Author,Title,Publication Year,number_of_studies,basic_study_info.initial_participants,basic_study_info.valid_participants,basic_study_info.excluded_participants,basic_study_info.exclusion_reasons,basic_study_info.data_collection_setting,basic_study_info.experiment_design,basic_study_info.experimental_procedure,basic_study_info.experimental_task,robot_characteristics.robot_models,robot_characteristics.robot_types,robot_characteristics.robot_functions,task_classification.high_level_task,task_classification.sub_task,interaction_intensity.classification,interaction_intensity.description,immersiveness.classification,immersiveness.description,robot_embodiment.classification,robot_embodiment.description,robot_autonomy_level.classification,robot_autonomy_level.description,trust_assessment.trust_measurement,trust_assessment.trust_questionnaires_used,trust_assessment.data_collection_for_trust_modeling,trust_assessment.description,trust_modeling_approach.classification,trust_modeling_approach.description,research_type.classification,research_type.subcategory,experimental_manipulation.manipulation_classification,experimental_manipulation.description,experimental_manipulation.impact_on_trust,additional_notes.notable_trends,additional_notes.key_findings,additional_notes.task_description,statistical_analysis.statistical_test,statistical_analysis.test_description,experimental_manipulation.manipulation_present,experimental_manipulation.factors_manipulated,experimental_manipulation.factors_that_impacted_trust,experimental_manipulation.factors_that_did_not_impact_trust,experimental_manipulation.factors_description,DOI,Url,Abstract Note
"Abd, Moaed A; Gonzalez, Iker; Nojoumian, Mehrdad; Engeberg, Erik D","Trust, Satisfaction and Frustration Measurements During Human-Robot Interaction",2017,1,10,10,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants interacted with a Baxter robot in a collaborative object sorting task. The robot delivered water bottles in different modes, and after each set of three deliveries, participants rated their trust, satisfaction, and frustration.",Participants received water bottles from a Baxter robot and placed them on a shelf.,Baxter,Humanoid Robots; Collaborative Robots,Research,Manipulation,Object Passing,direct-contact interaction,Participants physically interacted with the robot during the object passing task.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The study used a physical Baxter robot.,pre-programmed (non-adaptive),The robot followed pre-programmed delivery modes without adapting to the user.,Questionnaires,,,Trust was measured using a questionnaire after each set of deliveries.,no modeling,The study did not include any computational modeling of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's delivery mode was manipulated to include successful, slow, fast, dropping, and wrong location deliveries, which directly influenced the robot's performance and behavior.","Trust levels varied depending on the robot's delivery mode, with the dropping mode significantly decreasing trust.","The dropping mode resulted in significantly lower trust, higher frustration, and lower satisfaction compared to other delivery modes.","Human trust, satisfaction, and frustration levels depend on the robot's interaction mode, with the dropping mode having the most negative impact on trust.",The Baxter robot picked up water bottles and delivered them to the human participant. The human participant then took the bottle and placed it on a shelf.,Mann-Whitney U; Wilcoxon rank sum,"The study used the non-parametric Mann-Whitney U test (also known as the Wilcoxon Rank Sum Test) to statistically analyze the data. This test was used to compare the medians of two independent samples, specifically to determine if there were significant differences in the levels of trust, satisfaction, and frustration across different robot delivery modes. The null hypothesis was that the data in the x and y axes are samples from a continuous distribution with equal medians, against the alternative that they are not.",TRUE,Robot-accuracy; Robot-task-strategy,Robot-accuracy,Robot-task-strategy,"The study manipulated the robot's delivery mode, which directly affected its accuracy in delivering the water bottles. This is reflected in the different modes: successful delivery, slow delivery, fast delivery, dropping the object, and delivering to the wrong location. The 'Robot-accuracy' category is chosen because these manipulations directly influenced the robot's success in completing the task of delivering the object to the correct location and without dropping it. The robot's speed and location of delivery, while not directly impacting the success of the task (the bottle was still delivered), did influence the user's experience and effort required to complete the task, which is why 'Robot-task-strategy' is also included. The dropping mode, which is a clear failure of the robot to perform the task, significantly impacted trust, while the speed and location of delivery did not have as significant of an impact on trust, although they did impact frustration and satisfaction. Therefore, 'Robot-accuracy' is listed as a factor that impacted trust, and 'Robot-task-strategy' is listed as a factor that did not impact trust.",,,"Since the beginning of human race, we have always sought ways to develop bonds and create meaningful relationships with others. In these interactions, there are several parameters that determine how strong the bond is. These parameters include among many others, the trust towards the other person. Humanlike robots have been created with basic human to human interaction rules. Trust is a significant factor for the interaction with the robot, if a human trusts a robot, certainly the outcome from the interaction would be different from the case when a human does not trust a robot. For a human to be able to interact with the robot without any concern, trust must be developed between human and robot. In this paper, we introduce a starting point for quantifying Human-Robot interactions in which we measure the level of trust, satisfaction, and frustration. Due to the different interaction modes during the collaborative task, the human trust towards the robot varied due to interaction and experiences. Results based on feedback from 10 persons, when they interacted with a Baxter robot in a real time collaborative task showed the trust, frustration and satisfaction levels changed depending on the Baxter robot operation modes. The most significant delivery mode is the dropping mode in which the trust, frustration and satisfaction levels are significantly different in comparison with other delivery modes."
"Adami, Pooya; Rodrigues, Patrick B.; Woods, Peter J.; Becerik-Gerber, Burcin; Soibelman, Lucio; Copur-Gencturk, Yasemin; Lucas, Gale",Impact of VR-Based Training on Human–Robot Interaction for Remote Operating Construction Robots,2022,1,50,49,1,1 participant was excluded because they were not comfortable using VR equipment,Controlled Lab Environment,between-subjects,"Participants were randomly assigned to either VR-based training or in-person training. Before training, participants completed surveys measuring trust and self-efficacy. After training, they retook the surveys and completed a performance assessment with the actual robot, during which situational awareness and mental workload were measured.","Participants were asked to remotely operate a demolition robot, including starting the robot, moving it to a designated location, demonstrating a demolition action, and then shutting down the robot.",Brokk110,Industrial Robot Arms,Industrial; Research,Manipulation,Remote Manipulation,direct-contact interaction,Participants directly controlled the robot during the performance assessment.,real-world,Participants in the VR condition used a VR headset and treadmill for training.,physical,Participants interacted with a physical robot during the performance assessment.,wizard of oz (directly controlled),The robot was directly controlled by the human operator.,Questionnaires; Custom Scales,Jian et al. Trust Scale; N/A,,Trust was measured using modified questionnaires and Likert scales.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Mixed Reality Studies,Direct Manipulation,"The training method (VR-based vs. in-person) was manipulated to influence participants' expectations and experience with the robot, and the task difficulty was implicitly influenced by the training method.",VR-based training significantly increased trust and self-efficacy compared to in-person training.,"The study found that VR-based training significantly increased trust and self-efficacy more than in-person training, and also increased situational awareness. However, there was no significant difference in mental workload between the two groups.",VR-based training significantly increased construction workers' trust in the robot and robot operation self-efficacy compared to traditional in-person training.,"The robot was a demolition robot, and the human operator remotely controlled the robot to perform a demolition task.",ANOVA; t-test; Kolmogorov-Smirnov,"The study used a Kolmogorov-Smirnov test to verify the normality of the data. It then employed a mixed ANOVA to analyze the interaction between time (pre- and post-training) and training type (VR-based vs. in-person) on trust and self-efficacy scores. Additionally, a t-test was used to compare the situational awareness scores between the two training groups. The purpose of these tests was to determine the effectiveness of VR-based training compared to in-person training on trust, self-efficacy, and situational awareness.",TRUE,Task-environment,Task-environment,,"The study manipulated the training environment by assigning participants to either a VR-based training or an in-person training. The VR training provided a different environment for learning, allowing for more interactive and varied experiences with the robot compared to the in-person training. This difference in the training environment (VR vs. in-person) is a manipulation of the 'Task-environment' because it changes the context in which the task of learning to operate the robot is performed. The paper states that the VR environment allowed trainees to work with the robot in different scenarios and experience the consequences of their actions, which was not possible in the in-person training due to safety concerns. This difference in the training environment impacted trust, as the VR-based training group showed significantly higher trust and self-efficacy compared to the in-person training group. The paper explicitly states that the VR environment provided an immersive experience, allowing trainees to familiarize themselves with the robot's functions and gain confidence. Therefore, the 'Task-environment' was the manipulated factor that impacted trust.",10.1061/(ASCE)CP.1943-5487.0001016,https://ascelibrary.org/doi/10.1061/%28ASCE%29CP.1943-5487.0001016,"Despite the increased interest in automation and the expanded deployment of robots in the construction industry, using robots in a dynamic and unstructured working environment has caused safety concerns in operating construction robots. Improving human–robot interaction (HRI) can increase the adoption of robots on construction sites; for example, increasing trust in robots could help construction workers to accept new technologies. Confidence in operation (or self-efficacy), mental workload, and situational awareness are among other key factors that help such workers to remote operate robots safely. However, construction workers have very few opportunities to practice with robots to build trust, self-efficacy, and situational awareness, as well as resistance against increasing mental workload, before interacting with them on job sites. Virtual reality (VR) could afford a safer place to practice with the robot; thus, we tested if VR-based training could improve these four outcomes during the remote operation of construction robots. We measured trust in the robot, self-efficacy, mental workload, and situational awareness in an experimental study where construction workers remote-operated a demolition robot. Fifty workers were randomly assigned to either VR-based training or traditional in-person training led by an expert trainer. Results show that VR-based training significantly increased trust in the robot, self-efficacy, and situational awareness, compared to traditional in-person training. Our findings suggest that VR-based training can allow for significant increases in beneficial cognitive factors over more traditional methods and has substantial implications for improving HRI using VR, especially in the construction industry. DOI: 10.1061/(ASCE)CP.1943-5487.0001016. © 2022 American Society of Civil Engineers."
"Adami, Pooya; Singh, Rashmi; Borges Rodrigues, Patrick; Becerik-Gerber, Burcin; Soibelman, Lucio; Copur-Gencturk, Yasemin; Lucas, Gale","Participants matter: Effectiveness of VR-based training on the knowledge, trust in the robot, and self-efficacy of construction workers and university students",2023,1,50,50,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants completed a demographics survey, a knowledge assessment, and trust and self-efficacy surveys before and after a VR-based training session.",Participants completed a VR-based training for remote operation of a demolition robot.,Brokk110,Mobile Manipulators; Unmanned Ground Vehicles,Industrial; Educational; Research,Manipulation,Remote Manipulation,minimal interaction,Participants interacted with a simulated robot in a virtual environment.,simulation,Participants experienced the training using a head-mounted display and controllers.,simulated,The robot was a virtual representation in the VR environment.,wizard of oz (directly controlled),Participants directly controlled the robot's actions through a manual controller.,Questionnaires; Custom Scales,Automation Trust Scale (ATS),,Trust was measured using a modified version of the Automation Trust Scale.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The VR-based training was designed to improve knowledge, trust, and self-efficacy by providing a realistic simulation of the robot and its operation, and by providing a structured learning experience.",Trust in the robot increased significantly more for construction workers than for students after the training.,"Students had higher initial knowledge and trust, but workers showed greater gains in trust and self-efficacy after training. The study also found that the effectiveness of VR-based training on knowledge acquisition was different for students and workers.","VR-based training led to a significantly larger increase in knowledge acquisition for construction students than workers, while it improved trust in the robot and robot operation self-efficacy significantly more for construction workers than students.",The robot was a simulated demolition robot that participants controlled remotely using a manual controller. The human participant's task was to complete a series of training modules to learn how to operate the robot safely and effectively.,ANOVA; t-test; t-test; t-test,"The study used a mixed factorial ANOVA to compare the mean differences of knowledge acquisition, trust in the robot, and robot operation self-efficacy between groups (students vs. workers) across time (pre- vs. post-training). Paired sample t-tests were used to assess the significance of pre-to-post training changes within each group (students and workers). Independent sample t-tests were used to compare the means of the dependent variables between students and workers at pre- and post-training conditions. The overall purpose was to determine the effect of VR-based training on the three dependent variables and whether the effect differed between students and construction workers.",TRUE,Task-complexity,Task-complexity,,"The study manipulated the task complexity through a VR-based training program that consisted of seven learning modules, each with increasing complexity. The training was designed to improve knowledge, trust, and self-efficacy by providing a structured learning experience. The paper states, 'The resulting VR-based training is composed of seven learning modules, each of which is followed by a diagnostic assessment that evaluates whether the worker understood the contents of the learning module before the worker is allowed to move on to the next one.' This indicates a progression in task complexity. The study found that trust in the robot increased significantly more for construction workers than for students after the training, indicating that the manipulation of task complexity through the training program impacted trust levels. The training program was designed to increase knowledge, trust, and self-efficacy, and the results showed that the training had a differential impact on trust between the two groups. The training program itself is the manipulation, and it is designed to increase the complexity of the task as the user progresses through the modules. The paper states, 'Trainees obtained the essential learning material to remote operate the robot by completing the exercises.' This indicates that the training program was designed to increase the complexity of the task as the user progresses through the modules.",10.1016/j.aei.2022.101837,https://linkinghub.elsevier.com/retrieve/pii/S1474034622002956,"Virtual Reality (VR)-based training has gained attention from the scientific community in the Architecture, Engineering, and Construction (AEC) industry as a cost-effective and safe method that eliminates the safety risks that may impose on workers during the training compared to traditional training methods (e.g., in-person handson training, apprenticeship). Although researchers have developed VR-based training for construction workers, some have recruited students rather than workers to understand the effect of their VR-based training. However, students are different from construction workers in many ways, which can threaten the validity of such studies. Hence, research is needed to investigate the extent to which the findings of a VR-based training study are contingent on whether students or construction workers were used as the study sample. This paper strives to compare the effectiveness of VR-based training on university students’ and construction workers’ knowledge acquisition, trust in the robot, and robot operation self-efficacy in remote operation of a construction robot. Twenty-five construction workers and twenty-five graduate construction engineering students were recruited to complete a VR-based training for remote operating a demolition robot. We used quantitative analyses to answer our research questions. Our study shows that the results are dependent on the target sample in that students gained more knowledge, whereas construction workers gained more trust in the robot and more self-efficacy in robot operation. These findings suggest that the effectiveness of VR-based training on students may not neces­ sarily associate with its effectiveness on construction workers."
"Adamik, Mark; Dudzinska, Karolina; Herskind, Adrian J.; Rehm, Matthias",The Difference Between Trust Measurement and Behavior: Investigating the Effect of Personalizing a Robot's Appearance on Trust in HRI,2021,1,60,60,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were randomly assigned to either an experimental group, where they personalized a robot's appearance, or a control group, where they were shown an introductory video of the robot. All participants then completed a trust questionnaire, played a dice game against the robot, and completed a final trust questionnaire.","Participants played a dice game against a simulated robot, where they had to decide whether to trust the robot's announced dice roll or check it.",Nao,Humanoid Robots,Research,Game,Economic Game,minimal interaction,Participants interacted with a simulated robot through a web interface.,simulation,The interaction took place in a simulated environment using a web interface.,simulated,The robot was a simulated representation of a Nao robot.,pre-programmed (non-adaptive),The robot's behavior was pre-programmed and did not adapt to the user's actions.,Questionnaires; Behavioral Measures,Trust Perception Scale - HRI,Performance Metrics,Trust was measured using questionnaires and behavioral data from the game.,no modeling,The study did not use any computational models of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The experimental group could personalize the robot's appearance, while the control group could not, to see if this would influence trust.","Personalizing the robot's appearance did not significantly increase self-reported trust, but it did lead to more trusting behavior during the game.","The study found a discrepancy between subjective trust measures (questionnaires) and objective trust measures (behavioral data), with the latter showing a significant effect of personalization while the former did not.","Personalizing a robot's physical appearance does not increase self-reported trust, but it does lead to more trusting behavior during a game interaction.","The robot played a dice game with the human participant, where the robot would announce a dice roll and the human would decide whether to trust the robot or check the roll. The human could also lie about their own dice roll.",Wilcoxon rank sum; Chi-squared,"The study used a Wilcoxon test to compare pre- and post-test trust scores between the experimental and control groups, as well as to compare the intermediate trust scores. A Chi-squared test was used to compare the number of unnecessary lies between the two groups. These tests were used to determine if personalizing the robot's appearance had a significant impact on self-reported trust and behavior during the game.",TRUE,Robot-aesthetics,Robot-aesthetics,,"The study manipulated the robot's appearance by allowing participants in the experimental group to personalize the robot's colors and name, while the control group did not have this option. This falls under the category of 'Robot-aesthetics' as it directly relates to the visual appeal and design of the robot. The study found that this manipulation of robot aesthetics did impact trust behavior, as participants in the experimental group exhibited more trusting behavior during the game, even though it did not impact self-reported trust. Therefore, 'Robot-aesthetics' is included in 'factors_that_impacted_trust'. There were no other factors manipulated, so 'factors_that_did_not_impact_trust' is empty.",10.1109/RO-MAN50785.2021.9515487,https://ieeexplore.ieee.org/document/9515487/,"With the increased use of social robots in critical applications, like elder care and rehabilitation, it becomes necessary to investigate the user’s trust in robots to prevent over- and under-utilization of the robotic systems. While several studies have shown how trust increases through personalised behaviour, there is a lack of research concerned with the inﬂuence of personalised physical appearance. This study explores the effect of personalised physical appearance on trust in human-robotinteraction (HRI). In an online game, 60 participants interacted with a robot, where half of the participants were asked to personalise the robot prior to the game. Trust was measured through a trust-related questionnaire as well as by evaluating user behaviour during the game. Results indicate that personalised physical appearance does not directly correlate to higher trust perceptions, however, there was signiﬁcant evidence that players exhibit more trusting behaviours in a game against a personalised robot."
"Ahlskog, Johanna; Bahodi, Maria-Theresa; Lugmayr, Artur; Merritt, Timothy",Fostering Trust Through User Interface Design in Multi-Drone Search and Rescue,2024,1,10,10,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants completed two SAR missions, one with a heatmap visualization and one without, in a randomized order. During each mission, the screen was frozen three times for SAGAT questions. After each mission, participants completed questionnaires and semi-structured interviews.","Participants monitored a multi-drone system during a simulated SAR mission, with the goal of finding a missing person.",Unspecified,Unmanned Aerial Vehicles (UAVs); Swarm Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated drone system through a computer interface.,simulation,The interaction was through a simulated environment displayed on a computer screen.,simulated,The drones were represented as virtual objects within the simulation.,shared control (fixed rules),"The drone swarm followed a pre-programmed path, but the user could monitor and respond to system messages.",Questionnaires; Custom Scales,Human-Computer Trust Scale/Questionnaire (HCT/HCTM); NASA Task Load Index (NASA-TLX),Video Data; Performance Metrics,"Trust was measured using a questionnaire and performance metrics, with video data collected for analysis.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The presence or absence of a heatmap visualization of a lost person model was manipulated to influence task difficulty and user expectations, and system messages provided feedback.","The heatmap visualization increased the benevolence aspect of trust, and the overall trust was slightly higher in the condition with the heatmap.","The study found that the heatmap visualization increased the benevolence aspect of trust. Participants also expressed frustration with the limitations of managing autonomous drone swarms, particularly the lack of options beyond emergency landing or mission abortion.",The study found that a heatmap visualization of a lost person model increased the benevolence aspect of trust in a multi-drone SAR system.,"The robot (drone swarm) autonomously followed a pre-defined search path. The human participant monitored the drone swarm, responded to system messages, and searched for a missing person using the provided interface.",paired samples t-test,"A paired samples t-test was conducted to examine the influence of the heatmap on trust levels, specifically comparing trust scores between the two conditions (with and without the heatmap). The test focused on the three components of trust: ability, integrity, and benevolence.",TRUE,Robot-interface-design,Robot-interface-design,,"The study manipulated the presence or absence of a heatmap visualization of a lost person model overlaid on the map. This is a change to the interactive elements of the interface, specifically the map view, which falls under the category of 'Robot-interface-design'. The paper states, 'In one condition, a heat map provided a visualization of a lost person model overlaid on the map, in the other condition, the heatmap was not shown.' The results showed that the heatmap visualization increased the benevolence aspect of trust, as stated in the paper: 'The human-computer trust scale showed an increase in the benevolence aspect of trust (see Figure 10). The heatmap received positive'. Therefore, 'Robot-interface-design' is the factor that impacted trust. There were no other factors manipulated, and no factors that were manipulated that did not impact trust.",10.1145/3686038.3686052,https://dl.acm.org/doi/10.1145/3686038.3686052,
"Ahmad, Muneeb Imtiaz; Bernotat, Jasmin; Lohan, Katrin; Eyssel, Friederike",Trust and Cognitive Load During Human-Robot Interaction,2019,1,40,26,14,14 participants were excluded due to difficulties in collecting data for cognitive load through Tobii glasses,Controlled Lab Environment,mixed design,"Participants were given information about the robot, completed a pre-test questionnaire, played a matching pairs game with a robot with either a high or low error rate, and then completed a post-test questionnaire.",Participants played a matching pairs game with a robot teammate.,Husky; Pepper,Mobile Robots; Humanoid Robots,Research; Social,Game,Cooperative Game,minimal interaction,Participants interacted with the robot verbally and through a game interface.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,wizard of oz (directly controlled),"The Husky robot was controlled using a Wizard-of-Oz approach, while the Pepper robot used its own speech recognition capabilities.",Questionnaires; Physiological Measures,Godspeed Questionnaire,Eye-tracking Data,Trust was measured using a questionnaire and cognitive load was measured using pupillometry.,"parametric models (e.g., regression)",A linear regression model was used to predict trust based on cognitive load.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's error rate (3% or 50%) and type (humanoid or machine-like) were manipulated to influence trust.,"Trust decreased with increased cognitive load. The interaction between robot type and error rate significantly impacted trust ratings, with the Husky being more trusted at low error rates and Pepper more trusted at high error rates.","The triple interaction impact between robot type, error rate, and pre- and post-measures on trust was unexpected. Participants trusted the Husky more than Pepper in the low error-rate condition, and Pepper more than Husky in the high error-rate condition. This is a deviation from the typical finding that more anthropomorphic robots are more trusted.","There was an inversely proportional relationship between trust and cognitive load, suggesting that as cognitive load increased, trust decreased. The interaction between robot type and error rate significantly impacted trust ratings.","The robot acted as a teammate in a matching pairs game, providing help to the human participant. The human participant selected markers and could ask the robot for help in finding matching pairs.",Linear regression; Pearson correlation; ANOVA; repeated measures manova; t-test,"The study used a linear regression to predict trust based on cognitive load (number of peaks). A Pearson correlation was used to examine the relationship between cognitive load and trust ratings. A MANOVA was conducted to assess the impact of robot type and error rate on cognitive load and trust ratings after the interaction. A repeated measures MANOVA was used to analyze the change in trust ratings from pre- to post-interaction, considering robot type and error rate. Additionally, paired and independent sample t-tests were used to compare pre- and post-test questionnaire results.",TRUE,Robot-accuracy; Robot-aesthetics; Task-complexity,Robot-accuracy; Robot-aesthetics,Task-complexity,"The study manipulated the robot's error rate (3% or 50%), which directly impacts the robot's accuracy in the game, thus 'Robot-accuracy'. The study also used two different robots, a humanoid (Pepper) and a machine-like (Husky), which is a manipulation of 'Robot-aesthetics'. The game itself, requiring memory and communication with the robot, was designed to induce cognitive load, which is a manipulation of 'Task-complexity'. The results showed that the robot's error rate and the robot type (aesthetics) significantly impacted trust ratings, with the Husky being more trusted at low error rates and Pepper more trusted at high error rates. However, the study did not find a significant main impact of task complexity (cognitive load) on trust ratings, although it did find an inverse relationship between cognitive load and trust. Therefore, 'Task-complexity' is listed as a factor that did not impact trust.",,http://arxiv.org/abs/1909.05160,"This paper presents an exploratory study to understand the relationship between a humans’ cognitive load, trust, and anthropomorphism during human-robot interaction. To understand the relationship, we created a “Matching the Pair” game that participants could play collaboratively with one of two robot types, Husky or Pepper. The goal was to understand if humans would trust the robot as a teammate while being in the game-playing situation that demanded a high level of cognitive load. Using a humanoid vs. a technical robot, we also investigated the impact of physical anthropomorphism and we furthermore tested the impact of robot error rate on subsequent judgments and behavior. Our results showed that there was an inversely proportional relationship between trust and cognitive load, suggesting that as the amount of cognitive load increased in the participants, their ratings of trust decreased. We also found a triple interaction impact between robot-type, error-rate and participant’s ratings of trust. We found that participants perceived Pepper to be more trustworthy in comparison with the Husky robot after playing the game with both robots under high error-rate condition. On the contrary, Husky was perceived as more trustworthy than Pepper when it was depicted as featuring a low error-rate. Our results are interesting and call further investigation of the impact of physical anthropomorphism in combination with variable error-rates of the robot."
"Ahmad, Muneeb; Alzahrani, Abdullah; Robinson, Simon; Rahat, Alma",Modelling Human Trust in Robots During Repeated Interactions,2023,1,45,45,0,No participants were excluded,Controlled Lab Environment,within-subjects,Participants played a card game against a NAO robot across four sessions with a 5-minute break between each session. Participants completed questionnaires after each session. The robot was controlled using the Wizard of Oz method.,"Participants played a card game against a NAO robot, where they had to decide whether to trust the robot's claims about the cards it was playing.",Nao,Humanoid Robots,Research; Social,Game,Competitive Game,minimal interaction,Participants interacted with the robot verbally and through a card game.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical NAO robot.,wizard of oz (directly controlled),The robot's actions were controlled by a human experimenter using the Wizard of Oz method.,Questionnaires; Performance-Based Measures; Real-time Trust Measures,Schaefer's Trust Questionnaire/Scale,Performance Metrics,"Trust was measured using a questionnaire, performance metrics, and a real-time trust model.","parametric models (e.g., regression)",A mathematical model was used to predict trust based on experience and interaction.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's truthfulness (performance) was manipulated through a predetermined strategy, and the game itself framed the interaction as a competition, influencing the perceived risk and difficulty.","Trust increased significantly from the first to the last session, with no significant changes between the second, third, and fourth sessions.","The study found that trust increased significantly from the first to the last session, but did not change significantly between the second, third, and fourth sessions. Risk was positively related to control only in the third session. The robot's performance was consistent across all sessions.","The mathematical model of trust was validated by the experimental results, showing that trust perception scores and interaction session predicted the trust modeled score.",The robot declared its cards and the human decided whether to trust the robot's claim. The human also declared their cards and the robot decided whether to trust the human's claim. The game continued until one player disposed of all their cards.,Linear regression; ANOVA; Pearson correlation,"The study used multiple linear regression to predict the Trust Modelled Score (TMS) based on the Trust Perception Score (TPS) and session number. A repeated-measures ANOVA was conducted to determine the effect of interactive session on TMS and TPS scores. Finally, a Pearson correlation coefficient test was used to assess the linear relationship between TMS and TPS measurements across the four sessions.",TRUE,Robot-morality; Teaming,Robot-morality,,"The study manipulated the robot's truthfulness (whether it was bluffing or not) during the card game, which directly relates to the 'Robot-morality' category as it involves actions that violate moral codes (lying) outside of the game's rules and does not influence the robot's success rate on the task. The game itself was framed as a competition, which falls under the 'Teaming' category. The paper states, 'In competitive settings, the truthfulness of the robot is considered for establishing trust'. The results showed that trust increased significantly from the first to the last session, indicating that the manipulation of the robot's truthfulness impacted trust. The paper also states, 'The robot's performance was consistent across all sessions', which means that the robot's accuracy was not manipulated. The paper also states, 'The game itself framed the interaction as a competition, influencing the perceived risk and difficulty', which means that the teaming aspect of the game was a manipulation. However, the paper does not explicitly state that the competitive nature of the game impacted trust, but rather the robot's truthfulness. Therefore, 'Teaming' is not included in the 'factors_that_impacted_trust' list.",10.1145/3623809.3623892,https://dl.acm.org/doi/10.1145/3623809.3623892,"Modelling humans’ trust in robots is critical during human-robot interaction (HRI) to avoid under- or over-reliance on robots. Currently, it is challenging to calibrate trust in real-time. Consequently, we see limited work on calibrating humans’ trust in robots in HRI. In this paper we describe a mathematical model that attempts to emulate the three-layered (initial, situational, learned) framework of trust capable of potentially estimating humans’ trust in robots in real-time. We evaluated the trust model in an experimental setup that involved participants playing a trust game on four occasions. We validate the model based on linear regression analysis that showed that the trust perception score (TPS) and interaction session predicted the trust modelled score (TMS) computed by applying the trust model. We also show that TPS and TMS did not change significantly from the second to the fourth session. However, TPS and TMS captured in the last session increased significantly from the first session. The described work is an initial effort to model three layers of humans’ trust in robot in a repeated HRI setup and requires further testing and extension to improve its robustness across settings."
"Ahmad, Muneeb; Alzahrani, Abdullah",Crucial Clues: Investigating Psychophysiological Behaviors for Measuring Trust in Human-Robot Interaction,2023,1,45,43,2,2 participants were excluded due to issues in data collection,Controlled Lab Environment,within-subjects,"Participants played a card game against a Nao robot across four sessions while wearing physiological sensors. They completed a demographics questionnaire and were given game instructions. Data was collected during the game, and participants were thanked and compensated at the end.","Participants played a card game against a Nao robot, making decisions to trust or distrust the robot's card declarations.",Nao,Humanoid Robots; Expressive Robots,Research; Social,Game,Competitive Game,minimal interaction,"Participants interacted with the robot verbally and through a card game, but there was no physical touch.",real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical Nao robot.,wizard of oz (directly controlled),The robot's actions were directly controlled by a human operator using the Wizard of Oz method.,Physiological Measures; Behavioral Measures,,Physiological Signals; Eye-tracking Data,Trust was assessed using physiological measures and behavioral data related to trust and distrust decisions.,"parametric models (e.g., regression)","The study used machine learning classifiers such as Random Forest, Logistic Regression, SVM, Decision Tree, and AdaBoost to classify trust levels based on physiological data.",Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The game was designed to create situations where participants had to decide whether to trust or distrust the robot, influencing their trust levels through the game's mechanics and the robot's behavior.","The study found that HR and SKT were significantly different between trust and distrust groups, suggesting that these physiological measures can be used to assess trust in real-time. The Random Forest classifier achieved the best accuracy in classifying trust levels.","The study found that HR and SKT were significantly different between trust and distrust groups, but other physiological measures (EDA, BVP, BR, and BD) did not show significant differences. There was no significant interaction effect of session and decision on any of the physiological measures.","The study's key finding is that heart rate (HR) and skin temperature (SKT) are significantly different between trust and distrust states during human-robot interaction, and that these measures can be used to classify trust levels with a reasonable accuracy using machine learning.","The robot verbally declared a set of cards, and the human participant decided whether to trust or distrust the robot's claim. The robot's actions were controlled by a human operator using the Wizard of Oz method.",ANOVA; Bonferroni correction,"A repeated-measures ANOVA was used to determine the effect of the decision (trust vs distrust) and the interactive session (session 1, session 2, session 3, and session 4) on the physiological measures (EDA, BVP, HR, SKT, BR, and BD). A post-hoc Bonferroni test was then conducted to assess whether HR, SKT, and other measures differed significantly between the trust and distrust classes within each session.",TRUE,Teaming; Robot-morality,Robot-morality,Teaming,"The study manipulated 'Teaming' by having participants compete against the robot in a card game, creating a competitive environment where trust decisions were crucial for success. The study also implicitly manipulated 'Robot-morality' by having the robot sometimes lie about the cards it held, forcing participants to decide whether to trust or distrust the robot's claims. The robot's truthfulness (or lack thereof) directly influenced the participant's trust in the robot. The results showed that the robot's truthfulness (or lack thereof) impacted trust, as evidenced by the significant differences in HR and SKT between trust and distrust states. The competitive nature of the game ('Teaming') was a constant factor across all conditions and did not have a differential impact on trust levels, as the study did not compare competitive vs. collaborative scenarios. Therefore, 'Teaming' is listed as a manipulated factor but not as a factor that impacted trust.",10.1145/3577190.3614148,https://dl.acm.org/doi/10.1145/3577190.3614148,"Existing work on the measurements of trust during Human-Robot Interaction (HRI) indicates that psychophysiological behaviours (PBs) have the potential to measure trust. However, we see limited work on the use of multiple PBs in combination to calibrate human’s trust in robots in real-time during HRI. Therefore, this study aims to estimate human trust in robots by examining the diferences in PBs between trust and distrust states. It further investigates the changes in PBs across repeated HRI and also explores the potential of machine learning classifers in predicting trust levels during HRI. We collected participants’ electrodermal activity (EDA), blood volume pulse (BVP), heart rate (HR), skin temperature (SKT), blinking rate (BR), and blinking duration (BD) during repeated HRI. The results showed signifcant diferences in HR and SKT between trust and distrust groups and no signifcant interaction efect of session and decision for all PBs. Random Forest classifer achieved the best accuracy of 68.6% to classify trust, while SKT, HR, BR, and BD were the important features. These fndings highlight the value of PBs in measuring trust in real-time during HRI and encourage further investigation of trust measures with PBs in various HRI settings."
"Ajenaghughrure, Ighoyota Ben.; Sousa, Sonia C.; Kosunen, Ilkka Johannes; Lamas, David",Predictive model to assess user trust: a psycho-physiological approach,2019,1,10,10,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants played a demo game, followed by two controlled game sessions with different difficulty levels, and completed a trust questionnaire after each session. EEG data was recorded during the game sessions.","Participants played a customized version of 'Who Wants to Be a Millionaire', choosing between Google Assistant or a virtual audience for help with questions of varying difficulty.",Unspecified,Other,Research,Game,Economic Game,minimal interaction,Participants interacted with Google Assistant through a smartphone during a game.,real-world,Participants interacted with a real smartphone and a virtual audience in a game setting.,physical,Participants interacted with a physical smartphone running Google Assistant.,shared control (fixed rules),"Google Assistant provided answers based on its capabilities, with the user choosing when to use it.",Behavioral Measures; Physiological Measures; Questionnaires,Human-Computer Trust Scale/Questionnaire (HCT/HCTM),Physiological Signals,"Trust was assessed using a questionnaire, behavioral measures (reliance on Google Assistant), and EEG data.","parametric models (e.g., regression)",A classifier model was developed using physiological features to predict user trust levels.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The difficulty of the questions was manipulated to influence the perceived competence of Google Assistant, and thus, user trust.","Trust was higher when the questions were simple and Google Assistant was more accurate, and lower when the questions were difficult and Google Assistant was less accurate.","Some participants showed individual differences in trust, relying on Google Assistant even when it was less accurate, or not relying on it even when it was accurate, which may be due to individual heuristic biases.",A predictive model using EEG signals can detect users' trust levels in AI technology with a mean accuracy of 77.8%.,"The robot (Google Assistant) provided answers to questions, and the human participant chose whether to rely on the robot or a virtual audience for help.",Pearson correlation; tree-based estimator; 5-fold cross-validation,"The study used Pearson correlation coefficient to assess multi-collinearity and reduce feature dimensions. A tree-based estimator was used to remove non-relevant features by correlating each feature with the class variable (high and low trust). Finally, a 5-fold cross-validation technique was used to validate the classifier model's accuracy.",TRUE,Task-complexity; Robot-accuracy,Task-complexity; Robot-accuracy,,"The study manipulated the difficulty of the questions in the 'Who Wants to Be a Millionaire' game. This directly impacts 'Task-complexity' as the cognitive load and effort required to answer the questions varied between the simple and difficult question sets. The manipulation of question difficulty also implicitly manipulated 'Robot-accuracy' because Google Assistant was more accurate with simple questions and less accurate with difficult questions. The paper states, 'Competence was elicited through varying difficulty level of questions to be answered (simple questions technology can answer accurately and difficult questions technology can not answer accurately).' and 'Participants cognitive assessment of the trustworthiness of the AI technology (Google assistant) during the difficult game session indicates that the AI technology was not sufficiently trustworthy (low trust), as participants relied more often on the virtual audience during the difficult game session... Also, participants cognitive assessment of the trustworthiness of the AI technology (Google assistant) during the simple game session indicates that the AI technology was sufficiently trustworthy (high trust), as participants relied more on the AI technology (Google assistant) during the simple game session...'. This shows that both task complexity and robot accuracy impacted trust.",10.1145/3364183.3364195,https://dl.acm.org/doi/10.1145/3364183.3364195,"Artiﬁcial intelligence (AI) systems are becoming pervasive in modern day society. Nonetheless, AI is not infallible. Therefore as more task and controls get delegated to AI systems, the implications become dire and riskier (e.g. Google assistant falling to make an emergency call hands-free), which impacts users experience and make users unforgiving towards system failure. Trust can be a key element as it can help overcome the fear of loss and can supports users interactions. Therefore making it important to design methods and tools to foster trust between users and these technologies, especially capable of assessing users trust objectively and serving as an interface applicable in real-time during the interaction. Measuring the psycho-physiological signals of the user provides a way for objective assessment of users trust towards the technology, with the potential for real-time trust assessment, provided that we know the neural correlates of trust. This study aims to show that it is indeed possible to objectively detect users trust level in AI technologies, and provides details on what physiological signals are most suitable for real-time trust detection, as well as details on the predictive machine learning model used. The results show that our model achieved a mean accuracy of 77.8% and mean receiver operating characteristics (ROC) for the area under the curve (AUC) was 0.76."
"Ajenaghughrure, Ighoyota Ben.; da Costa Sousa, Sonia Claudia; Lamas, David",Risk and Trust in artificial intelligence technologies: A case study of Autonomous Vehicles,2020,1,18,18,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants played a driving game with varying risk levels, completed trust questionnaires before and after each game session, and had their electrodermal activity recorded.","Participants played a self-driving car game where they had to navigate obstacles under different risk conditions, choosing to rely on the AI or manually control the vehicle.",Unspecified,Autonomous Vehicles,Research,Game,Cooperative Game,minimal interaction,Participants interacted with a virtual car in a game environment.,simulation,Participants interacted with a driving simulator game.,simulated,The robot was a virtual car in a driving game.,shared control (fixed rules),"The robot could drive autonomously, but participants could take over control using a joystick.",Behavioral Measures; Physiological Measures; Questionnaires,Human-Computer Trust Scale/Questionnaire (HCT/HCTM),Physiological Signals,"Trust was measured using questionnaires, electrodermal activity, and joystick activation.",no modeling,"Trust was not modeled computationally, only descriptive statistics were used.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The risk level of the driving scenarios was manipulated to influence the perceived safety and reliability of the autonomous vehicle.,"Trust decreased as risk increased, and initial trust was higher than trust after interaction in high-risk conditions.","There was no significant difference in trust between very high and high risk conditions, or between low and no risk conditions, suggesting participants may perceive risk on a binary level (high/low).","Risk significantly influences users' trust in autonomous vehicles, with trust decreasing as risk increases.","The robot (virtual car) autonomously navigated a course with obstacles, while the human participant could take over control using a joystick to avoid crashes.",ANOVA; t-test; ANOVA; t-test; Pearson correlation,"The study used a repeated measures ANOVA to determine if there were significant differences in trust scores before and after interactions with the autonomous vehicle under varying risk conditions. Post hoc tests with Bonferroni correction were used to identify where the differences occurred. A one-way repeated measures ANOVA was used to compare the effect of different risk levels on trust scores during interactions with the AV. A dependent-samples t-test was used to analyze the joystick activation data to see if there were differences in user take-over control across risk conditions. Finally, a Pearson correlation test was used to examine the relationship between users' EDA responses and trust levels.",TRUE,Task-complexity; Robot-autonomy,Task-complexity,Robot-autonomy,"The study manipulated the risk level of the driving scenarios, which directly impacts the complexity of the task. The paper states, 'The very high-risk conditions game rounds include 13 unique obstacles that involve ethical decision and loss of live is certain... The high-risk conditions game rounds include 13 unique obstacles that are difficult to maneuver with certainty of live threatening injuries... The low risk conditions game rounds include obstacles that are not too difficult to maneuver, and none-live threatening injuries is certain... The no risk conditions game rounds include obstacles that are easy to maneuver with certainty of total collision avoidance.' This clearly shows a manipulation of task complexity. The study also allowed participants to take over control of the vehicle, which is a manipulation of robot autonomy, as the level of control was shared between the AI and the participant. The results showed that trust decreased as risk increased, indicating that task complexity impacted trust. However, the study found no significant difference in trust between very high and high risk conditions, or between low and no risk conditions, suggesting that the manipulation of robot autonomy did not have a significant impact on trust levels, as participants seemed to perceive risk on a binary level (high/low) rather than a continuous scale.",10.1109/HSI49210.2020.9142686,https://ieeexplore.ieee.org/document/9142686/,"This study investigates how risk influences users’ trust before and after interactions with technologies such as autonomous vehicles (AVs’). Also, the psychophysiological correlates of users’ trust from users’’ eletrodermal activity responses. Eighteen (18) carefully selected participants embark on a hypothetical trip playing an autonomous vehicle driving game. In order to stay safe, throughout the drive experience under four risk conditions (very high risk, high risk, low risk and no risk) that are based on automotive safety and integrity levels (ASIL D, C, B, A), participants exhibit either high or low trust by evaluating the AVs’ to be highly or less trustworthy and consequently relying on the Artificial intelligence or the joystick to control the vehicle. The result of the experiment shows that there is significant increase in users’ trust and user’s delegation of controls to AVs’ as risk decreases and vice-versa. In addition, there was a significant difference between user’s initial trust before and after interacting with AVs’ under varying risk conditions. Finally, there was a significant correlation in users’ psychophysiological responses (electrodermal activity) when exhibiting higher and lower trust levels towards AVs’. The implications of these results and future research opportunities are discussed."
"Akash, Kumar; Wan-Lin Hu; Reid, Tahira; Jain, Neera",Dynamic modeling of trust in human-machine interactions,2017,1,581,518,63,63 outliers were removed according to the interquartile range (IQR) rule,Online Crowdsourcing,between-subjects,"Participants interacted with a computer-based simulation of a car with an obstacle detection sensor. They were asked to repeatedly evaluate the sensor's report and choose to either trust or distrust it. The trials were divided into reliable and faulty phases, with the accuracy of the algorithm switching between reliable and faulty according to a pseudo-random binary sequence in the third phase.",Participants were asked to evaluate the accuracy of an obstacle detection sensor in a simulated driving scenario and choose to trust or distrust the sensor's report.,Unspecified,Autonomous Vehicles,Research,Evaluation,Image Analysis,minimal interaction,Participants interacted with a simulation and made trust decisions based on the sensor's reports.,simulation,The interaction was conducted through a computer-based simulation of a driving scenario.,simulated,The robot was represented as a simulated obstacle detection sensor in a driving simulation.,pre-programmed (non-adaptive),"The sensor's behavior was pre-programmed, with a fixed probability of being accurate or faulty.",Behavioral Measures,,Performance Metrics,Trust was measured by the probability of participants choosing to trust the sensor's report.,"parametric models (e.g., regression)",A linear third-order model was used to capture the dynamic variations of human trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers directly manipulated the reliability of the obstacle detection sensor, switching between reliable and faulty trials, and provided feedback on the correctness of the participant's trust decision.","Participants showed high trust in reliable trials and low trust in faulty trials, with trust levels changing dynamically based on the sensor's performance.","The study found that it took approximately eight to ten trials for participants to establish a new trust level, and that trust levels continued to mildly increase or decrease near the steady state. The study also found that demographic factors such as national culture and gender had a significant effect on trust behavior.","A third-order linear model was developed to capture the dynamic changes in human trust, and the model was parameterized based on human subject data, showing that trust is influenced by experience, cumulative trust, and expectation bias.","The simulated robot (obstacle detection sensor) provided reports on road conditions, and the human participant decided whether to trust or distrust the report, receiving feedback on their decision.",t-test; nonlinear least squares estimation,"The study used paired t-tests to compare the trust behavior data of all participants with that of different demographic groups (US vs India, male vs female) to determine if there were statistically significant differences. Nonlinear least squares estimation was used to fit the parameters of the proposed trust model to the collected data, both for the overall population and for each demographic group. This involved minimizing the error between the model's predictions and the observed trust behavior.",TRUE,Robot-accuracy,Robot-accuracy,,"The researchers manipulated the accuracy of the obstacle detection sensor, switching between reliable and faulty trials. This is explicitly stated in the paper: 'In database 3, the accuracy of the algorithm was switched between reliable and faculty according to a pseudo-random binary sequence (PRBS) in order to excite all possible dynamics of the participant's trust responses.' The manipulation of the sensor's accuracy directly impacted the participants' trust levels, as they showed higher trust in reliable trials and lower trust in faulty trials. This is described in the results section: 'The study elicited the variation of trust level as expected: participants showed high trust level (i.e., probability of trust response) in reliable trials and low trust level in faulty trials.'",10.23919/ACC.2017.7963172,http://ieeexplore.ieee.org/document/7963172/,"In an increasingly automated world, trust between humans and autonomous systems is critical for successful integration of these systems into our daily lives. In particular, for autonomous systems to work cooperatively with humans, they must be able to sense and respond to the trust of the human. This inherently requires a control-oriented model of dynamic human trust behavior. In this paper, we describe a gray-box modeling approach for a linear third-order model that captures the dynamic variations of human trust in an obstacle detection sensor. The model is parameterized based on data collected from 581 human subjects, and the goodness of ﬁt is approximately 80% for a general population. We also discuss the effect of demographics, such as national culture and gender, on trust behavior by re-parameterizing our model for subpopulations of data. These demographic-based models can be used to help autonomous systems further predict variations in human trust dynamics."
"Akash, Kumar; Hu, Wan-Lin; Jain, Neera; Reid, Tahira",A Classification Model for Sensing Human Trust in Machines Using EEG and GSR,2018,1,48,45,3,"3 participants were excluded due to anomalous EEG spectra, possibly due to bad channels or dislocation of EEG electrodes",Controlled Lab Environment,within-subjects,"Participants were equipped with EEG and GSR sensors, completed a baseline task, and then interacted with a computer-based simulation where they evaluated sensor reports and chose to trust or distrust them. They received feedback on the correctness of their response.",Participants evaluated the reliability of a simulated car sensor that detected obstacles and chose to trust or distrust the sensor report.,Unspecified,Autonomous Vehicles,Research,Evaluation,Image Analysis,minimal interaction,Participants interacted with a computer simulation and made trust decisions based on sensor reports.,simulation,The interaction was conducted through a computer-based simulation of a driving scenario.,simulated,The robot was represented as a simulated sensor within the computer interface.,pre-programmed (non-adaptive),"The sensor's performance was pre-programmed to be either reliable or faulty, without adapting to the user.",Physiological Measures,,Physiological Signals,"Trust was assessed using real-time psychophysiological measurements, specifically EEG and GSR.","parametric models (e.g., regression)",A quadratic discriminant classifier was used to predict trust levels based on extracted psychophysiological features.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the simulated sensor was directly manipulated by making it either 100% accurate or 50% accurate, which was intended to influence the participants' trust levels.","The sensor's reliability significantly impacted trust, with higher trust in reliable trials and lower trust in faulty trials.","The study found that customized trust-sensor models, based on individual feature sets, had higher accuracy than general models. The study also found that the phasic component of GSR was a significant predictor of trust levels.","Psychophysiological measurements, specifically EEG and GSR, can be used to estimate human trust in intelligent systems in real time, with customized models showing higher accuracy than general models.","The simulated car sensor reported either 'obstacle detected' or 'clear road', and the human participant chose to either 'trust' or 'distrust' the report. The human received feedback on the correctness of their response.",relieff; sequential forward floating selection (sffs); quadratic discriminant analysis (qda),"The study used ReliefF, a filter method, to initially shortlist features based on their ability to distinguish between samples. This was followed by Sequential Forward Floating Selection (SFFS), a wrapper method, which used the misclassification rate of a Quadratic Discriminant Analysis (QDA) classifier to select the final feature subset. The QDA classifier was then used to predict trust levels based on the selected features. Fivefold cross-validation was used to evaluate the performance of the QDA classifier.",TRUE,Robot-accuracy,Robot-accuracy,,"The study explicitly manipulated the reliability of the simulated car sensor. In 'reliable trials', the sensor was 100% accurate, while in 'faulty trials', it was only 50% accurate. This manipulation of the sensor's accuracy directly influenced the participants' trust levels, as stated in the paper: 'The independent variable was the participants' experience due to the sensor performance, and the dependent variable was their trust level. The sensor performance was varied to elicit the dynamic response in each participant's trust level.' This clearly indicates that the researchers intentionally changed the sensor's accuracy to observe its impact on trust. Therefore, 'Robot-accuracy' is the most appropriate category. The paper also states, 'The sensor's reliability significantly impacted trust, with higher trust in reliable trials and lower trust in faulty trials.' This confirms that the manipulated factor, 'Robot-accuracy', impacted trust. There were no other factors manipulated in the study.",10.1145/3132743,https://doi.org/10.1145/3132743,"Today, intelligent machines interact and collaborate with humans in a way that demands a greater level of trust between human and machine. A first step toward building intelligent machines that are capable of building and maintaining trust with humans is the design of a sensor that will enable machines to estimate human trust level in real time. In this article, two approaches for developing classifier-based empirical trust-sensor models are presented that specifically use electroencephalography and galvanic skin response measurements. Human subject data collected from 45 participants is used for feature extraction, feature selection, classifier training, and model validation. The first approach considers a general set of psychophysiological features across all participants as the input variables and trains a classifier-based model for each participant, resulting in a trust-sensor model based on the general feature set (i.e., a “general trust-sensor model”). The second approach considers a customized feature set for each individual and trains a classifier-based model using that feature set, resulting in improved mean accuracy but at the expense of an increase in training time. This work represents the first use of real-time psychophysiological measurements for the development of a human trust sensor. Implications of the work, in the context of trust management algorithm design for intelligent machines, are also discussed."
"Akash, Kumar; Reid, Tahira; Jain, Neera",Improving Human-Machine Collaboration Through Transparency-based Feedback – Part II: Control Design and Synthesis,2019,1,79,79,0,No participants were excluded,Online Crowdsourcing,within-subjects,Participants completed a reconnaissance mission study where they searched buildings and decided whether to wear protective gear based on a robot's recommendation. The robot's transparency level varied across trials.,"Participants were tasked with searching buildings and classifying them as safe or unsafe, deciding whether to wear protective gear based on a robot's recommendation.",Unspecified,Mobile Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a virtual robot through a computer interface.,simulation,The interaction was presented through a simulated environment on a computer screen.,simulated,The robot was represented as a virtual entity within the simulation.,pre-programmed (non-adaptive),"The robot provided recommendations based on pre-programmed rules, without adapting to the user.",Behavioral Measures,,Performance Metrics,Trust was inferred through compliance with the robot's recommendations.,POMDP,A Partially Observable Markov Decision Process (POMDP) was used to model human trust and workload.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's transparency level was directly manipulated, affecting the amount of information provided to the participant, which influenced the perceived risk and workload.","Higher transparency increased trust when initial trust was low, but decreased trust when initial trust was high. Higher transparency also increased workload.","The study found that higher transparency does not always increase trust, and can decrease trust when it is already high. Also, higher transparency always increased workload.","Higher transparency is more likely to increase human trust when the existing trust is low but also is more likely to decrease trust when it is already high. Furthermore, higher transparency always increases workload.",The robot provided recommendations on whether to wear protective gear. The human decided whether to comply with the robot's recommendation and searched the building.,baum-welch algorithm,"The study used an extended version of the Baum-Welch algorithm to estimate the state transition and observation probability functions for a Partially Observable Markov Decision Process (POMDP) model. This algorithm was used to analyze the collected human subject data and model the dynamics of human trust and workload based on the robot's transparency, reliability, and the presence or absence of danger. The algorithm was applied to both trust and workload models independently, and the data was split into training and testing sets for cross-validation.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the robot's transparency level, which directly affects the content of the robot's verbal communication. The low transparency robot reported if the building was safe/unsafe. The medium transparency robot additionally included details regarding which type of danger had been detected. The high transparency robot included all of the information provided by the low and medium transparency robots in addition to a percent confidence in the report. This manipulation of the information content directly impacted the participants' trust levels, as higher transparency increased trust when initial trust was low, but decreased trust when initial trust was high. The paper states, 'Our results indicate that higher transparency is more likely to increase human trust when the existing trust is low but also is more likely to decrease trust when it is already high.' This clearly indicates that the manipulated factor, 'Robot-verbal-communication-content', had an impact on trust. There were no other factors manipulated in the study.",10.1016/j.ifacol.2019.01.026,https://linkinghub.elsevier.com/retrieve/pii/S240589631930028X,"To attain improved human-machine collaboration, it is necessary for autonomous systems to infer human trust and workload and respond accordingly. In turn, autonomous systems require models that capture both human trust and workload dynamics. In a companion paper, we developed a trust-workload partially observable Markov decision process (POMDP) model framework that captured changes in human trust and workload for contexts that involve interaction between a human and an intelligent decision-aid system. In this paper, we define intuitive reward functions and show that these can be readily transformed for integration with the proposed POMDP model. We synthesize a near-optimal control policy using transparency as the feedback variable based on solutions for two cases: 1) increasing human trust and reducing workload, and 2) improving overall performance along with the aforementioned objectives for trust and workload. We implement these solutions in a reconnaissance mission study in which human subjects are aided by a virtual robotic assistant in completing a series of missions. We show that it is not always beneficial to aim to improve trust; instead, the control objective should be to optimize a context-specific performance objective when designing intelligent decision-aid systems that influence trust-workload behavior."
"Akash, Kumar; Reid, Tahira; Jain, Neera",Improving Human-Machine Collaboration Through Transparency-based Feedback – Part II: Control Design and Synthesis,2019,1,81,81,0,No participants were excluded,Online Crowdsourcing,within-subjects,Participants completed three reconnaissance missions with a virtual robotic assistant. The first mission used a random transparency level as a baseline. The subsequent two missions used transparency levels determined by two different POMDP solutions. The order of the POMDP solutions was randomized.,Participants classified buildings as safe or unsafe with the help of a virtual robotic assistant that provided recommendations on whether to wear protective gear.,Unspecified,Service and Assistive Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a virtual robot through a computer interface.,simulation,The interaction was conducted in a simulated environment.,simulated,The robot was a virtual representation.,shared control (adaptive),The robot's transparency level adapted based on the POMDP model and the participant's inferred trust and workload.,Behavioral Measures; Performance-Based Measures,,Performance Metrics,Trust was assessed through compliance with the robot's recommendations and performance metrics.,POMDP,"A POMDP model was used to model human trust and workload, and to determine the robot's transparency level.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's transparency level was directly manipulated based on two different POMDP solutions, which influenced the robot's behavior and the feedback provided to the user, and indirectly influenced the task difficulty.","The control policy based on Case 2 rewards decreased trust, while the control policy based on Case 1 did not significantly affect trust. Both control policies reduced beta-errors and improved overall performance.","The control policy based on Case 2 rewards, which prioritized performance, was more effective at reducing beta-errors and improving overall performance, even though it decreased trust. This suggests that optimizing for performance may be more important than maximizing trust in some contexts.",Optimizing for context-specific performance objectives is more beneficial than simply aiming to increase trust when designing intelligent decision-aid systems.,The virtual robot provided recommendations on whether to wear protective gear before entering a building. The human participant decided whether to follow the robot's recommendation and classified the building as safe or unsafe.,ANOVA; t-test,"The study used repeated measures one-way analysis of variance (ANOVA) tests to determine whether the use of feedback (different transparency control policies) had any significant effect on five performance metrics: number of compliant trials, average response time, number of injured trials, number of correct decisions, and total mission time. Post hoc analyses were conducted using paired t-tests on all possible pairwise contrasts when a significant ANOVA F test was obtained. The ANOVA tests were used to compare the means of the different conditions (baseline, Case 1, and Case 2), and the t-tests were used to determine which specific pairs of conditions differed significantly.",TRUE,Robot-verbal-communication-content; Robot-autonomy,Robot-verbal-communication-content,Robot-autonomy,"The study manipulated the robot's transparency level, which falls under 'Robot-verbal-communication-content' because it directly changes the information communicated to the user about the robot's reasoning and recommendations. The robot's transparency level was determined by two different POMDP solutions, which is a form of 'Robot-autonomy' because it changes how the robot makes decisions about what information to share with the user. The study found that the control policy based on Case 2 rewards, which prioritized performance, decreased trust, indicating that 'Robot-verbal-communication-content' impacted trust. The study did not find a significant impact on trust due to the different POMDP solutions, indicating that 'Robot-autonomy' did not impact trust.",10.1016/j.ifacol.2019.01.026,https://linkinghub.elsevier.com/retrieve/pii/S240589631930028X,"To attain improved human-machine collaboration, it is necessary for autonomous systems to infer human trust and workload and respond accordingly. In turn, autonomous systems require models that capture both human trust and workload dynamics. In a companion paper, we developed a trust-workload partially observable Markov decision process (POMDP) model framework that captured changes in human trust and workload for contexts that involve interaction between a human and an intelligent decision-aid system. In this paper, we define intuitive reward functions and show that these can be readily transformed for integration with the proposed POMDP model. We synthesize a near-optimal control policy using transparency as the feedback variable based on solutions for two cases: 1) increasing human trust and reducing workload, and 2) improving overall performance along with the aforementioned objectives for trust and workload. We implement these solutions in a reconnaissance mission study in which human subjects are aided by a virtual robotic assistant in completing a series of missions. We show that it is not always beneficial to aim to improve trust; instead, the control objective should be to optimize a context-specific performance objective when designing intelligent decision-aid systems that influence trust-workload behavior."
"Akash, Kumar; Jain, Neera; Misu, Teruhisa",Toward Adaptive Trust Calibration for Level 2 Driving Automation,2020,1,16,10,6,6 participants were excluded because eye-tracking data could not be recorded completely or with sufficient quality,Controlled Lab Environment,within-subjects,"Participants completed a practice drive, then completed eight driving sessions with varying traffic density, intersection complexity, and AR cues. They could take over control of the automated driving by braking or steering.",Participants monitored an autonomous driving system in a simulated urban environment and could take over control if needed.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated autonomous driving system and could take over control.,simulation,Participants interacted with a driving simulator in a virtual urban environment.,simulated,The autonomous vehicle was simulated in a virtual environment.,wizard of oz (directly controlled),The automated driving was simulated by replaying a past researcher's drive via the 'Wizard of Oz' technique.,Behavioral Measures,,Eye-tracking Data; Performance Metrics,Trust was inferred from reliance behavior and workload was inferred from eye-gaze data.,POMDP,A POMDP model was used to model human trust and workload dynamics.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated automation reliability, automation transparency (AR cues), and scene complexity to influence trust and workload.","The presence of AR cues generally increased trust, while high automation reliability saturated trust to a high level. Increased transparency increased trust but also increased cognitive workload.",The model showed that the effect of increased transparency on human trust and workload depends on scene complexity and automation reliability. The model also showed that high automation reliability saturates the probability of High Trust to a very high level.,"The study developed a POMDP framework to model coupled human trust and workload dynamics in a simulated driving scenario, demonstrating that automation transparency can be optimized to calibrate trust.","The robot (simulated autonomous vehicle) drove through a simulated urban environment, and the human monitored the driving and could take over control by braking or steering.",,"The paper describes a POMDP model for human trust and workload dynamics. The model parameters were estimated using a modified Baum-Welch algorithm. The model selection was done using a 3-fold cross-validation and the Akaike information criterion (AIC). However, no traditional statistical tests like t-tests or ANOVA were explicitly mentioned.",TRUE,Robot-verbal-communication-content; Robot-accuracy; Task-complexity,Robot-verbal-communication-content; Robot-accuracy,Task-complexity,"The researchers manipulated several factors. 'Robot-verbal-communication-content' was manipulated through the presence or absence of AR cues, which provided information about the environment (e.g., bounding boxes around cars and pedestrians). This is a manipulation of the content of the information provided by the robot. 'Robot-accuracy' was manipulated by varying the distance the automated vehicle stopped from the stop line, which directly impacts the perceived reliability of the automation. 'Task-complexity' was manipulated by varying traffic density and the presence of pedestrians, which changes the cognitive demands of the monitoring task. The paper states that the presence of AR cues generally increased trust, and high automation reliability saturated trust to a high level, indicating that 'Robot-verbal-communication-content' and 'Robot-accuracy' impacted trust. The paper also states that traffic density was found to be insignificant, indicating that 'Task-complexity' did not impact trust.",10.1145/3382507.3418885,https://dl.acm.org/doi/10.1145/3382507.3418885,"Properly calibrated human trust is essential for successful interaction between humans and automation. However, while human trust calibration can be improved by increased automation transparency, too much transparency can overwhelm human workload. To address this tradeoff, we present a probabilistic framework using a partially observable Markov decision process (POMDP) for modeling the coupled trust-workload dynamics of human behavior in an action-automation context. We specifically consider hands-off Level 2 driving automation in a city environment involving multiple intersections where the human chooses whether or not to rely on the automation. We consider automation reliability, automation transparency, and scene complexity, along with human reliance and eye-gaze behavior, to model the dynamics of human trust and workload. We demonstrate that our model framework can appropriately vary automation transparency based on real-time human trust and workload belief estimates to achieve trust calibration."
"Akkuzu, Anastasia; Calvo-Barajas, Natalia; Castellano, Ginevra",Behavioural Observations as Objective Measures of Trust in Child-Robot Interaction: Mutual Gaze,2023,1,52,52,0,No participants were excluded,Controlled Lab Environment,,"Children interacted with a robot in a collaborative storytelling game. Video data of the interactions was collected, and pre- and post-interaction surveys were administered to assess social trust and liking. Mutual gaze was extracted from the video data using OpenFace. Statistical analyses were performed to identify correlations between mutual gaze, social trust, liking, age, and interaction duration.",Children participated in a collaborative storytelling game with a robot.,Unspecified,Expressive Robots,Social; Research,Social,Conversation,minimal interaction,Children interacted with the robot in a storytelling game.,real-world,Children interacted with a physical robot in a real-world setting.,physical,A physical robot was present during the interaction.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Behavioral Measures; Questionnaires,,Video Data,Trust was assessed using pre- and post-interaction surveys and behavioral measures of mutual gaze.,no modeling,"The study did not model trust computationally, but used statistical analysis.",Empirical HRI Studies,Physical Robot Studies,No Manipulation,"The study did not manipulate any specific factors related to trust, but rather observed the relationship between mutual gaze and trust/liking.","The study found no significant correlation between mutual gaze and social trust, but a weak correlation between mutual gaze and liking.","The study found a weak negative correlation between age and mutual gaze, and a strong positive correlation between mutual gaze in the first and second half of the interaction, and between the first and second interaction. The study also noted that the data was not normally distributed.","Mutual gaze is not a reliable indicator of social trust, but is more closely tied to liking.","The robot engaged in a collaborative storytelling game with the child. The child was asked to participate in the storytelling game with the robot, and their gaze was tracked.",Shapiro-Wilk; Spearman correlation,"The study used a Shapiro-Wilk test to assess the normality of the data, which revealed that the data was not normally distributed. Consequently, Spearman correlation matrices were used to identify relationships between the percent of mutual gaze and social trust and liking scores, as well as to explore the relationship between mutual gaze and age, and the temporal effects of mutual gaze within and between interactions.",FALSE,,,,"The study did not manipulate any factors. The study observed the relationship between mutual gaze and trust/liking. The study did not manipulate any of the factors listed in the prompt. Therefore, no factors were manipulated, and no factors impacted or did not impact trust.",10.1145/3623809.3623960,https://dl.acm.org/doi/10.1145/3623809.3623960,"In developing a computational model of trust, this paper summarises the findings in a previous study exploring mutual gaze as a behavioural parameter of social trust and liking [1]. Drawing on the data collected in a related paper [6], which provides us with video clips of children interacting with a robot during a collaborative storytelling game, we look at the interactions between metrics assessing social trust and liking, and the development of mutual gaze as an objective measure of social trust and liking. We achieve this through several statistical analyses between the percent of mutual gaze in each interaction, scores from social trust and liking metrics, age of the participant, and duration. The findings of our study support the use of mutual gaze as an objective measure for liking, but there is still not sufficient evidence to support the use of mutual gaze as an objective measure to identify and capture social trust as a whole. Furthermore, interaction context impacts the amount of mutual gaze in an interaction, and the age of the participant has an impact on the amount of mutual gaze that occurs."
"Alarcon, Gene M.; Gibson, Anthony M.; Jessup, Sarah A.","Trust Repair in Performance, Process, and Purpose Factors of Human-Robot Ttust",2020,1,52,52,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were introduced to a robot, then completed a trust game where they loaned money to the robot across five rounds. Participants were randomly assigned to either a trust or distrust condition. In the distrust condition, the robot returned less money than promised in rounds 3 and 4, and returned the promised amount in round 5. Participants completed trustworthiness measures after each round.",Participants played a modified investor/dictator game where they loaned money to a robot partner across five rounds.,Unspecified,Unmanned Ground Vehicles,Research,Game,Economic Game,minimal interaction,Participants interacted with the robot through a computer interface.,simulation,The interaction was conducted through a computer-based simulation.,physical,"Participants were introduced to a physical robot, but the interaction was through a computer interface.",pre-programmed (non-adaptive),The robot's actions were pre-recorded and did not adapt to the participant's behavior.,Questionnaires; Custom Scales,Mayer and Davis' Trust/Trustworthiness Scales (1999),,Trust was measured using a shortened version of an interpersonal trustworthiness scale.,"parametric models (e.g., regression)",Mixed effects models were used to analyze changes in trustworthiness perceptions over time.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's return of investment was manipulated to create trust violations in the distrust condition, and the task was framed as a military scenario where the robot may act self-interested.","Trust in performance was repaired after a violation, but trust in process and purpose was not fully repaired. Trust decreased after the violation in all three factors.","Process and purpose perceptions were more sensitive to trust violations than performance perceptions. Trust repair was achieved in performance perceptions, but not in process and purpose perceptions.","Trust violations negatively impacted perceptions of performance, process, and purpose, with process and purpose being more sensitive to violations. Trust repair was only achieved in performance perceptions.","The robot acted as a 'Runner' in a virtual maze, collecting boxes. The human participant acted as a 'Banker', loaning money to the robot and receiving a return on their investment. The robot's performance was consistent, but the return of investment was manipulated to create trust violations.",Mixed-effects model,"The study used linear mixed effects models to analyze changes in trustworthiness perceptions (performance, process, and purpose) over time across two conditions (trust vs. distrust). The models included time, runner behavior (trust vs. distrust), and their interaction as predictors. A first-order autoregressive correlation structure was used for all analyses. The purpose of these models was to examine the impact of trust violations and repairs on the three trustworthiness perceptions.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the robot's return on investment, which directly impacted the task performance metrics (i.e., the amount of money the participant received back). In the distrust condition, the robot returned less money than promised in rounds 3 and 4, which is a manipulation of the robot's accuracy in fulfilling its financial obligations. This manipulation was explicitly designed to create a trust violation. The paper states, 'in the distrust condition, the Runner returned less money than promised for Time points 3 and 4, which we considered a trust violation.' This directly relates to the robot's accuracy in the task. The results showed that this manipulation of the robot's accuracy significantly impacted trust perceptions, as evidenced by the decrease in performance, process, and purpose perceptions after the trust violation. The paper states, 'After Time 2 (i.e., when the trust violation occurred) performance perceptions declined significantly for the distrust condition.' and 'After the distrust behaviors, perceived purpose perceptions declined significantly.' and 'Process perceptions declined significantly at Time 3 (where the distrust behavior first occurred).'",10.1109/ICHMS49158.2020.9209453,https://ieeexplore.ieee.org/document/9209453/,"The current study explored the influence of trust and distrust behaviors on performance, process, and purpose (trustworthiness) perceptions over time when participants were paired with a robot partner. We examined the changes in trustworthiness perceptions after trust violations and trust repair after those violations. Results indicated performance, process, and purpose perceptions were all affected by trust violations, but perceptions of process and purpose decreased more than performance following a distrust behavior. Similarly, trust repair was achieved in performance perceptions, but trust repair in perceived process and purpose was absent. When a trust violation occurred, process and purpose perceptions deteriorated and failed to recover from the violation. In addition, the trust violation resulted in untrustworthy perceptions of the robot. In contrast, trust violations decreased partner performance perceptions, and subsequent trust behaviors resulted in a trust repair. These findings suggest that people are more sensitive to distrust behaviors in their perceptions of process and purpose than they are in performance perceptions."
"Alarcon, Gene M.; Gibson, Anthony M.; Jessup, Sarah A.; Capiola, August",Exploring the differential effects of trust violations in human-human and human-robot interactions,2021,1,124,124,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were assigned to the banker role in a modified Trust Game called Checkmate, interacting with either a human or robot runner. The runner's behavior was manipulated to either return the promised amount or less than promised. Participants completed background surveys, a training round, and five experimental rounds. Trustworthiness was measured after each round, and trust intentions were measured before and after the experiment. Behavioral trust was assessed through loan amounts.","Participants played the role of a banker in the Checkmate game, loaning money to a runner (either human or robot) who then completed a maze task and returned a portion of the earnings.",Nao,Humanoid Robots; Expressive Robots,Research; Social,Game,Economic Game,minimal interaction,"Participants interacted with a robot or human through a game interface, with no physical touch.",real-world,Participants interacted with a physically present robot or human in a lab setting.,physical,The robot was physically present during the interaction.,pre-programmed (non-adaptive),The robot's actions were pre-recorded and did not adapt to the participant's behavior.,Behavioral Measures; Questionnaires; Custom Scales,Trust in Automation Scale (TAS); Mayer and Davis' Trust/Trustworthiness Scales (1999),,"Trust was assessed using questionnaires, a custom scale, and behavioral measures of loan amounts.",no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The runner's behavior was manipulated to either return the promised amount or less than promised, influencing the participant's expectations and perceptions of trustworthiness.","Trustworthiness perceptions, trust intentions, and trust behaviors decreased when the runner engaged in distrust behaviors.","The study found no significant differences in trust-related outcomes between human and robot partners, supporting the CASA model, which contradicts some previous findings on automation bias. There were also significant Experimenter x RB and Experimenter x RB x Time interactions on Trustworthiness and Trust Intentions, but these were not reported in the main text.","Participants showed lower trustworthiness perceptions, trust intentions, and trust behaviors after the runner engaged in non-performance-based distrust behaviors, regardless of whether the runner was a human or a robot.","The robot (or human confederate) acted as a runner in the Checkmate game, completing a maze task and returning a portion of the earnings to the human participant, who acted as the banker and made loan decisions.",ANOVA; pairwise contrasts; cumulative link mixed model; likelihood ratio test; analysis of deviance tests,"The study used a mixed factorial ANOVA to analyze differences in trustworthiness perceptions and trust intentions across time, runner behavior (trust vs. distrust), and partner type (human vs. robot). Post-hoc pairwise contrasts with Bonferroni correction were used to further examine significant interactions. A cumulative link mixed model was used to analyze trust behaviors (loan amounts) across the same factors, and a likelihood ratio test was used to compare model fit. Analysis of Deviance tests were used to test the main effects and interactions on trust behaviors.",TRUE,Teaming; Robot-morality,Robot-morality,Teaming,"The study manipulated 'Teaming' by having participants interact with either a human or a robot partner in a collaborative game setting. This is a manipulation of the partner type, which is a form of teaming. The study also manipulated 'Robot-morality' by having the runner (human or robot) either return the promised amount of money (trust condition) or less than promised (distrust condition). This manipulation directly impacts the perceived integrity and benevolence of the runner, which is a form of morality in the context of the game. The results showed that the 'Robot-morality' manipulation significantly impacted trust, as participants showed lower trustworthiness perceptions, trust intentions, and trust behaviors after the runner engaged in non-performance-based distrust behaviors. However, the 'Teaming' manipulation (human vs. robot partner) did not significantly impact trust-related outcomes, as there were no significant differences in trust between human and robot partners.",10.1016/j.apergo.2020.103350,https://linkinghub.elsevier.com/retrieve/pii/S0003687020302982,"There is sparse research directly investigating the effects of trust manipulations in human-human and humanrobot interactions. Moreover, studies on human-human versus human-robot trust have leveraged unusual or low vulnerability contexts to investigate such effects and have focused mostly on robot performance. In the present research, we seek to remedy these limitations and compare trust in human-human versus human-robot collaborations in an augmented and adapted version of the Trust Game. We used a mixed factorial design to examine the effects of trust and trust violations on human-human and human-robot interactions over time with an emphasis on anthropomorphic robots in a social context. We found consistent and significant effects of partner behavior. Specifically, partner distrust behaviors led to participants’ lower levels of trustworthiness perceptions, trust intentions, and trust behaviors over time compared to partner trust behaviors. We found no significant effect of partnering with a human versus an anthropomorphic robot over time across the three dependent var­ iables, supporting the computers as social actors (CASA; Nass and Moon, 2000) paradigm. This study demon­ strated that there may be instances where the effects of trust violations from an anthropomorphized robot partner are not meaningfully different from those of a human partner in a social context."
"Alarcon, Gene M.; Capiola, August; Hamdan, Izz Aldin; Lee, Michael A.; Jessup, Sarah A.",Differential biases in human-human versus human-robot interactions,2023,1,131,131,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were introduced to a human or robot partner, completed baseline questionnaires, performed an endowment earning task, viewed a presentation about the Checkmate game, and then completed the Checkmate task with pre-recorded videos of their partner. Participants completed trust intention scales before and after the task, and were debriefed after the task.","Participants played the role of a banker in the Checkmate game, lending money to a runner (confederate) who collected boxes in a virtual maze. The runner would then return a portion of the earnings to the banker.",Nao,Humanoid Robots; Expressive Robots,Research; Social,Game,Economic Game,passive observation,Participants observed pre-recorded videos of the robot performing the task.,media,Participants watched videos of the robot performing the task.,physical,"The robot was physically present in the room, but participants interacted with it through pre-recorded videos.",pre-programmed (non-adaptive),The robot's actions were pre-recorded and did not adapt to the participant's behavior.,Behavioral Measures; Questionnaires,Mayer and Davis' Trust/Trustworthiness Scales (1999),,Trust was measured using a modified version of the Mayer and Davis' trustworthiness scale and behavioral measures of risk-taking.,"parametric models (e.g., regression)",Linear mixed-effects models and repeated measures ANOVA were used to analyze the data.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the runner's behavior in the Checkmate game to create performance, consideration, and morality violations, which were intended to affect perceptions of ability, benevolence, and integrity, respectively.",Performance violations had a stronger negative effect on ability/performance perceptions for the robot compared to the human. Benevolence/purpose and integrity/process perceptions decreased more for the robot than the human after any violation. There were no significant differences in trust intentions or risk-taking behaviors between partners or conditions.,"The study found that while trustworthiness perceptions were more negatively impacted for robots after trust violations, trust intentions and risk-taking behaviors were not significantly different between human and robot partners. This suggests a dissociation between perceptions and behaviors.","The study's key finding is that people's perceptions of a robot's trustworthiness are more negatively affected by trust violations than a human's, even though this does not translate to differences in trust intentions or risk-taking behaviors.","The robot (or human confederate) acted as a runner in a virtual maze, collecting boxes and returning a portion of the earnings to the human participant, who acted as the banker. The human participant decided how much money to lend to the runner.",linear mixed-effects models; ANOVA,"Linear mixed-effects models were used to analyze the effects of the manipulations on trustworthiness perceptions and risk-taking behaviors. Repeated measures ANOVA was used to analyze the effects of the manipulations on trust intentions, which were measured at the beginning and end of the experiment. These tests were used to examine the effects of partner (human vs. robot) and manipulation (performance, consideration, morality violations) on the dependent variables across different rounds of the experiment.",TRUE,Robot-accuracy; Robot-morality; Robot-social-attitude,Robot-accuracy; Robot-morality; Robot-social-attitude,,"The study manipulated the runner's behavior in the Checkmate game to create performance, consideration, and morality violations. The 'Performance' violation was manipulated by degrading the runner's performance, causing the runner to crash into buildings and explore regions where boxes were not present, which directly impacts the robot's accuracy in the task. The 'Consideration' violation was manipulated by having the runner collect mostly red boxes, which the participants were told would not be shared with them, thus impacting the robot's social attitude (benevolence/purpose). The 'Morality' violation was manipulated by having the runner return less money than promised, which is a violation of moral codes outside of games and does not influence the robot's success rate on the task. All three manipulations were designed to affect perceptions of ability/performance, benevolence/purpose, and integrity/process, respectively, and all three were found to impact trust perceptions. The study explicitly states that the manipulations were designed to create 'trust violations' and that these violations were intended to affect perceptions of ability, benevolence, and integrity, which are all related to trust. The results section also shows that these manipulations had a significant impact on trust perceptions.",10.1016/j.apergo.2022.103858,https://linkinghub.elsevier.com/retrieve/pii/S0003687022001818,"The research on human-robot interactions indicates possible differences toward robot trust that do not exist in human-human interactions. Research on these dif­ ferences has traditionally focused on performance degradations. The current study sought to explore differences in human-robot and human-human trust interactions with performance, consideration, and morality trustworthiness manipulations, which are based on ability/performance, benevolence/purpose, and integrity/process manipulations, respectively, from previous research. We used a mixed factorial hierarchical linear model design to explore the effects of trustworthiness manipu­ lations on trustworthiness perceptions, trust intentions, and trust behaviors in a trust game. We found partner (human versus robot) differences across all three trustworthiness perceptions, indicating biases towards robots may be more expansive than previously thought. Additionally, there were marginal effects of partner differences on trust intentions. Interestingly, there were no differences between partners on trust behaviors. Results indicate human biases toward robots may be more complex than considered in the literature."
"Alarcon, Gene M.; Lyons, Joseph B.; Hamdan, Izz Aldin; Jessup, Sarah A.",Affective Responses to Trust Violations in a Human-Autonomy Teaming Context: Humans Versus Robots,2024,1,131,131,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were introduced to a human or robot partner, completed a math task, received instructions on the Checkmate game, played two practice rounds, and then five experimental rounds, completing surveys after each round.","Participants played the role of 'banker' in the Checkmate game, lending money to a 'runner' (human or robot confederate) who collected boxes in a virtual maze.",Nao,Humanoid Robots,Research,Game,Economic Game,minimal interaction,Participants observed pre-recorded videos of the robot or human partner performing the task.,media,Participants watched videos of the robot or human partner completing the maze task.,physical,"Participants were introduced to a physical robot, but interacted with it through pre-recorded videos.",pre-programmed (non-adaptive),"The robot's actions were pre-recorded and automated, with no adaptation to the participant's behavior.",Questionnaires,,,Trust was measured using a shortened version of the PANAS scale to assess positive and negative affect.,no modeling,"The study did not use computational models of trust, focusing on statistical analysis of the collected data.",Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the runner's behavior (ability, benevolence, integrity) by pre-recording videos of the runner making errors, collecting red boxes, or returning less money than promised, influencing the participant's trust.","Ability violations by the robot decreased positive affect more than human violations, while benevolence and integrity violations by humans decreased positive affect more than robot violations. Negative affect was higher with human partners overall.","The study found that ability violations had a stronger effect on negative affect than integrity violations, which is not typically found in the literature. Also, participants experienced more negative affect with a human partner than a robot partner, which is unexpected.","Ability violations by a robot led to a greater decrease in positive affect compared to human violations, while benevolence and integrity violations by humans led to a greater decrease in positive affect compared to robot violations.","The robot (or human confederate) acted as a 'runner' in a virtual maze, collecting boxes and returning a portion of the earnings to the human participant, who acted as the 'banker'. The human participant decided how much money to lend to the runner.",Mixed-effects model,"The study used repeated measures mixed-effects models to analyze the data. These models were used to examine the effects of partner type (human vs. robot), manipulation type (ability, benevolence, integrity violations), and round on positive and negative affect. The F-statistic and Type II sum of squares were used to interpret the effects from the models. The models were used to test the hypotheses related to how different types of trust violations by human and robot partners impact participants' positive and negative affect over time.",TRUE,Robot-accuracy; Robot-morality; Teaming,Robot-accuracy; Robot-morality; Teaming,,"The study manipulated the runner's behavior to simulate different types of trust violations. 'Robot-accuracy' was manipulated by having the runner (robot or human) make errors in the maze, impacting the number of boxes collected and thus the amount of money earned. This directly affected the task performance and was explicitly manipulated by the researchers. 'Robot-morality' was manipulated by having the runner collect red boxes (which they were told they could keep) instead of blue/white boxes (which could be shared), and by having the runner return less money than promised. These actions represent a violation of moral codes within the game context, and were intentionally manipulated. 'Teaming' was manipulated by having participants interact with either a human or a robot partner, which influenced the participants' affective responses to the trust violations. The study found that all three of these factors impacted trust, as evidenced by the changes in positive and negative affect depending on the type of violation and the partner (human or robot). The paper explicitly states that the type of violation (ability, benevolence, integrity) and the partner type (human or robot) were manipulated to examine their impact on trust and affect. The results section shows that these manipulations had a significant impact on the participants' affective responses. There were no factors that were manipulated that did not impact trust.",10.1007/s12369-023-01017-w,https://link.springer.com/10.1007/s12369-023-01017-w,"Despite the increasing use of robots in a variety of applications, little is known about the emotional responses humans experience when a robot or a human commits a trust violation. The current paper compares the affective responses of humans paired with either a human or a robot confederate who committed trust violations. Additionally, the current paper utilizes new manipulations in the literature to experimentally manipulate the type of trust violation, namely ability, benevolence, and integrity violations. As expected, when a robot committed an ability violation participants’ positive affect decreased more than if the violation was performed by a human. When an integrity or benevolence violation occurred, participants had a greater decrease in positive affect when a human performed the violations than when a robot violated trust. Overall, participants experienced more negative affect with a human partner than a robot partner. Also, ability violations had stronger effects on negative affect than integrity violations. Results indicate humans do have different affective responses when trust is violated, depending on the type of violation as well as the partner performing the violation. Implications are discussed."
"Albayram, Yusuf; Jensen, Theodore; Khan, Mohammad Maifi Hasan; Fahim, Md Abdullah Al; Buck, Ross; Coman, Emil",Investigating the Effects of (Empty) Promises on Human-Automation Interaction and Trust Repair,2020,1,200,189,11,"7 participants did not provide a proper answer to the open-ended question on the instruction page, 3 participants allocated all 20 images to the automation in each of the final 4 rounds, 1 participant had a technical problem during their first attempt at gameplay",Online Crowdsourcing,between-subjects,"Participants played an online game where they collaborated with an automated system to classify images across five rounds, with different reliability and promise conditions. After each round, they received feedback and allocated images for the next round. Finally, they completed a survey.",Participants classified images of vehicles as 'Dangerous' or 'Not Dangerous' with the help of an automated system.,Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Game,Cooperative Game,minimal interaction,Participants interacted with the automated system through a computer interface.,simulation,The interaction was through a simulated game environment.,simulated,The robot was represented as an automated system within the game.,pre-programmed (non-adaptive),The automated system had a fixed level of accuracy and did not adapt to the user.,Behavioral Measures; Questionnaires,Mayer and Davis' Trust/Trustworthiness Scales (1999); System Usability Scale (SUS),Performance Metrics,Trust was measured using a combination of behavioral measures (image allocation) and questionnaires.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the reliability of the automated system and the type of promise message given to participants after each round, influencing their expectations and perceptions of the system.","High reliability led to increased trust and reliance on the automation. Optimistic promises initially increased reliance, but this effect diminished when the system did not improve. No-promise conditions led to greater frustration.","Participants initially increased their allocation to the automation after an optimistic promise, but this trend reversed when the system did not improve. Participants in the no-promise group reported greater frustration than those in the promise groups.","System reliability significantly influenced trust, with higher reliability leading to greater trust and reliance. Optimistic promises initially increased reliance, but this effect diminished when the system did not improve.","The automated system classified images of vehicles as 'Dangerous' or 'Not Dangerous', and the human participant also classified images and allocated images to the automated system for the next round.",ANOVA; ANOVA; Mann-Whitney U; Kruskal-Wallis; Chi-squared; Fisher's exact test,"The study used ANOVA and MANOVA to assess the main effects and interactions of system reliability and promise type on trust and reliance, examining variables such as total allocation, first calibration, perceived ability, integrity, and benevolence. Mann-Whitney U-test was used to compare perceived reliability between groups. Kruskal-Wallis test was used to compare demographic variables across groups. Chi-square and Fisher's exact tests were used for comparisons with categorical data, specifically for gender and race.",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-accuracy; Robot-verbal-communication-content,,"The study manipulated 'Robot-accuracy' by having two levels of reliability for the automated system (low and high), which directly impacted the system's success rate in identifying images. This is explicitly stated in the paper: 'To observe how participants calibrated their trust as they grew familiar with the system's capabilities [17], we used two reliability levels for the automation: low and high.' The study also manipulated 'Robot-verbal-communication-content' by varying the promise messages given to participants after each round (no-promise, optimistic, realistic). This is described in the paper: 'Promise type determined the message about future performance that participants saw after each round.' Both of these manipulations were found to impact trust. The paper states: 'High reliability participants allocated more images to the automation than those collaborating with low reliability automation, and reported greater perceived trustworthiness and less frustration.' and 'Optimistic promise participants allocated significantly more images than realistic promise participants initially, but not overall. Optimistic promises were rated as Session 1A: Trust and Teamwork HAI '20, November 10-13, 2020, Virtual Event, NSW, Australia less believable than no-promise and realistic promise messages. Moreover, participants who received no promise reported a greater degree of frustration than either group who received a promise.' There were no factors that were manipulated that did not impact trust.",10.1145/3406499.3415064,https://dl.acm.org/doi/10.1145/3406499.3415064,"Setting expectations for future behavior with promises is one way to manage human-human trusting relationships. To investigate the effect of promises made by an automated system, we conducted a 2 (reliability: low, high) x 3 (promise type: no-promise, optimistic, realistic) between-subject study where participants collaborated with an Automated Target Detection (ATD) system to classify images in multiple rounds of gameplay. We found that an optimistic promise (i.e., _I promise to do better_) initially led to significantly more reliance on automation than a realistic promise (i.e., _I cannot do better than this_), but not in the long-term. High reliability participants relied more on the automation and reported greater perceived trustworthiness compared to low reliability participants. In addition, participants in the no promise group reported a greater degree of frustration compared to the other groups. We discuss the implications of our findings for trust repair in automated systems."
"Alhaji, Basel; Prilla, Michael; Rausch, Andreas",Trust Dynamics and Verbal Assurances in Human Robot Physical Collaboration,2021,1,32,32,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants completed a demographic questionnaire, trained with the robot in a mixed reality environment, read a scenario description, and then completed two phases of the experiment: Trust Dynamics (TD) and Trust Calibration (TC). In the TD phase, participants experienced reliable and unreliable robot behavior. In the TC phase, participants experienced correct and incorrect feedback with either reliable or unreliable robot behavior, depending on the group they were assigned to. Questionnaires were administered after each battery in the TD phase and at the end of each run in the TC phase.",Participants collaborated with a virtual robot to disassemble a simplified model of an electric car traction battery by loosening screws.,Franka Emika Panda,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Tool Use,minimal interaction,Participants interacted with a virtual robot in a mixed reality environment.,simulation,The interaction took place in a mixed reality environment using a HoloLens 2.,simulated,The robot was a virtual representation in a mixed reality environment.,shared control (fixed rules),"The robot autonomously detected the battery and screws, but the human chose which screw to loosen.",Questionnaires,Muir's Trust Questionnaire; Godspeed Questionnaire,,Trust was measured using a single item from Muir's questionnaire and other factors were measured using single items from other questionnaires.,no modeling,Trust was not modeled computationally in this study.,Empirical HRI Studies,Mixed Reality Studies,Direct Manipulation,"The reliability of the robot was manipulated by having it succeed or fail at loosening screws, and feedback was manipulated by having the robot provide correct or incorrect verbal assurances about its ability to loosen screws. These manipulations were intended to influence trust positively or negatively.","Reliable behavior increased trust, while unreliable behavior decreased trust. Correct feedback increased trust when the robot was unreliable, but had no effect when the robot was reliable. Incorrect feedback decreased trust when the robot was reliable, but had no effect when the robot was unreliable.","Trust dynamics were stronger while dissipating than while accumulating. The factors that correlated with trust during accumulation (dependability, reliability, predictability, and faith) did not correlate with trust during dissipation. Incorrect feedback only affected trust when the robot was reliable, and correct feedback only affected trust when the robot was unreliable.","Trust dynamics are stronger while dissipating than while accumulating, and verbal feedback can be used to calibrate trust, especially when failures are expected.",The robot autonomously detected the battery and screws and loosened them using a screwdriver. The human placed the battery in the workspace and selected which screw the robot should loosen.,Wilcoxon rank sum; Mann-Whitney U; Spearman correlation,"The study used Wilcoxon Signed-Rank tests for pairwise comparisons of dependent samples (within-group comparisons), such as comparing trust levels across different batteries within the same reliability condition. Mann-Whitney U tests were used for pairwise comparisons of independent samples (between-group comparisons), such as comparing trust levels between different groups or sequences. Spearman correlation analysis was used to identify the factors that correlate with trust as it accumulates and dissipates. The purpose of these tests was to analyze the impact of robot reliability, feedback, and other factors on human trust, and to understand the dynamics of trust over time.",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-accuracy; Robot-verbal-communication-content,,"The study explicitly manipulated the robot's accuracy by having it either reliably loosen all screws or fail on rusty screws, which directly impacts task performance. This is described in the 'Experimental Conditions' section where it states 'Reliable behavior: in the reliable case, the robot successfully loosens all rusty and normal screws... Unreliable behavior: in this case, the robot loosens the normal screws correctly but fails to loosen the rusty ones in different ways.' The study also manipulated the content of the robot's verbal communication by having it provide either correct or incorrect feedback about its ability to loosen screws, as described in the 'Feedback' section: 'Correct feedback: in this case the robot verbally provides the human with assurances/warnings about its ability to loosen a specific screw... Incorrect feedback: similar to the correct feedback case, in this case the robot verbally provides the human with assurances/warnings about its ability to loosen a specific screw. Incorrectly though.' Both of these manipulations were found to impact trust. The 'Results' section shows that 'Results of Wilcoxon signed-rank test show statistically significant difference between the reliable and unreliable conditions' and 'correct feedback appears to have a positive impact on trust when the behavior is unreliable' and 'incorrect feedback did decrease human trust when it accompanies reliable behavior'. No other factors were explicitly manipulated.",10.3389/frai.2021.703504,https://www.frontiersin.org/articles/10.3389/frai.2021.703504/full,"Trust is the foundation of successful human collaboration. This has also been found to be true for human-robot collaboration, where trust has also influence on over- and under-reliance issues. Correspondingly, the study of trust in robots is usually concerned with the detection of the current level of the human collaborator trust, aiming at keeping it within certain limits to avoid undesired consequences, which is known as trust calibration. However, while there is intensive research on human-robot trust, there is a lack of knowledge about the factors that affect it in synchronous and co-located teamwork. Particularly, there is hardly any knowledge about how these factors impact the dynamics of trust during the collaboration. These factors along with trust evolvement characteristics are prerequisites for a computational model that allows robots to adapt their behavior dynamically based on the current human trust level, which in turn is needed to enable a dynamic and spontaneous cooperation. To address this, we conducted a two-phase lab experiment in a mixed-reality environment, in which thirty-two participants collaborated with a virtual CoBot on disassembling traction batteries in a recycling context. In the first phase, we explored the (dynamics of) relevant trust factors during physical human-robot collaboration. In the second phase, we investigated the impact of robot’s reliability and feedback on human trust in robots. Results manifest stronger trust dynamics while dissipating than while accumulating and highlight different relevant factors as more interactions occur. Besides, the factors that show relevance as trust accumulates differ from those appear as trust dissipates. We detected four factors while trust accumulates (perceived reliability, perceived dependability, perceived predictability, and faith) which do not appear while it dissipates. This points to an interesting conclusion that depending on the stage of the collaboration and the direction of trust evolvement, different factors might shape trust. Further, the robot’s feedback accuracy has a conditional effect on trust depending on the robot’s reliability level. It preserves human trust when a failure is expected but does not affect it when the robot works reliably. This provides a hint to designers on when assurances are necessary and when they are redundant."
"Alhaji, Basel; Büttner, Sebastian; Sanjay Kumar, Shushanth; Prilla, Michael",Trust dynamics in human interaction with an industrial robot,2024,1,32,32,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants disassembled car batteries with a robot in two phases: a no-forecast phase where the robot either succeeded or failed, and a forecast phase where the robot predicted its success or failure before acting. Questionnaires were administered after each battery in the no-forecast phase and after each run in the forecast phase.",Participants disassembled car batteries by removing screws using voice commands to control a robot arm.,Franka Emika Panda,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically interacted with the robot by placing batteries and giving voice commands.,real-world,The study used a real robot and physical objects in a lab setting.,physical,The robot was a physical industrial robot arm.,shared control (fixed rules),The robot followed pre-programmed actions based on voice commands from the participant.,Questionnaires,Muir's Trust Questionnaire,,Trust was measured using a single-item questionnaire after each battery disassembly.,"parametric models (e.g., regression)",Linear regression models were used to analyze the relationship between trust and other factors.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's performance was manipulated by introducing errors, and its behavior was manipulated by providing verbal forecasts of success or failure. Human expectations were influenced by the robot's forecasts.","Trust increased with successful robot performance and decreased with failures. Verbal forecasts had a positive effect on trust during faulty behavior, even when incorrect.",Trust dissolved more quickly than it formed. Incorrect forecasts of faulty behavior still increased trust compared to no forecasts. There were no statistically significant differences between genders.,"Verbal forecasts of task success by the robot mitigated trust dissolution during faulty behavior, even when the forecasts were incorrect.","The robot removed screws from batteries, and the human placed batteries in the work area and gave voice commands to the robot.",Mann-Whitney U; Wilcoxon signed-rank test; Spearman correlation; Linear regression,"The study used Mann-Whitney U tests to compare groups based on the sequence of conditions and gender. Wilcoxon signed-rank tests with Bonferroni correction were used to compare trust ratings across different steps in the no-forecast phase. Spearman correlation analysis was used to identify factors correlated with trust. Linear regression models were built to assess the explanatory power of these factors on trust, with multicollinearity checks using the Variance Inflation Factor (VIF).",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-accuracy; Robot-verbal-communication-content,,"The study manipulated 'Robot-accuracy' by having the robot either successfully remove screws or fail when encountering rusty screws. This is explicitly stated in the 'Experiment task & design' section: 'To stimulate trust in a given direction, we use the independent variable occurrence of errors with two levels. They are error-free in which the robot loosens all screws correctly (normal and rusty screws), which we assume (and we shall verify) will affect trust positively (increase trust); and faulty in which the robot fails in its task when it faces a rusty screw where a pre-programmed failure occurs...'. The study also manipulated 'Robot-verbal-communication-content' by having the robot provide verbal forecasts about its ability to remove screws before attempting the task. This is described in the 'Experiment task & design' section: 'In this phase, which we refer to as forecast phase hereinafter, if the robot faces a rusty screw, the robot reacts to the user's command with a forecast, stating whether it will or will not be able to remove the screw.' Both of these manipulations were found to impact trust. The paper states that 'Trust increased with successful robot performance and decreased with failures. Verbal forecasts had a positive effect on trust during faulty behavior, even when incorrect.' There were no factors that were manipulated that did not impact trust.",10.1080/0144929X.2024.2316284,https://www.tandfonline.com/doi/full/10.1080/0144929X.2024.2316284,"Trust is important for collaboration. In hybrid teams of humans and robots, trust enables smooth collaboration and reduces risks. Just as collaboration between humans and robots diﬀers from interpersonal collaboration, so does the nature of trust in human-robot interaction (HRI). Therefore, further investigations on trust formation and dissolution in HRI, factors aﬀecting it, and means for keeping trust on an appropriate level are needed. However, our knowledge of interpersonal trust and trust in autonomous agents cannot be transferred directly to HRI. In this paper, we present a study with 32 participants on trust formation and dissolution as well as forecasting to inﬂuence trust in an industry robot. Results show diﬀerences in dynamics and factors of trust formation and dissolution. Additionally, we ﬁnd that the eﬀect of forecasting on trust depends on task success. These ﬁndings support the design of trustful human-robot interaction and corresponding robotic team members."
"Aliasghari, Pourya; Ghafurian, Moojan; Nehaniv, Chrystopher L.; Dautenhahn, Kerstin",How Do We Perceive Our Trainee Robots? Exploring the Impact of Robot Errors and Appearance When Performing Domestic Physical Tasks on Teachers’ Trust and Evaluations,2023,2,217,173,44,"44 participants were excluded for failing at least 2 out of 13 attention checks, and for not providing meaningful answers to open-ended questions",Online Crowdsourcing,within-subjects,"Participants completed a demographics questionnaire, then rated the severity of six robot errors in a food preparation task, and finally indicated their personal food preparation preferences.",Participants rated the severity of different errors made by a robot while performing a food preparation task.,Unspecified,Humanoid Robots,Research,Evaluation,Text Evaluation,passive observation,Participants only read descriptions of robot errors.,media,The interaction was based on written descriptions of robot errors.,hypothetical,"The robot was only described in text, with no visual representation.",not autonomous,"The robot's actions were described in text, without any real autonomy.",Custom Scales,,,Trust was assessed using a custom scale to rate the severity of robot errors.,no modeling,No computational model of trust was used; descriptive statistics were used.,Observational & Survey Studies,Quantitative Surveys,Direct Manipulation,"The researchers manipulated the type of error the robot made (forgetting, replacing, adding cleaner) to see how it affected the perceived severity of the error.","The study did not directly measure trust, but rather the perceived severity of errors, which is related to trust. Adding cleaner was rated as a more severe error than forgetting or replacing an ingredient.","Adding cleaner was consistently rated as a more severe error than other types of errors, regardless of the food item.","Adding a cleaning product to food was rated as a significantly more severe error than other types of errors, such as forgetting or replacing an ingredient.","The robot was described as making errors while preparing food, and the human participant rated the severity of these errors.",ANOVA; t-test,One-way repeated-measures ANOVA with Greenhouse-Geisser correction was used to analyze the differences in severity ratings of different error types across food items. Two-sided independent-sample t-tests were used to compare the severity ratings of forgetting and replacing errors based on participants' food preferences.,TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the type of error the robot made (forgetting, replacing, adding cleaner) to see how it affected the perceived severity of the error. This directly relates to the robot's accuracy in performing the task, as different errors represent different levels of deviation from the correct action. The study found that the type of error significantly impacted the perceived severity, with adding cleaner being rated as the most severe. This indicates that the manipulation of robot accuracy directly influenced the participants' perception of the robot's reliability, which is closely related to trust. There were no other factors manipulated in this study.",10.1145/3582516,https://dl.acm.org/doi/10.1145/3582516,"To be successful, robots that can learn new tasks from humans should interact effectively with them while being trained, and humans should be able to trust the robots’ abilities after teaching. Typically, when human learners make mistakes, their teachers tolerate those errors, especially when students exhibit acceptable progress overall. But how do errors and appearance of a trainee robot affect human teachers’ trust while the robot is generally improving in performing a task? First, an online survey with 173 participants investigated perceived severity of robot errors in performing a cooking task. These findings were then used in an interactive online experiment with 138 participants, in which the participants were able to remotely teach their food preparation preferences to trainee robots with two different appearances. Compared with an untidy-looking robot, a tidy-looking robot was rated as more professional, without impacting participants’ trust. Furthermore, while larger errors at the end of iterative training had a greater impact, even a small error could significantly reduce trust in a trainee robot performing the domestic physical task of food preparation, regardless of the robot’s appearance. The present study extends human–robot interaction knowledge about teachers’ perception of trainee robots, particularly when teachers observe them accomplishing domestic physical tasks."
"Aliasghari, Pourya; Ghafurian, Moojan; Nehaniv, Chrystopher L.; Dautenhahn, Kerstin",How Do We Perceive Our Trainee Robots? Exploring the Impact of Robot Errors and Appearance When Performing Domestic Physical Tasks on Teachers’ Trust and Evaluations,2023,2,260,138,122,"122 participants were excluded for failing at least three attention and consistency checks, and for not rating small errors as less severe than big errors",Online Crowdsourcing,mixed design,"Participants completed a demographics questionnaire, personality and trust questionnaires, watched videos of robot learning, taught a robot a food preparation task over six rounds, and then answered questions about their trust in the robot.","Participants taught a virtual robot how to prepare food by selecting ingredients, and then observed the robot performing the task with varying levels of errors.",Pepper,Humanoid Robots; Expressive Robots,Research,Manipulation,Cooking/Food Preparation,minimal interaction,"Participants interacted with the robot through a virtual interface, selecting ingredients and observing the robot's actions.",media,The interaction was presented through videos of a robot performing a task.,physical,"The robot was a physical robot, but the interaction was through prerecorded videos.",pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user's input.,Questionnaires; Custom Scales,Ten Item Personality Inventory (TIPI); Disposition to Trust Questionnaire,,Trust was assessed using questionnaires and custom scales after each teaching round and at the end of the experiment.,"parametric models (e.g., regression)",Linear mixed-effects models and generalized linear models were used to analyze the data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers manipulated the robot's performance by introducing errors of varying severity and the robot's appearance by changing its clothing style to see how these factors affected trust.,"Trust decreased when the robot made errors, even small ones, after a period of correct performance. The robot's clothing style only affected perceived professionalism, not trust.","Participants rated the first robot they interacted with more positively, regardless of its appearance. Small errors at the end of the learning process had a significant negative impact on trust.",Even a small error made by a trainee robot after a period of correct performance can significantly reduce human teachers' trust in the robot.,"The human participant taught the robot how to prepare food by selecting ingredients, and the robot performed the task with pre-programmed actions, sometimes making errors.",Generalized linear models; linear mixed-effects models (lmms); Chi-squared; t-test,"Generalized Linear Models (GLMs) with a binomial family were used to analyze categorical data, such as trust indicators, considering confounding factors. Linear Mixed-effects Models (LMMs) were employed to analyze continuous data, such as behavior evaluation measures, accounting for random effects of participants. Chi-squared tests were used to compare categorical data, and t-tests were used to compare means between conditions. Pairwise comparisons were adjusted using the Holm-Bonferroni method for multiple hypothesis testing.",TRUE,Robot-accuracy; Robot-aesthetics,Robot-accuracy,Robot-aesthetics,"In this study, the researchers manipulated the robot's performance by introducing errors of varying severity (big and small errors) at different stages of the learning process, which directly impacts the robot's accuracy. The robot's appearance was also manipulated by changing its clothing style to be either tidy or untidy, which falls under the category of robot aesthetics. The study found that the robot's accuracy (specifically the presence and severity of errors) significantly impacted trust levels, with errors leading to a decrease in trust. However, the robot's clothing style (aesthetics) only affected perceived professionalism and did not impact trust. Therefore, robot accuracy is a factor that impacted trust, while robot aesthetics did not.",10.1145/3582516,https://dl.acm.org/doi/10.1145/3582516,"To be successful, robots that can learn new tasks from humans should interact effectively with them while being trained, and humans should be able to trust the robots’ abilities after teaching. Typically, when human learners make mistakes, their teachers tolerate those errors, especially when students exhibit acceptable progress overall. But how do errors and appearance of a trainee robot affect human teachers’ trust while the robot is generally improving in performing a task? First, an online survey with 173 participants investigated perceived severity of robot errors in performing a cooking task. These findings were then used in an interactive online experiment with 138 participants, in which the participants were able to remotely teach their food preparation preferences to trainee robots with two different appearances. Compared with an untidy-looking robot, a tidy-looking robot was rated as more professional, without impacting participants’ trust. Furthermore, while larger errors at the end of iterative training had a greater impact, even a small error could significantly reduce trust in a trainee robot performing the domestic physical task of food preparation, regardless of the robot’s appearance. The present study extends human–robot interaction knowledge about teachers’ perception of trainee robots, particularly when teachers observe them accomplishing domestic physical tasks."
"Alimardani, Maryam; Harinandansingh, Jishnu; Ravin, Lindsey; De Haas, Mirjam",Motivational Gestures in Robot-Assisted Language Learning: A Study of Cognitive Engagement using EEG Brain Activity,2022,1,25,22,3,3 participants were removed from analysis due to technical issues during the experiment or noisy EEG data,Controlled Lab Environment,within-subjects,"Participants learned Vimmi words with a NAO robot in two conditions: Gesture and No-Gesture. In the Gesture condition, the robot provided verbal feedback and motivational gestures, while in the No-Gesture condition, it only provided verbal feedback. EEG data was collected during the interaction, and participants completed word recognition tests immediately after each condition and one week later.","Participants learned and practiced Vimmi words with a NAO robot, receiving feedback on their performance.",Nao,Humanoid Robots; Expressive Robots,Educational; Research; Social,Social,Tutoring,minimal interaction,Participants interacted with the robot through verbal instructions and feedback during a learning task.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical NAO robot.,wizard of oz (directly controlled),The robot's behavior was controlled using a Wizard of Oz technique.,Questionnaires,,Physiological Signals,Trust was assessed using self-reported engagement questionnaires and EEG signals.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's behavior was manipulated by adding motivational gestures to the verbal feedback in one condition, while the other condition only had verbal feedback. This was intended to influence engagement and learning.","Self-reported engagement was higher in the gesture condition, but there was no significant impact on learning gain or cognitive engagement.","The study found a significant difference in self-reported engagement but not in cognitive engagement as measured by EEG, suggesting a potential disconnect between subjective and objective measures of engagement. The ceiling effect in word knowledge scores also suggests that the task may not have been challenging enough.","The addition of motivational gestures to the robot's verbal feedback improved self-reported engagement, but did not impact learning gain or cognitive engagement.",The robot provided verbal and gestural feedback to the participant during a language learning task. The participant learned new words and answered questions about them.,Shapiro-Wilk; Wilcoxon signed-rank test; t-test,"The Shapiro-Wilk test was used to check for normality of the data. The Wilcoxon signed-rank test was used to compare word recognition test scores between the Gesture and No-Gesture conditions, as the data violated normality assumptions. A paired t-test was used to compare cognitive engagement (EEG Engagement Indices) and self-reported engagement scores between the two conditions, as these data were normally distributed.",TRUE,Robot-nonverbal-communication,,,"The study manipulated the robot's nonverbal communication by having it perform motivational gestures (positive and negative) in one condition (Gesture) and not in the other (No-Gesture). This is explicitly stated in the paper: 'In one condition, the robot provided verbal feedback and motivational gestures during the practice (Gesture condition) and in the other condition, the robot did not perform any gestures although the participants received verbal feedback from the robot whenever the answer was correct or incorrect (No-Gesture condition)'. The paper also mentions that 'The motivational gestures included both positive and negative gestures (Fig. 1c). For each category five gestures were prepared and randomly displayed by the robot depending on the participant's answer.' This clearly indicates a manipulation of the robot's physical movements and gestures, which falls under the 'Robot-nonverbal-communication' category. The study found that self-reported engagement was higher in the gesture condition, but there was no significant impact on learning gain or cognitive engagement. Since the study did not find a significant impact on trust, neither 'Robot-nonverbal-communication' nor any other factor is listed under 'factors_that_impacted_trust' or 'factors_that_did_not_impact_trust'.",10.1109/RO-MAN53752.2022.9900508,https://ieeexplore.ieee.org/document/9900508/,"Social robots have been shown effective in pedagogical settings due to their embodiment and social behavior that can improve a learner’s motivation and engagement. In this study, the impact of a social robot’s motivational gestures in robot-assisted language learning (RALL) was investigated. Twenty-five university students participated in a language learning task tutored by a NAO robot under two conditions (within-subjects design); in one condition the robot provided positive and negative feedback on participant’s performance using both verbal and non-verbal behavior (Gesture condition), in another condition the robot only employed verbal feedback (No-Gesture condition). To assess cognitive engagement and learning in each condition, we collected EEG brain activity from the participants during the interaction and evaluated their word knowledge during an immediate and delayed post-test. No significant difference was found with respect to cognitive engagement as quantified by the EEG Engagement Index during the practice phase. Similarly, the word test results indicated an overall high performance in both conditions, suggesting similar learning gain regardless of the robot’s gestures. These findings do not provide evidence in favor of robot’s motivational gestures during language learning tasks but at the same time indicate challenges with respect to the design of effective social behavior for pedagogical robots."
"Alzahrani, Abdullah; Robinson, Simon; Ahmad, Muneeb",Exploring Factors Affecting User Trust Across Different Human-Robot Interaction Settings and Cultures,2022,1,36,36,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants watched three videos of human-robot interaction in three different scenarios, completed questionnaires to rate the robot using TPS and TRS, and wrote the factors affecting their trust in robots. Participants then engaged in a focus group discussion.",Participants watched videos of three different HRI scenarios and rated their trust in the robot in each scenario.,Unspecified,Legged Robots; Telepresence Robots; Industrial Robot Arms,Care; Care; Industrial,Evaluation,Survey/Questionnaire Completion,passive observation,Participants observed videos of robot interactions.,media,Participants watched videos of the robot interactions.,physical,The robots were shown in videos.,shared control (fixed rules),"The robots operated with some level of autonomy, but with fixed rules.",Questionnaires; Custom Scales,Schaefer's Trust Questionnaire/Scale,Speech Data,"Trust was measured using a questionnaire and a custom scale, and speech data was collected during focus groups.",no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the HRI scenario by using different robots and tasks, which influenced the perceived risk and relevance of trust.","Trust perception varied across scenarios in the Saudi Arabia study, but not significantly in the UK study. Trust relevance varied across scenarios in both studies. Trust perception and relevance also varied across cultures.","The study found that trust perception varied across scenarios in the Saudi Arabia study, but not significantly in the UK study. The study also found that trust perception and relevance varied across cultures. The number of robot-related and environment-related factors was significantly higher in the UK study compared to the Saudi Arabia study.","Factors affecting trust vary across different HRI scenarios and cultures, and the relevance of trust for a given task is an indicator of a participant's trust.","Participants watched videos of a dog robot guiding a blind person, a teleoperated robot in healthcare, and a manufacturing robot. Participants then rated their trust in the robot in each scenario and discussed factors affecting their trust.",Chi-squared; ANOVA; t-test,"The study used a Chi-Square Goodness of Fit Test to determine if the frequency of human, robot, and environment-related factors was equal across the three scenarios. One-way ANOVA was used to compare the Trust Perception Score (TPS) and Trust Relevance Score (TRS) across the three scenarios. Independent samples t-tests were used to compare the TPS and TRS across the two studies (Saudi Arabia and the UK).",TRUE,Task-complexity; Task-environment; Robot-autonomy,Task-complexity; Task-environment,Robot-autonomy,"The study manipulated the HRI scenario by using different robots and tasks, which influenced the perceived risk and relevance of trust. The three scenarios presented different levels of task complexity and different task environments. The first scenario involved a dog robot guiding a blind person, which is a complex task in a dynamic environment. The second scenario involved a teleoperated robot in healthcare, which is a less complex task in a controlled environment. The third scenario involved a manufacturing robot, which is a relatively simple task in a structured environment. These differences in task complexity and environment were intentionally introduced by the researchers. The robot autonomy was also different across the scenarios, with the dog robot having a higher level of autonomy than the teleoperated robot and the manufacturing robot. However, the study did not find a significant impact of the robot autonomy on trust. The study found that trust perception varied across scenarios in the Saudi Arabia study, but not significantly in the UK study. Trust relevance varied across scenarios in both studies. This indicates that the task complexity and environment had an impact on trust, while the robot autonomy did not have a significant impact on trust.",10.1145/3527188.3561920,https://dl.acm.org/doi/10.1145/3527188.3561920,"Trust is one of the necessary factors for building a successful humanrobot interaction (HRI). This paper investigated how human trust in robots differs across HRI scenarios in two cultures. We conducted two studies in two countries: Saudi Arabia (study 1) and the United Kingdom (study 2). Each study presented three HRI scenarios: a dog robot guiding people with sight impairments, a teleoperated robot in healthcare, and a manufacturing robot. Study 1 shows that participants’ trust perception score (TPS) was significantly different across the three scenarios. However, Study 2 results show a slightly significant variation in TPS across the scenarios. We also found that the relevance of trust for a given task is an indicator of a participant’s trust. Furthermore, the findings showed that trust scores or factors affecting users’ trust vary across cultures. The findings identified novel factors that might affect human trust, such as controllability, usability and risk. The findings direct the HRI community to consider a dynamic and evolving design for modelling human-robot trust because factors affecting humans’ trust are evolving and will vary across different settings and cultures."
"Alzahrani, Abdullah; Ahmad, Muneeb",Real-Time Trust Measurement in Human-Robot Interaction: Insights from Physiological Behaviours,2024,1,43,43,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants played a card game against a Nao robot in four sessions, with physiological data recorded during decision periods.","Participants played the Bluff Game against a Nao robot, making trust/distrust decisions based on the robot's claims.",Nao,Humanoid Robots; Expressive Robots,Research; Social,Game,Competitive Game,minimal interaction,Participants interacted with the robot in a game setting with verbal instructions.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical Nao robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed script during the game.,Physiological Measures,,Physiological Signals,Trust was assessed using physiological measures such as heart rate and skin temperature.,"parametric models (e.g., regression)",Machine learning classifiers were used to predict trust levels based on physiological data.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The game was designed to elicit trust and distrust by having the robot make claims that participants could either accept or reject.,Heart rate and skin temperature showed significant differences between trust and distrust states.,"HR and SKT were significant indicators of trust, while other PBs did not show significant differences.","Heart rate and skin temperature significantly differed between trust and distrust states, and machine learning classifiers could predict trust levels with reasonable accuracy.","The robot verbally interacted with the participant during a card game, making claims that the participant could either accept or reject. The participant made decisions based on the robot's claims.",ANOVA; Bonferroni correction,"A repeated-measures ANOVA was used to examine the effects of decision (trust vs. distrust) on physiological behaviors (PBs) such as EDA, BVP, HR, SKT, BR, and BD. Post-hoc Bonferroni tests were then conducted to compare HR and SKT between trust and distrust groups across different sessions. The ANOVA aimed to determine if there were significant differences in PBs between trust and distrust states, while the post-hoc tests identified where these differences occurred across the different sessions.",TRUE,Teaming,Teaming,,"The study manipulated the context of the interaction by having participants play a competitive game against the robot. This is classified as 'Teaming' because the core manipulation is whether the human is competing against the robot. The paper states, 'Participants engaged in four game sessions with a Nao robot, playing an interactive card game designed to elicit trust and distrust.' The results showed that HR and SKT significantly differed between trust and distrust states, indicating that the competitive context influenced trust levels. The paper also states, 'In competitive settings, the desire to outperform the robot leads to increased physiological arousal, indicated by higher EDA, BVP, HR, BR, and BD.' This further supports the choice of 'Teaming' as the manipulated factor that impacted trust.",10.1145/3678957.3688620,https://dl.acm.org/doi/10.1145/3678957.3688620,"Existing work has shown that physiological behaviours (PBs) can effectively measure trust. However, there is a limited exploration of using multiple PBs concurrently to calibrate human trust in robots during real-time HRI. Additionally, most datasets are based on one-off interactions or a single context. This project addresses this gap by examining differences in PBs between trust and distrust states and investigating how these PBs change over repeated interactions in different contexts. We conducted two experiments to collect data on electrodermal activity (EDA), blood volume pulse (BVP), heart rate (HR), skin temperature (SKT), blinking rate (BR), and blinking duration (BD) from participants across multiple HRI sessions. The results showed significant differences in HR and SKT between trust and distrust states in Study 1, and significant differences in HR in Study 2. Furthermore, the Decision Tree classifier achieved the highest accuracy of 79% in classifying trust when using the incremental transfer learning algorithm for collective datasets. These results highlight the potential of using PBs for realtime trust measurement in HRI and suggest further exploration of incremental transfer learning methods to enhance trust prediction across different interaction contexts."
"Amine, Ahmad; Aldilati, Mostafa; Hasan, Hadi; Maalouf, Noel; Elhajj, Imad H.","Human-Robot Interaction using VAHR: Virtual Assistant, Human, and Robots in the Loop",2023,1,10,10,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed tasks using both a traditional keyboard/mouse interface and the VAHR system, with a warm-up run before each test run. The tasks included robot navigation and a jigsaw puzzle. Subjective and objective metrics were collected.","Participants controlled two mobile robots to deliver packages to specified zones, and completed a jigsaw puzzle.",Unspecified,Mobile Robots; Mobile Manipulators,Research,Navigation,Path Following,minimal interaction,Participants interacted with the robots through a virtual interface and voice commands.,simulation,The interaction took place in a simulated environment using ROS and Gazebo.,simulated,The robots were simulated in a virtual environment.,shared control (fixed rules),The robots followed pre-programmed navigation paths and responded to voice commands.,Questionnaires,NASA Task Load Index (NASA-TLX),Performance Metrics,Trust was assessed using the NASA TLX questionnaire and performance metrics.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the control method (VAHR vs. keyboard/mouse) and the task difficulty by including a jigsaw puzzle, which influenced the interaction medium and the mental load.","The study found that while objective performance was better with VAHR, subjective trust was lower, suggesting that familiarity with the control method influences trust.","The subjective assessment of performance contradicted the objective evaluation scores, with subjects showing superior performance while completing the tasks using VAHR but reporting lower trust. The NASA TLX scores showed improvement in mental and temporal demand in the second trial, but physical demand increased.","The VAHR system required 41% less Robot Attention Demand and ensured 91% more Fan-out time compared to the standard method, highlighting its potential for efficient human-robot interaction.","The robot navigated to different zones to pick up and deliver packages, while the human controlled the robots using either voice commands through Alexa or a keyboard and mouse, and also completed a jigsaw puzzle.",ANOVA,"An ANOVA test was conducted to assess the statistical significance of the objective results (RAD, FO, time taken, and number of solved puzzle pieces) obtained under both control methods (VAHR and keyboard/mouse). Individual ANOVA tests were also conducted on each subjective metric of the NASA TLX to check for statistical significance.",TRUE,Robot-interface-design; Task-complexity,Robot-interface-design,Task-complexity,"The study manipulated the control method (VAHR vs. keyboard/mouse), which is categorized as 'Robot-interface-design' because it changes the way the user interacts with the robot system. The study also included a jigsaw puzzle as a distracting task, which increased the 'Task-complexity' by adding a secondary cognitive load. The results showed that the interface design (VAHR) impacted trust, as subjective trust was lower with VAHR despite better objective performance. The task complexity did not impact trust, as the subjective trust was only impacted by the interface design and not the presence of the puzzle. The paper states, 'However, subjective metrics revealed a need for human operators to build confidence and trust with this new method of operation.' This indicates that the interface design was the primary factor influencing trust, not the task complexity.",,http://arxiv.org/abs/2303.17582,"Robots have become ubiquitous tools in various industries and households, highlighting the importance of human-robot interaction (HRI). This has increased the need for easy and accessible communication between humans and robots. Recent research has focused on the intersection of virtual assistant technology, such as Amazon’s Alexa, with robots and its effect on HRI. This paper presents the Virtual Assistant, Human, and Robots in the loop (VAHR) system, which utilizes bidirectional communication to control multiple robots through Alexa. VAHR’s performance was evaluated through a human-subjects experiment, comparing objective and subjective metrics of traditional keyboard and mouse interfaces to VAHR. The results showed that VAHR required 41% less Robot Attention Demand and ensured 91% more Fan-out time compared to the standard method. Additionally, VAHR led to a 62.5% improvement in multi-tasking, highlighting the potential for efﬁcient human-robot interaction in physicallyand mentally-demanding scenarios. However, subjective metrics revealed a need for human operators to build conﬁdence and trust with this new method of operation."
"Andersen, Kamilla Egedal; Köslich, Simon; Pedersen, Bjarke Kristian Maigaard Kjær; Weigelin, Bente Charlotte; Jensen, Lars Christian",Do We Blindly Trust Self-Driving Cars,2017,1,73,73,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants interacted with a car simulator in one of four conditions. The first condition provided no audible information. The other three conditions provided continuous computer-synthesized speech about the SDC's actions, with variations in the information provided at the second intersection.",Participants were asked to monitor a self-driving car in a simulator and assume control by braking if they perceived a dangerous situation.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a car simulator, with limited verbal interaction.",simulation,The study used a car simulator to create an interactive virtual environment.,simulated,The robot was a simulated self-driving car within the simulator.,fully autonomous (limited adaptation),The self-driving car operated autonomously but with limited adaptation to unexpected situations.,Behavioral Measures,,,Trust was assessed by measuring whether participants assumed control of the vehicle.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the accuracy of the information provided by the SDC, and the presence or absence of information, to see how it affected the user's trust and willingness to take control.","Participants trusted the autonomous system even when it provided incorrect information, and were less likely to take control when the system provided no information.","Participants were less likely to assume control when the SDC provided no information, and also when the SDC provided incorrect information, suggesting an over-reliance on the system even when it was wrong.","Participants tend to over-trust autonomous systems, even when they should not, leading to potentially dangerous situations.","The robot (simulated SDC) drove through a virtual environment, and the human participant monitored the SDC and could take control by braking.",Chi-squared,"The study used chi-square tests to analyze the differences in the percentage of participants who assumed control (braked) at different points in the simulation across different experimental conditions. Specifically, chi-square tests were used to compare the braking behavior at the first intersection, the crossing pedestrian, and the second intersection. Further chi-square tests were used to compare the detection of incorrect information and the decision to assume control after detecting the error.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the content of the verbal communication provided by the self-driving car (SDC). Specifically, the SDC provided either no information, correct information, or incorrect information about the surrounding traffic at the second intersection. This manipulation of the information content directly influenced participants' trust and their willingness to take control of the vehicle. The paper states, 'In the other scenarios the interfaces continuously commented on what the SDC did and why, through computer synthesis speech. These conditions (2-4) differed only at the second intersection, where they announced a different number of registered SDCs -five being the correct number'. The results show that participants were less likely to assume control when the SDC provided no information, and also when the SDC provided incorrect information, indicating that the content of the verbal communication had a direct impact on trust. The study did not manipulate any other factors from the provided list.",10.1145/3029798.3038428,https://dl.acm.org/doi/10.1145/3029798.3038428,"Trust is an essential factor in ensuring robust human-robot interaction. However, recent work suggests that people can be too trusting of the technology with which they interact during emergencies, causing potential harm to themselves. To test whether this “over-trust” also extends to normal dayto-day activities, such as driving a car, we carried out a series of experiments with an autonomous car simulator. Participants (N=73) engaged in a scenario with no, correct or false audible information regarding the state of traﬃc around the self-driving vehicle, and were told they could assume control at any point in the interaction. Results show that participants trust the autonomous system, even when they should not, leading to potential dangerous situations."
"Angelopoulos, Georgios; Lacroix, Dimitri; Wullenkord, Ricarda; Rossi, Alessandra; Rossi, Silvia; Eyssel, Friederike",Measuring Transparency in Intelligent Robots,2024,2,371,322,49,"13 failed to complete the study, 36 were excluded for not passing both attention checks",Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of eight conditions manipulating explainability, legibility, and predictability of a robot's behavior using image vignettes. They then completed questionnaires on perceived transparency, trust, and acceptance.",Participants viewed image vignettes of a robot moving towards a charging station and then completed questionnaires.,Unspecified,Mobile Robots,Other,Evaluation,Survey/Questionnaire Completion,passive observation,Participants observed the robot through image vignettes.,media,Participants viewed static images of the robot.,simulated,The robot was represented through image vignettes.,pre-programmed (non-adaptive),The robot's behavior was pre-programmed and did not adapt to the user.,Questionnaires; Multidimensional Measures,Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using the Multi-Dimensional Measure of Trust (MDMT) questionnaire.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The explainability, legibility, and predictability of the robot's behavior were manipulated using image vignettes to influence perceived transparency and trust.",Manipulated explainability and legibility had a significant main effect on performance and moral trust. Predictability had a significant effect on performance trust but not moral trust.,"No interaction effect between manipulated explainability, legibility, and predictability was found on the factors of the scale, but all had main effects on the different subscales of the TOROS.","The perceived transparency of a robot consists of three factors: Illegibility, Explainability, and Predictability, and these factors are sensitive to the experimental manipulation of the robot's behavior.","Participants viewed image vignettes of a robot moving towards a charging station. Participants then completed questionnaires about the robot's transparency, their trust in the robot, and their acceptance of the robot.",Factor analysis; principal axis factoring; kaiser-meyer olkin; bartlett's test of sphericity; cronbach's alpha; mcdonald's omega; composite reliability; average variance extracted; Item Response Theory; ANCOVA,"The study used Exploratory Factor Analysis (EFA) with Principal Axis Factoring (PAF) to assess the factorial validity of the 64-item scale, including the Kaiser-Meyer Olkin (KMO) measure and Bartlett's test of sphericity. Item reduction was performed based on factor loadings. Inter-item correlation analysis was used to minimize redundancy. The reliability of the scale was assessed using Cronbach's Alpha and McDonald's Omega. Convergent validity was evaluated using Composite Reliability (CR) and Average Variance Extracted (AVE). Item Response Theory (IRT) was used to analyze item difficulty and discrimination. Finally, ANCOVAs were conducted to examine the effects of manipulated explainability, legibility, and predictability on the scale factors, trust, and acceptance, with prior experience with robots as a covariate.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the explainability, legibility, and predictability of the robot's behavior using image vignettes. These manipulations directly relate to the content of the robot's communication, specifically how well it explains its actions and intentions. Therefore, 'Robot-verbal-communication-content' is the most appropriate category. The paper states, 'We found a significant main effect of manipulated explainability, legibility, and predictability on transparency...Similarly, we obtained significant main effects of manipulated explainability, legibility, and predictability on Factor 1, Factor 2, and Factor 3.' and 'Regarding trust towards the robot, we found significant main effects of manipulated explainability and legibility on performance...and moral trust towards the robot. However, although the main effect of manipulated predictability on performance trust was significant, we did not find a significant effect of manipulated predictability on moral trust towards the robot.' This indicates that the manipulated factors, categorized as 'Robot-verbal-communication-content', impacted trust. There were no factors that were manipulated that did not impact trust.",,http://arxiv.org/abs/2408.16865,"As robots become increasingly integrated into our daily lives, the need to make them transparent has never been more critical. Yet, despite its importance in human-robot interaction, a standardized measure of robot transparency has been missing until now. This paper addresses this gap by presenting the first comprehensive scale to measure perceived transparency in robotic systems, available in English, German, and Italian languages. Our approach conceptualizes transparency as a multidimensional construct, encompassing explainability, legibility, predictability, and meta-understanding. The proposed scale was a product of a rigorous three-stage process involving 1,223 participants. Firstly, we generated the items of our scale, secondly, we conducted an exploratory factor analysis, and thirdly, a confirmatory factor analysis served to validate the factor structure of the newly developed TOROS scale. The final scale encompasses 26 items and comprises three factors: Illegibility, Explainability, and Predictability. TOROS demonstrates high cross-linguistic reliability, inter-factor correlation, model fit, internal consistency, and convergent validity across the three cross-national samples. This empirically validated tool enables the assessment of robot transparency and contributes to the theoretical understanding of this complex construct. By offering a standardized measure, we facilitate consistent and comparable research in human-robot interaction in which TOROS can serve as a benchmark."
"Angelopoulos, Georgios; Lacroix, Dimitri; Wullenkord, Ricarda; Rossi, Alessandra; Rossi, Silvia; Eyssel, Friederike",Measuring Transparency in Intelligent Robots,2024,2,927,901,26,"17 failed to complete the study, 1 was excluded for not passing the video attention check, 8 were removed for insufficient language proficiency",Online Crowdsourcing,between-subjects,"Participants were randomly assigned to either a low or high transparency condition and watched a video of a robot. They then completed questionnaires on perceived transparency, trust, and acceptance.",Participants watched a video of a robot and then completed questionnaires.,Pepper,Humanoid Robots; Expressive Robots,Other,Evaluation,Survey/Questionnaire Completion,passive observation,Participants observed the robot through video vignettes.,media,Participants watched videos of the robot in action.,physical,The robot was a physical robot shown in the video.,pre-programmed (non-adaptive),The robot's behavior was pre-programmed and did not adapt to the user.,Questionnaires; Multidimensional Measures,Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using the Multi-Dimensional Measure of Trust (MDMT) questionnaire.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The overall transparency of the robot's behavior was manipulated using video vignettes to influence perceived transparency and trust.,Performance trust was significantly higher in the high transparency condition. Moral trust and acceptance were also higher in the high transparency condition. Anxiety was lower in the high transparency condition.,German participants perceived higher transparency than English participants. There was a significant main effect of language on perceived transparency and on Factor 1 of the scale. There was a marginally significant interaction between the transparency manipulation and participant language on Factor 2.,"The TOROS scale was validated across three languages (English, German, and Italian) and demonstrated good convergent validity with factors related to transparency, namely trust towards robots and acceptance of robots.","Participants watched a video of a Pepper robot moving towards a charging station. Participants then completed questionnaires about the robot's transparency, their trust in the robot, and their acceptance of the robot.",Multilevel Model; comparative fit index; tucker-lewis index; root mean square error of approximation; standardized root mean square residual; measurement invariance testing; cronbach's alpha; mcdonald's omega; composite reliability; average variance extracted; t-test; ANOVA; ANCOVA,"The study used Confirmatory Factor Analysis (CFA) to verify the factor structure of the 26-item scale across English, German, and Italian versions, using Comparative Fit Index (CFI), Tucker-Lewis index (TLI), Root Mean Square Error of Approximation (RMSEA), and Standardized Root Mean Square Residual (SRMR). Measurement invariance testing was conducted to assess the comparability of the scale across languages. Internal consistency was assessed using Cronbach's Alpha and McDonald's Omega. Convergent validity was evaluated using Composite Reliability (CR) and Average Variance Extracted (AVE). Two-tailed t-tests and two-way ANOVAs were used to analyze the effects of the experimental manipulation of transparency and participant language on perceived transparency, the scale factors, trust, and acceptance. ANCOVA was used to analyze the effect of manipulated transparency on Factor 2.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the overall transparency of the robot's behavior using video vignettes. This manipulation directly relates to the content of the robot's communication, specifically how well it explains its actions and intentions. Therefore, 'Robot-verbal-communication-content' is the most appropriate category. The paper states, 'participants perceived the robot's behavior as significantly more transparent in the high transparency condition compared to the low transparency condition...For Factor 1, the high transparency condition showed significantly higher scores than the low transparency condition...For Factor 2, the high transparency condition again scored significantly higher than the low transparency condition...Lastly, Factor 3 also showed significantly higher scores in the high transparency condition compared to the low transparency condition.' and 'Performance trust was significantly higher in the high transparency condition...Moral trust was significantly higher in the high transparency condition...Acceptance was significantly higher in the high transparency condition...Anxiety was lower in the high transparency condition.' This indicates that the manipulated factor, categorized as 'Robot-verbal-communication-content', impacted trust. There were no factors that were manipulated that did not impact trust.",,http://arxiv.org/abs/2408.16865,"As robots become increasingly integrated into our daily lives, the need to make them transparent has never been more critical. Yet, despite its importance in human-robot interaction, a standardized measure of robot transparency has been missing until now. This paper addresses this gap by presenting the first comprehensive scale to measure perceived transparency in robotic systems, available in English, German, and Italian languages. Our approach conceptualizes transparency as a multidimensional construct, encompassing explainability, legibility, predictability, and meta-understanding. The proposed scale was a product of a rigorous three-stage process involving 1,223 participants. Firstly, we generated the items of our scale, secondly, we conducted an exploratory factor analysis, and thirdly, a confirmatory factor analysis served to validate the factor structure of the newly developed TOROS scale. The final scale encompasses 26 items and comprises three factors: Illegibility, Explainability, and Predictability. TOROS demonstrates high cross-linguistic reliability, inter-factor correlation, model fit, internal consistency, and convergent validity across the three cross-national samples. This empirically validated tool enables the assessment of robot transparency and contributes to the theoretical understanding of this complex construct. By offering a standardized measure, we facilitate consistent and comparable research in human-robot interaction in which TOROS can serve as a benchmark."
"Aroyo, Alexander Mois; Rea, Francesco; Sciutti, Alessandra",Will You Rely on a Robot to Find a Treasure?,2017,1,5,5,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were introduced to the robot, filled a questionnaire, then played a treasure hunt game with the robot offering hints. After the game, they filled another questionnaire and answered questions about risk and trust.","Participants played a treasure hunt game, finding hidden eggs in a room, with the option to ask a robot for hints.",iCub,Humanoid Robots,Research; Social,Game,Cooperative Game,direct-contact interaction,Participants physically interacted with the robot by touching its torso to receive hints.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The robot was a physical humanoid robot.,shared control (fixed rules),"The robot provided pre-programmed hints and responded to touch, but did not adapt to the user's behavior.",Behavioral Measures; Questionnaires,Godspeed Questionnaire,Video Data,Trust was assessed using questionnaires and by observing if participants followed the robot's pointing gestures.,no modeling,No computational model of trust was used in this study.,Empirical HRI Studies,Physical Robot Studies,Indirect Manipulation,"Time pressure was introduced by limiting the game time, and the robot offered help spontaneously, influencing participants' expectations about its role.","Participants initially avoided robot help, but relied on it more as time pressure increased. They followed the robot's advice when they did ask for help. The robot's perceived anthropomorphism and animacy increased after the interaction.","Participants initially preferred to rely on their own skills, but resorted to robot help under time pressure. The perceived safety of the robot decreased after the experiment, possibly due to the stress of the game.","Participants tended to rely on the robot's help only when under pressure, but then followed its advice without hesitation. The robot's perceived anthropomorphism and animacy increased after the interaction.",The robot offered hints by pointing and giving verbal clues. The human participant searched for hidden eggs and could ask the robot for help by touching its torso.,t-test,"Paired sample t-tests were used to compare the participants' responses to the Godspeed questionnaire before and after the game. Specifically, the tests examined changes in the perception of the robot's anthropomorphism, animacy, and perceived safety.",TRUE,Task-constraints; Robot-autonomy,Task-constraints,Robot-autonomy,"The study manipulated 'Task-constraints' by introducing time pressure, limiting the game time to 20 minutes. This is evident in the paper: 'The experimenter provided a sheet with the instructions of the game, which explained that they had to play a Treasure Hunt game in 20 minutes...'. The paper also states that 'as the time was passing they began requesting more hints to the robot... This implies that participants did not want to ask for robot's suggestion but rather preferred to rely on their skills until the time pressure became stronger'. This shows that time pressure impacted trust. The robot's autonomy was also a factor, as the robot offered help spontaneously, but the level of autonomy was fixed and did not change during the experiment. The robot provided pre-programmed hints and responded to touch, but did not adapt to the user's behavior. This is described in the paper as 'After 30 seconds, the robot provided a free hint and invited the subject to touch its torso for more hints.' and 'There were four hints per egg: one based on pointing at the location of the hidden egg and three verbal hints, from more broad to very precise.' The robot's autonomy did not change during the experiment and did not impact trust directly, as the participants' trust was influenced by the time pressure, not the robot's fixed autonomy.",10.1145/3029798.3038394,https://dl.acm.org/doi/10.1145/3029798.3038394,"Collaborative interaction between human partners is based on trust. Similarly in HRI, trust is crucial aspect that regulates the confidence in the robot partner. Through the challenge of a Treasure Hunt, we intend to explore whether people tend to trust and rely on robot help, even when they are not instructed to do so. Results suggest that participants resort to robot support only when under pressure, but then follow with no hesitation its advice. In addition, we show that robot’s perceived anthropomorphism and animacy increase, also when the robot help did not lead to subjects’ victory at the game."
"Avetisyan, Lilit; Ayoub, Jackie; Yang, X. Jessie; Zhou, Feng",Building Trust Profiles in Conditionally Automated Driving,2023,1,74,70,4,4 participants were excluded due to missing data,Controlled Lab Environment,between-subjects,"Participants completed a consent form, demographic survey, personality and trust questionnaires, a training session, and then completed a driving simulation with takeover requests. They also rated their trust levels every 25 seconds and after the simulation, and rated their anticipated emotional responses to AVs.",Participants drove a simulated vehicle in automated mode and were required to take over control when prompted by takeover requests (TORs) in different scenarios.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a driving simulator and responded to takeover requests.,simulation,Participants used a desktop-based driving simulator with three LCD monitors.,simulated,The autonomous vehicle was simulated in a driving simulator.,shared control (fixed rules),The automated vehicle operated autonomously but required human takeover in specific scenarios.,Questionnaires; Real-time Trust Measures,Big Five Inventory Scale; Disposition to Trust Questionnaire,Performance Metrics,"Trust was measured using questionnaires, real-time trust ratings, and performance metrics.","parametric models (e.g., regression)","A K-means clustering model was used to identify trust profiles, and a multinomial logistic regression model was used to validate the profiles.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the reliability of the automated driving system by introducing false alarms and misses in takeover requests, which influenced the perceived performance of the system and the difficulty of the task.","The study found that different levels of automation performance (control, false alarm, and miss conditions) influenced trust, with false alarms and misses generally reducing trust. The study also identified three trust profiles (believers, disbelievers, and oscillators) based on dynamic trust, dispositional trust, initial learned trust, personality, and emotions.","The study identified three distinct trust profiles (believers, disbelievers, and oscillators) based on various factors, including dynamic trust, dispositional trust, initial learned trust, personality, and emotions. The condition alone was not sufficient to determine the identified trust profiles, indicating that other factors also played important roles.","The study identified three distinct trust profiles (believers, disbelievers, and oscillators) in conditionally automated driving, which were influenced by system performance, emotions, personality, and dispositional and initial learned trust.","The robot (simulated autonomous vehicle) drove in automated mode, and the human participant monitored the driving and took over control when prompted by takeover requests. The human also completed a non-driving related task (Tetris) during the automated driving.",ANOVA; post-hoc analysis with tukey-kramer test; Logistic regression,"The study used one-way ANOVA tests to compare personality traits and emotions across the identified trust profiles. Post-hoc analysis with Tukey-Kramer test was used to identify specific differences between groups when ANOVA showed significant results. A multinomial logistic regression model was used to validate the identified trust profiles, and SHAP explainer was used to interpret the model and understand the importance of individual features in the model's decision-making process.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the reliability of the automated driving system by introducing false alarms and misses in takeover requests. This directly impacts the accuracy of the robot's (simulated AV) performance, as it sometimes fails to correctly identify situations requiring human intervention (misses) or incorrectly requests intervention when not needed (false alarms). This manipulation of the system's performance directly influenced the participants' trust levels, as described in the paper: 'This approach was adopted to evaluate the impact of different levels of automation performance on trust, as previous studies indicated that both false alarms and misses reduced trust in automated systems [27].' The paper also states that 'The features of condition (miss, false alarm, or control), emotions of freaked out and confused were found to be the most significant ones in predicting the trust profiles'. The condition refers to the manipulation of the robot's accuracy. The study did not manipulate any other factors from the provided list.",,http://arxiv.org/abs/2306.16567,"Trust is crucial for ensuring the safety, security, and widespread adoption of automated vehicles (AVs), and if trust is lacking, drivers and the public may not be willing to use them. This research seeks to investigate trust profiles in order to create personalized experiences for drivers in AVs. This technique helps in better understanding drivers’ dynamic trust from a persona’s perspective. The study was conducted in a driving simulator where participants were requested to take over control from automated driving in three conditions that included a control condition, a false alarm condition, and a miss condition with eight takeover requests (TORs) in different scenarios. Drivers’ dispositional trust, initial learned trust, dynamic trust, personality, and emotions were measured. We identified three trust profiles (i.e., believers, oscillators, and disbelievers) using a K-means clustering model. In order to validate this model, we built a multinomial logistic regression model based on SHAP explainer that selected the most important features to predict the trust profiles with an F1-score of 0.90 and accuracy of 0.89. We also discussed how different individual factors influenced trust profiles which helped us understand trust dynamics better from a persona’s perspective. Our findings have important implications for designing a personalized in-vehicle trust monitoring and calibrating system to adjust drivers’ trust levels in order to improve safety and experience in automated driving."
"Avetisyan, Lilit; Ayoub, Jackie; Yang, X. Jessie; Zhou, Feng",Building Contextualized Trust Profiles in Conditionally Automated Driving,2024,1,74,70,4,4 participants were excluded due to missing data,Controlled Lab Environment,between-subjects,"Participants completed a consent form, demographic survey, personality and trust questionnaires, a training session, and two driving simulations with takeover requests. They also rated their trust levels dynamically and after the simulation, and rated their anticipated emotional responses to AVs.","Participants were asked to drive in a simulator and take over control from an automated vehicle when prompted by a takeover request (TOR), while also performing a secondary task (Tetris).",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a driving simulator and were prompted to take over control of the vehicle.,simulation,The study used a desktop-based driving simulator to simulate the driving experience.,simulated,The automated vehicle was simulated in a driving simulator.,shared control (fixed rules),The automated vehicle operated autonomously but required human takeover in specific scenarios.,Questionnaires; Real-time Trust Measures,Big Five Inventory Scale; Disposition to Trust Questionnaire,Performance Metrics,"Trust was measured using questionnaires, real-time trust ratings, and performance metrics.","parametric models (e.g., regression)",A multinomial logistic regression model was used to predict contextualized trust profiles.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the reliability of the automated vehicle by introducing false alarms and misses in takeover requests, which influenced participants' expectations and trust.","The study found that different levels of automation performance (control, false alarm, and miss conditions) significantly impacted trust, with false alarms and misses generally reducing trust.","The study identified three distinct trust profiles (myopic pragmatists, confident copilots, and reluctant automators) with different trust dynamics, emotional responses, and personality traits. Myopic pragmatists showed volatile trust levels based on recent experiences, while confident copilots maintained high trust, and reluctant automators exhibited low trust and negative emotions.","The study identified three distinct contextualized trust profiles (myopic pragmatists, confident copilots, and reluctant automators) based on dynamic trust, personality, and emotions, and developed a model to predict these profiles with high accuracy.","The robot (simulated AV) drove along a predefined route and initiated takeover requests. The human participant was required to monitor the driving, perform a secondary task, and take over control when prompted.",K-means; ANOVA; Logistic regression; shap explainer,"The study used K-means clustering to identify three distinct trust profiles based on 48 normalized features including personality, emotions, dispositional, initial learned, and real-time dynamic trust. ANOVA was used to compare the means of different trust measures, personality traits, and emotions across the identified trust profiles. A multinomial logistic regression model was constructed to predict the identified trust profiles, and the SHAP explainer was used to determine the most influential features in the model.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy,Task-complexity,"The study manipulated the reliability of the automated vehicle by introducing false alarms and misses in takeover requests (TORs). This directly impacts the 'Robot-accuracy' as it changes the success rate of the automated system in correctly identifying when a takeover is needed. The study also included a secondary task (Tetris) which adds to the cognitive load and thus manipulates 'Task-complexity'. The paper states that the different levels of automation performance (control, false alarm, and miss conditions) significantly impacted trust, with false alarms and misses generally reducing trust. This indicates that 'Robot-accuracy' was a factor that impacted trust. While the secondary task was present, the paper does not indicate that the complexity of the task itself had an impact on trust, but rather the performance of the AV. Therefore, 'Task-complexity' is listed as a factor that was manipulated but did not impact trust.",10.1109/THMS.2024.3452411,https://ieeexplore.ieee.org/document/10680191/,"Trust is crucial for ensuring the safety, security, and widespread adoption of automated vehicles (AVs), and if trust is lacking, drivers and the general public may hesitate to embrace this technology. This research seeks to investigate contextualized trust proﬁles in order to create personalized experiences for drivers in AVs with varying levels of reliability. A driving simulator experiment involving 70 participants revealed three distinct contextualized trust proﬁles (i.e., conﬁdent copilots, myopic pragmatists, and reluctant automators) identiﬁed through K-means clustering, and analyzed in relation to drivers’ dynamic trust, dispositional trust, initial learned trust, personality traits, and emotions. The experiment encompassed eight scenarios where participants were requested to take over control from the AV in three conditions: a control condition, a false alarm condition, and a miss condition. To validate the models, a multinomial logistic regression model was constructed using the shapley additive explanations explainer to determine the most inﬂuential features in predicting contextualized trust proﬁles, achieving an F1-score of 0.90 and an accuracy of 0.89. In addition, an examination of how individual factors impact contextualized trust proﬁles provided valuable insights into trust dynamics from a user-centric perspective. The outcomes of this research hold signiﬁcant implications for the development of personalized in-vehicle trust monitoring and calibration systems to modulate drivers’ trust levels, thereby enhancing safety and user experience in automated driving."
"Ayoub, Jackie; Yang, X. Jessie; Zhou, Feng",Modeling dispositional and initial learned trust in automated vehicles with predictability and explainability,2021,1,1175,1054,121,"Participants who gave nonsensical answers (i.e., unreasonable driving experience compared to their age, using letters instead of numbers to represent the number of driving years, using the same pattern to answer all the questions, and completing the survey too quickly)",Online Crowdsourcing,,Participants completed an online survey on Amazon Mechanical Turk. Data was cleaned by removing invalid responses. An XGBoost model was trained and evaluated using 10-fold cross validation. SHAP was used to explain the model predictions.,"Participants answered questions about their knowledge, experience, feelings, risk and benefit perceptions, and behavioral assessment related to automated vehicles.",Unspecified,Autonomous Vehicles,,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about automated vehicles and answered survey questions.,media,The interaction was based on text descriptions of automated vehicles.,hypothetical,The robot was only described hypothetically in the survey questions.,not autonomous,"The automated vehicle's actions were not part of the study, and the robot was only described hypothetically.",Questionnaires,,,"Trust was measured using a survey with questions related to knowledge, experience, feelings, risk and benefit perceptions, and behavioral assessment.","parametric models (e.g., regression)",An XGBoost model was used to predict trust based on survey responses.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study assessed trust without intentionally altering any specific factors.,,"The study found that perceived benefits, risk, excitement, knowledge about AVs, and eagerness to adopt new technology were the most important factors in predicting trust in AVs. The study also found interaction effects between some of these factors, such as between perceived benefits and experience with AVs.","The study proposed a machine learning model using XGBoost and SHAP that estimates people's dispositional and initial learned trust in AVs with good predictability and explainability, identifying critical factors affecting trust.","The robot is an automated vehicle, and the human participant answers survey questions about their perceptions and attitudes towards it.",xgboost; Pearson correlation; Logistic regression,"The study used an XGBoost model, trained with 10-fold cross-validation, to predict trust in automated vehicles based on survey responses. Pearson correlation coefficient was used to remove highly correlated predictor variables before training the XGBoost model. The learning objective used in the XGBoost model was logistic regression. The performance of the XGBoost model was compared with other machine learning models using various performance metrics, including accuracy, ROC_AUC, precision, recall, and F1 measure.",FALSE,,,,"The study did not manipulate any factors. It was an observational survey study where participants answered questions about their perceptions and attitudes towards automated vehicles. The study aimed to predict trust based on these responses, but no experimental manipulation was performed. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust due to manipulation.",10.1016/j.trf.2020.12.015,https://linkinghub.elsevier.com/retrieve/pii/S1369847820306008,"Technological advances in the automotive industry are bringing automated driving closer to road use. However, one of the most important factors affecting public acceptance of automated vehicles (AVs) is the public’s trust in AVs. Many factors can inﬂuence people’s trust, including perception of risks and beneﬁts, feelings, and knowledge of AVs. This study aims to use these factors to predict people’s dispositional and initial learned trust in AVs using a survey study conducted with 1175 participants. For each participant, 23 features were extracted from the survey questions to capture his/her knowledge, perception, experience, behavioral assessment, and feelings about AVs. These features were then used as input to train an eXtreme Gradient Boosting (XGBoost) model to predict trust in AVs. With the help of SHapley Additive exPlanations (SHAP), we were able to interpret the trust predictions of XGBoost to further improve the explainability of the XGBoost model. Compared to traditional regression models and black-box machine learning models, our ﬁndings show that this approach was powerful in providing a high level of explainability and predictability of trust in AVs, simultaneously."
"Ayoub, Jackie; Avetisian, Lilit; Yang, X. Jessie; Zhou, Feng",Real-Time Trust Prediction in Conditionally Automated Driving Using Physiological Measures,2023,1,74,59,15,15 participants were excluded due to malfunction of physiological sensors and the driving simulator,Controlled Lab Environment,between-subjects,"Participants completed a consent form, demographic survey, training session, and eye-tracker calibration. They then completed a driving simulation with automated mode and takeover requests, while physiological data was collected. Participants rated their trust every 25 seconds.","Participants were asked to drive in a simulator, engage the automated mode, and take over control when prompted by the system. They also performed a non-driving related task (Tetris) and rated their trust in the automated system.",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the automated driving system through a simulator, with limited physical interaction.",simulation,The study used a driving simulator to create an interactive virtual environment.,simulated,The automated vehicle was represented as a simulation in the driving simulator.,shared control (fixed rules),The automated vehicle operated independently but with fixed rules for takeover requests.,Physiological Measures; Questionnaires; Real-time Trust Measures,,Eye-tracking Data; Physiological Signals,"Trust was measured using self-reported ratings, physiological data, and eye-tracking data.","parametric models (e.g., regression)","The study used machine learning models, including XGBoost, to predict trust based on physiological and eye-tracking data.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the reliability of the automated driving system by introducing false alarms and misses, which influenced the perceived performance and difficulty of the task.",Trust was significantly lower in the miss condition compared to the control and false alarm conditions.,"The false alarm condition did not significantly reduce participants' trust, which was unexpected. The study also found that specific physiological measures were correlated with trust levels.",The XGBoost model was able to predict drivers' trust in real time with an f1-score of 89.1% using physiological measures.,"The robot (automated vehicle) performed the driving task, while the human monitored the system and took over control when requested. The human also performed a secondary task (Tetris) and provided trust ratings.","ANOVA; Tukey HSD; machine learning models (xgboost, logistic regression, decision tree, nave bayes, knn)","A one-way ANOVA was used to determine if there was a significant difference in trust among the three experimental conditions (control, misses, and false alarms). A Tukey post-hoc test was then used to determine which conditions differed significantly from each other. Several machine learning models, including XGBoost, logistic regression, decision tree, Nave Bayes, and KNN, were used to predict trust based on physiological and self-reported data. The performance of these models was compared using metrics such as accuracy and f1-score.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the reliability of the automated driving system by introducing 'misses' (system failed to identify hazards) and 'false alarms' (system incorrectly identified hazards). This directly impacts the robot's accuracy in performing its task, which is to correctly identify hazards and request takeovers. The paper states, 'Specifically, we focused on the effects of system malfunctions in SAE Level 3 automated vehicles... To explore this, we intentionally introduced two distinct types of malfunctions, namely misses and false alarms.' The results showed that the 'misses' condition significantly reduced trust compared to the control and false alarm conditions, indicating that the manipulation of robot accuracy had an impact on trust. The false alarm condition did not significantly reduce trust compared to the control condition, indicating that this specific manipulation of robot accuracy did not impact trust.",10.1109/TITS.2023.3295783,https://ieeexplore.ieee.org/document/10201385/,"Trust calibration poses a significant challenge in the interaction between drivers and automated vehicles (AVs) in the context of human-automation collaboration. To effectively calibrate trust, it becomes crucial to accurately measure drivers’ trust levels in real time, allowing for timely interventions or adjustments in the automated driving. One viable approach involves employing machine learning models and physiological measures to model the dynamic changes in trust. This study introduces a technique that leverages machine learning models to predict drivers’ real-time dynamic trust in conditional AVs using physiological measurements. We conducted the study in a driving simulator where participants were requested to take over control from automated driving in three conditions that included a control condition, a false alarm condition, and a miss condition. Each condition had eight takeover requests (TORs) in different scenarios. Drivers’ physiological measures were recorded during the experiment, including galvanic skin response (GSR), heart rate (HR) indices, and eye-tracking metrics. Using five machine learning models, we found that eXtreme Gradient Boosting (XGBoost) performed the best and was able to predict drivers’ trust in real time with an f1-score of 89.1% compared to a baseline model of K -nearest neighbor classifier of 84.5%. Our findings provide good implications on how to design an in-vehicle trust monitoring system to calibrate drivers’ trust to facilitate interaction between the driver and the AV in real time."
"Ayoub, Jackie; Avetisian, Lilit; Yang, X. Jessie; Zhou, Feng",Real-Time Trust Prediction in Conditionally Automated Driving Using Physiological Measures,2023,1,74,59,15,15 participants were excluded due to malfunction of physiological sensors and the driving simulator,Controlled Lab Environment,between-subjects,"Participants completed a consent form, a demographic survey, and a training session. They were then calibrated with an eye-tracker and had GSR and PPG sensors attached. Participants then completed a driving simulation with takeover requests, and rated their trust every 25 seconds.","Participants were asked to drive in a simulator and take over control from automated driving when prompted, while also performing a non-driving related task.",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the automated driving system through a driving simulator.,simulation,The study used a driving simulator to create an interactive virtual environment.,simulated,The robot was a simulated automated vehicle within the driving simulator.,shared control (fixed rules),The automated vehicle operated independently but with fixed rules for takeover requests.,Questionnaires; Physiological Measures; Real-time Trust Measures,,Physiological Signals; Eye-tracking Data,"Trust was measured using self-reported ratings, physiological data, and eye-tracking metrics.","parametric models (e.g., regression)","The study used machine learning models, including XGBoost, to predict trust based on physiological data.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the reliability of the automated driving system by introducing false alarms and misses, which influenced the perceived performance and task difficulty.",Trust was significantly lower in the miss condition compared to the control and false alarm conditions.,"The false alarm condition did not significantly reduce participants' trust, which was unexpected. The study also found that a high number of fixations at the center screen led to a decrease in trust, while a low number of fixations led to an increase in trust.",The XGBoost model was able to predict drivers' trust in real time with an f1-score of 89.1% using physiological measures.,"The automated vehicle drove autonomously until a takeover request was issued, at which point the human driver took control. The human also performed a non-driving related task (Tetris) during the automated driving phase.",ANOVA; Tukey HSD,"A one-way ANOVA was used to determine if there was a significant difference in trust among the three experimental conditions (control, misses, and false alarms). A Tukey post-hoc test was then used to perform pairwise comparisons between the conditions to identify which specific conditions differed significantly in terms of trust.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy,,"The study manipulated the reliability of the automated driving system by introducing 'misses' (where the system failed to identify hazards) and 'false alarms' (where the system incorrectly identified hazards). This directly impacts the 'Robot-accuracy' as it changes the success rate of the automated system in correctly identifying hazards. The study also implicitly manipulated 'Task-complexity' by introducing the non-driving related task (Tetris) alongside the driving task, which increased the cognitive load on the participants. The 'Robot-accuracy' manipulation was found to significantly impact trust, with trust being lower in the 'miss' condition compared to the control and 'false alarm' conditions. The paper states, 'A one-way analysis of variance (ANOVA) showed a significant difference in trust among the three conditions (i.e., control, misses, and FA), F(2, 47) = 22.323, p < 0.001. A Tukey post-hoc test revealed that trust was significantly higher in the control (M = 7.967) and FA (M = 7.699) conditions compared to the misses condition (M = 5.597) ( p < 0.001). There was no significant difference between the FA and the control condition.' The 'Task-complexity' was not explicitly tested for its impact on trust, but it was a constant factor across all conditions, and therefore not a manipulated factor that impacted trust.",10.1109/TITS.2023.3295783,https://ieeexplore.ieee.org/document/10201385/,"Trust calibration poses a significant challenge in the interaction between drivers and automated vehicles (AVs) in the context of human-automation collaboration. To effectively calibrate trust, it becomes crucial to accurately measure drivers’ trust levels in real time, allowing for timely interventions or adjustments in the automated driving. One viable approach involves employing machine learning models and physiological measures to model the dynamic changes in trust. This study introduces a technique that leverages machine learning models to predict drivers’ real-time dynamic trust in conditional AVs using physiological measurements. We conducted the study in a driving simulator where participants were requested to take over control from automated driving in three conditions that included a control condition, a false alarm condition, and a miss condition. Each condition had eight takeover requests (TORs) in different scenarios. Drivers’ physiological measures were recorded during the experiment, including galvanic skin response (GSR), heart rate (HR) indices, and eye-tracking metrics. Using five machine learning models, we found that eXtreme Gradient Boosting (XGBoost) performed the best and was able to predict drivers’ trust in real time with an f1-score of 89.1% compared to a baseline model of K -nearest neighbor classifier of 84.5%. Our findings provide good implications on how to design an in-vehicle trust monitoring system to calibrate drivers’ trust to facilitate interaction between the driver and the AV in real time."
"Azevedo-Sa, Hebert; Jayaraman, Suresh Kumaar; Yang, X. Jessie; Robert, Lionel P.; Tilbury, Dawn M.",Context-Adaptive Management of Drivers’ Trust in Automated Vehicles,2020,1,40,40,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a pre-experiment survey, received explanations about the AV and NDRT, completed a training drive, and then completed two trials on the AV simulator while performing the NDRT, followed by post-trial surveys.",Participants operated an AV simulator while simultaneously performing a visual search NDRT.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated AV and performed a secondary task.,simulation,The interaction took place in a simulated driving environment.,simulated,The robot was a simulated autonomous vehicle.,shared control (fixed rules),The AV operated autonomously but with fixed rules for interaction and warnings.,Behavioral Measures; Real-time Trust Measures,,Eye-tracking Data; Performance Metrics,Trust was assessed using real-time behavioral measures and performance metrics.,"parametric models (e.g., regression)",A Kalman filter-based trust estimator was used to model trust dynamics.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The AV's communication style was manipulated to encourage or warn the driver based on their trust level, influencing their situation awareness and risk perception.","The AV's encouraging messages increased trust in undertrusting drivers, while warning messages decreased trust in overtrusting drivers.","The study found that the trust calibrator was effective for 29 out of 40 participants, but the reasons for the lack of decreased trust miscalibration for the remaining 11 participants are unknown.","The proposed trust management framework, using a trust calibrator, was effective in mitigating trust miscalibration by adjusting drivers' trust levels through adaptive communication.","The robot (simulated AV) provided verbal messages to the driver, encouraging or warning them based on their trust level. The human participant operated the AV simulator while performing a visual search task, and had to take over control when the AV encountered obstacles.",Mixed-effects model,"A linear mixed-effects model was used to analyze the impact of different communication styles on drivers' trust in the AV. The model examined the differences in drivers' trust estimates between consecutive events (ΔT), after they had heard messages from the AV. Fixed effects included the different communication styles (encouraging, warning moderate, warning harsh), and a random intercept was assigned to each participant to account for individual differences. The purpose was to determine how the AV's communication style influenced changes in drivers' trust levels.",TRUE,Robot-verbal-communication-style,Robot-verbal-communication-style,,"The study manipulated the style of verbal communication from the AV to the driver. The AV provided encouraging, moderately warning, or harshly warning messages based on the driver's trust level. This is described in the paper as 'the communication style of the AV is then selected after the trust miscalibration is identified' and 'the AV interacted with the driver through verbal messages corresponding to the communication style defined in the trust calibrator block'. The different communication styles directly impacted the drivers' trust levels, as stated in the results: 'The encouraging messages helped drivers to increase their trust in the AV... The warning messages had the effect of decreasing their trust in the AV'. Therefore, 'Robot-verbal-communication-style' is the most appropriate category for the manipulated factor and the factor that impacted trust. There were no other factors manipulated in the study.",10.1109/LRA.2020.3025736,https://ieeexplore.ieee.org/document/9204417/,"Automated vehicles (AVs) that intelligently interact with drivers must build a trustworthy relationship with them. A calibrated level of trust is fundamental for the AV and the driver to collaborate as a team. Techniques that allow AVs to perceive drivers’ trust from drivers’ behaviors and react accordingly are, therefore, needed for context-aware systems designed to avoid trust miscalibrations. This letter proposes a framework for the management of drivers’ trust in AVs. The framework is based on the identiﬁcation of trust miscalibrations (when drivers’ undertrust or overtrust the AV) and on the activation of different communication styles to encourage or warn the driver when deemed necessary. Our results show that the management framework is effective, increasing (decreasing) trust of undertrusting (overtrusting) drivers, and reducing the average trust miscalibration time periods by approximately 40%. The framework is applicable for the design of SAE Level 3 automated driving systems and has the potential to improve the performance and safety of driver–AV teams."
"Azevedo-Sa, Hebert; Jayaraman, Suresh Kumaar; Esterwood, Connor T.; Yang, X. Jessie; Robert, Lionel P.; Tilbury, Dawn M.",Real-Time Estimation of Drivers’ Trust in Automated Driving Systems,2020,1,80,80,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants completed a pre-experiment survey, a training session, and two trials in a driving simulator, followed by post-trial surveys. During each trial, participants operated a simulated vehicle with an ADS while performing a visual search NDRT, and reported their trust change after each event.",Participants operated a simulated vehicle equipped with an ADS while concurrently performing a visual search NDRT.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated ADS in a driving simulator.,simulation,Participants interacted with a simulated driving environment.,simulated,The robot was a simulated autonomous driving system.,shared control (fixed rules),The ADS operated autonomously but required human intervention for obstacles.,Questionnaires; Real-time Trust Measures,Muir's Trust Questionnaire,Eye-tracking Data; Performance Metrics,Trust was measured using self-report questionnaires and real-time behavioral measures.,"parametric models (e.g., regression)",A linear time-invariant state-space model was used to model trust dynamics.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the ADS's reliability by introducing false alarms and misses, which influenced the perceived performance of the system and the difficulty of the task.","Trust generally decreased after false alarms and misses, and increased after true alarms.","Participants' self-reported trust increased after true alarms and decreased after false alarms and misses, but some participants showed less frequent variations in trust reports, possibly due to learning effects or the road type.","The study successfully estimated drivers' trust in ADS using a Kalman filter-based approach, with accuracy improving over time.","The robot (ADS) provided lane keeping, cruise control, and collision avoidance, while the human operated the vehicle and performed a visual search task, taking over control when the ADS detected obstacles.",linear mixed-effects models; maximum likelihood estimation,The study used linear mixed-effects models with maximum likelihood estimation to identify the parameters of the state-space model for trust dynamics. These models included a random offset per participant to capture individual biases and mitigate their effects on the results. The parameters were used to develop a Kalman filter-based trust estimator.,TRUE,Robot-accuracy; Task-complexity,Robot-accuracy,Task-complexity,"The researchers manipulated the reliability of the ADS by introducing false alarms and misses, which directly impacted the accuracy of the system in detecting obstacles. This is classified as 'Robot-accuracy' because it directly relates to the system's performance in a safety-critical task. The study also implicitly manipulated 'Task-complexity' by having participants perform a visual search NDRT concurrently with the driving task. The difficulty of the NDRT was not directly manipulated, but the presence of the NDRT and the need to switch attention between the driving and NDRT tasks increased the overall cognitive load and task complexity. The study found that the 'Robot-accuracy' (specifically, the occurrence of false alarms and misses) significantly impacted trust, with trust decreasing after such events. While the NDRT was present and added to the task complexity, the study did not explicitly manipulate the difficulty of the NDRT, and the paper does not suggest that the NDRT complexity had a direct impact on trust. Therefore, 'Task-complexity' is listed as a manipulated factor but not as a factor that impacted trust.",10.1007/s12369-020-00694-1,http://link.springer.com/10.1007/s12369-020-00694-1,"Trust miscalibration issues, represented by undertrust and overtrust, hinder the interaction between drivers and self-driving vehicles. A modern challenge for automotive engineers is to avoid these trust miscalibration issues through the development of techniques for measuring drivers’ trust in the automated driving system during real-time applications execution. One possible approach for measuring trust is through modeling its dynamics and subsequently applying classical state estimation methods. This paper proposes a framework for modeling the dynamics of drivers’ trust in automated driving systems and also for estimating these varying trust levels. The estimation method integrates sensed behaviors (from the driver) through a Kalman ﬁlter-based approach. The sensed behaviors include eye-tracking signals, the usage time of the system, and drivers’ performance on a non-driving-related task. We conducted a study (n = 80) with a simulated SAE level 3 automated driving system, and analyzed the factors that impacted drivers’ trust in the system. Data from the user study were also used for the identiﬁcation of the trust model parameters. Results show that the proposed approach was successful in computing trust estimates over successive interactions between the driver and the automated driving system. These results encourage the use of strategies for modeling and estimating trust in automated driving systems. Such trust measurement technique paves a path for the design of trust-aware automated driving systems capable of changing their behaviors to control drivers’ trust levels to mitigate both undertrust and overtrust."
"Azevedo-Sa, Hebert; Yang, X. Jessie; Robert, Lionel P.; Tilbury, Dawn M.",A Unified Bi-Directional Model for Natural and Artificial Trust in Human–Robot Collaboration,2021,1,284,284,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants rated capability requirements for four driving tasks, watched videos of an AV executing three of the tasks, and then rated their trust in the AV to execute the fourth task.",Participants assessed the capability requirements of driving tasks and rated their trust in an AV after watching videos of the AV performing similar tasks.,Unspecified,Autonomous Vehicles,Research,Evaluation,Rating,passive observation,Participants watched videos of the robot performing tasks.,media,Participants watched videos of a simulated AV performing driving tasks.,simulated,The robot was presented as a simulated AV in videos.,pre-programmed (non-adaptive),The AV's actions were pre-programmed and did not adapt to the user.,Questionnaires,,Video Data; Performance Metrics,Trust was measured using a 7-point Likert scale after participants watched videos of the AV.,"parametric models (e.g., regression)",The study used a bi-directional trust model and compared it with other parametric models.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the success or failure of the AV in the videos, and the task difficulty was varied by using different driving scenarios.",The study found that the proposed bi-directional trust model outperformed other models in predicting human trust in the AV.,The bi-directional trust model outperformed existing models for multi-task natural trust transfer prediction.,The proposed bi-directional trust model outperformed existing models in predicting human trust in a robot for multi-task scenarios.,Participants watched videos of a simulated AV performing driving tasks and then rated their trust in the AV to perform a different driving task.,Cross-validation; mean absolute error (mae); negative log-likelihood (nll),The study used 10-fold cross-validation to train and evaluate the proposed bi-directional trust model (BTM). The performance of the BTM was compared with a Bayesian Gaussian process model (GP) and a linear Gaussian model (OPT). The models' performance was evaluated using mean absolute error (MAE) and negative log-likelihood (NLL) metrics. The cross-entropy between the trust measurements from the dataset and the model outputs was used as the loss function to be minimized during the optimization process.,TRUE,Robot-accuracy; Task-complexity,Robot-accuracy,Task-complexity,"The study manipulated 'Robot-accuracy' by showing videos of the AV either succeeding or failing in the driving tasks. This is explicitly stated in the paper: 'The videos showed the AV succeeding or failing to execute each observation task.' This manipulation directly impacted trust, as the success or failure of the AV influenced participants' trust ratings. The study also implicitly manipulated 'Task-complexity' by using different driving scenarios, as stated in the paper: 'Initially, only images and verbal descriptions of four driving tasks were presented...'. However, the paper does not explicitly state that the task complexity was a factor that impacted trust, but rather that the success or failure of the AV was the main driver of trust. Therefore, 'Task-complexity' is included in 'factors_manipulated' but not in 'factors_that_impacted_trust'.",10.1109/LRA.2021.3088082,https://ieeexplore.ieee.org/document/9450023/,"We introduce a novel capabilities-based bi-directional multi-task trust model that can be used for trust prediction from either a human or a robotic trustor agent. Tasks are represented in terms of their capability requirements, while trustee agents are characterized by their individual capabilities. Trustee agents’ capabilities are not deterministic; they are represented by belief distributions. For each task to be executed, a higher level of trust is assigned to trustee agents who have demonstrated that their capabilities exceed the task’s requirements. We report results of an online experiment with 284 participants, revealing that our model outperforms existing models for multi-task trust prediction from a human trustor. We also present simulations of the model for determining trust from a robotic trustor. Our model is useful for control authority allocation applications that involve human–robot teams."
"Azevedo-Sa, Hebert; Yang, Jessie; Robert, Lionel; Tilbury, Dawn",A Unified Bi-directional Model for Natural and Artificial Trust in Human–Robot Collaboration,2021,1,284,284,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants first rated the capability requirements for four driving tasks. Then, they watched videos of a simulated AV executing three of the tasks, and indicated whether the AV succeeded or failed. Finally, they rated their trust in the AV to execute the fourth remaining task.","Participants rated the capability requirements for driving tasks, watched videos of an AV performing tasks, and then rated their trust in the AV to perform a new task.",Unspecified,Autonomous Vehicles,Research,Evaluation,Rating,passive observation,Participants observed videos of the robot performing tasks.,media,Participants watched videos of the robot performing tasks.,simulated,The robot was presented as a simulated vehicle in videos.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires,,Performance Metrics,Trust was measured using a 7-point Likert scale and performance metrics.,"parametric models (e.g., regression)",The study used a capabilities-based bi-directional trust model and compared it with other parametric models.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the robot's performance by showing videos of the AV either succeeding or failing at tasks, and the task difficulty was implicitly manipulated by the different task requirements.",The study found that the proposed bi-directional trust model outperformed other models in predicting human trust in the robot.,The bi-directional trust model outperformed existing models for multi-task natural trust transfer prediction.,The proposed bi-directional trust model outperformed existing models in predicting human trust in a robot for multi-task scenarios.,"The robot, a simulated AV, performed driving tasks, and the human participant watched videos of the robot's performance and rated their trust in the robot to perform a new task.",Cross-validation; mean absolute error (mae); negative log-likelihood (nll),The study used 10-fold cross-validation to train and evaluate the proposed bi-directional trust model (BTM) and compared its performance with a Bayesian Gaussian process model (GP) and a linear Gaussian model (OPT). The models' performance was evaluated using mean absolute error (MAE) and negative log-likelihood (NLL) metrics. The cross-entropy between the distributions of the dataset and the model outputs was used as the loss function to be minimized during the parameter optimization process.,TRUE,Robot-accuracy; Task-complexity,Robot-accuracy,Task-complexity,"The study manipulated 'Robot-accuracy' by showing videos of the AV either succeeding or failing at tasks. This directly influenced the perceived performance of the robot, which is a key aspect of robot accuracy. The study also implicitly manipulated 'Task-complexity' by presenting different driving tasks with varying capability requirements (sensing and processing). While the task requirements were rated by participants, the inherent complexity of the tasks was a factor that was varied by the researchers. The study found that the robot's performance (success or failure) directly impacted trust, as indicated by the trust ratings after watching the videos. The task complexity, while varied, was not explicitly found to impact trust levels directly, but rather through the lens of the robot's performance on those tasks. The study focused on how the robot's performance on tasks of varying complexity influenced trust, not the complexity itself. Therefore, 'Robot-accuracy' is the factor that impacted trust, while 'Task-complexity' was a factor that was manipulated but did not directly impact trust.",10.2139/ssrn.3859320,https://www.ssrn.com/abstract=3859320,"We introduce a novel capabilities-based bidirectional multi-task trust model that can be used for trust prediction from either a human or a robotic trustor agent. Tasks are represented in terms of their capability requirements, while trustee agents are characterized by their individual capabilities. Trustee agents’ capabilities are not deterministic; they are represented by belief distributions. For each task to be executed, a higher level of trust is assigned to trustee agents who have demonstrated that their capabilities exceed the task’s requirements. We report results of an online experiment with 284 participants, revealing that our model outperforms existing models for multi-task trust prediction from a human trustor. We also present simulations of the model for determining trust from a robotic trustor. Our model is useful for control authority allocation applications that involve human–robot teams."
"Azevedo-Sa, Hebert; Zhao, Huajing; Esterwood, Connor; Yang, X. Jessie; Tilbury, Dawn M.; Robert, Lionel P.",How internal and external risks affect the relationships between trust and driver behavior in automated driving systems,2021,1,37,37,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a pre-experiment survey, a training session with the simulator and NDRT, were equipped with an eye-tracking headset, and then completed four trials with different conditions. After each trial, participants completed a post-trial survey.",Participants drove a simulated vehicle with ADS assistance while performing a visual search NDRT on a touchscreen.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated vehicle and automation system.,simulation,Participants interacted with a driving simulator.,simulated,The robot was a simulated automated driving system.,shared control (fixed rules),"The ADS provided lane-keeping, cruise control, and collision alerts, but the driver could take over at any time.",Behavioral Measures; Questionnaires; Real-time Trust Measures,Muir's Trust Questionnaire,Eye-tracking Data; Performance Metrics,"Trust was measured using post-trial questionnaires, real-time trust changes after each alert, and behavioral measures such as monitoring ratio.","parametric models (e.g., regression)","Linear mixed effects models were used to analyze the relationships between risk, trust, NDRT performance, and monitoring.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the ADS (internal risk) was manipulated by introducing false alarms, and visibility (external risk) was manipulated by using foggy weather. These manipulations were intended to influence trust in the ADS.","Low ADS reliability reduced trust, while low visibility did not significantly impact post-trial trust, but did impact dynamic trust.","The study found that internal risk (low reliability) reduced ADS trust, but external risk (low visibility) did not significantly impact post-trial trust. Also, low visibility moderated the impact of ADS trust on monitoring, but low reliability did not.","The study's key finding is that the impact of risk on ADS trust depends on the type of risk, with internal risk (low reliability) reducing trust, while external risk (low visibility) did not have a significant negative impact on post-trial trust.","The robot (ADS) provided lane-keeping, cruise control, collision alerts, and recommendations for when the driver should take over. The human participant drove the simulated vehicle, monitored the driving situation, and performed a visual search task on a touchscreen.",t-test; Mixed-effects model,"Pairwise t-tests were used as a manipulation check to compare perceived risk levels between different conditions. Linear mixed effects (LME) models were used to investigate the relationships among risk, trust, NDRT performance, and monitoring ratios. These models aimed to identify parameters that significantly differed from zero, indicating the influence of associated factors on the output variables. Specifically, LME models were used to analyze post-trial trust, dynamic trust, NDRT scores, and monitoring ratios, considering the effects of reliability, visibility, and their interactions.",TRUE,Robot-accuracy; Task-environment,Robot-accuracy,Task-environment,"The study manipulated the reliability of the ADS by introducing false alarms, which directly impacts the accuracy of the robot's actions and recommendations, thus falling under 'Robot-accuracy'. The study also manipulated the visibility of the driving environment by using foggy weather, which changes the working conditions and is therefore classified as 'Task-environment'. The results showed that the manipulation of 'Robot-accuracy' (ADS reliability) significantly impacted trust, as low reliability reduced trust. However, the manipulation of 'Task-environment' (visibility) did not significantly impact post-trial trust, although it did impact dynamic trust. Therefore, 'Robot-accuracy' is listed as a factor that impacted trust, and 'Task-environment' is listed as a factor that did not impact post-trial trust.",10.1016/j.trc.2021.102973,https://linkinghub.elsevier.com/retrieve/pii/S0968090X21000103,"Automated driving systems (ADSs) allow vehicles to engage in self-driving under specific con­ ditions. Along with the potential safety benefits, the increase in productivity through non-drivingrelated tasks (NDRTs) is often cited as a motivation behind the adoption of ADSs. Although ad­ vances have been made in understanding both the promotion of ADS trust and its impact on NDRT performance, the influence of risk remains largely understudied. To fill this gap, we con­ ducted a within-subjects experiment with 37 licensed drivers using a simulator. Internal risk was manipulated by ADS reliability and external risk by visibility, producing a 2 (ADS reliability) × 2 (visibility) design. The results indicate that high reliability increases ADS trust and further en­ hances the positive impact of ADS trust on NDRT performance, while low visibility reduces the negative impact of ADS trust on driver monitoring. Results also suggest that trust increases over time if the system is reliable and that visibility did not have a significant impact on ADS trust. These findings are important for the design of intelligent ADSs that can respond to drivers’ trusting behaviors."
"Azevedo-Sa, Hebert; Yang, X. Jessie; Robert Jr., Lionel P.; Tilbury, Dawn M.",Using Trust in Automation to Enhance Driver-(Semi)Autonomous Vehicle Interaction and Improve Team Performance,2021,1,80,80,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants experienced two trials (straight and curvy roads) in a driving simulator, with 12 interaction events in each trial. They self-reported their trust in the ADS and their trusting behaviors were measured.","Participants drove a simulated vehicle with an automated driving system (ADS) and experienced true alarms, false alarms, and misses from the ADS.",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated autonomous vehicle in a driving simulator.,simulation,The interaction took place in a driving simulator.,simulated,The robot was a simulated autonomous vehicle.,fully autonomous (limited adaptation),The ADS operated autonomously but with limited adaptation to the specific driving conditions.,Behavioral Measures; Questionnaires; Real-time Trust Measures,Muir's Trust Questionnaire,Eye-tracking Data; Performance Metrics,"Trust was measured using self-reported questionnaires, eye-tracking data, and performance metrics.","parametric models (e.g., regression)",A linear mixed effects model was used to model trust dynamics.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the ADS was manipulated by introducing true alarms, false alarms, and misses, which influenced the driver's expectations and trust.","The study found that the reliability of the ADS, specifically the occurrence of false alarms and misses, influenced the driver's trust levels.","The study found that the success of the trust estimation process highly depends on a precise initial estimate of trust in the ADS. The variation of drivers' trust in the ADS is a slow process, and the noise terms have large variances, which slow down the convergence of the curves.","The study developed a method to track trust dynamically in short-term interactions between drivers and ADSs, showing that trust can be estimated from observation variables.","The robot (ADS) provided automated driving capabilities, and the human participant monitored the system, experiencing true alarms, false alarms, and misses. The human also completed a non-driving related task.",Mixed-effects model; Kalman filter,"The study used linear mixed effects models to establish a state-space model for trust dynamics, examining how trust changes after the ADS meets or fails to meet driver expectations. A Kalman filter was then used to design a trust estimator based on the identified parameters from the linear mixed effects model. The Kalman filter was used to track the self-reported trust levels from observation variables.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the reliability of the ADS by introducing true alarms, false alarms, and misses. This directly impacts the accuracy of the robot's actions in identifying obstacles, which is a core aspect of 'Robot-accuracy'. The paper states, 'We aim to model how risk factors (e.g.: false alarms and misses from the ADS) and the short term interactions associated with these risk factors influence the dynamics of drivers' trust in the ADS.' This clearly indicates that the manipulation of the ADS's performance (accuracy) was intended to influence trust. The results section also confirms that the success of the trust estimation process depends on the initial estimate of trust, which is influenced by the accuracy of the ADS. The paper also states, 'In this work we explored how trust develops over interactions between drivers and ADSs, and how the reliability of the ADS affects this trust development.' This further supports the choice of 'Robot-accuracy' as the manipulated factor that impacted trust.",,http://arxiv.org/abs/2106.02136,"Trust in robots has been gathering attention from multiple directions, as it has special relevance in the theoretical descriptions of human-robot interactions. It is essential for reaching high acceptance and usage rates of robotic technologies in society, as well as for enabling effective human-robot teaming. Researchers have been trying to model the development of trust in robots to improve the overall rapport between humans and robots. Unfortunately, the miscalibration of trust in automation is a common issue that jeopardizes the effectiveness of automation use. It happens when a user's trust levels are not appropriate to the capabilities of the automation being used. Users can be: under-trusting the automation -- when they do not use the functionalities that the machine can perform correctly because of a lack of trust; or over-trusting the automation -- when, due to an excess of trust, they use the machine in situations where its capabilities are not adequate. The main objective of this work is to examine driver's trust development in the ADS. We aim to model how risk factors (e.g.: false alarms and misses from the ADS) and the short-term interactions associated with these risk factors influence the dynamics of drivers' trust in the ADS. The driving context facilitates the instrumentation to measure trusting behaviors, such as drivers' eye movements and usage time of the automated features. Our findings indicate that a reliable characterization of drivers' trusting behaviors and a consequent estimation of trust levels is possible. We expect that these techniques will permit the design of ADSs able to adapt their behaviors to attempt to adjust driver's trust levels. This capability could avoid under- and over-trusting, which could harm their safety or their performance."
"Babel, Franziska; Kraus, Johannes; Miller, Linda; Kraus, Matthias; Wagner, Nicolas; Minker, Wolfgang; Baumann, Martin","Small Talk with a Robot? The Impact of Dialog Content, Talk Initiative, and Gaze Behavior of a Social Robot on Trust, Acceptance, and Proximity",2021,1,31,31,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants interacted with a NAO robot in a lab setting. They engaged in two dialogs, one small talk and one service task, in a randomized order. The robot's gaze (directed or random) and talk initiative (human or robot) were manipulated between subjects. Participants completed questionnaires before, during, and after the interactions, and their proximity to the robot was measured.",Participants engaged in a small talk conversation and a service task conversation with a robot.,Nao,Humanoid Robots,Research; Social,Social,Conversation,minimal interaction,Participants had verbal interactions with the robot in a controlled setting.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical humanoid robot.,wizard of oz (directly controlled),The robot's actions were controlled by a human operator behind the scenes.,Behavioral Measures; Questionnaires,,,Trust was assessed using self-report questionnaires and behavioral measures of proximity.,no modeling,The study did not use computational models of trust.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,The study manipulated the robot's gaze behavior (directed or random) and talk initiative (human or robot) as well as the dialog content (small talk or service task) to see how these factors influenced trust.,Robot initiative led to higher trust in the service task when it was the first interaction. Directed gaze was more accepted during small talk. Self-reported trust was associated with less physical distance to the robot.,"The study found that the effect of robot initiative on trust was dependent on the order of the dialog content. Specifically, robot initiative led to higher trust in the service task only when it was the first interaction. Also, the majority of participants did not approach the robot, and those who did were mostly in the random gaze condition.","The study found that different gaze and proactive strategies are efficient to foster trust and acceptance in social robots for different dialog contents, and that self-reported trust was associated with the distance participants kept from the robot.","The robot engaged in either small talk or a service task conversation with the participant. The human participant followed a script, either asking questions or answering them, depending on the talk initiative condition. The robot's gaze was either directed at the participant or random.",t-test; ANOVA; Shapiro-Wilk; Levene's test; χ² test; Pearson correlation,"The study used t-tests for manipulation checks and to compare means. Mixed analysis of variance (ANOVA) was used to test for differences between measurements after the first and second conversations, and across different conditions (talk initiative and gaze). Shapiro-Wilk and Levene's tests were used to check for normality and homogeneity of variances, respectively. A chi-squared (χ²) test was used to analyze the distribution of participants in the approaching group. Correlation analysis was used to examine the relationship between proximity and baseline trust ratings.",TRUE,Robot-nonverbal-communication; Robot-verbal-communication-content; Robot-verbal-communication-style,Robot-verbal-communication-style,Robot-nonverbal-communication; Robot-verbal-communication-content,"The study manipulated several factors related to the robot's communication. 'Robot-nonverbal-communication' was chosen because the robot's gaze behavior was manipulated between directed and random gaze, which is a nonverbal cue. 'Robot-verbal-communication-content' was chosen because the dialog content was manipulated between small talk and a service task, which changes the content of the verbal communication. 'Robot-verbal-communication-style' was chosen because the talk initiative was manipulated between human and robot, which changes the style of the verbal communication. The study found that the talk initiative (Robot-verbal-communication-style) impacted trust, specifically that robot initiative led to higher trust in the service task when it was the first interaction. The study found that the robot's gaze behavior (Robot-nonverbal-communication) did not impact trust, although it did impact perceived anthropomorphism and acceptance. The study also found that the dialog content (Robot-verbal-communication-content) did not impact trust, although it did impact acceptance, reliability, and performance ratings.",10.1007/s12369-020-00730-0,http://link.springer.com/10.1007/s12369-020-00730-0,"Appropriate human likeness for social robots is said to increase trust and acceptance. Whether this applies to human communication features like dialog initiative needs to be investigated. Dialog initiative could be unacceptable for a robot, depending on the dialog content. Hence, the presented study investigates how a social robot’s proactive verbal and non-verbal communication behavior affects trust and acceptance depending on dialog content and content presentation order. A laboratory study (n = 31) with a humanoid robot was conducted. Talk initiative (human/robot) and the robot’s gaze behavior (directed/random) were manipulated. Dialog content was alternated between a service task and small talk. The subject’s trust, acceptance and human-robot proximity were assessed. Whereas a directed gaze was perceived as more humanlike and was more accepted during small talk, no gaze preference for the service task emerged. There was no preference for who initiated the small talk but for the service task, robot initiative led to higher trust in the robot when the service task was the ﬁrst interaction. Participant’s self-reported trust in the robot was associated with the distance they kept to the robot. Different gaze and proactive strategies seem to be efﬁcient to foster trust and acceptance in social robots for different dialog contents and thus should be considered when designing interaction strategies for social robots."
"Babel, Franziska; Kraus, Johannes; Baumann, Martin",Findings From A Qualitative Field Study with An Autonomous Robot in Public: Exploration of User Reactions and Conflicts,2022,1,344,32,13,"2 participants were excluded because German was not their mother tongue and due to incapacity of study participation, 11 participants were excluded due to unusable audio material",Real-World Environment,,"Passersby were observed interacting with a cleaning robot at a train station, some were then interviewed and filled out a questionnaire.",Passersby were observed while a cleaning robot performed its cleaning task in a public space.,CR700,Mobile Robots; Service and Assistive Robots; Unmanned Ground Vehicles (UGVs),Other: Cleaning,Navigation,Path Following,passive observation,Participants observed the robot in a real-world setting without direct interaction.,real-world,The study took place in a real-world environment with a physical robot.,physical,The robot was a physical entity present in the real-world environment.,fully autonomous (limited adaptation),"The robot operated autonomously, but with limited adaptation to unexpected situations.",Questionnaires,Trust in Automation Scale (TAS),Video Data,Trust was measured using a questionnaire and video data was collected.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,No Manipulation,"The study did not manipulate any specific factors related to trust, but rather observed natural interactions.",,Passersby who perceived the robot as unpredictable also trusted and accepted it less. There was a conflict between the high trust and acceptance ratings and the observed evasive behavior and negative emotions.,"The study found that most passersby felt safe in the presence of the robot, but unpredictability of the robot's actions led to lower trust and acceptance.","The robot autonomously cleaned the floor, while the human passersby walked through the train station.",Mann-Whitney U,A non-parametric Mann-Whitney U-test was used to compare the average trust and acceptance scores between two groups of participants: those who perceived the robot as less predictable (lower quartile) and those who perceived it as very predictable (upper quartile). This test was used due to the small sample size.,FALSE,,,,"The study did not manipulate any factors. The study observed natural interactions between passersby and a cleaning robot in a public space. There were no intentional changes to the robot's behavior, communication, or task performance. The study focused on observing and documenting the reactions of passersby to the robot's presence and actions. The robot's autonomy was a constant factor, not a manipulated one. The study did not manipulate any task-related factors, such as task complexity or constraints. The study did not manipulate any environmental factors. The study did not involve any teaming or competition between humans and robots. Therefore, no factors from the provided list were manipulated.",10.1007/s12369-022-00894-x,https://link.springer.com/10.1007/s12369-022-00894-x,"Soon service robots will be employed in public spaces with frequent human-robot interaction (HRI). To achieve a safe, trustworthy and acceptable HRI, service robots need to be equipped with interaction strategies suitable for the robot, user, and context. To gain realistic insights into the initial user reactions and challenges that arise when a mechanoid, autonomous service robot in public is applied, a ﬁeld study with three data sources was conducted. In a ﬁrst step, lay users’ intuitive reactions to a cleaning robot at a train station were observed (N = 344). Second, passersby’s preferences for HRI interaction strategies were explored in interviews (n = 54). As a third step, trust and acceptance of the robot were assessed with questionnaires (n = 32). Identiﬁed challenges were social robot navigation in crowded places also applicable to vulnerable passersby, inclusive communication modalities, information of staff and public about the service robot application and the need for conﬂict resolution strategies to avoid an inefﬁcient robot (e.g., testing behavior, path is blocked). This study provides insights into naive HRI in public and illustrates challenges, provides recommendations supported by literature and highlights aspects for future research to inspire a research agenda in the ﬁeld of public HRI."
"Babel, Franziska; Hock, Philipp; Kraus, Johannes; Baumann, Martin","It Will Not Take Long! Longitudinal Effects of Robot Conflict Resolution Strategies on Compliance, Acceptance and Trust",2022,1,116,103,13,"11 participants dropped out in session 2, particular browsers and an unstable internet connection",Online Crowdsourcing,mixed design,"Participants completed a virtual dish-sorting task and interacted with a virtual robot that used one of three conflict resolution strategies (appeal, command, diminution) with or without positive reinforcement (thanking) over six trials divided into two sessions separated by one week. Participants' compliance, strategy acceptance, and trust in the robot were assessed after each trial.",Participants completed a dish-sorting task in a virtual kitchen while a virtual robot requested them to move aside to clean the floor.,REEM,Humanoid Robots; Service and Assistive Robots,Social; Research,Social,Persuasion,minimal interaction,Participants interacted with a virtual robot in a simulated environment.,simulation,Participants interacted with a virtual robot in a virtual kitchen environment.,simulated,The robot was a virtual representation of the REEM robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions and responses.,Questionnaires,LETRAS-G,,Trust was measured using a questionnaire.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's conflict resolution strategy (appeal, command, diminution) and the presence or absence of positive reinforcement (thanking) were manipulated to influence trust and compliance.","The diminution strategy with reinforcement increased trust over time, while command with reinforcement decreased acceptance over time. Compliance rates did not differ significantly between strategies.","The diminution strategy with reinforcement was the most effective and trustworthy over time. The command strategy with reinforcement showed a decrease in acceptance over time. There was a trend towards reciprocity in compliance rates for the diminution and appeal strategies, but this did not reach statistical significance. The study also found that the robot achieved significantly less compliance than a household member, except for the diminution with reinforcement strategy.",Diminishing the request combined with positive reinforcement was the most effective strategy for a service robot to achieve user compliance and increased trustworthiness over multiple interactions.,The robot asked the participant to move aside so it could clean the floor using one of three conflict resolution strategies. The human participant was engaged in a dish-sorting task and had to decide whether to comply with the robot's request or continue with their task.,Cochran's Q; Repeated measures ANOVA; Chi-squared; LSD post-hoc,The study used Cochran's Q test to assess differences in compliance rates over time within each strategy. Repeated-measures ANOVAs were used to analyze baseline differences in questionnaire data and to examine changes in acceptance and trust over time within each strategy. Multiple Chi-Square tests were used to compare compliance rates between different strategies and to assess the effect of positive reinforcement on compliance. Fisher's LSD post-hoc tests were used to further investigate significant differences found in the ANOVAs.,TRUE,Robot-verbal-communication-style; Robot-verbal-communication-content,Robot-verbal-communication-style,Robot-verbal-communication-content,"The study manipulated the robot's conflict resolution strategy, which is categorized as 'Robot-verbal-communication-style' because it changes how the robot communicates its request (appeal, command, diminution). The presence or absence of thanking, which is a change in what is communicated, is categorized as 'Robot-verbal-communication-content'. The study found that the 'diminution' strategy with reinforcement (thanking) increased trust over time, while the 'command' strategy with reinforcement decreased acceptance over time. This indicates that the style of communication ('Robot-verbal-communication-style') impacted trust, while the content of the communication (thanking or not) did not have a direct impact on trust, but rather on acceptance. The study explicitly states that the diminution strategy with reinforcement increased trustworthiness over time, and the command strategy with reinforcement decreased acceptance over time. The study also states that the compliance rates did not differ significantly between strategies, indicating that the content of the communication did not impact trust.",10.1109/HRI53351.2022.9889492,https://ieeexplore.ieee.org/document/9889492/,"Domestic service robots become increasingly prevalent and autonomous, which will make task priority conﬂicts more likely. The robot must be able to effectively and appropriately negotiate to gain priority if necessary. In previous humanrobot interaction (HRI) studies, imitating human negotiation behavior was effective but long-term effects have not been studied. Filling this research gap, an interactive online study (N = 103) with two sessions and six trials was conducted. In a conﬂict scenario, participants repeatedly interacted with a domestic service robot that applied three different conﬂict resolution strategies: appeal, command, diminution of request. The second manipulation was reinforcement (thanking) of compliance behavior (yes/no). This led to a 3x2x6 mixed-subject design. User acceptance, trust, user compliance to the robot, and selfreported compliance to a household member were assessed. The diminution of a request combined with positive reinforcement was the most effective strategy and perceived trustworthiness increased signiﬁcantly over time. For this strategy only, selfreported compliance rates to the human and the robot were similar. Therefore, applying this strategy potentially seems to make a robot equally effective as a human requester. This paper contributes to the design of acceptable and effective robot conﬂict resolution strategies for long-term use."
"Bainbridge, Wilma A.; Hart, Justin; Kim, Elizabeth S.; Scassellati, Brian",The effect of presence on human-robot interaction,2008,1,65,59,6,"2 participants were excluded due to technical problems in the physical condition, 2 participants were excluded due to technical problems in the virtual condition, 2 participants were excluded due to technical problems in the augmented-virtual condition",Controlled Lab Environment,between-subjects,"Participants were introduced to a robot (physical or video-displayed) and completed a series of book-moving tasks, including a greeting, simple cooperation, unusual cooperation, and proximity task. They then completed a questionnaire.",Participants moved books based on the robot's pointing gestures.,Nico,Humanoid Robots,Research; Social,Manipulation,Object Passing,direct-contact interaction,Participants directly interacted with the robot by moving objects based on its instructions.,real-world,"Participants interacted with a physical robot in a real-world setting, or a video display of the robot.",physical,The robot was physically present in one condition and video-displayed in the other conditions.,wizard of oz (directly controlled),The robot's actions were controlled by an experimenter using a Wizard-of-Oz setup.,Behavioral Measures; Questionnaires,,Video Data; Performance Metrics,"Trust was assessed using behavioral measures (compliance with unusual requests, personal space) and a post-interaction questionnaire.",no modeling,No computational model of trust was used; the study focused on statistical analysis of behavioral and questionnaire data.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The study manipulated the robot's presence (physical vs. video-displayed) and included an unusual task (throwing books in the garbage) to influence trust.,"Participants showed greater trust in the physically present robot, as evidenced by higher compliance with unusual requests and greater personal space afforded to the robot.","Subjects in the physical condition were more likely to comply with the unusual request to throw books in the garbage can, and they also afforded the robot more personal space. The augmented-virtual condition improved upon the ambiguity in the pointing gestures of a virtual robot, but did not change the overall results.",Physical presence of a robot significantly increases trust and respect in human-robot interaction compared to a video-displayed robot.,"The robot pointed to books, and the human moved the books to the indicated locations. The robot also waved to the participant at the beginning of the interaction.",ANOVA; t-test; t-test; ANOVA; t-test; t-test,"The study used ANOVA to compare response times across the three conditions (physical, virtual, and augmented-virtual) for both the simple and unusual task cooperation. T-tests were used to compare the number of subjects who complied with the unusual request (throwing books in the garbage) and the number of subjects who walked around the robot in the proximity task between the physical condition and the virtual condition, and between the physical condition and the augmented-virtual condition. Specifically, t-tests were used to compare the means of two groups for the unusual task cooperation and the proximity task cooperation.",TRUE,Robot-aesthetics; Task-environment,Robot-aesthetics,Task-environment,"The study manipulated the robot's presence by having it be either physically present or video-displayed. This manipulation is categorized as 'Robot-aesthetics' because it changes the visual and physical embodiment of the robot, which directly impacts how it is perceived. The study also manipulated the task environment by having the physical robot in one room and the video-displayed robot in another room, which is categorized as 'Task-environment'. The results showed that the physical presence of the robot (Robot-aesthetics) significantly impacted trust, as participants were more likely to comply with unusual requests and respect personal space when interacting with the physical robot. The task environment did not impact trust, as the environment was kept consistent across conditions, and the differences in room location were not found to influence trust.",10.1109/ROMAN.2008.4600749,http://ieeexplore.ieee.org/document/4600749/,"This study explores how a robot’s physical or virtual presence affects unconscious human perception of the robot as a social partner. Subjects collaborated on simple bookmoving tasks with either a physically present humanoid robot or a video-displayed robot. Each task examined a single aspect of interaction: greetings, cooperation, trust, and personal space. Subjects readily greeted and cooperated with the robot in both conditions. However, subjects were more likely to fulfill an unusual instruction and to afford greater personal space to the robot in the physical condition than in the video-displayed condition. The same tendencies occurred when the virtual robot was supplemented by disambiguating 3-D information."
"Banh, Amy; Rea, Daniel J.; Young, James E.; Sharlin, Ehud",Inspector Baxter: The Social Aspects of Integrating a Robot as a Quality Inspector in an Assembly Line,2015,1,5,5,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed assembly tasks on circuit boards, then submitted them to a robot for inspection. The robot displayed social cues and provided a report. Participants then completed a questionnaire and an interview.",Participants assembled circuit boards by attaching components and then submitted them to a robot for quality inspection.,Baxter,Humanoid Robots; Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research; Social,Evaluation,Survey/Questionnaire Completion,minimal interaction,Participants watched the robot inspect their work and received a report.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical robot.,wizard of oz (directly controlled),The robot's inspection results and report were fabricated by the researchers.,Questionnaires,Godspeed Questionnaire,Video Data,Trust was measured using a questionnaire and video data was collected.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's facial expressions, body language, and inspection reports were manipulated to influence participants' perceptions.","Positive social behaviors increased positive feelings, while negative behaviors decreased positive feelings. Disagreement with the robot's assessment negatively impacted trust.","Participants rated the robot as more intelligent in the negative condition, despite disliking the robot. Some participants interpreted the contradictory condition as mocking.","People's opinions of the robot changed based on the robot's social behaviors, and people rated the robot more negatively if they disagreed with the robot's opinions of their work.","Participants assembled circuit boards by attaching components. The robot then picked up the board, displayed social cues, and provided a report on the quality of the assembly.",Repeated measures ANOVA,"Repeated-measures ANOVAs were used to analyze the questionnaire data. However, due to the low number of participants, no significant results were found. The analysis aimed to identify differences in participants' ratings of the robot across the four experimental conditions.",TRUE,Robot-emotional-display; Robot-nonverbal-communication; Robot-verbal-communication-content,Robot-emotional-display; Robot-nonverbal-communication; Robot-verbal-communication-content,,"The study manipulated the robot's facial expressions (satisfied, unsatisfied, neutral), body language (nodding or shaking head), and the content of the report it provided to participants. These manipulations were done to observe how they affected the participants' perception of the robot. The robot's facial expressions are a direct manipulation of 'Robot-emotional-display'. The robot's head nodding or shaking is a manipulation of 'Robot-nonverbal-communication'. The content of the report, which included statements about errors or acceptance, is a manipulation of 'Robot-verbal-communication-content'. The paper explicitly states that these manipulations were intended to influence participants' perceptions of the robot, including trust. The results showed that positive social behaviors increased positive feelings, while negative behaviors decreased positive feelings, and disagreement with the robot's assessment negatively impacted trust. Therefore, all three manipulated factors impacted trust.",10.1145/2814940.2814955,https://dl.acm.org/doi/10.1145/2814940.2814955,"We are interested in the social implications of working alongside robots. In this paper we look at a humanoid robot quality inspector, acting alongside workers in an assembly line. This setting is viable in small scale assembly lines where human assembly workers provide flexible, rapid assembly. A robotic quality inspector could enhance the quality assurance process, but places the robot in a position of relative seniority to the assembly workers. We present the results of an initial in-lab pilot study designed with our industry collaborators. In our pilot, a humanoid robot visually inspected participants’ assembled products in a shared workspace and provided critiques that follow simple models of robotic social feedback. Our findings suggest that people’s opinions of the robot (trust, impression of intelligence, etc.) changed based on the robot’s social behaviors while it is judging the participant’s work. Additionally, people rated the robot more negatively if they disagreed with the robot’s opinions of their work, regardless of the robot social behavior and the value of its critique."
"Banks, Jaime","Good Robots, Bad Robots: Morally Valenced Behavior Effects on Perceived Mind, Morality, and Trust",2020,2,402,402,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of four conditions (2 agent types x 2 moral valences). They completed demographic questions, pre-stimulus questions, and then watched videos of a human or robot responding to moral dilemmas. After each video, they answered questions about the agent and the behavior. Finally, they completed social evaluation questions.",Participants watched videos of a human or robot responding to moral dilemmas and then evaluated the agent's behavior and their own trust in the agent.,RoboThespian,Humanoid Robots,Research,Evaluation,Survey/Questionnaire Completion,passive observation,"Participants watched videos of the robot and human, without direct interaction.",media,Participants watched videos of the robot and human performing actions.,physical,"The robot was a physical robot, but participants only viewed it through video.",pre-programmed (non-adaptive),The robot's actions were pre-scripted and did not adapt to the user.,Questionnaires; Multidimensional Measures,Multi-Dimensional Measure of Trust (MDMT); Godspeed Questionnaire,,Trust was measured using the Multidimensional Measure of Trust and the Godspeed likeability subscale.,"parametric models (e.g., regression)",Canonical correlation analysis was used to model the relationship between moral evaluations and trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The study manipulated the moral valence of the robot's behavior (upholding or violating moral foundations) and the agent type (human or robot).,Negative behavior ratings were associated with reduced trust ratings.,"Moral judgments of behavior were largely agent-agnostic, but robots were not afforded the potential to be bad without blame. There was a near-absence of mindedness in the observed models.","Bad behavior is seen as an indicator of a bad actor regardless of the performing agent, and this negatively influences perceived morality and trust.",The robot and human each responded to moral dilemmas by giving scripted responses. The human participant watched the videos and answered questions about the agent and their behavior.,ANCOVA; canonical correlation analysis,"MANCOVAs were conducted individually for each moral foundation to assess how evaluations of agent-behavior goodness and responsibility vary by moral foundation, with moral-foundation sacredness and existing agent attitudes as covariates. Canonical correlation analysis was used to explore how moral foundations contribute to evaluations of agent mind, morality, and trust, separately for each agent type, by examining the relationship between sets of variables.",TRUE,Robot-morality; Robot-social-attitude,Robot-morality,Robot-social-attitude,"The study manipulated the moral valence of the robot's behavior by having the robot either uphold or violate moral foundations, which directly relates to 'Robot-morality'. The study also manipulated the agent type (human or robot), which can be considered a manipulation of 'Robot-social-attitude' as it influences how participants perceive the agent's social role and behavior. The results showed that the moral valence of the behavior ('Robot-morality') impacted trust, with negative behavior leading to reduced trust. The agent type ('Robot-social-attitude') did not directly impact trust, as the effect of moral behavior was largely agent-agnostic.",10.1007/s12369-020-00692-3,http://link.springer.com/10.1007/s12369-020-00692-3,"Both robots and humans can behave in ways that engender positive and negative evaluations of their behaviors and associated responsibility. However, extant scholarship on the link between agent evaluations and valenced behavior has generally treated moral behavior as a monolithic phenomenon and largely focused on moral deviations. In contrast, contemporary moral psychology increasingly considers moral judgments to unfold in relation to a number of moral foundations (care, fairness, authority, loyalty, purity, liberty) subject to both upholding and deviation. The present investigation seeks to discover whether social judgments of humans and robots emerge differently as a function of moral foundation-speciﬁc behaviors. This work is conducted in two studies: (1) an online survey in which agents deliver observed/mediated responses to moral dilemmas and (2) a smaller laboratory-based replication with agents delivering interactive/live responses. In each study, participants evaluate the goodness of and blame for six foundation-speciﬁc behaviors, and evaluate the agent for perceived mind, morality, and trust. Across these studies, results suggest that (a) moral judgments of behavior may be agent-agnostic, (b) all moral foundations may contribute to social evaluations of agents, and (c) physical presence and agent class contribute to the assignment of responsibility for behaviors. Findings are interpreted to suggest that bad behaviors denote bad actors, broadly, but machines bear a greater burden to behave morally, regardless of their credit- or blame-worthiness in a situation."
"Banks, Jaime","Good Robots, Bad Robots: Morally Valenced Behavior Effects on Perceived Mind, Morality, and Trust",2020,2,92,92,0,No participants were excluded,Controlled Lab Environment,between-subjects,Participants were assigned to either a human or robot condition and a moral valence (upholding or violating). They interacted with the agent by reading moral dilemmas and listening to the agent's response. They then completed a paper evaluation form and a web-based post-interaction questionnaire.,"Participants interacted with a human or robot by reading moral dilemmas and listening to the agent's response, then evaluated the agent's behavior and their own trust in the agent.",RoboThespian,Humanoid Robots,Research,Social,Conversation,minimal interaction,Participants interacted with the robot by reading prompts and listening to responses.,real-world,Participants interacted with the robot in a real-world setting.,physical,The robot was physically present during the interaction.,wizard of oz (directly controlled),The robot's actions were controlled by a human operator using a Wizard of Oz procedure.,Questionnaires; Multidimensional Measures,Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using the Multidimensional Measure of Trust.,"parametric models (e.g., regression)",Canonical correlation analysis was used to model the relationship between moral evaluations and trust.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,The study manipulated the moral valence of the robot's behavior (upholding or violating moral foundations) and the agent type (human or robot).,"Negative behavior ratings were associated with reduced trust ratings. For robots, low responsibility paired with higher goodness promoted stronger implicit mind, morality, and trust.","Moral judgments of behavior were largely agent-agnostic, but robots were not afforded the potential to be bad without blame. For live interactions with robots, low responsibility paired with higher goodness promoted stronger implicit mind, morality, and trust.","For robots, responsibility is not a consideration in social evaluations such that they may bear a greater burden to behave morally, regardless of their credit-or blame-worthiness in a situation.",The robot and human each responded to moral dilemmas by giving scripted responses. The human participant read the prompts to the agent and listened to the agent's response.,ANCOVA; canonical correlation analysis,"MANCOVAs were conducted individually for each moral foundation to assess how evaluations of agent-behavior goodness and responsibility vary by moral foundation, with associated Bonferroni-corrected significance level. Canonical correlation analysis was used to explore how moral foundations contribute to evaluations of agent mind, morality, and trust, separately for each agent type, by examining the relationship between sets of variables.",TRUE,Robot-morality; Robot-social-attitude,Robot-morality,Robot-social-attitude,"Similar to Study 1, Study 2 manipulated the moral valence of the robot's behavior ('Robot-morality') by having the robot either uphold or violate moral foundations. The agent type (human or robot) was also manipulated, which is categorized as 'Robot-social-attitude'. The results indicated that the moral valence of the behavior ('Robot-morality') impacted trust, with negative behavior leading to reduced trust. The agent type ('Robot-social-attitude') did not directly impact trust, as the effect of moral behavior was largely agent-agnostic, although there was a small interaction effect for fairness. The study also found that for robots, low responsibility paired with higher goodness promoted stronger implicit mind, morality, and trust, further emphasizing the impact of 'Robot-morality' on trust.",10.1007/s12369-020-00692-3,http://link.springer.com/10.1007/s12369-020-00692-3,"Both robots and humans can behave in ways that engender positive and negative evaluations of their behaviors and associated responsibility. However, extant scholarship on the link between agent evaluations and valenced behavior has generally treated moral behavior as a monolithic phenomenon and largely focused on moral deviations. In contrast, contemporary moral psychology increasingly considers moral judgments to unfold in relation to a number of moral foundations (care, fairness, authority, loyalty, purity, liberty) subject to both upholding and deviation. The present investigation seeks to discover whether social judgments of humans and robots emerge differently as a function of moral foundation-speciﬁc behaviors. This work is conducted in two studies: (1) an online survey in which agents deliver observed/mediated responses to moral dilemmas and (2) a smaller laboratory-based replication with agents delivering interactive/live responses. In each study, participants evaluate the goodness of and blame for six foundation-speciﬁc behaviors, and evaluate the agent for perceived mind, morality, and trust. Across these studies, results suggest that (a) moral judgments of behavior may be agent-agnostic, (b) all moral foundations may contribute to social evaluations of agents, and (c) physical presence and agent class contribute to the assignment of responsibility for behaviors. Findings are interpreted to suggest that bad behaviors denote bad actors, broadly, but machines bear a greater burden to behave morally, regardless of their credit- or blame-worthiness in a situation."
"Banks, Jaime; Koban, Kevin; Haggadone, Brad","Avoiding the Abject and Seeking the Script: Perceived Mind, Morality, and Trust in a Persuasive Social Robot",2023,1,412,291,121,121 participants were excluded because they were part of a pilot study,Online Crowdsourcing,between-subjects,"Participants first provided informed consent and completed a check to ensure they could see and hear video. They then completed items for baseline frustration tolerance and demographics. A video was displayed featuring the stimulus robot, followed by a baseline indication of liking, trust, and mental/moral capacities. Participants then viewed a video of the robot making a request based on either logical or moral appeals. Participants could continue to help or stop helping, completing up to 100 puzzles. Finally, they were asked about their carefulness and whether they had seen the robot before.","Participants were asked to solve CAPTCHA puzzles to help the robot learn how to solve them, with the request framed as either a logical or moral appeal.",RoboThespian,Humanoid Robots; Expressive Robots,Research; Social,Social,Persuasion,minimal interaction,Participants watched videos of the robot and interacted by completing a task based on the robot's request.,media,Participants watched videos of the robot interacting with them.,physical,Participants saw videos of a physical robot.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the participant's behavior.,Questionnaires,Mayer and Davis' Trust/Trustworthiness Scales (1999),,Trust was measured using a questionnaire.,"parametric models (e.g., regression)",Regression analyses were used to model the relationship between trust and compliance.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's request was framed as either a logical or moral appeal, and the robot's behavior was manipulated through the content of the video.","The study found that capacity trustworthiness moderated the relationship between mental/moral capacities and compliance for logical appeals, while relational trustworthiness did not moderate the relationship for moral appeals. Perceived agentic capacity was negatively associated with compliance.","Perceived agentic capacity was negatively associated with compliance, which was unexpected. The study also found that self-reported carefulness did not correspond with actual accuracy. The moderation effect of capacity trustworthiness on the relationship between mental/moral capacities and compliance was only found for logical appeals, not moral appeals.","The study's key finding is that the relationship between a robot's perceived mental/moral capacities and compliance is moderated by capacity trustworthiness for logical appeals, and that perceived agentic capacity negatively impacts compliance.","The robot presented a request to participants to solve CAPTCHA puzzles, framed as either a logical or moral appeal. Participants then decided whether to continue solving puzzles, and their compliance was measured by the number of puzzles solved and their self-reported carefulness.",welch's t-tests; Confirmatory factor analysis; Chi-squared; Negative binomial regression; Logistic regression; Linear regression,"The study used Welch's t-tests to compare the effectiveness of the logical and moral appeals in the pilot study. Confirmatory factor analyses (CFA) and chi-squared testing were used to validate the two-factor structure of the trust measure. Negative binomial regression was used to model count data (number of puzzles solved). Logistic regression was used to analyze the binary outcome of initial compliance (whether participants chose to continue solving puzzles). Linear regression was used to analyze the relationship between mind perception, moral agency, and self-reported carefulness, as well as the percentage of correct answers.",TRUE,Robot-verbal-communication-content,,,"The study manipulated the content of the robot's verbal communication by framing its request to solve CAPTCHA puzzles as either a logical or moral appeal. This is explicitly stated in the paper: 'Then, participants viewed a video of the robot delivering a request based on either logical appeals or moral appeals (randomly assigned)'. The manipulation is directly related to the content of the message, not the style or tone, thus 'Robot-verbal-communication-content' is the most appropriate category. The study did not find any direct impact of the manipulated factor on trust itself, but rather on the relationship between perceived mental/moral capacities and compliance, moderated by trustworthiness. Therefore, no factors are listed under 'factors_that_impacted_trust' or 'factors_that_did_not_impact_trust'.",10.1145/3572036,https://dl.acm.org/doi/10.1145/3572036,"Social robots are being groomed for human influence, including the implicit and explicit persuasion of humans. Humanlike characteristics are understood to enhance robots’ persuasive impact; however, little is known of how perceptions of two key human capacities—mind and morality—function in robots’ persuasive potential. This experiment tests the possibility that perceived robot mind and morality will correspond with greater persuasive impact, moderated by relational trustworthiness for a moral appeal and by capacity trustworthiness for a logical appeal. Via an online survey, a humanoid robot asks participants to help it learn to overcome CAPTCHA puzzles to access important online spaces—either on grounds that it is logical or moral to do so. Based on three performance indicators and one self-report indicator of compliance, analysis indicates that (a) seeing the robot as able to perceive and act on the world selectively improves compliance, and (b) perceiving agentic capacity diminishes compliance, though capacity trustworthiness can moderate that reduction. For logical appeals, social-moral mental capacities promote compliance, moderated by capacity trustworthiness. Findings suggest that, in this compliance scenario, the accessibility of schemas and scripts for engaging robots as social-moral actors may be central to whether/how perceived mind, morality, and trust function in machine persuasion."
"Bartlett, Cade E.; Cooke, Nancy J.",Human-Robot Teaming in Urban Search and Rescue,2015,1,16,16,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were assigned to either a remote or intelligent condition. In the remote condition, the external participant created a search plan and directed the internal participant. In the intelligent condition, the internal participant created the search plan and acted more autonomously. Both conditions involved a virtual search and rescue task.","Participants completed a simulated search and rescue task in a virtual environment, with one participant acting as the operator (external) and the other as the robot (internal).",Unspecified,Other,Research,Navigation,Remote Navigation,minimal interaction,Participants interacted through a virtual environment with verbal communication.,simulation,The interaction took place in a virtual environment using Minecraft.,simulated,The robot was represented by a participant in a virtual environment.,shared control (fixed rules),The robot's actions were either directly controlled by the external participant or were based on a pre-determined plan.,Questionnaires,,Speech Data,Trust was measured using a self-report questionnaire and speech data was collected.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The level of autonomy of the internal participant was manipulated, with the intelligent condition allowing for more independent action, and the remote condition requiring explicit instructions from the external participant. This was intended to influence the level of trust in the internal participant.",There was no significant difference in trust between the conditions.,"The study found that the intelligent condition resulted in higher team performance and lower workload for the external participant, but there was no significant difference in trust between the conditions. The external participant in the intelligent condition had a higher SA, which was unexpected.","The intelligent condition, where the internal participant had more autonomy, resulted in higher team performance compared to the remote condition.","The internal participant navigated a virtual environment, searching for targets, while the external participant monitored the environment and marked the map. In the remote condition, the external participant directed the internal participant's actions, while in the intelligent condition, the internal participant acted more autonomously.",ANOVA; t-test,"The study used a 2x2 ANOVA design to analyze data, with the two conditions (intelligent and remote) being between teams and the internal/external roles being within teams. Univariate ANOVA with covariates was used to compare team performance between the two conditions. T-tests were used to compare the external participant's situation awareness (SA) and the internal participant's SA between the two conditions. Another 2x2 ANOVA was used to analyze the trust data, and another 2x2 ANOVA was used to analyze the workload data.",TRUE,Robot-autonomy,,Robot-autonomy,"The study explicitly manipulated the level of autonomy of the 'internal' participant, who was acting as the robot. In the 'remote' condition, the internal participant was directly controlled by the external participant, while in the 'intelligent' condition, the internal participant had more autonomy and could make decisions independently. This manipulation of autonomy is described in the 'Procedure' section: 'In the remote condition only the external participant was given the non-matching map, along with instructions to draw a search plan which they must adhere to, or communicate changes to the plan if they deviate from it. The internal participant was instructed to act only on the external participant's explicit directions as they carry out the task. In the intelligent condition the internal participant independently made a search plan and both the internal and external participants were given a copy of it... The internal participant in the intelligent condition was not instructed to only act on the external participant's instructions, allowing him or her to act freely and according to the internal's own judgment.' The results section states that 'When comparing how much internal remote (M = 3.29, SD = 0.96), internal intelligent (M = 3.04, SD = 1.18), external remote (M = 3.04, SD = 0.65), and external intelligent (M = 3.13, SD = 0.66) trusted their teammate there was no significant difference for either role or condition [F(1,36) = 0.08, p = 0.774] or for interaction effects [F(1,36) = 0.36, p = 0.552] at the p<0.05 level.' This indicates that while autonomy was manipulated, it did not significantly impact trust levels. Therefore, 'Robot-autonomy' is the only factor manipulated, and it did not impact trust.",10.1177/1541931215591051,http://journals.sagepub.com/doi/10.1177/1541931215591051,"Although current urban search and rescue (USAR) robots are little more than remotely controlled cameras, the end goal is for them to work alongside humans as trusted teammates. Natural language communications and performance data are collected as a team of humans works to carry out a simulated search and rescue task in an uncertain virtual environment. Conditions are tested emulating a remotely controlled robot versus an intelligent one. Differences in performance, situation awareness (SA), trust, and workload are measured. The Intelligent robot condition resulted in higher levels of performance and operator SA."
"Bass, B.; Fink, N.; Price, M.; Sturre, L.; Hentschel, E.; Pak, R.","How Does Anthropomorphism Affect User's Trust, Compliance, and Performance On a Decision Making Automation Task?",2011,1,60,,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were assigned to one of three conditions: no automation, text-based automation, or anthropomorphic automation. They answered 30 diabetes management questions, with automation providing suggestions in the two automation conditions. Participants rated their confidence in their answers and the automation after each question. Post-task surveys assessed trust, satisfaction, and workload.","Participants answered 30 multiple-choice questions about diabetes management, with or without automation assistance.",Unspecified,Other,Care; Research,Evaluation,Text Evaluation,minimal interaction,Participants interacted with a computer interface and received text-based or image-based suggestions.,media,The study used a computer interface with text and images.,simulated,The robot was represented by a text suggestion or an image of a doctor on a smartphone interface.,shared control (fixed rules),"The automation provided suggestions based on pre-set rules, but the user made the final decision.",Questionnaires; Custom Scales,,Performance Metrics,"Trust was measured using Likert scales after each question and post-task surveys, along with performance metrics.",no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the presence and anthropomorphism of automation, with the anthropomorphic condition including an image of a doctor. The automation's reliability was also manipulated by including incorrect answers.","The study expected that anthropomorphic automation would increase trust, compliance, and performance compared to text-based automation and no automation.","The study expected that anthropomorphic automation would lead to higher trust, compliance, and performance, but the actual results are not reported in this paper.","The study aimed to investigate the effect of anthropomorphic automation on user's trust, compliance, and performance in a diabetes management task.","The robot, represented by a smartphone application, provided suggestions for diabetes management decisions. The human participant answered multiple-choice questions and could choose to follow the robot's suggestions or not.",ANOVA,"A multiple analysis of variance (MANOVA) will be used to analyze the data. The response factor will be the type of automation (no automation, text-based automation, and anthropomorphic automation). The MANOVA will be used to examine the effects of automation type on trust, compliance, subjective satisfaction, and performance.",TRUE,Robot-interface-design; Robot-accuracy,Robot-interface-design,Robot-accuracy,"The study manipulated the presence and anthropomorphism of automation. The 'Robot-interface-design' was manipulated by presenting the automation as either text-based or with an image of a doctor on a smartphone interface. This change in visual presentation is a manipulation of the interface design. The study also manipulated the 'Robot-accuracy' by including incorrect answers in the automation's suggestions, with a reliability of 67%. The paper states that they expect that the anthropomorphic interface will increase trust, compliance, and performance, indicating that the interface design is expected to impact trust. The paper does not state that the accuracy of the robot will impact trust, but rather that it is a factor that will be manipulated. The paper does not report the results of the study, so we cannot determine if the accuracy manipulation impacted trust.",10.1177/1071181311551407,http://pro.sagepub.com/lookup/doi/10.1177/1071181311551407,"Automation is becoming increasingly prevalent, as new technology develops to replace tasks humans once performed. Users‘ adoption of the technology depends on several factors, particularly how much they trust the automation. One important, yet understudied factor that may influence trust in automation is how  ̳humanlike‘ the interface is, or how anthropomorphic the features of the computer are. The current study seeks to examine the effect of anthropomorphic automation on user‘s trust, compliance, and performance on a challenging diabetes related decision-making task. We hypothesize that anthropomorphic automation will engender more social responses (e.g., trust), which would affect compliance and performance."
"Baumann, Anna-Elisabeth; Goldman, Elizabeth J.; Meltzer, Alexandra; Poulin-Dubois, Diane",People Do Not Always Know Best: Preschoolers’ Trust in Social Robots,2023,2,95,95,17,"8 participants were excluded due to parental or sibling interference, 2 participants were excluded due to experimenter error, 1 participant was excluded due to prior robot exposure, 2 participants were excluded due to completing the study on a screen deemed too small, 4 participants were excluded due to fussiness",Online Crowdsourcing,mixed design,"Participants completed a naive biology task, followed by a selective trust task, and then the robot trial of the naive biology task again. Parents also completed the CSUS and CPBQ questionnaires.","Participants were asked to identify the correct labels for familiar and novel objects, and to categorize the internal properties of animals, artifacts, and a robot.",Nao,Humanoid Robots; Expressive Robots,Research; Social,Social,Tutoring,minimal interaction,Participants interacted with the robot through pre-recorded videos.,media,The interaction was presented through pre-recorded videos.,physical,"The robot was a physical robot, but the interaction was through video.",pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Behavioral Measures,,Video Data,Trust was assessed through behavioral measures of endorsement and asking behavior.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot was presented as competent by labeling familiar objects correctly, while a human informant labeled them incorrectly. The robot was also humanoid in appearance.","5-year-olds trusted the competent robot more than the incompetent human, while 3-year-olds showed no preference.","3-year-olds showed no clear preference for either the robot or the human informant, despite the robot being competent. 5-year-olds correctly categorized the robot as mechanical but still chose to learn from it.","5-year-old children preferred to learn from a competent robot over an incompetent human, while 3-year-olds showed no preference.",The robot and human each labeled familiar and novel objects. The human participant was asked to endorse one of the labels provided by the robot or the human.,ANOVA; t-test; Mann-Whitney U,"The study used repeated-measures ANOVA to compare the proportion of correct trials across different trial types (ask, endorse, judgment) and age groups. Independent t-tests were used to further investigate significant interactions between trial type and age. Additionally, Mann-Whitney tests were used as a non-parametric alternative for some comparisons after normality corrections. The study also used repeated-measures ANOVA to compare the proportion of correct trials across different object types (animal, artifact, robot) and age groups, with a Greenhouse-Geisser correction applied due to violation of sphericity. Chance analyses were also conducted to assess performance against random chance.",TRUE,Robot-accuracy; Robot-aesthetics,Robot-accuracy,Robot-aesthetics,"The study manipulated 'Robot-accuracy' by having the robot (Nao) label familiar objects correctly while a human informant labeled them incorrectly. This was done to test if children would trust the competent robot more. The study also implicitly manipulated 'Robot-aesthetics' by using a humanoid robot (Nao). The paper explicitly states that the robot's morphology was manipulated across the two studies, but within this study, the robot's humanoid appearance was a constant factor. The results showed that 'Robot-accuracy' impacted trust, as 5-year-olds preferred the competent robot. The study found that 'Robot-aesthetics' did not impact trust within this study, as the 3-year-olds did not show a preference for either the robot or the human, despite the robot's humanoid appearance.",10.1080/15248372.2023.2178435,https://www.tandfonline.com/doi/full/10.1080/15248372.2023.2178435,"In this paper, we investigated whether Canadian preschoolers prefer to learn from a competent robot over an incompetent human using the classic trust paradigm. An adapted Naive Biology task was also admi­ nistered to assess children’s perception of robots. In Study 1, 3-yearolds and 5-year-olds were presented with two informants; A social, humanoid robot (Nao) who labeled familiar objects correctly, while a human informant labeled them incorrectly. Both informants then labeled unfamiliar objects with novel labels. It was found that 3-yearold children equally endorsed the labels provided by the robot and the human, but 5-year-old children learned significantly more from the competent robot. Interestingly, 5-year-olds endorsed Nao’s labels even though they accurately categorized the robot as having mechanical insides. In contrast, 3-year-old children associated Nao with biological or mechanical insides equally. In Study 2, new samples of 3-year-olds and 5-year-olds were tested to determine whether the human-like appearance of the robot informant impacted children’s trust judg­ ments. The procedure was identical to that of Study 1, except that a non-humanoid robot, Cozmo, replaced Nao. It was found that 3-yearold children still trusted the robot and the human equally and that 5-year-olds preferred to learn new labels from the robot, suggesting that the robot’s morphology does not play a key role in their selective trust strategies. It is concluded that by 5 years of age, preschoolers show a robust sensitivity to epistemic characteristics (e.g., compe­ tency), but that younger children’s decisions are equally driven by the animacy of the informant."
"Baumann, Anna-Elisabeth; Goldman, Elizabeth J.; Meltzer, Alexandra; Poulin-Dubois, Diane",People Do Not Always Know Best: Preschoolers’ Trust in Social Robots,2023,2,105,89,16,"10 participants were excluded due to parental or sibling interference, 1 participant was excluded due to familiarity with the robot, 1 participant was excluded due to technical difficulties, 4 participants were excluded due to fussiness",Online Crowdsourcing,mixed design,"Participants completed a naive biology task, followed by a selective trust task, and then the robot trial of the naive biology task again. Parents also completed the CSUS and CPBQ questionnaires.","Participants were asked to identify the correct labels for familiar and novel objects, and to categorize the internal properties of animals, artifacts, and a robot.",Cozmo,Expressive Robots,Research; Social,Social,Tutoring,minimal interaction,Participants interacted with the robot through pre-recorded videos.,media,The interaction was presented through pre-recorded videos.,physical,"The robot was a physical robot, but the interaction was through video.",pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Behavioral Measures,,Video Data,Trust was assessed through behavioral measures of endorsement and asking behavior.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot was presented as competent by labeling familiar objects correctly, while a human informant labeled them incorrectly. The robot was non-humanoid in appearance.","5-year-olds trusted the competent robot more than the incompetent human, while 3-year-olds showed no preference.","3-year-olds showed no clear preference for either the robot or the human informant, despite the robot being competent and non-humanoid. 5-year-olds correctly categorized the robot as mechanical but still chose to learn from it.","5-year-old children preferred to learn from a competent robot over an incompetent human, while 3-year-olds showed no preference, regardless of the robot's human-like appearance.",The robot and human each labeled familiar and novel objects. The human participant was asked to endorse one of the labels provided by the robot or the human.,ANOVA; t-test; Mann-Whitney U; Linear regression; Partial least squares,"The study employed repeated-measures ANOVA to compare the proportion of correct trials across different trial types (ask, endorse, judgment) and age groups, and to compare the proportion of correct trials across different object types (animal, artifact, robot) and age groups, with a Greenhouse-Geisser correction applied due to violation of sphericity. Independent t-tests were used to further investigate significant interactions. Mann-Whitney tests were used as a non-parametric alternative for some comparisons. A stepwise linear regression was used to examine the influence of age, naïve biology scores, and questionnaire scores (CPBQ and CSUS) on the endorse trials. Structural equation modeling (SEM) was used to investigate potential indirect effects of variables mediating the association between age and trust scores. Chance analyses were also conducted to assess performance against random chance.",TRUE,Robot-accuracy; Robot-aesthetics,Robot-accuracy,Robot-aesthetics,"The study manipulated 'Robot-accuracy' by having the robot (Cozmo) label familiar objects correctly while a human informant labeled them incorrectly. This was done to test if children would trust the competent robot more. The study also manipulated 'Robot-aesthetics' by using a non-humanoid robot (Cozmo). The paper explicitly states that the robot's morphology was manipulated across the two studies. The results showed that 'Robot-accuracy' impacted trust, as 5-year-olds preferred the competent robot. The study found that 'Robot-aesthetics' did not impact trust, as the 3-year-olds did not show a preference for either the robot or the human, despite the robot's non-humanoid appearance. The comparison between the two studies also shows that the robot's aesthetics did not impact trust.",10.1080/15248372.2023.2178435,https://www.tandfonline.com/doi/full/10.1080/15248372.2023.2178435,"In this paper, we investigated whether Canadian preschoolers prefer to learn from a competent robot over an incompetent human using the classic trust paradigm. An adapted Naive Biology task was also admi­ nistered to assess children’s perception of robots. In Study 1, 3-yearolds and 5-year-olds were presented with two informants; A social, humanoid robot (Nao) who labeled familiar objects correctly, while a human informant labeled them incorrectly. Both informants then labeled unfamiliar objects with novel labels. It was found that 3-yearold children equally endorsed the labels provided by the robot and the human, but 5-year-old children learned significantly more from the competent robot. Interestingly, 5-year-olds endorsed Nao’s labels even though they accurately categorized the robot as having mechanical insides. In contrast, 3-year-old children associated Nao with biological or mechanical insides equally. In Study 2, new samples of 3-year-olds and 5-year-olds were tested to determine whether the human-like appearance of the robot informant impacted children’s trust judg­ ments. The procedure was identical to that of Study 1, except that a non-humanoid robot, Cozmo, replaced Nao. It was found that 3-yearold children still trusted the robot and the human equally and that 5-year-olds preferred to learn new labels from the robot, suggesting that the robot’s morphology does not play a key role in their selective trust strategies. It is concluded that by 5 years of age, preschoolers show a robust sensitivity to epistemic characteristics (e.g., compe­ tency), but that younger children’s decisions are equally driven by the animacy of the informant."
"Becker, Dennis; Rueda, Diana; Beese, Felix; Torres, Brenda Scarleth Gutierrez; Lafdili, Myriem; Ahrens, Kyra; Fu, Di; Strahl, Erik; Weber, Tom; Wermter, Stefan",The Emotional Dilemma: Influence of a Human-like Robot on Trust and Cooperation,2023,1,47,41,6,"3 participants were excluded due to technical issues, 3 participants were discarded based on the control question",Controlled Lab Environment,between-subjects,Participants played a coin entrustment game with either an emotional or non-emotional robot for 16 rounds; the robot defected in round 8 and then attempted to regain trust.,"Participants played a coin entrustment game with a robot, deciding how many coins to entrust and whether to return the robot's coins.",NICO,Humanoid Robots; Expressive Robots,Research; Social,Game,Economic Game,minimal interaction,"Participants interacted with the robot through a game interface, with no physical touch.",real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),"The robot's actions were pre-programmed and did not adapt to the participant's behavior, except for the emotional expressions.",Questionnaires; Behavioral Measures,Godspeed Questionnaire; Discrete Emotions,Performance Metrics,Trust was measured using questionnaires and behavioral data from the coin entrustment game.,no modeling,The study did not use computational models of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's emotional expressions (facial, gestural, and vocal) were manipulated to be either emotional or neutral, influencing the perceived animacy and emotionality of the robot.",The emotional robot received less trust and participants were less likely to cooperate compared to the neutral robot.,"The emotional robot was perceived as more animated but also induced more anxiety, leading to reduced trust and cooperation. The perceived intelligence of the robot was positively correlated with trust.","A robot that displays human-like emotions can induce more anxiety, leading to reduced trust and cooperation compared to a neutral robot.","The robot displayed either emotional or neutral expressions and made decisions in the coin entrustment game, while the human participant decided how many coins to entrust and whether to return the robot's coins.",t-test; Mann-Whitney U; Fisher's exact test; Spearman correlation,"The study used several statistical tests to analyze the data. Student's t-tests were used to compare the means of the Godspeed questionnaire items and the Discrete Emotions questionnaire items between the emotional and non-emotional robot groups. Mann-Whitney U tests were used to compare the amount of coins entrusted between the two groups and across different rounds. Fisher's exact tests were used to compare the cooperation rates between the two groups and across different rounds. Finally, Spearman correlation was used to estimate the relationship between the questionnaire items and the behavioral measures (entrusted coins and cooperation rate).",TRUE,Robot-emotional-display,Robot-emotional-display,,"The study explicitly manipulated the robot's emotional display. The robot either displayed emotional expressions (happy or sad) or neutral expressions through facial expressions, gestures, and vocal interjections. This manipulation is described in the 'Emotional Expressions' section, where it details how the emotional robot used interjections with emotional prosody, happy/sad gestures, and corresponding facial expressions, while the non-emotional robot used neutral gestures and expressions. The results section shows that the emotional robot was perceived as more animated and induced more anxiety, leading to reduced trust and cooperation. Therefore, 'Robot-emotional-display' is the factor that impacted trust. There were no other factors manipulated in the study.",,http://arxiv.org/abs/2307.02924,"Increasing anthropomorphic robot behavioral design could affect trust and cooperation positively. However, studies have shown contradicting results and suggest a taskdependent relationship between robots that display emotions and trust. Therefore, this study analyzes the effect of robots that display human-like emotions on trust, cooperation, and participants’ emotions. In the between-group study, participants play the coin entrustment game with an emotional and a non-emotional robot. The results show that the robot that displays emotions induces more anxiety than the neutral robot. Accordingly, the participants trust the emotional robot less and are less likely to cooperate. Furthermore, the perceived intelligence of a robot increases trust, while a desire to outcompete the robot can reduce trust and cooperation. Thus, the design of robots expressing emotions should be task dependent to avoid adverse effects that reduce trust and cooperation."
"Beelen, Thomas; Velner, Ella; Truong, Khiet P.; Ordelman, Roeland; Huibers, Theo; Evers, Vanessa",Children’s Trust in Robots and the Information They Provide,2023,1,35,30,5,5 participants were excluded because they did not know what to answer more than once on the trust questionnaire,Educational Setting,within-subjects,"Participants interacted with two robots (trustworthy and untrustworthy) via video call, completed a quiz with each robot, and then participated in a semi-structured interview.",Participants played a quiz with two different robots and indicated whether they accepted the robot's answer.,Furhat,Expressive Robots,Research; Social,Game,Puzzle/Logic Game,minimal interaction,"Participants interacted with the robot via video call, with limited direct interaction.",media,"The interaction was conducted via video call, providing a visual representation of the robot.",physical,"The robot was a physical Furhat robot, but the interaction was mediated through a video call.",wizard of oz (directly controlled),The robot was teleoperated by a researcher who controlled its speech and actions.,Questionnaires; Behavioral Measures,Schaefer's Trust Questionnaire/Scale; Godspeed Questionnaire,Video Data; Speech Data; Performance Metrics,Trust was measured using questionnaires and a behavioral measure of information acceptance.,no modeling,Trust was not modeled computationally; the analysis used descriptive statistics and statistical tests.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The trustworthiness of the robot was manipulated by varying its capabilities, displayed emotions, and past accuracy, which was intended to influence the children's trust in the robot.","The manipulation was successful in creating two levels of trust, with the trustworthy robot being perceived as more trustworthy, likeable, intelligent, and knowledgeable. However, trust did not significantly impact information acceptance.","The study found that children tend to accept information from the robot, regardless of the trust condition. There was a significant effect of one question in the quiz on information acceptance. Children often compared robots to humans, showing a tendency to anthropomorphize robots.","The study successfully manipulated trust in robots, but found no significant impact of trust on children's acceptance of information provided by the robot.","The robot presented quiz questions and suggested answers, while the human participant read the questions aloud and chose whether to accept the robot's answer.",Shapiro-Wilk; wilcoxon-pratt signed-rank tests; Fisher's exact test; Generalized linear models,"The study used a Shapiro-Wilk test to check for normality of data distribution. Since the data was not normally distributed, Wilcoxon-Pratt signed-rank tests were used to assess the effect of the trust condition on trust measures (both measurement points) and on perception measures (likeability, intelligence, knowledgeability). Fisher's exact test was used to check the effect of a specific question on information acceptance. A generalized linear mixed model (GLMM) was used to examine if the trust condition influenced the likelihood of accepting the information presented by the robot.",TRUE,Robot-emotional-display; Robot-accuracy; Robot-social-attitude,Robot-emotional-display; Robot-accuracy; Robot-social-attitude,,"The study manipulated the robot's trustworthiness through three key aspects: the robot's capabilities (technological trust), its displayed emotions (social trust), and its past accuracy (epistemic trust). These were integrated into a video testimonial, the robot's behavior during the interaction, and the quiz answers. The 'trustworthy' robot (Bart) was presented as capable, reliable, and displayed positive emotions, while the 'untrustworthy' robot (Henk) was presented as incapable, displayed negative emotions, and made errors. This directly corresponds to 'Robot-emotional-display' as the robot's emotional expressions were intentionally varied. 'Robot-accuracy' is chosen because the trustworthy robot always gave the correct answer, while the untrustworthy robot always gave an incorrect answer, directly impacting task performance. 'Robot-social-attitude' is chosen because the trustworthy robot showed benevolence by stating it was there to help, while the untrustworthy robot remained neutral or slightly negative in its expressions. The paper explicitly states that these manipulations were intended to influence the children's trust in the robot, and the results show that the manipulation was successful in creating two levels of trust. The factors that impacted trust were all the manipulated factors, as the study found significant differences in trust levels between the two conditions. There were no factors that were manipulated that did not impact trust.",10.1145/3544549.3585801,https://dl.acm.org/doi/10.1145/3544549.3585801,"Previous work has shown that children tend to trust embodied conversational agents such as social robots. Also, that children have difficulty assessing the credibility of information. The study reported in this paper addresses how children’s attitudes toward and trust in a robot affect their acceptance of information provided by the robot. We conducted a within-subjects study (N=30) where children engaged with a ‘trustworthy’ versus an ‘untrustworthy’ robot. Due to the pandemic period, this interaction was carried out via video call. The children played a quiz with the robot where we measured whether they accepted the information provided by the robot. Results show that the manipulation of trustworthiness was successful. We did not find evidence for a causal relationship between trust in the robot and acceptance of the information. Furthermore, semi-structured interviews offered a more in-depth understanding of how children perceived the two different robots and their preference for the trustworthy robot."
"Bejarano, Alexandra; Williams, Tom","No Name, No Voice, Less Trust: Robot Group Identity Performance, Entitativity, and Trust Distribution",2023,1,94,94,0,No participants were excluded,Online Crowdsourcing,between-subjects,Participants watched two videos of a human interacting with a group of three robots at an airport help desk. The first video showed the robots introducing themselves using different identity performance strategies. The second video showed the robots making an error. Participants completed questionnaires after each video.,Participants watched videos of a human interacting with a group of robots and answered questionnaires about their perceptions of the robots' entitativity and trust.,Nao,Humanoid Robots,Social; Research,Social,Social Perception,passive observation,Participants passively observed videos of a human interacting with robots.,media,The interaction was presented through videos of a human interacting with robots.,physical,The robots were physically present in the videos.,pre-programmed (non-adaptive),The robots followed a pre-programmed script without adapting to the user.,Questionnaires; Multidimensional Measures,Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using the Reliability and Capability subscales of the MDMT questionnaire.,no modeling,Trust was not modeled computationally; descriptive statistics were used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers manipulated the robot group's identity performance strategy by changing the names and voices used by the robots, which influenced how participants perceived the group.","The identity performance strategy influenced how trust was distributed among the robots, with the 'Only One' strategy leading to uneven trust distribution and the 'All Unique' and 'All Shared' strategies leading to even trust distribution. Trust loss was evenly distributed across all conditions.","The study found that the 'Only One' strategy led to lower perceived entitativity and uneven trust distribution, while the 'All Unique' and 'All Shared' strategies resulted in higher entitativity and even trust distribution. Contrary to the hypothesis, entitativity did not mediate the relationship between identity performance strategies and trust distribution. Trust loss was evenly distributed across all conditions, which was unexpected.","The presence or absence of identities within a robot group influences how humans distribute trust amongst loci within that group, with unnamed, voiceless robot bodies being seen as less trustworthy.","The human participant in the video approached a help desk and asked for a boarding pass. The robots then processed the request and provided the boarding pass. In the second video, the human returned to the help desk to report an error with the boarding pass, and the robots provided a new one.",Bayesian ANOVA; Bayesian t-test; Bayesian ANOVA,The study used Bayesian Repeated Measures ANOVAs (RM-ANOVAs) with Bayes Factor (BF) analyses to examine the effects of identity performance strategy on entitativity and trust distribution. Inclusion BFs were calculated through Bayesian Model Averaging. Post-hoc Bayesian t-tests were used to further analyze significant effects. A Bayesian ANOVA was used to analyze trust loss distribution. The RM-ANOVAs were used to compare the means of different conditions and to assess the effect of the independent variables on the dependent variables. The t-tests were used to compare the means of two groups. The ANOVA was used to compare the means of multiple groups.,TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the robot group's identity performance strategy by changing the names and voices used by the robots. This is a manipulation of the content of the robot's verbal communication, as the specific names and whether they were shared or unique changed the perceived identity of the robots. The study found that this manipulation of verbal communication content impacted how trust was distributed among the robots, with the 'Only One' strategy leading to uneven trust distribution and the 'All Unique' and 'All Shared' strategies leading to even trust distribution. The study also found that the manipulation of verbal communication content impacted the perceived entitativity of the robot group. There were no other factors manipulated in the study. The study did not find any factors that did not impact trust.",,,"Human interactions with robot groups are more complex than interactions with individual robots. This is especially true for groups of robots that do not have humanlike 1-1 associations between bodies and identities, such as when multiple robots share a single identity. This is further complicated by the lack of direct observability of the relationship between body and identity, which may be inferred by users on the basis of various robot group identity performance strategies. Previous research on Deconstructed Trustee Theory has argued that this complexity is critical, as different perceived bodyidentity configurations may lead users to build and develop trust in distinct ways. In this paper, we thus investigate (n=94) the ways that different robot group identity performance strategies might influence the distribution of trust amongst robot group members, as well as the impact of these strategies on perceptions of robot group entitativity."
"Ben Ajenaghughrure, Ighoyota; Sousa, Sónia; Lamas, David",Psychophysiological Modelling of Trust in Technology: Comparative Analysis of Psychophysiological Signals:,2021,1,31,31,1,1 participant was excluded due to noisy EEG data,Controlled Lab Environment,within-subjects,Participants played a driving game in a virtual reality environment with four risk conditions. Psychophysiological data was collected during the game. Participants completed trust questionnaires before and after each game session.,Participants played a virtual reality driving game where they had to navigate an autonomous vehicle through different risk conditions.,Unspecified,Autonomous Vehicles,Research,Game,Other Game subtask: Participants played a driving game with varying risk levels.,minimal interaction,Participants interacted with a simulated autonomous vehicle in a virtual environment.,simulation,Participants were immersed in a virtual reality driving simulation.,simulated,The robot was a simulated autonomous vehicle in a virtual environment.,fully autonomous (limited adaptation),"The autonomous vehicle operated without direct human control, but with limited adaptation to the environment.",Behavioral Measures; Physiological Measures; Questionnaires,Trust in Technology Questionnaire,Eye-tracking Data; Physiological Signals; Performance Metrics,"Trust was assessed using questionnaires, behavioral measures (joystick usage), and physiological signals (EEG, EDA, ECG, EMG, eye-tracking).","deep learning (e.g., neural networks, reinforcement learning)",A stack ensemble trust classifier model was developed using machine learning algorithms.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The risk level of the driving scenarios was manipulated to influence trust. The risk was varied across four conditions.,Trust decreased as the risk level increased. Participants relied less on the autonomous vehicle in high-risk conditions.,"EEG and multimodal psychophysiological signals were found to be the most reliable for assessing trust. The majority of the selected features in the multimodal model were EEG features, raising questions about feature selection for multimodal signals.",EEG signals are the most reliable for assessing users' trust in real-time during interaction with an autonomous system.,"The robot (simulated autonomous vehicle) navigated a virtual environment, and the human participant monitored the vehicle and could take control using a joystick when needed.",ANOVA,"A one-way repeated measures ANOVA was used to compare trust scores obtained from participants before the game and after each game session (very high risk, high risk, low risk, no risk). It was also used to compare joystick usage (non-reliance) across the different risk conditions. The purpose was to determine if there were statistically significant differences in trust and reliance based on the risk level of the driving scenarios.",TRUE,Task-constraints,Task-constraints,,"The study manipulated the risk level of the driving scenarios, which directly impacts the task constraints. The paper states, 'The game affords participants the opportunity to experience an AV under four categories of risk conditions...'. This manipulation of risk (very-high, high, low, no risk) directly influences the performance limits and challenges faced by the participant during the driving task. The results show that 'users trust before beginning the game (initial trust) was higher with statistical significant difference when compared to users trust during the very high risk and high risk game session were lower'. This indicates that the manipulation of risk, a task constraint, impacted trust. There were no factors that were manipulated that did not impact trust.",10.5220/0010237701610173,https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0010237701610173,"Trust, Machine Learning, Psychophysiology, Autonomous Vehicle, Artificial Intelligence."
"Bender, Nadine; Faramawy, Samir El; Kraus, Johannes Maria; Baumann, Martin",The role of successful human-robot interaction on trust,2021,1,15,15,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants first completed a pre-questionnaire, then carried a table with a human partner, and then with a robot. After the interaction, participants completed post-questionnaires and interviews.","Participants carried a table with a human partner and then with a robot, moving it between two rooms.",Unspecified,Mobile Manipulators; Collaborative Robots (Cobots); Service and Assistive Robots,Care; Research,Manipulation,Object Passing,direct-contact interaction,Participants physically collaborated with the robot to carry a table.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,The study involved a physical robot interacting with participants.,shared control (fixed rules),"The robot operated autonomously but with fixed rules, and the human had control over the robot's movement.",Questionnaires,Checklist for Trust between People and Automation,,Trust was measured using a questionnaire.,no modeling,"Trust was not modeled computationally, and only descriptive statistics were used.",Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers manipulated the success of the interaction by having the robot stop if the participant steered it too close to an obstacle, creating a failure condition.","Successful interaction led to higher trust and perceived adaptiveness, while unsuccessful interaction led to lower trust and perceived adaptiveness.","The study found that perceived adaptiveness and trust were highly correlated and significantly lower in the failure group, highlighting the importance of successful interaction for trust.",Successful cooperative task completion has a positive effect on trust and perceived adaptiveness of the robot.,"The robot assisted the human in carrying a table, and the human controlled the direction and movement of the table, with the robot stopping if it got too close to an obstacle.",Mann-Whitney U; Spearman correlation,The study used the Mann-Whitney-U test to compare the means of different groups (successful vs. unsuccessful interaction) on various constructs like trust (TRU) and perceived adaptiveness (PAD). Spearman's rank correlation coefficient was used to analyze the monotonic correlations between different variables.,TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the success of the interaction by having the robot stop if the participant steered it too close to an obstacle, creating a failure condition. This directly impacts the robot's accuracy in completing the task, as it sometimes fails to complete the task due to the obstacle avoidance. The paper states, 'An interaction failure was recorded when the robot stopped because the participant steered the robot too close to an obstacle and the robot had to avoid a collision.' This manipulation of success/failure directly relates to the robot's accuracy in performing the task. The results showed that successful interaction led to higher trust, indicating that the manipulated factor, Robot-accuracy, impacted trust. There were no other factors manipulated in the study.",,,"The foundation of this paper is an experiment of 15 participants interacting directly with an autonomous robot. The task for the participants was to carry a table, in two different setups, together with a robot, which is intended to support older people with heavy lifting tasks. By collecting and analyzing observational, quantitative, and qualitative data the interaction was investigated with a specific emphasis on trust in the robot. The overall aim was a better understanding of people’s emotional and evaluative reactions when they engage with a functioning robot in a relatable everyday scenario. This study shows that successful cooperative task completion has a positive effect on trust and other related evaluations, like the perceived adaptiveness regarding the robot’s behavior."
"Benton, Rachel A.; McDonald, Mitchell D.; Sackett, Maxwell A.; Poncy, John R.; Collins McLaughlin, Anne; Rovira, Ericka M.; Novitzky, Michael",Maybe I Can Do Better Myself: The Impact of Automation Levels on HRI,2022,1,64,64,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants completed surveys, cognitive tasks, and then played two 8-minute iterations of a capture the flag game with a robot teammate, followed by a final trust survey.",Participants played a digital capture the flag game with a robot teammate against two robot opponents.,Unspecified,Mobile Robots,Research,Game,Cooperative Game,minimal interaction,Participants interacted with a robot teammate through a digital game interface.,simulation,The interaction took place within a digital game simulation.,simulated,The robot was represented as a digital character within the game.,shared control (fixed rules),"The robot operated with either low or high autonomy, with fixed rules for each level.",Questionnaires,Trust Perception Scale - HRI,,Trust was measured using a pre- and post-interaction questionnaire.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's autonomy level was manipulated by changing its ability to sense the environment and make decisions, and participants were aware of the autonomy level.","The study aims to investigate how different levels of autonomy impact trust, but the specific results are not provided in this paper.",,The study aims to investigate how individual differences and robot autonomy levels affect performance and trust in human-robot teams.,"The robot teammate assists the human in a capture the flag game, with the human issuing commands to the robot in the low autonomy condition, and the robot acting more autonomously in the high autonomy condition. The human controls their own character in the game.",t-test; t-test; bivariate correlation,"The study uses independent sample t-tests to compare mean performance between groups based on autonomy levels, initial trust, attentional control, and working memory capacity. Specifically, it compares high vs low autonomy groups, high initial trust groups across autonomy levels, high vs low attentional control groups, and high vs low working memory capacity groups. A bivariate correlation is used to examine the relationship between negative attitudes towards robots and the number of communication attempts with the robot teammate.",TRUE,Robot-autonomy,,,"The study explicitly manipulates the level of autonomy of the robot teammate. In the low autonomy condition, the participant issues commands to the robot, and the robot does not sense its environment. In the high autonomy condition, the robot is aware of the environment and can make decisions, although the participant can still issue commands and override the robot's actions. This manipulation of decision-making authority and control over the robot's actions directly corresponds to the 'Robot-autonomy' category. The paper states that the study aims to investigate how different levels of autonomy impact trust, but the specific results are not provided in this paper, so we cannot determine which factors impacted or did not impact trust.",10.1177/1071181322661287,http://journals.sagepub.com/doi/10.1177/1071181322661287,"Although previous studies have developed scales to measure levels of robot autonomy and ways to quantify trust in human-robot teams, there is still much interest in determining how trust, individual differences, and levels of autonomy impact these teams. The primary objective of this study is to identify how pre-existing attitudes about robots, individual differences in normal cognition, and levels of robot autonomy affect performance, communication, and trust with a robot teammate. This study pairs participants with a robot teammate to compete against two robotic opponents in a simulated game of capture the flag. Game performance and interactions with the robot teammate will be collected as outcome measures. Subjective measures will include pre-existing negative attitudes toward robots (Nomura et al., 2006a) and trust (Schaefer, 2016)."
"Bernotat, Jasmin; Eyssel, Friederike; Sachse, Janik",Shape It – The Influence of Robot Body Shape on Gender Perception in Robots,2017,1,83,83,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were recruited online and were shown a picture of either a male or female robot torso. They then completed a questionnaire assessing their perceptions of the robot, including gender attribution, trait attribution, task suitability, and trust. Covariates such as robot anxiety and technology commitment were also measured.","Participants rated a robot torso image on various scales related to gender perception, trait attribution, task suitability, and trust.",Unspecified,Humanoid Robots,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants observed images of robot torsos.,media,Participants viewed static images of robot torsos.,simulated,The robot was represented by colored drawings.,not autonomous,The robot's actions were not autonomous; it was a static image.,Questionnaires; Custom Scales,,,Trust was assessed using a custom questionnaire with items related to cognitive and affective trust.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's waist-to-hip ratio and shoulder width were manipulated to create male and female prototypes, influencing perceived gender.","Participants showed more affective trust toward the female robot shape, and surprisingly, more cognitive trust as well. The male robot did not elicit more cognitive trust.","The study found that participants showed more affective and cognitive trust toward the female robot, which was unexpected as the hypothesis was that the male robot would elicit more cognitive trust. The results were mainly confirmed for the female robot, possibly because the male gender is a default and the female robot violated expectations.","Robot body shape, specifically waist-to-hip ratio and shoulder width, influences gender perception, trait attribution, task suitability ratings, and trust toward robots.",Participants viewed a static image of a robot torso and completed a questionnaire about their perceptions of the robot. The robot did not perform any actions.,t-test; ANCOVA; Pearson correlation,"The study used t-tests to compare means in the pilot study, specifically to check if the robot prototypes were perceived as male and female as intended, and to ensure they did not differ on robot-typicality, humanlikeness, and machinelikeness. MANCOVAs were used to test the main hypotheses, examining the effects of robot shape (male vs. female) on gender attribution, trait attribution, task suitability, and trust, while controlling for covariates such as societal gender stereotypes, robot anxiety, technology commitment, anthropomorphism, social desirability, and ambivalent sexism. Pearson correlations were used to explore the relationships between significant covariates and the dependent variables.",TRUE,Robot-aesthetics,Robot-aesthetics,,"The study manipulated the robot's waist-to-hip ratio (WHR) and shoulder width (SW) to create male and female prototypes. This manipulation directly altered the visual appearance of the robot, which falls under the category of 'Robot-aesthetics'. The paper states, 'We created colored drawings of robot prototypes and manipulated WHR and SW according to [3,4].' and 'The robot with WHR 0.5, 80% SW was rated as female... participants showed more affective and surprisingly, even more cognitive trust toward the female robot compared to the male robot.' This indicates that the aesthetic manipulation of the robot's body shape impacted trust. There were no other manipulations present in the study.",,http://link.springer.com/10.1007/978-3-319-70022-9_8,"Previous research has shown that gender-related stereotypes are even applied to robots. In HRI, a robot’s appearance, for instance, visual facial gender cues such as hairstyle of a robot have successfully been used to elicit gender-stereotypical judgments about male and female prototypes, respectively. To complement the set of features to visually indicate a robot’s gender, we explored the impact of waist-to-hip ratio (WHR) and shoulder width (SW) in robot prototypes. Speciﬁcally, we investigated the effect of male vs. female appearance on perceived robot gender, the attribution of gender stereotypical traits, the robots’ suitability for stereotypical tasks, and participants’ trust toward the robots. Our results have demonstrated that the manipulation of WHR and SW correctly elicited gendered perceptions of the two prototypes. However, the perception of male robot gender did not affect the attribution of agentic traits and cognitive trust. Nevertheless, participants tended to rate the male robot as more suitable for stereotypically male tasks. In line with our predictions, participants preferred to use the female robot shape for stereotypically female tasks. They tended to attribute more communal traits and showed more affective trust toward the robot that was designed with a female torso versus a male robot torso. These results demonstrate that robot body shape activates stereotypes toward robots. These in turn, deeply impact people’s attitudes and trust toward robots which determine people’s motivation to engage in HRI."
"Bernotat, Jasmin; Eyssel, Friederike; Sachse, Janik",The (Fe)male Robot: How Robot Body Shape Impacts First Impressions and Trust Towards Robots,2019,1,107,107,0,No participants were excluded,Online Crowdsourcing,between-subjects,Participants were recruited online and were told they would evaluate a new robot prototype. They were shown either a male or female robot torso and completed a survey using 7-point Likert scales.,"Participants rated the robot on various scales, including perceived gender, agency, communion, task suitability, and cognitive and affective trust.",Unspecified,Other,Research,Evaluation,Rating,passive observation,Participants observed images of robot torsos.,media,Participants viewed static images of robot torsos.,simulated,The robot was represented by drawings of robot torsos.,not autonomous,The robot was a static image and did not perform any actions.,Questionnaires; Custom Scales,,,Trust was assessed using custom questionnaires.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's body shape was manipulated by changing the waist-to-hip ratio and shoulder width to create male and female versions, influencing perceived gender.","The female robot evoked more affective trust and, unexpectedly, more cognitive trust than the male robot.","The male robot was not perceived as more agentic or preferred for stereotypically male tasks, contrary to the hypotheses. The female robot was preferred for stereotypically female tasks and evoked more affective trust, as predicted. Unexpectedly, the female robot also evoked more cognitive trust than the male robot. Participants tended to rate both robots as more female, more communal, and preferred them for stereotypically male tasks, and ascribed more cognitive trust than affective trust.","Manipulating a robot's body shape through waist-to-hip ratio and shoulder width affects the attribution of gender and trust, with a female body shape leading to higher affective and cognitive trust.",Participants viewed a drawing of a robot torso and rated it on various scales. The robot did not perform any actions.,t-test; paired samples t-test; ANCOVA; Pearson correlation; ANOVA,"The study used independent samples t-tests to compare responses between groups in the pilot study I, and a paired samples t-test to compare the degree of close interaction required by different tasks in pilot study II. The main study employed MANCOVAs to examine the effects of robot body shape on various dependent variables (perceived gender, agency, communion, task preferences, cognitive and affective trust), controlling for several covariates. Pearson correlations were used to explore the relationships between covariates and dependent variables. Additionally, ANOVAs were used to investigate overall tendencies in the data, such as whether participants rated the robots as more female than male, more agentic than communal, preferred them for stereotypically male tasks, and ascribed more cognitive than affective trust.",TRUE,Robot-aesthetics,Robot-aesthetics,,"The study manipulated the robot's body shape by altering the waist-to-hip ratio (WHR) and shoulder width (SW) to create male and female versions. This manipulation directly affects the visual appearance of the robot, which falls under the 'Robot-aesthetics' category. The paper explicitly states that this manipulation of body shape impacted both affective and cognitive trust, thus 'Robot-aesthetics' is also listed as a factor that impacted trust. There were no other factors manipulated in the study, so there are no factors that did not impact trust.",10.1007/s12369-019-00562-7,http://link.springer.com/10.1007/s12369-019-00562-7,"Humanlikeness, including robot gender, impacts people’s impression of social robots (Eyssel and Hegel in J Appl Soc Psychol 42(9):2213–2230, 2012) and actual human robot interaction (HRI) (Kuchenbrandt et al. in Int J Soc Robot 6(3):417–427, 2014; Reich-Stiebert and Eyssel in Proceedings of the 2017 ACM/IEEE international conference on human–robot interaction, ACM, pp 166–176, 2017). Although robot gender has been manipulated in various ways in previous research (Alexander et al. in Proceedings of the annual meeting of the Cognitive Science Society, vol 36, 2014; Eyssel and Hegel, 2012), robot body shape as a gender cue has been neglected in this context. Therefore, the current research investigated the effects of manipulating a robot torso’s waist-to-hip ratio and shoulder width on social judgments of a robot. As hypothesized, a robot with a female body shape was perceived as more communal, it was preferred for stereotypically female tasks, and evoked more cognitive and affective trust than a robot with a male body shape. Unexpectedly, both robot types were perceived as equally agentic and they were deemed equally suitable for stereotypically male tasks. Above and beyond, participants’ motivation to respond in a socially desirable manner, their societal beliefs about agentic and communal traits considered appropriate for men and women, sexist attitudes, gender, and technology commitment affected their impression formation about robots. We point to the risks of designing gendered robots and recommend to manipulate robot gender deliberately with regard to the effects this might have on HRI."
"Bhat, Shreyas; Lyons, Joseph B.; Shi, Cong; Yang, X. Jessie",Clustering Trust Dynamics in a Human-Robot Sequential Decision-Making Task,2022,1,46,45,1,One participant's data was discarded as the participant marked all survey questions in the middle and used significantly less time compared to other participants,Controlled Lab Environment,within-subjects,"Participants provided informed consent and completed pre-experiment surveys. They were oriented to the experiment and the two-fold objective of minimizing time and maximizing health was emphasized. Participants were told that the robotic agent was imperfect, but they were not informed of the exact reliability level. They were also told that the robotic agent's recommendations would help them achieve the two-fold objective. Participants then proceeded to the experimental trials, wherein they had to search through 100 houses sequentially. After searching each house, the participants were asked to report their level of trust on the autonomous agent's recommendations. At the end of the experiment, participants reported their post-experiment trust toward the automated aid using two scales and their level of workload.","Participants were tasked with searching through 100 houses sequentially, deciding whether to breach a site directly or deploy a Robotic Armored Rescue Vehicle (RARV) based on recommendations from an intelligent drone, with the goal of minimizing health loss and time.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a simulated drone through a screen, making decisions based on its recommendations.",simulation,The interaction took place in a 3D simulated environment using the Unreal Engine.,simulated,The robot was a simulated drone presented in a 3D environment.,shared control (adaptive),"The robot provided recommendations, and the human could choose to accept or reject them, with the robot adapting its recommendations based on the human's trust.",Questionnaires; Real-time Trust Measures,Muir's Trust Questionnaire; NASA Task Load Index (NASA-TLX),Performance Metrics,"Trust was measured using real-time slider ratings and post-experiment questionnaires, with performance metrics used for modeling.","parametric models (e.g., regression)",A personalized beta distribution model was used to update trust parameters based on performance and feedback.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's recommendations were designed to influence the immediate reward, which in turn affected the participant's trust. Participants were also told that the robot was imperfect but would help them achieve their goals.","Trust increased when the robot's recommendation led to a higher reward and decreased when it led to a lower reward. The study also identified three distinct types of trust dynamics: Bayesian decision makers, oscillators, and disbelievers.","The study identified three distinct types of trust dynamics, with disbelievers being less extroverted, less agreeable, and having lower expectations toward the robotic agent, and oscillators experiencing more frustration. The study also found that the trust measures had some conceptual differences, with the Muir and Moray measure focusing on elements of trustworthiness while the Lyons and Guznov measure focused on one's willingness to be vulnerable to the drone aid.","The study found three distinct types of trust dynamics: Bayesian decision makers, oscillators, and disbelievers, and that a reward-based performance metric can capture moment-to-moment trust changes.","The robot provided recommendations on whether to use a RARV or breach a site directly. The human participant decided whether to follow the recommendation, aiming to minimize health loss and time.",K-means; ANOVA; Bonferroni correction,"The study used k-means clustering to group participants based on their trust dynamics, using the average logarithm of trust and RMS error as features. One-way ANOVA was used to identify significant differences in personality traits, post-experiment trust reports, and workload between the identified clusters. Post-hoc analysis with Bonferroni adjustment was used to determine which specific groups differed significantly from each other after the ANOVA revealed significant differences.",TRUE,Robot-accuracy; Robot-autonomy,Robot-accuracy,Robot-autonomy,"The study manipulated 'Robot-accuracy' by varying the immediate reward based on the robot's recommendations. The paper states, 'We expect that a participant's trust would be likely to increase if following the recommendation by the drone gets them a higher reward that not following the recommendation and vice versa.' This indicates that the robot's recommendations were designed to influence the immediate reward, which in turn affected the participant's trust. The study also manipulated 'Robot-autonomy' by having the robot provide recommendations, but the human could choose to accept or reject them. The paper states, 'The agent provides recommendations to their human partner about the action that they should take, but the final decision of action selection lies with the human.' While the robot's recommendations were designed to influence the human's decision, the human retained the final decision-making authority. The study found that the robot's accuracy (i.e., whether its recommendation led to a higher or lower reward) impacted trust, as evidenced by the statement, 'It is quite clear that a red triangle is often followed by a decrease in trust and a green triangle is followed by an increase in trust.' However, the level of autonomy (shared control) did not directly impact trust levels, as the study focused on how the accuracy of the recommendations influenced trust, not the level of control the robot had.",10.1109/LRA.2022.3188902,https://ieeexplore.ieee.org/document/9816108/,"In this paper, we present a framework for trust-aware sequential decision-making in a human-robot team wherein the human agent’s trust in the robotic agent is dependent on the reward obtained by the team. We model the problem as a ﬁnite-horizon Markov Decision Process with the trust of the human on the robot as a state variable. We develop a reward-based performance metric to drive the trust update model, allowing the robotic agent to make trust-aware recommendations. We conduct a human-subject experiment with a total of 45 participants and analyze how the human agent’s trust evolves over time. Results show that the proposed trust update model is able to accurately capture the human agent’s trust dynamics. Moreover, we cluster the participants’ trust dynamics into three categories, namely, Bayesian decision makers, oscillators, and disbelievers, and identify personal characteristics that could be used to predict which type of trust dynamics a person will belong to. We ﬁnd that the disbelievers are less extroverted, less agreeable, and have lower expectations toward the robotic agent, compared to the Bayesian decision makers and oscillators. The oscillators tend to get signiﬁcantly more frustrated than the Bayesian decision makers."
"Bhat, Shreyas; Lyons, Joseph B.; Shi, Cong; Yang, X. Jessie",Evaluating the Impact of Personalized Value Alignment in Human-Robot Interaction: Insights into Trust and Team Performance Outcomes,2024,1,54,54,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed three missions, each with a different robot interaction strategy (non-learner, non-adaptive learner, adaptive learner), using a Latin square design to minimize order effects. Before each mission, participants rated their preferences between saving health and saving time. After each site search, participants reported their trust in the robot. Post-mission, participants completed questionnaires on trust, reliance intentions, workload, and performance.","Participants worked with a robot to search for threats in a town, deciding whether to use an armored robot for protection at each site.",Unspecified,Mobile Robots; Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated robot through a screen.,simulation,The interaction took place in a 3D simulated environment.,simulated,The robot was represented as a virtual entity in the simulation.,shared control (adaptive),The robot adapted its recommendations based on the human's inferred reward function.,Questionnaires; Real-time Trust Measures,Muir's Trust Questionnaire; Reliance Intention Scale,Performance Metrics,"Trust was measured using questionnaires and a real-time slider, and performance metrics were used for modeling.",POMDP,The interaction was modeled as a trust-aware Markov Decision Process (MDP) with Bayesian Inverse Reinforcement Learning (IRL) to estimate human reward weights.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's interaction strategy was manipulated to either not learn, learn but not adapt, or learn and adapt to the human's reward function, influencing goal alignment and autonomy.","When starting with an informed prior, personalized value alignment did not significantly benefit trust or team performance. However, when an informed prior was unavailable, aligning to the human's values led to higher trust, agreement, and reliance intentions.","The study found that value alignment was only beneficial when an informed prior on the human's reward weights was unavailable. There was a significant difference in post-mission reliance intentions in Experiment 1, where participants were more willing to rely on the non-learner strategy than the adaptive-learner strategy, which was unexpected.",Personalized value alignment is beneficial for trust and reliance when a good prior on the human's reward weights is unavailable.,"The robot provided recommendations on whether to use an armored robot for protection, and the human decided whether to follow the recommendation, balancing time and health objectives.",ANOVA,"Repeated measures analyses of variance (ANOVAs) were conducted to compare the three interaction strategies (non-learner, non-adaptive learner, and adaptive learner) on various dependent measures such as trust, agreement, reliance intentions, workload, and team performance. Greenhouse-Geisser corrections were applied when Mauchly's test of sphericity was violated. Pairwise comparisons with Bonferroni adjustments were used for post-hoc analysis when significant differences were found.",TRUE,Robot-adaptability; Robot-autonomy,Robot-adaptability,Robot-autonomy,"The study manipulated the robot's learning and adaptation strategy, which falls under 'Robot-adaptability'. The robot either did not learn, learned but did not adapt, or learned and adapted to the human's reward function. This directly influenced how the robot made recommendations. The robot's level of control over its recommendations, based on whether it adapted to the human's reward function, is classified as 'Robot-autonomy'. The study found that 'Robot-adaptability' impacted trust, as the adaptive-learner strategy led to higher trust when an informed prior was unavailable. However, the level of autonomy itself did not impact trust, as the non-adaptive learner and adaptive learner both had the same level of autonomy, but different trust outcomes. The key manipulation was whether the robot adapted its behavior based on the human's preferences, not the level of control it had over its recommendations.",10.1145/3610977.3634921,https://dl.acm.org/doi/10.1145/3610977.3634921,"This paper examines the efect of real-time, personalized alignment of a robot’s reward function to the human’s values on trust and team performance. We present and compare three distinct robot interaction strategies: a non-learner strategy where the robot presumes the human’s reward function mirrors its own; a nonadaptive-learner strategy in which the robot learns the human’s reward function for trust estimation and human behavior modeling, but still optimizes its own reward function; and an adaptive-learner strategy in which the robot learns the human’s reward function and adopts it as its own. Two human-subject experiments with a total number of ��� = 54 participants were conducted. In both experiments, the human-robot team searches for potential threats in a town. The team sequentially goes through search sites to look for threats. We model the interaction between the human and the robot as a trust-aware Markov Decision Process (trust-aware MDP) and use Bayesian Inverse Reinforcement Learning (IRL) to estimate the reward weights of the human as they interact with the robot. In Experiment 1, we start our learning algorithm with an informed prior of the human’s values/goals. In Experiment 2, we start the learning algorithm with an uninformed prior. Results indicate that when starting with a good informed prior, personalized value alignment does not seem to beneft trust or team performance. On the other hand, when an informed prior is unavailable, alignment to the human’s values leads to high trust and higher perceived performance while maintaining the same objective team performance."
"Bhat, Shreyas; Lyons, Joseph B.; Shi, Cong; Yang, X. Jessie",Value Alignment and Trust in Human-Robot Interaction: Insights from Simulation and User Study,2024,1,24,24,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed three missions, each with a different robot interaction strategy (non-learner, non-adaptive learner, adaptive learner) in a counterbalanced order. Before each mission, participants rated their preference between saving health and saving time. After each site search, participants reported their trust in the robot. Post-mission, participants completed questionnaires assessing trust, reliance intentions, workload, and performance.","Participants were tasked with completing a reconnaissance mission, searching through 40 sites, with the goal of minimizing damage to the soldier and minimizing mission completion time. They received recommendations from an intelligent agent on whether to use an armored robot for protection at each site.",Unspecified,Unmanned Aerial Vehicles; Unmanned Ground Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a simulated robot through a screen interface, receiving recommendations and making choices.",simulation,The interaction took place in a 3D simulated environment using the Unreal Engine.,simulated,The robot was represented as a virtual entity within the simulation.,shared control (adaptive),"The robot provided recommendations, and the human made the final decision, with the robot adapting its recommendations based on the human's preferences in the adaptive learner condition.",Questionnaires; Custom Scales; Behavioral Measures,Muir's Trust Questionnaire; NASA Task Load Index (NASA-TLX); Reliance Intention Scale,Performance Metrics,"Trust was assessed using questionnaires, a custom scale for reliance intentions, and behavioral measures such as the number of agreements with the robot's recommendations.","parametric models (e.g., regression)",The robot used a Bayesian Inverse Reinforcement Learning framework to learn human preferences and update its reward weights.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's interaction strategy was manipulated to be either non-learning, non-adaptive learning, or adaptive learning, influencing the alignment of the robot's goals with the human's and the robot's autonomy level. The feedback provided to the participants was also manipulated by showing the drone's assessed level of threat, the recommendation given by the system, the action selected by the participant, the ground truth of threat presence, and the time it took them to search the site.",The adaptive-learner strategy led to higher average trust and post-mission trust compared to the non-learner and non-adaptive learner strategies. The adaptive learner strategy also led to a higher number of agreements and higher reliance intentions.,"The study found that adapting to human values kept trust high, while trust was lost when interacting with a misaligned robot. The adaptive learner strategy also resulted in lower workload and higher perceived performance, even though objective performance was similar across all conditions.","Value alignment is beneficial for trust, especially under high-risk conditions, and an adaptive learning strategy can maintain trust by aligning with human values.","The robot provided recommendations on whether to use an armored robot for protection at each search site. The human decided whether to follow the recommendation or not, and their actions were used to update the robot's model of their preferences.",ANOVA,"Repeated measures analyses of variance (ANOVAs) were conducted to compare the three interaction strategies (non-learner, non-adaptive learner, and adaptive learner) on various dependent variables such as average trust, post-mission trust, number of agreements, reliance intentions, workload, and performance. Greenhouse-Geisser corrections were applied where necessary. Pairwise comparisons with Bonferroni adjustments were used for post-hoc analysis to determine significant differences between specific interaction strategies.",TRUE,Robot-adaptability; Robot-autonomy,Robot-adaptability,,"The study manipulated the robot's learning strategy, which falls under 'Robot-adaptability'. The robot was either a non-learner, a non-adaptive learner, or an adaptive learner. The adaptive learner adjusted its recommendations based on the human's preferences, while the other two did not. This manipulation directly impacted trust, as the adaptive learner strategy led to higher trust. The study also manipulated the level of autonomy of the robot by having it provide recommendations, but the human made the final decision, which falls under 'Robot-autonomy'. However, the level of autonomy was consistent across all conditions, so it did not impact trust. The paper states, 'The adaptive-learner strategy led to higher average trust and post-mission trust compared to the non-learner and non-adaptive learner strategies.' This indicates that the manipulation of the robot's adaptability was the key factor influencing trust, while the level of autonomy was consistent across all conditions.",,http://arxiv.org/abs/2405.18324,"With the advent of AI technologies, humans and robots are increasingly teaming up to perform collaborative tasks. To enable smooth and effective collaboration, the topic of value alignment (operationalized herein as the degree of dynamic goal alignment within a task) between the robot and the human is gaining increasing research attention. Prior literature on value alignment makes an inherent assumption that aligning the values of the robot with that of the human benefits the team. This assumption, however, has not been empirically verified. Moreover, prior literature does not account for human’s trust in the robot when analyzing human-robot value alignment. Thus, a research gap needs to be bridged by answering two questions: How does alignment of values affect trust? Is it always beneficial to align the robot’s values with that of the human? We present a simulation study and a human-subject study to answer these questions. Results from the simulation study show that alignment of values is important for trust when the overall risk level of the task is high. We also present an adaptive strategy for the robot that uses Inverse Reinforcement Learning (IRL) to match the values of the robot with those of the human during interaction. Our simulations suggest that such an adaptive strategy is able to maintain trust across the full spectrum of human values. We also present results from an empirical study that validate these findings from simulation. Results indicate that real-time personalized value alignment is beneficial to trust and perceived performance by the human when the robot does not have a good prior on the human’s values."
"Biermann, Hannah; Brauner, Philipp; Ziefle, Martina",How context and design shape human-robot trust and attributions,2020,1,388,228,160,160 participants were excluded because they did not complete the survey or rushed through the survey,Online Crowdsourcing,mixed design,"Participants completed an online questionnaire with three parts: demographics, expectations of robots in care and production, and evaluation of robots in different contexts and designs. Participants were presented with textual descriptions and images of robots in care and production contexts with either an anthropomorphic or functional design.",Participants evaluated robots in different contexts and designs using Likert scales and semantic differentials.,Unspecified,Other,Care; Industrial; Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the robot and interaction scenario.,media,Participants were presented with static images of robots.,simulated,The robots were represented through images.,not autonomous,"The robot's actions were described in text and images, but no actual robot was present.",Questionnaires; Custom Scales,Jian et al. Trust Scale; Disposition to Trust Questionnaire,,Trust was measured using a questionnaire and a custom semantic differential scale.,no modeling,The study used descriptive and inferential statistics but did not model trust.,Observational & Survey Studies,Quantitative Surveys,Direct Manipulation,The study manipulated the robot's appearance (anthropomorphic vs. functional) and the usage context (production vs. care) to see how these factors influenced trust.,"The combination of context and design affected trust, with functional robots in production being trusted more than anthropomorphic robots or robots in the care context. The context and design influenced the attributions of the robots.","The study found that the combination of context and design influenced trust, rather than either factor alone. The differences in perception between functional and anthropomorphic designs were more pronounced in the care context than in the production context.","The study's key finding is that the combination of robot design and usage context significantly influences human-robot trust, with functional robots in production being trusted more and anthropomorphic robots being preferred in care contexts.","Participants read descriptions and viewed images of robots in different contexts and designs, then answered questions about their trust and perceptions of the robots.",t-test; ANOVA; Pearson correlation,"The study used t-tests to compare means between different groups, specifically to analyze the expectations of robots in care and production contexts and the use intention of HRC. Repeated measures ANOVA was used to examine the impact of context and design on human-robot trust and the perception of robots. Correlation analyses were used to explore the relationships between user factors (e.g., disposition to trust, technology affinity) and the intention to use robots in different contexts.",TRUE,Robot-aesthetics; Task-environment,Robot-aesthetics; Task-environment,,"The study manipulated the robot's appearance (anthropomorphic vs. functional), which falls under 'Robot-aesthetics'. The study also manipulated the usage context (production vs. care), which falls under 'Task-environment'. The paper explicitly states that the combination of context and design affected trust, indicating that both 'Robot-aesthetics' and 'Task-environment' impacted trust. The paper states that neither the context nor the design had a significant main effect on trust, but their interaction did, so there are no factors that did not impact trust.",10.1515/pjbr-2021-0008,https://www.degruyter.com/document/doi/10.1515/pjbr-2021-0008/html,"In increasingly digitized working and living environments, human-robot collaboration is growing fast with human trust toward robotic collaboration as a key factor for the innovative teamwork to succeed. This article explores the impact of design factors of the robotic interface (anthropomorphic vs functional) and usage context (production vs care) on human–robot trust and attributions. The results of a scenario-based survey with N = 228 participants showed a higher willingness to collaborate with production robots compared to care. Context and design inﬂuenced the trust attributed to the robots: robots with a technical appearance in production were trusted more than anthropomorphic robots or robots in the care context. The evaluation of attributions by means of a semantic diﬀerential showed that diﬀerences in robot design were less pronounced for the production context in comparison to the care context. In the latter, anthropomorphic robots were associated with positive attributes. The results contribute to a better understanding of the complex nature of trust in automation and can be used to identify and shape use case-speciﬁc risk perceptions as well as perceived opportunities to interacting with collaborative robots. Findings of this study are pertinent to research (e.g., experts in human–robot interaction) and industry, with special regard given to the technical development and design."
"Birmingham, Chris; Hu, Zijian; Mahajan, Kartik; Reber, Eli; Mataric, Maja J",Can I Trust You? A User Study of Robot Mediation of a Support Group,2020,1,81,71,10,10 participants did not complete the surveys or filled them in without coding the reverse questions correctly,Controlled Lab Environment,within-subjects,"Participants were seated around a table with a robot mediator, completed a pre-study trust survey, engaged in a 20-minute support group interaction guided by the robot, completed a post-study trust survey, and participated in a group interview.","Participants engaged in a support group discussion about academic stress, guided by a robot mediator.",Nao,Humanoid Robots; Expressive Robots,Social; Research,Social,Conversation,minimal interaction,Participants interacted verbally with the robot and other participants in a group setting.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical Nao robot.,wizard of oz (directly controlled),The robot's speech and gaze were controlled by a human operator.,Questionnaires; Custom Scales,Dyadic Trust Scale; Specific Interpersonal Trust Scale; Negative Attitude towards Robots Scale (NARS); Big Five Inventory Scale; Empathy Inventory,Video Data; Speech Data,"Trust was measured using questionnaires and custom scales, and video and speech data were collected.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's questions and disclosures were manipulated to increase in sensitivity over time, and the task was framed as a support group interaction.",The study found a significant increase in trust between participants and between participants and the robot.,"The study found that trust in the robot became more monolithic after the interaction, while trust in other participants became less monolithic. There was also a negative correlation between trust in the robot before the interaction and the Negative Attitudes towards Robots Scale.",Robot-mediated support group interactions significantly increased dyadic trust between participants and between participants and the robot.,The robot asked questions and made disclosures to encourage participants to share their academic stress. Participants responded to the robot's prompts and shared their experiences with each other.,t-test; Wilcoxon rank sum; Multilevel Model,"A t-test for comparing paired samples was used to determine the significance of changes in trust before and after the support group session. A Wilcoxon Signed Rank Test was used to calculate effect size. Factor analysis was performed to understand the latent variables affecting the results, and to validate the internal consistency of the survey responses.",TRUE,Robot-verbal-communication-content; Task-complexity,Robot-verbal-communication-content,Task-complexity,"The study manipulated the content of the robot's verbal communication by varying the sensitivity of the questions and disclosures over time, starting with low sensitivity and progressing to high sensitivity. This is explicitly stated in the paper: 'The robot's questions and disclosures were open-ended and specifically designed to encourage the participants to share with one another... The content ranged from low sensitivity, such as, ""What do you like about school?"" to high sensitivity, such as, ""Sometimes I worry about if I belong here, does anyone else feel the same way?""'. This manipulation of question sensitivity directly impacted trust, as the study found a significant increase in trust between participants and between participants and the robot. The task complexity was also implicitly manipulated by the increasing sensitivity of the questions, which could be considered as increasing the cognitive load and emotional risk for the participants. However, the study did not find any significant impact of the task complexity on trust, as the main finding was related to the robot's communication content. The study did not explicitly manipulate any other factors from the provided list.",10.1109/ICRA40945.2020.9196875,https://ieeexplore.ieee.org/document/9196875/,"Socially assistive robots have the potential to improve group dynamics when interacting with groups of people in social settings. This work contributes to the understanding of those dynamics through a user study of trust dynamics in the novel context of a robot mediated support group. For this study, a novel framework for robot mediation of a support group was developed and validated. To evaluate interpersonal trust in the multi-party setting, a dyadic trust scale was implemented and found to be uni-factorial, validating it as an appropriate measure of general trust. The results of this study demonstrate a signiﬁcant increase in average interpersonal trust after the group interaction session, and qualitative post-session interview data report that participants found the interaction helpful and successfully supported and learned from one other. The results of the study validate that a robot-mediated support group can improve trust among strangers and allow them to share and receive support for their academic stress."
"Biros, David P.; Daly, Mark; Gunsch, Gregg",The Influence of Task Load and Automation Trust on Deception Detection,2004,1,40,40,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were trained on a military command and control simulation, then completed a decision-making task with manipulated information warfare and task load conditions. They completed questionnaires before and after the simulation.",Participants directed air assets against enemy assets using either manual controls or system recommendations in a simulated military command and control environment.,Unspecified,Other,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a computer simulation, making decisions based on system recommendations.",simulation,Participants used a computer simulation to perform a military command and control task.,simulated,The robot was represented as a simulated system within the computer interface.,shared control (fixed rules),"The system provided recommendations, but participants could choose to accept or reject them.",Questionnaires; Behavioral Measures,,Performance Metrics,Trust was measured using questionnaires and by tracking the number of accepted system recommendations.,"parametric models (e.g., regression)","The study used correlation analysis and ANOVA to analyze the relationship between trust, task load, and automation use.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated task load by increasing the number of resources participants were responsible for and manipulated trust by introducing the idea of information warfare, which questioned the reliability of the system.","Trust in the system was reduced by the information warfare manipulation, and increased task load led to greater reliance on automation even when trust was low.","Participants tended to over-rely on automation when task load was high, even when they had low trust in the system due to the information warfare manipulation. This suggests that task load can override trust in automation.","Trust in system automation is positively correlated with the use of system automation, but increased task load can lead to over-reliance on automation even when trust is low.","The simulated system provided recommendations for directing air assets, and the human participant could choose to accept or reject these recommendations. The human's primary task was to monitor the situation and make decisions about how to direct the air assets.",pearson product-moment correlations; ANOVA,"The study used Pearson product-moment correlations to test the relationships between predictability, dependability, and trust, as well as between trust and automation use. ANOVA was used to test the difference in automation use between the environmental conditions of Information Warfare (IW) and Non-IW, and between different task loads. ANOVA was also used to determine if user task load has a moderating effect on the relationship between system automation trust and use of system automation.",TRUE,Task-complexity; Robot-accuracy,Robot-accuracy,Task-complexity,"The study manipulated 'Task-complexity' by increasing the number of resources a participant was responsible for, as stated in the paper: ""Task load was increased by a factor of approximately 2.5."" This manipulation was intended to increase the cognitive demands on the participants. The study also manipulated 'Robot-accuracy' by introducing the idea of information warfare, which questioned the reliability of the system's recommendations. The paper states: ""IW was operationalized by planting the idea of IW (i.e., successful computer attack) in the minds of participants via the scenario description...the reliability of systems recommendations were in question."" This manipulation directly impacted the perceived accuracy and dependability of the system. The paper explicitly states that the information warfare manipulation reduced trust in the system: ""the treatments in which information provided was not in question (i.e., non-IW) are significantly different from the treatments in which information was in question (i.e., IW) in terms of overall system automation trust."" This indicates that 'Robot-accuracy' impacted trust. While task load was manipulated, the paper states that participants tended to over-rely on automation when task load was high, even when they had low trust in the system. This suggests that while task load influenced automation use, it did not directly impact trust levels, therefore it is classified as not impacting trust.",10.1023/B:GRUP.0000021840.85686.57,https://doi.org/10.1023/B:GRUP.0000021840.85686.57,"The purpose of this research was to investigate the effects that user task load level has on the relationship between an individual's trust in and subsequent use of a system's automation. Military decision-makers trust and use information system automation to make many tactical judgments and decisions. In situations of information uncertainty (information warfare environments), decision-makers must remain aware of information reliability issues and temperate their use of system automation if necessary. An individual's task load may have an effect on his use of a system's automation in environments of information uncertainty."
"Blair, Kasha; Sandry, Joshua; Rice, Stephen",An Expansion of System Wide Trust Theory Using In-Vehicle Automation,2012,2,80,80,0,No participants were excluded,Educational Setting,mixed design,Participants completed a questionnaire providing reliability and trust ratings for in-vehicle automated devices after being given information about the reliability of an initial device.,Participants rated the perceived reliability and trust of five in-vehicle automated devices.,Unspecified,Autonomous Vehicles,Other,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the automated devices.,media,The interaction was based on written descriptions of the devices.,hypothetical,The devices were described in text without any visual representation.,not autonomous,The devices were described in text without any real autonomy.,Questionnaires,,,Trust was measured using a questionnaire with a rating scale.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the initial automated device was manipulated to be 70%, 85%, or 100%, influencing participants' expectations and perceptions of the other devices.",Trust ratings decreased when the initial device was described as less reliable.,"Ratings within each condition tended to decrease in a linear fashion, possibly due to a rating bias.",The initial reliability of a single automated device had a negative impact on trust and perceived reliability ratings for subsequent devices.,Participants read descriptions of automated in-vehicle devices and provided ratings for their perceived reliability and trust.,ANOVA,"A mixed ANOVA was used to analyze both the reliability rating data and the trust rating data. For reliability ratings, a 3 (Reliability Condition) X 4 (Device Reliability) mixed ANOVA was conducted. For trust ratings, a 3 (Reliability Condition) X 5 (Device Trust) mixed ANOVA was conducted. These tests were used to determine the main effects of reliability condition and device, as well as any interaction effects on the ratings.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the reliability of the initial automated device (Device 1) by presenting it as either 70%, 85%, or 100% reliable. This manipulation directly affects the accuracy of the robot's (automated device's) performance, which is why 'Robot-accuracy' is the appropriate category. The results showed that the initial reliability level impacted both subsequent reliability estimates and subsequent trust ratings of both specific and non-specific in-vehicle automated devices. Therefore, 'Robot-accuracy' is also listed as a factor that impacted trust. There were no other factors manipulated, so there are no factors that did not impact trust.",10.1177/1071181312561336,http://journals.sagepub.com/doi/10.1177/1071181312561336,"In the present experiments, we extended system-wide trust theory (SWT) to make predictions regarding in-vehicle automated devices. Reformulation of SWT into system-wide assessment theory (SWAT) was successful in predicting participants’ perceived reliability and trust ratings of in-vehicle automated devices when participants were presented with only a limited amount of information about the reliability of an existing in-vehicle automated device. Over a series of two related experiments, findings revealed that the initial reliability level impacted both subsequent reliability estimates and subsequent trust ratings of both specific and non-specific in-vehicle automated devices."
"Blair, Kasha; Sandry, Joshua; Rice, Stephen",An Expansion of System Wide Trust Theory Using In-Vehicle Automation,2012,2,85,85,0,No participants were excluded,Educational Setting,mixed design,Participants completed a questionnaire providing reliability and trust ratings for specific in-vehicle automated devices after being given information about the reliability of an initial device.,Participants rated the perceived reliability and trust of five specific in-vehicle automated devices.,Unspecified,Autonomous Vehicles,Other,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the automated devices.,media,The interaction was based on written descriptions of the devices.,hypothetical,The devices were described in text without any visual representation.,not autonomous,The devices were described in text without any real autonomy.,Questionnaires,,,Trust was measured using a questionnaire with a rating scale.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the initial automated device was manipulated to be 70%, 85%, or 100%, influencing participants' expectations and perceptions of the other devices.",Trust ratings decreased when the initial device was described as less reliable.,"Ratings for the parallel parking aid were lower than other devices in the 100% reliability condition, and ratings within each condition tended to decrease in a linear fashion, possibly due to a rating bias.","The initial reliability of a single automated device had a negative impact on trust and perceived reliability ratings for subsequent devices, even when the devices were specific and unrelated.",Participants read descriptions of specific automated in-vehicle devices and provided ratings for their perceived reliability and trust.,ANOVA,"A mixed ANOVA was used to analyze both the reliability rating data and the trust rating data. For reliability ratings, a 3 (Reliability Condition) X 4 (Device Reliability) mixed ANOVA was conducted. For trust ratings, a 3 (Reliability Condition) X 5 (Device Trust) mixed ANOVA was conducted. These tests were used to determine the main effects of reliability condition and device, as well as any interaction effects on the ratings.",TRUE,Robot-accuracy,Robot-accuracy,,"Similar to Study 71, the study manipulated the reliability of the initial automated device (Device 1) by presenting it as either 70%, 85%, or 100% reliable. This manipulation directly affects the accuracy of the robot's (automated device's) performance, which is why 'Robot-accuracy' is the appropriate category. The results showed that the initial reliability level impacted both subsequent reliability estimates and subsequent trust ratings of both specific in-vehicle automated devices. Therefore, 'Robot-accuracy' is also listed as a factor that impacted trust. There were no other factors manipulated, so there are no factors that did not impact trust.",10.1177/1071181312561336,http://journals.sagepub.com/doi/10.1177/1071181312561336,"In the present experiments, we extended system-wide trust theory (SWT) to make predictions regarding in-vehicle automated devices. Reformulation of SWT into system-wide assessment theory (SWAT) was successful in predicting participants’ perceived reliability and trust ratings of in-vehicle automated devices when participants were presented with only a limited amount of information about the reliability of an existing in-vehicle automated device. Over a series of two related experiments, findings revealed that the initial reliability level impacted both subsequent reliability estimates and subsequent trust ratings of both specific and non-specific in-vehicle automated devices."
"Bodala, Indu P.; Kok, Bing Cai; Sng, Weicong; Soh, Harold",Modeling the Interplay of Trust and Attention in HRI: An Autonomous Vehicle Study,2020,1,48,36,12,"3 subjects had incomplete behavioral data, 9 subjects whose eye-tracking data were unavailable due to technical issues with the eye-tracker",Controlled Lab Environment,mixed design,"Participants interacted with a simulated autonomous vehicle in two phases. In the first phase, they indicated if they perceived a critical event. In the second phase, they could intervene to override the AV. Participants also performed a secondary arithmetic task with varying difficulty levels. Eye-tracking data and trust ratings were collected.",Participants monitored a simulated autonomous vehicle and performed a secondary arithmetic task.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated AV and could intervene.,simulation,Participants interacted with a simulated AV environment.,simulated,The robot was a simulated autonomous vehicle.,shared control (fixed rules),The AV operated autonomously but participants could intervene.,Custom Scales; Questionnaires,NASA Task Load Index (NASA-TLX),Eye-tracking Data; Performance Metrics,"Trust was measured using a continuous scale and the NASA-TLX, and eye-tracking data was collected.","parametric models (e.g., regression)",A probabilistic model was used to capture the relationship between trust and attention.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The difficulty of the secondary task was manipulated to influence workload, and participants could intervene with the AV, influencing its autonomy.",Higher workload and lower trust led to lower attention to the AV.,"The study found that higher trust leads to lower attention, and higher workload leads to lower attention.","Trust affects attention, which in turn affects the human's decision to intervene with the autonomous vehicle.","The robot (simulated AV) drove autonomously, and the human monitored the AV and performed a secondary arithmetic task, with the option to intervene.",Bayesian ANOVA; Bayesian t-test; Hamiltonian Monte Carlo; Pearson correlation,"The study used a one-way Bayesian ANOVA to assess the effect of workload manipulation on the total workload score. Post-hoc pairwise Bayesian t-tests were used to compare workload between different groups, controlling for multiple comparisons. The Hamiltonian Monte Carlo algorithm was used to infer the posterior distribution of latent variables in the graphical model. Correlation analysis was used to compare the mean posterior predictions for trust and eye-tracking data with the actual data. The model's predictions for binary variables were evaluated using AUC.",TRUE,Task-complexity; Robot-autonomy,Task-complexity,,"The study manipulated the difficulty of the secondary arithmetic task, which directly impacts the cognitive demands and thus falls under 'Task-complexity'. Specifically, the paper states, 'The secondary task was designed to be either medium-paced in the Medium-Load (ML) condition or fastpaced in the High-Load (HL) condition.' This manipulation was intended to influence workload. Additionally, the study design allowed participants to intervene and override the AV in the second phase, which is a change in the level of control and thus falls under 'Robot-autonomy'. The paper states, 'In the subsequent phase, participants were allowed to intervene, that is, they could override the operations of the AV, thereby affecting the outcome of a critical event.' The results section indicates that the workload manipulation (Task-complexity) impacted attention and trust, as stated, 'the negative sign of the coefficients imply (1) that higher trust leads to lower levels of attention and (2) the higher workload induced by the secondary task leads to lower attention.' While the study manipulated robot autonomy by allowing interventions, the paper does not explicitly state that this manipulation directly impacted trust levels. Therefore, only 'Task-complexity' is listed under 'factors_that_impacted_trust'.",10.1145/3371382.3378262,https://dl.acm.org/doi/10.1145/3371382.3378262,"In this work, we study and model how two factors of human cognition, trust and attention, affect the way humans interact with autonomous vehicles. We develop a probabilistic model that succinctly captures how trust and attention evolve across time to drive behavior, and present results from a human-subjects experiment where participants interacted with a simulated autonomous vehicle while engaging with a secondary task. Our main findings suggest that trust affects attention, which in turn affects the human’s decision to intervene with the autonomous vehicle."
"Bonneviot, Flavie; Coeugnet, Stéphanie; Brangier, Eric","How to improve pedestrians' trust in automated vehicles: new road infrastructure, external human–machine interface with anthropomorphism, or conventional road signaling?",2023,1,731,731,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants completed an online survey after watching videos of automated vehicles with different HMIs in standard and non-standard crossing situations, and they were asked to evaluate their feelings and behaviors.","Participants were asked to imagine crossing the street in front of an automated vehicle with different human-machine interfaces and to report their feelings, trust, and crossing behaviors.",Unspecified,Autonomous Vehicles,Research,Navigation,Street Crossing,passive observation,Participants observed videos of the automated vehicle and interfaces.,media,Participants watched videos of the interaction scenarios.,simulated,The robot was presented through videos.,pre-programmed (non-adaptive),The automated vehicle's behavior was pre-programmed and did not adapt to the user.,Questionnaires; Custom Scales,,,Trust was measured using questionnaires and custom scales.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the type of human-machine interface (LED, anthropomorphic, road infrastructure) and the crossing situation (standard, non-standard) to influence trust.","The presence of HMIs increased trust and decreased distrust, with the road infrastructure and anthropomorphic interface showing the most positive impact on trust.","Participants showed an overtrust towards the interfaces, especially the road infrastructure, which led to increased willingness to engage in dangerous crossing behaviors. There was a conflict between the stated preference for the BOLD interface and the higher trust and safety ratings for the Sirocco and Alfy interfaces.",The study found that road infrastructure and anthropomorphic interfaces were more effective at increasing pedestrian trust and safety compared to conventional road signaling on the vehicle.,"The robot (automated vehicle) was shown in videos stopping at a pedestrian crossing, and the human participant was asked to imagine crossing the street in front of it, evaluating their feelings and behaviors.",ANOVA; t-test; Pearson correlation,"The study used multivariate analyses of variance (ANOVA) with 3 to 5 factors to compare mean values of responses, and Student's t-tests for group comparisons. Multivariate Pearson's correlations were performed with trust and distrust variables. Rankings were analyzed according to a draw without replacement. These tests were used to assess the impact of different human-machine interfaces (HMIs) on participants' trust, distrust, perceived safety, anticipation, uncertainty, and crossing behaviors when interacting with automated vehicles. The ANOVA was used to determine if there were significant differences between the means of different groups (e.g., different HMIs), while the t-tests were used for pairwise comparisons. Pearson's correlations were used to examine the relationships between trust and distrust variables.",TRUE,Robot-interface-design,Robot-interface-design,,"The study manipulated the type of human-machine interface (HMI) presented to participants. Three different HMIs were used: BOLD (LED strips), Alfy (anthropomorphic avatar), and Sirocco (road infrastructure). The paper explicitly states that the study investigated the impact of these three different HMIs on pedestrians' trust. The different HMIs represent changes to interactive elements, such as screen displays, lighting, and indicators, which falls under the category of 'Robot-interface-design'. The results showed that the different HMIs had a significant impact on trust, with Sirocco and Alfy inducing higher trust levels than BOLD. Therefore, 'Robot-interface-design' is also listed as a factor that impacted trust. There were no factors that were manipulated that did not impact trust.",10.3389/fpsyg.2023.1129341,https://www.frontiersin.org/articles/10.3389/fpsyg.2023.1129341/full,"Introduction               Automated vehicles need to gain the trust of all road users in order to be accepted. To make technology trustworthy, automated vehicles must transmit crucial information to pedestrians through a human-machine interface, allowing pedestrians to accurately predict and act on their next behavior. However, the unsolved core issue in the field of vehicle automation is to know how to successfully communicate with pedestrians in a way that is efficient, comfortable, and easy to understand. This study investigated the impact of three human-machine interfaces specifically designed for pedestrians' trust during the street crossing in front of an automated vehicle. The interfaces used different communication channels to interact with pedestrians, i.e., through a new road infrastructure, an external human-machine interface with anthropomorphism, or with conventional road signaling.                                         Methods               Mentally projected in standard and non-standard use cases of human-machine interfaces, 731 participants reported their feelings and behavior through an online survey.                                         Results               Results showed that human-machine interfaces were efficient to improve trust and willingness to cross the street in front of automated vehicles. Among external human-machine interfaces, anthropomorphic features showed significant advantages in comparison with conventional road signals to induce pedestrians' trust and safer crossing behaviors. More than the external human-machine interfaces, findings highlighted the efficiency of the trust-based road infrastructure on the global street crossing experience of pedestrians with automated vehicles.                                         Discussion               All of these findings support trust-centered design to anticipate and build safe and satisfying human-machine interactions."
"Booth, Serena; Tompkin, James; Pfister, Hanspeter; Waldo, Jim; Gajos, Krzysztof; Nagpal, Radhika",Piggybacking Robots: Human-Robot Overtrust in University Dormitory Security,2017,1,108,108,10,Approximately 10 participants were excluded because they had already heard of the study,Real-World Environment,between-subjects,A robot asked participants to let it into or out of a secure dormitory. The robot was either unmodified or disguised as a food delivery robot. Participants were interviewed after the interaction.,Participants were asked to either assist a robot in entering or exiting a secure dormitory.,TurtleBot,Mobile Robots,Research,Navigation,Guiding,minimal interaction,Participants interacted with the robot verbally and by holding the door open.,real-world,The interaction took place in a real-world dormitory setting with a physical robot.,physical,A physical robot was used in the study.,wizard of oz (directly controlled),The robot was teleoperated by a human.,Behavioral Measures; Questionnaires,,Video Data,Trust was measured using a 5-point Likert scale and by observing whether participants assisted the robot.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's appearance was manipulated by disguising it as a food delivery robot, and the social context was manipulated by having participants interact individually or in groups.",The robot was more likely to gain entry when disguised as a food delivery robot. Groups were more likely to assist the robot than individuals. Participants who identified the robot as a bomb threat were also more likely to assist it.,Participants who identified the robot as a bomb threat were more likely to assist it. There was a conflation of autonomy and sentience among participants. Groups were more likely to assist the robot than individuals.,"People are more likely to assist a robot in entering a secure area when it appears to be delivering food, and groups are more likely to assist than individuals, indicating overtrust in robots can be a security risk.",The robot asked participants to let it into or out of a secure dormitory. Participants either held the door open for the robot or did not.,Fisher's exact test; Chi-squared; Logistic regression; point-biserial correlation coefficient,"The study used Fisher's Exact Test to compare the response distribution between two dormitories. Chi-squared tests were used to analyze the significance of differences in assistance rates between groups and individuals, between exiting and entering conditions, and between the robot's disguised and unmodified appearances. Logistic regression was used to analyze binary outcomes of whether participants assisted the robot, estimating the likelihood of a positive outcome given changes in independent variables. Finally, a point-biserial correlation coefficient was used to assess the relationship between self-reported trust in autonomous systems and study trial outcomes.",TRUE,Robot-aesthetics; Task-environment,Robot-aesthetics,Task-environment,"The study manipulated the robot's appearance by disguising it as a food delivery robot (Robot-aesthetics). This was done by adding a box with the Robot Grub logo and cookies on top of the TurtleBot. The study also implicitly manipulated the task environment by having the robot ask for assistance either to enter or exit the dormitory (Task-environment). The paper states that the robot was significantly more likely to gain entry when it appeared to be delivering cookies (Robot-aesthetics), indicating that this manipulation impacted trust. The study found that participants were as likely to assist the robot in exiting as in entering, indicating that the manipulation of the task environment did not significantly impact trust.",10.1145/2909824.3020211,https://dl.acm.org/doi/10.1145/2909824.3020211,"Canovertrustinrobotscompromisephysicalsecurity? We positionedarobotoutsideasecure-accessstudentdormitory and madeitaskpassersbyforaccess.Individualparticipants wereaslikelytoassisttherobotinexitingthedormitory (40%assistancerate,4/10individuals)asinentering(19%, 3/16individuals). Groupsofpeoplewere morelikelythanindividualstoassisttherobotinentering(71%,10/14groups). Whentherobot wasdisguisedasafooddeliveryagentfor theﬁctionalstart-upRobot Grub,individuals were more likelytoassisttherobotinentering(76%,16/21individuals). Lastly,participantswhoidentiﬁedtherobotasabombthreat demonstratedatrendtowardassistingtherobot(87%,7/8 individuals,6/7groups). Thus,overtrust—theunfounded beliefthattherobotdoesnotintendtodeceiveorcarry risk—canrepresentasigniﬁcantthreattophysicalsecurity atauniversitydormitory."
"Bordbar, Fareed; Salehzadeh, Roya; Cousin, Christian; Griffin, Darrin J.; Jalili, Nader",Analyzing Human-Robot Trust in Police Work Using a Teleoperated Communicative Robot,2021,1,54,37,17,17 participants were excluded because they failed to provide the identifying number assigned to them in order to link their pretest and post-test data,Controlled Lab Environment,within-subjects,"Participants completed pre-test surveys, attended communication and robotics training sessions, and then completed post-test surveys.",Participants were trained to pilot a teleoperated robot and use its communication capabilities.,TurtleBot,Mobile Robots; Mobile Manipulators,Research; Social,Navigation,Remote Navigation,minimal interaction,Participants interacted with the robot through teleoperation and audio/video communication.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,wizard of oz (directly controlled),The robot was directly controlled by the participants through teleoperation.,Questionnaires; Custom Scales,Negative Attitude towards Robots Scale (NARS); Human-Robot Trust Scale/Questionnaire (HRTS/HRITS),,Trust was assessed using questionnaires and custom scales.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the robot's appearance and behavior through its design and teleoperation, and influenced human expectations through training, and the interaction medium through the use of a teleoperated robot.","The study found that LEOs had a high level of trust in the teleoperated communicative robot, and their negative attitudes towards the social influence of robots were reduced.","The study found that humanoid appearance was not considered important by LEOs, which is different from some social robotics studies. The study also found that battery life was a very important factor for LEOs.",LEOs had a positive experience operating the robot and displayed a high level of trust in the communicative robot platform.,"The robot was teleoperated by LEOs, who used it to communicate with others through audio and video. The human participant piloted the robot and used its communication capabilities.",t-test; t-test,"The study used paired t-tests to analyze pre- and post-test responses on the NARS scale, specifically examining changes in negative attitudes towards the social influence of robots. Additionally, t-tests were used to compare the mean scores of the Affective Learning Scale (ALS) and the Human-Robot Interaction Trust Scale (HRITS) against the mid-point of their respective scales to determine if the means were significantly higher, indicating a positive impact of the experiment and a high level of trust.",TRUE,Robot-verbal-communication-content; Robot-interface-design; Robot-aesthetics; Robot-autonomy,Robot-verbal-communication-content; Robot-interface-design; Robot-autonomy,Robot-aesthetics,"The study manipulated several factors related to the robot and its interaction with the LEOs. 'Robot-verbal-communication-content' was manipulated through the training provided to the LEOs on communication strategies, which influenced how they used the robot to communicate. The robot's design, including the touchscreen, cameras, and audio equipment, represents a manipulation of 'Robot-interface-design'. The physical appearance of the robot, including the addition of a touchscreen and pan/tilt camera, is considered a manipulation of 'Robot-aesthetics'. The level of control given to the LEOs through teleoperation is a manipulation of 'Robot-autonomy'. The study found that the communication capabilities, interface design, and the level of control (teleoperation) impacted trust, as evidenced by the positive feedback and high trust scores. However, the study explicitly states that humanoid appearance (a component of 'Robot-aesthetics') was not considered important by LEOs, indicating that this factor did not impact trust in this context. The study also mentions that battery life was important, but this is not a manipulated factor, but rather a design consideration that influenced trust.",10.1109/RO-MAN50785.2021.9515489,https://ieeexplore.ieee.org/document/9515489/,"Recent advances in robotics have accelerated their widespread use in nontraditional domains such as law enforcement. The inclusion of robotics allows for the introduction of time and space in dangerous situations, and protects law enforcement ofﬁcers (LEOs) from the many potentially dangerous situations they encounter. In this paper, a teleoperated robot prototype was designed and tested to allow LEOs to remotely and transparently communicate and interact with others. The robot featured near face-to-face interactivity and accuracy across multiple verbal and non-verbal modes using screens, microphones, and speakers. In cooperation with multiple law enforcement agencies, results are presented on this dynamic and integrative teleoperated communicative robot platform in terms of attitudes towards robots, trust in robot operation, and trust in human-robot-human interaction and communication."
"Branyon, Jessica; Pak, Richard","Investigating Older Adults’ Trust, Causal Attributions, and Perception of Capabilities in Robots as a Function of Robot Appearance, Task, and Reliability",2015,1,100,100,0,No participants were excluded,Online Crowdsourcing,mixed design,"Participants viewed video vignettes of a robot performing tasks, then answered questions about trust, causal attributions, and perceived capabilities.",Participants watched videos of a robot completing either cognitive or physical tasks and rated their trust in the robot.,Baxter,Humanoid Robots; Collaborative Robots (Cobots),Research,Evaluation,Rating,passive observation,Participants watched videos of the robot performing tasks.,media,Participants viewed video vignettes of the robot.,simulated,The robot's face was superimposed onto the Baxter robot in the video.,pre-programmed (non-adaptive),The robot's actions were pre-programmed in the video vignettes.,Questionnaires,,,Trust was measured using a Likert scale questionnaire.,no modeling,The study did not use any computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's age appearance, task domain, and reliability were manipulated to see how they affect trust.",Trust was expected to be highest when the robot's appearance was congruent with the task and its performance was reliable.,The study hypothesizes that older adults will make more dispositional attributions and have lower trust levels overall.,"The study aims to investigate how a robot's appearance, task domain, and reliability influence older adults' trust in robots.","The robot performs either cognitive or physical tasks, while the human participant observes the robot's performance in video vignettes and provides ratings.",ANOVA,"A repeated measures analysis of variance (ANOVA) was used to investigate the effects of manipulating robot appearance, task domain, and reliability on subjective trust, causal attributions, and perceived capabilities. The ANOVA included participant age as a between-subjects variable and robot age, task domain, and robot reliability as within-subjects factors.",TRUE,Robot-aesthetics; Robot-accuracy; Task-complexity,Robot-aesthetics; Robot-accuracy,Task-complexity,"The study manipulated the robot's appearance by using different facial stimuli to represent a younger and older adult, which falls under 'Robot-aesthetics'. The robot's reliability was manipulated by showing the robot either successfully or unsuccessfully completing the task, which is categorized as 'Robot-accuracy'. The task domain was manipulated by having the robot perform either cognitive or physical tasks, which is categorized as 'Task-complexity'. The paper states that the robot's appearance and reliability influenced trust levels, while the task domain was treated as an exploratory variable and a main effect of task domain was hypothesized but not explicitly stated to have impacted trust in the results section. Therefore, 'Robot-aesthetics' and 'Robot-accuracy' are listed as factors that impacted trust, and 'Task-complexity' is listed as a factor that did not impact trust.",10.1177/1541931215591335,http://journals.sagepub.com/doi/10.1177/1541931215591335,"The purpose of this study is to examine the extent to which appearance, task, and reliability of a robot is susceptible to stereotypic thinking. Stereotypes can influence the types of causal attributions that people make about the performance of others. Just as causal attributions may affect an individual’s perception of other people, it may similarly affect perceptions of technology. Stereotypes can also influence perceived capabilities of others. In situations where stereotypes are activated, an individual’s perceived capabilities are typically diminished. The tendency to adjust perceptions of capabilities of others may translate into levels of trust placed in the individual’s abilities. A factorial survey using video vignettes will be utilized to assess young and older adults’ attitudes toward a robot’s behavior and appearance. We hypothesize that a robot’s older appearance will yield lower levels of trust, more dispositional attributions, and lower perceptions of capabilities while high reliability should positively impact trust."
"Bridgwater, Tom; Giuliani, Manuel; van Maris, Anouk; Baker, Greg; Winfield, Alan; Pipe, Tony",Examining Profiles for Robotic Risk Assessment: Does a Robot's Approach to Risk Affect User Trust?,2020,1,40,40,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were asked to supervise a robot making decisions in a virtual nuclear environment. They observed the robot's decisions under four different risk profiles, and could question and alter the robot's choices. After each profile, participants completed a trust questionnaire. Finally, they ranked the profiles by trust and the highest-ranked profile was used in a bonus round.","Participants supervised a robot navigating a virtual corridor, making decisions at 'gates' involving risk, and could question or alter the robot's choices.",Unspecified,Mobile Robots,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the robot through a simulation, with options to question and alter its decisions.",simulation,The interaction took place in a simulated virtual nuclear environment.,simulated,The robot was a virtual representation within the simulation.,shared control (fixed rules),"The robot made decisions based on pre-defined risk profiles, but participants could intervene.",Behavioral Measures; Questionnaires; Ranking,Jian et al. Trust Scale,,"Trust was assessed using a questionnaire, behavioral measures of intervention, and a ranking task.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's risk profile was directly manipulated, with four different approaches (risk-averse, risk-seeking, risk-neutral, and human-like) being tested. The task was framed as a supervision task in a hazardous environment.","The risk-seeking robot was significantly less trusted than the other profiles. There was no significant difference in trust between the risk-averse, risk-neutral, and human-like profiles.","The level of questioning did not correlate with trust, and participants with prior knowledge of expected value showed less trust in the human-like risk profile. The study also found that performance did not affect the trust ranking of the robots.","A risk-seeking robot is significantly less trusted than risk-averse, risk-neutral, or human-like robots.","The robot moved through a virtual corridor, making decisions at 'gates' based on its risk profile. The human participant observed the robot's decisions, and could question or alter them.",Friedman test; Wilcoxon signed-rank test; greenhouse-geisser correction; pairwise comparisons,"The study used a Friedman test to determine if there was a significant difference in the level of intervention and ranking between the different robot risk profiles. Wilcoxon signed-rank tests were then used to determine if there were significant differences between specific pairs of profiles, with a Bonferroni correction applied for multiple comparisons. A Greenhouse-Geisser correction was used to address violations of sphericity in the trust questionnaire data, followed by pairwise comparisons to identify significant differences between profiles. The analysis also examined the interaction effect of previous knowledge of expected value on trust scores using an F-test.",TRUE,Robot-task-strategy,Robot-task-strategy,,"The study manipulated the robot's approach to risk, which directly influenced its task strategy. The robot used four different risk profiles: risk-averse, risk-seeking, risk-neutral, and a human-like approach based on prospect theory. This manipulation is described in the introduction: 'This work investigates four robotic approaches to risk and, through a user study, explores the levels of trust placed in each. These approaches are: risk averse, risk seeking, risk neutral and a human approach to risk.' The paper also states that 'The robot indicates its decision and participants are given the options to question the robot, question the robot further and ultimately accept or alter its decisions. This is then repeated along the same corridor for each of the four risk profiles'. The different risk profiles directly impacted the robot's decision-making process and thus its task strategy. The results showed that the risk-seeking strategy was significantly less trusted than the other three, as stated in the abstract: 'It is shown that a robot that is risk seeking is significantly less trusted than a risk averse robot, a risk neutral robot and a robot utilising human approach to risk.' Therefore, the 'Robot-task-strategy' was the factor that impacted trust. There were no other factors manipulated in the study.",10.1145/3319502.3374804,https://dl.acm.org/doi/10.1145/3319502.3374804,"As autonomous robots move towards ubiquity, the need for robots to make decisions under risk that are trustworthy becomes increasingly significant; both to aid acceptance and to fully utilise their autonomous capabilities. We propose that incorporating a human approach to risk assessment into a robot’s decision making process will increase user trust. This work investigates four robotic approaches to risk and, through a user study, explores the levels of trust placed in each. These approaches are: risk averse, risk seeking, risk neutral and a human approach to risk. Risk is artificially stimulated through performance-based compensation, in line with previous studies. The study was conducted in a virtual nuclear environment created using the Unity games engine. Forty participants were asked to complete a robot supervision task, in which they observed a robot making risk based decisions and were able to question the robot, question the robot further and ultimately accept or alter the robot’s decision. It is shown that a robot that is risk seeking is significantly less trusted than a risk averse robot, a risk neutral robot and a robot utilising human approach to risk. There was found to be no significant difference between the levels of trust placed in the risk averse, risk neutral and human approach to risk. It is also found that the level to which participants question a robot’s decisions does not form an accurate measure of trust. The results suggest that when designing a robot that must make risk based decisions during teleoperation in a hazardous environment, an engineer should avoid a risk seeking robot. However, that same engineer may choose whichever of the remaining risk profiles best suits the implementation, with knowledge that the trust in their system is unlikely to be significantly affected."
"Brink, Kimberly A.; Wellman, Henry M.",Robot teachers for children? Young children trust robots depending on their perceived accuracy and agency.,2020,2,60,53,7,7 children were excluded because they failed to accurately answer the four name-check questions during the accuracy trials,Controlled Lab Environment,within-subjects,"Children watched videos of two Nao robots giving contrasting testimony on the names of familiar and unfamiliar objects, judged which robot's testimony to trust, and rated their perceptions of the robots' capacities using the Robot Beliefs Interview (RBI).",Children were asked to identify which of two robots was accurate and then to choose which robot to ask for the name of a novel object and endorse the name given by that robot.,Nao,Humanoid Robots,Educational; Research,Social,Tutoring,passive observation,Children watched videos of the robots but did not interact with them directly.,media,The interaction was presented through videos of the robots.,physical,The robots were physically present in the videos.,pre-programmed (non-adaptive),The robots followed a pre-set script in the videos.,Behavioral Measures; Questionnaires,,,Trust was measured using behavioral measures (asking and endorsing) and a questionnaire about robot beliefs.,"parametric models (e.g., regression)",Regression analysis was used to predict performance based on agency and experience.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robots' accuracy was manipulated by having one robot always name objects correctly and the other always incorrectly, and the robots differed in color.","Children trusted the accurate robot more than the inaccurate robot, and this was influenced by their perception of the robot's agency.",Children's trust in the accurate robot increased with their perception of the robot's psychological agency.,Young children appropriately trust information from robots based on their accuracy and perceived psychological agency.,"The robots provided names for objects, and the children chose which robot to trust and endorse the name given by that robot.",t-test; Linear regression,"The study used t-tests to compare children's performance on accuracy, ask, and endorsement questions against chance levels. Regression analysis was used to predict children's performance on the selective trust task based on factors like perceived agency, experience, age, sex, and presentation order. Specifically, the regression analysis aimed to determine if children's attributions of psychological agency to the robots predicted their trust in the accurate robot.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the accuracy of the robots by having one robot always name objects correctly and the other always name them incorrectly. This is explicitly stated in the paper: 'One robot correctly named all four objects while the other incorrectly named all four.' This manipulation of accuracy directly impacted children's trust, as they were more likely to trust the accurate robot. The paper states: 'Children's responses to our selective trust questions demonstrated that they trusted the accurate robot over the inaccurate robot: They typically asked for and agreed with information provided by the accurate robot.'",10.1037/dev0000884,http://doi.apa.org/getdoi.cfm?doi=10.1037/dev0000884,"Children acquire extensive knowledge from others. Today, children receive information from not only people but also technological devices, like social robots. Two studies assessed whether young children appropriately trust technological informants. One hundred and four 3-year-olds learned the names of novel objects from either a pair of social robots or inanimate machines, where 1 informant was previously shown to be accurate and the other inaccurate. Children trusted information from an accurate social robot over an inaccurate one, as they have been shown to do for human informants, and even more so when they perceived the robots as having psychological agency. However, children did not learn selectively from inanimate, but accurate, machines. Children can learn from technological devices (e.g., social robots) but trust their information more when the device appears to have mindful agency."
"Brink, Kimberly A.; Wellman, Henry M.",Robot teachers for children? Young children trust robots depending on their perceived accuracy and agency.,2020,2,47,33,14,14 children were excluded because they failed to accurately answer all four name-check questions,Controlled Lab Environment,within-subjects,"Children watched videos of two blob-like machines giving contrasting testimony on the names of familiar and unfamiliar objects, judged which machine's testimony to trust, and rated their perceptions of the machines' capacities using a modified Robot Beliefs Interview (RBI).",Children were asked to identify which of two machines was accurate and then to choose which machine to ask for the name of a novel object and endorse the name given by that machine.,Unspecified,Other,Educational; Research,Social,Tutoring,passive observation,Children watched videos of the machines but did not interact with them directly.,media,The interaction was presented through videos of the machines.,physical,The machines were physically present in the videos.,pre-programmed (non-adaptive),The machines followed a pre-set script in the videos.,Behavioral Measures; Questionnaires,,,Trust was measured using behavioral measures (asking and endorsing) and a questionnaire about machine beliefs.,"parametric models (e.g., regression)",Regression analysis was used to predict performance based on agency and experience.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The machines' accuracy was manipulated by having one machine always name objects correctly and the other always incorrectly, and the machines were amorphous blob shapes.","Children did not consistently trust the accurate machine, and their perception of agency did not predict their trust.","Children did not consistently trust the accurate machine, and their perception of agency did not predict their trust, unlike in Study 1.",Children do not simply trust any accurate informant; they require cues of psychological agency to trust and learn from a technological device.,"The machines provided names for objects, and the children chose which machine to trust and endorse the name given by that machine.",t-test; Linear regression; Pearson correlation,"The study used t-tests to compare children's performance on accuracy, ask, and endorsement questions against chance levels. A correlation analysis was used to examine the relationship between children's attributions of agency and their willingness to trust the machines. Additionally, a cross-study analysis was conducted using a t-test to compare the total correct performance of children in Study 1 (robots) and Study 2 (machines). A regression analysis was also performed across both studies to predict overall performance based on agency, experience, informant type, age, sex, and order.",TRUE,Robot-accuracy,,Robot-accuracy,"Similar to Study 1, the study manipulated the accuracy of the machines, with one always naming objects correctly and the other incorrectly. This is stated in the paper: 'Because one of these machines consistently emitted accurate names for common objects, and one consistently did not, accuracy was held constant across Studies 1 and 2'. However, unlike Study 1, this manipulation of accuracy did not impact children's trust. The paper states: 'While children were able to identify which machine was inaccurate, they did not consistently ask the accurate machine for the object name or endorse the object name stated by the accurate machine.'",10.1037/dev0000884,http://doi.apa.org/getdoi.cfm?doi=10.1037/dev0000884,"Children acquire extensive knowledge from others. Today, children receive information from not only people but also technological devices, like social robots. Two studies assessed whether young children appropriately trust technological informants. One hundred and four 3-year-olds learned the names of novel objects from either a pair of social robots or inanimate machines, where 1 informant was previously shown to be accurate and the other inaccurate. Children trusted information from an accurate social robot over an inaccurate one, as they have been shown to do for human informants, and even more so when they perceived the robots as having psychological agency. However, children did not learn selectively from inanimate, but accurate, machines. Children can learn from technological devices (e.g., social robots) but trust their information more when the device appears to have mindful agency."
"Britten, Nicholas; Johns, Mishel; Hankey, Jon; Kurokawa, Ko",Do you trust me? Driver responses to automated evasive maneuvers,2023,1,36,34,2,2 participants were excluded because a DAS error made their video data unavailable,Real-World Environment,between-subjects,"Participants experienced a Wizard-of-Oz simulated CAD vehicle on a test track, with either a braking or swerving evasive maneuver. Data was collected on driver interventions, control inputs, and eye glances.","Participants were instructed to drive a vehicle in CAD mode on a test track, and the vehicle performed an evasive maneuver (braking or swerving) to avoid an obstacle.",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants experienced a simulated autonomous vehicle in a real-world setting, but did not physically interact with the vehicle's controls during the evasive maneuver.",real-world,"The study was conducted in a real-world driving environment, with a physical vehicle.",physical,The robot was a physical vehicle modified to simulate autonomous driving.,wizard of oz (directly controlled),The vehicle's autonomous behavior was simulated by a hidden human experimenter.,Behavioral Measures,,Video Data; Eye-tracking Data; Performance Metrics; robot data,"Trust was assessed through behavioral measures such as driver interventions, control inputs, and eye glances.",no modeling,Trust was not modeled computationally; the study used descriptive statistics.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The study manipulated the type of evasive maneuver (braking or swerving) performed by the simulated autonomous vehicle, which influenced the task difficulty and robot behavior.","The study found that most drivers did not intervene during the evasive maneuvers, suggesting they trusted the system. The type of maneuver did not significantly affect the likelihood of intervention.","Most drivers did not intervene during the evasive maneuvers, and there was no significant difference in intervention rates between braking and swerving maneuvers. Drivers tended to look towards the obstacle during the maneuver.",Most drivers trusted the CAD system enough not to intervene during a system-initiated evasive maneuver after a brief period of exposure.,The robot (simulated autonomous vehicle) performed either a braking or swerving maneuver to avoid an obstacle. The human participant was instructed to monitor the vehicle while in CAD mode and was not required to intervene.,Chi-squared; Chi-squared; Chi-squared,"The study used chi-square tests of independence to examine the association between the type of evasive maneuver (braking or swerving) and several behavioral outcomes. Specifically, these tests were used to determine if there was a significant relationship between maneuver type and whether participants intervened, reached for the steering wheel, or reached for the brake pedal. The purpose of these tests was to determine if the type of maneuver influenced the likelihood of these driver behaviors.",TRUE,Robot-task-strategy,,Robot-task-strategy,"The study manipulated the type of evasive maneuver performed by the simulated autonomous vehicle, which is a change in the task completion strategy of the robot. The robot either braked or swerved to avoid an obstacle. This is a change in the robot's task strategy, not its accuracy, as both maneuvers were designed to successfully avoid the obstacle. The study found that the type of maneuver (braking or swerving) did not significantly affect the likelihood of driver intervention, suggesting that this manipulation did not impact trust. The study explicitly states 'A chi-square test of independence showed that there was no significant association between maneuver type and the number of participants who chose to intervene during the evasive maneuver'. Therefore, the factor 'Robot-task-strategy' was manipulated but did not impact trust.",10.3389/fpsyg.2023.1128590,https://www.frontiersin.org/articles/10.3389/fpsyg.2023.1128590/full,"An increasing number of Conditionally Automated Driving (CAD) systems are being developed by major automotive manufacturers. In a CAD system, the automated system is in control of the vehicle within its operational design domain. Therefore, in CAD the vehicle is capable of tactical control of the vehicle and needs to be able to maneuver evasively by braking or steering to avoid objects. During these evasive maneuvers, the driver may attempt to take back control of the vehicle by intervening. A driver interrupting a CAD vehicle while properly performing an evasive maneuver presents a potential safety risk. To investigate this issue, 36 participants were recruited to participate in a Wizard-of-Oz research study. The participants experienced one of two evasive maneuvers of moderate intensity on a test track. The evasive maneuver required the CAD system to brake or steer to avoid the box placed in the lane of travel of the test vehicle. Drivers glanced toward the obstacle but did not intervene or prepare to intervene in response to the evasive maneuver. Importantly, the drivers who chose to intervene did so safely. These findings suggest that after experiencing a CAD vehicle for a brief period, most participants trusted the system enough to not intervene during a system-initiated evasive maneuver."
"Broad, Alexander; Schultz, Jarvis; Derry, Matthew; Murphey, Todd; Argall, Brenna",Trust Adaptation Leads to Lower Control Effort in Shared Control of Crane Automation,2017,1,22,20,2,2 participants were removed from the study before analysis due to a poor understanding of the task requirements,Controlled Lab Environment,between-subjects,"Participants were randomly assigned to either a static or adaptive trust cohort. Both cohorts completed an initialization phase with three trials of shared control with fixed trust values on a low-difficulty maze task. Then, both cohorts completed five trials of shared control on low, medium, and high difficulty maze tasks. The adaptive cohort had their trust level updated after each trial, while the static cohort's trust level remained fixed after the initialization phase. All participants also completed five trials of direct control on the low-difficulty maze.",Participants used a joystick to provide a reference trajectory for a simulated planar crane to move a suspended payload through a maze to a target location.,Unspecified,Mobile Manipulators,Research,Navigation,Path Following,minimal interaction,Participants interacted with a simulated robot through a joystick.,simulation,The interaction took place in a simulated planar crane environment.,simulated,The robot was a simulated planar crane.,shared control (adaptive),The robot adapted its control authority based on the user's performance.,Performance-Based Measures; Real-time Trust Measures,,Performance Metrics,Trust was measured using a performance-based metric based on the deviation from the reference trajectory and updated in real-time.,"parametric models (e.g., regression)","Trust was modeled using a Gaussian distribution of deviations from the reference trajectory, updated with a learning rate.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the level of autonomy by adapting the control authority based on a trust metric, and the task difficulty by using different maze configurations. Feedback was manipulated by providing the user with a trust metric that was used to modulate the user input.","The adaptive trust metric led to a significant decrease in the controller effort required to track the user's reference trajectory, suggesting that the system learned to modulate the user's input effectively. The adaptive trust cohort outperformed the static trust cohort in the final maze configuration.","The adaptive trust cohort showed a significant decrease in controller effort compared to the static cohort in the final maze configuration, but not in the initial configurations. The study also found that the results held regardless of whether the initial control authority allocation was an over- or under-estimate of the user's expertise.","An adaptive trust metric, based on a control-theoretic formulation, improved the ability of the shared-control system to produce reference trajectories that required significantly less effort for the controller to track than those provided by users with a static trust metric.","The robot, a simulated planar crane, moved a suspended payload through a maze. The human used a joystick to provide a reference trajectory for the robot to follow.",t-test,"The study used a two-tailed Student's t-test to compare the average controller magnitude (U) between the adaptive and static trust cohorts in each maze configuration. The t-test was also used to analyze the change in average controller magnitude within the adaptive trust cohort, comparing users who finished with higher trust versus those with lower trust than their initial trust value. The purpose of these tests was to determine if there were statistically significant differences in controller effort between the two cohorts and within the adaptive cohort based on trust adaptation.",TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,Task-complexity,"The study manipulated 'Robot-autonomy' by having an adaptive trust cohort where the robot's control authority was adjusted based on the user's performance, and a static trust cohort where the control authority remained fixed after an initial training period. This is explicitly stated in the paper: 'The adaptive cohort had their trust level updated after each trial, while the static cohort's trust level remained fixed after the initialization phase.' The study also manipulated 'Task-complexity' by using three different maze configurations of increasing difficulty (low, medium, and high), as described in the paper: 'We test three different task configurations of increasing difficulty (Fig. 2). First, a low difficulty configuration... Then, a medium difficulty configuration... Finally, a high difficulty configuration...'. The results showed that the adaptive trust metric led to a significant decrease in controller effort, indicating that 'Robot-autonomy' impacted trust. The paper states: 'We also find (Fig. 5) a statistically significant difference between the average controller magnitude, U, required to track reference trajectories provided by users in the static trust cohort versus those in the adaptive trust cohort, in the final maze configuration (p < 0.01).' The paper also states that both cohorts showed a significant decrease in controller effort from the initial to final maze configuration, indicating that the task complexity did not impact trust, but rather the user's ability to learn the system dynamics. The paper states: 'In both the static (p < 0.01) and adaptive (p < 0.01) trust cohorts, we see a statistically significant decrease in the average controller magnitude required to track the user's reference trajectory in the final maze configuration when compared with the initial maze configuration. This suggests that users in both cohorts are able to learn pertinent aspects of the system dynamics and how to provide stable reference trajectories from the viewpoint of the automated controller.'",10.1109/LRA.2016.2593740,http://ieeexplore.ieee.org/document/7518605/,"We present a shared-control framework predicated on a measure of trust in the operator, that is calculated automatically based on the quality of the interactions between a human and autonomous system. This measure of trust is built upon a controltheoretic foundation that rewards stable operation of the system to give more trusted users additional control authority. The level of control authority is used to modify the human input, and as a result, we observe a minimization of the required effort of the controller. We validate this work within a planar crane environment with a receding horizon controller to assist with the regulation of the system dynamics. The human deﬁnes the reference trajectory for the controller. In an experimental study users navigate a suspended payload through a set of maze conﬁgurations. We ﬁnd that adaptation of the trust metric over time provides the beneﬁt of substantially (p < 0.01) improving the automated system’s ability to modulate the user’s input, resulting in stable reference trajectories that require less effort to track. In effect, the human and automation spend less time ﬁghting each other during task execution, suggesting that the automated system and user each have a better understanding of the other’s ability."
"Bryant, De'Aira; Borenstein, Jason; Howard, Ayanna",Why Should We Gender?: The Effect of Robot Gendering and Occupational Stereotypes on Human Trust and Perceived Competency,2020,1,50,150,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of three conditions (male, female, or gender-neutral robot). They watched a short video of the robot and then completed a questionnaire about the robot's perceived competency and trust.",Participants rated the perceived occupational competency and trust in a robot for 14 different occupations.,Pepper,Humanoid Robots; Expressive Robots,Research; Social,Evaluation,Survey/Questionnaire Completion,passive observation,Participants watched a video of the robot.,media,Participants watched a short video of the robot.,physical,Participants saw a video of a physical robot.,pre-programmed (non-adaptive),The robot performed a pre-recorded script.,Questionnaires,,,Trust was measured using a questionnaire.,"parametric models (e.g., regression)",Ordinal logistic regression was used to determine how effective a predictor perceived competency was for perceived trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's gender was manipulated through voice and name, which was intended to influence trust based on gender stereotypes.",The study found no significant effect of robot gender on trust. Perceived competency was a better predictor of trust than robot gender or participant gender.,"A majority of participants in the gender-neutral condition identified the robot as male, suggesting a potential bias in the perception of gender-neutral robots. There was a small effect of participant gender on perceived competency for the package deliverer occupation, where female participants rated the robot as more competent.",Robot gender does not significantly impact human trust in a robot's occupational competency; perceived competency is a stronger predictor of trust.,"The robot presented a short introduction video, and the human participant completed a questionnaire rating the robot's perceived competency and trust for various occupations.",Kruskal-Wallis; Logistic regression,"The Kruskal-Wallis test, a non-parametric test, was used to determine if there were any statistically significant differences in the perception of occupational competency and trust between the three experimental groups (male, female, and gender-neutral robot). This test was applied to each of the 14 occupations. An ordinal logistic regression analysis was conducted to determine how effective perceived competency was as a predictor for perceived trust, while also considering robot gender and participant gender.",TRUE,Robot-verbal-communication-content; Robot-aesthetics,,Robot-verbal-communication-content; Robot-aesthetics,"The study manipulated the robot's gender through its name and voice, which falls under 'Robot-verbal-communication-content' because it changes the content of what the robot communicates (its identity). The robot's gender was also manipulated through the use of a male, female, or gender-neutral voice, which is a change in the robot's appearance and thus falls under 'Robot-aesthetics'. The study found that neither the robot's gender (manipulated through name and voice) nor the participant's gender had a significant impact on trust. Therefore, both 'Robot-verbal-communication-content' and 'Robot-aesthetics' are listed under 'factors_that_did_not_impact_trust'. The study did not find any factors that impacted trust, so 'factors_that_impacted_trust' is left empty.",10.1145/3319502.3374778,https://dl.acm.org/doi/10.1145/3319502.3374778,"The attribution of human-like characteristics onto humanoid robots has become a common practice in Human-Robot Interaction by designers and users alike. Robot gendering, the attribution of gender onto a robotic platform via voice, name, physique, or other features is a prevalent technique used to increase aspects of user acceptance of robots. One important factor relating to acceptance is user trust. As robots continue to integrate themselves into common societal roles, it will be critical to evaluate user trust in the robot’s ability to perform its job. This paper examines the relationship among occupational gender-roles, user trust and gendered design features of humanoid robots. Results from the study indicate that there was no significant difference in the perception of trust in the robot’s competency when considering the gender of the robot. This expands the findings found in prior efforts that suggest performance-based factors have larger influences on user trust than the robot’s gender characteristics. In fact, our study suggests that perceived occupational competency is a better predictor for human trust than robot gender or participant gender. As such, gendering in robot design should be considered critically in the context of the application by designers. Such precautions would reduce the potential for robotic technologies to perpetuate societal gender stereotypes."
"Bryant, De'Aira; Xu, Jin; Rogers, Kantwon; Howard, Ayanna",The Effect of Conceptual Embodiment on Human-Robot Trust During a Youth Emotion Classification Task,2021,1,312,241,71,"60 participants were excluded due to a lack of a diverse male stimuli condition to compare, 11 participants were excluded for failing the manipulation check or not providing consent",Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of four experimental conditions. They completed an emotion classification task with robot assistance, then answered demographic questions, a trust questionnaire, and an open-ended question about the robot's appearance.",Participants classified the emotion of youth in images with the assistance of a robot recommender.,Unspecified,Other,Research; Social,Evaluation,Image Analysis,minimal interaction,"Participants interacted with the robot through a screen, receiving recommendations but not physically interacting with it.",media,"The interaction was based on static images of faces, providing a low level of immersion.",hypothetical,"The robot's embodiment was left to the participant's conceptualization, with no physical or simulated representation provided.",pre-programmed (non-adaptive),The robot provided pre-programmed recommendations without adapting to the user's input.,Questionnaires; Behavioral Measures,Jian et al. Trust Scale,,Trust was measured using a questionnaire and average final confidence as a behavioral measure.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated task certainty (image clarity) and gender alignment (participant vs. image gender) to influence trust. They also influenced human expectations by not providing a visual representation of the robot, allowing participants to conceptualize it.","Task certainty and gender alignment significantly impacted trust, with higher trust reported in certain conditions and gender mismatch conditions. Conceptual embodiment also influenced trust, with participants who conceptualized the robot as humanoid reporting higher trust.","Participants in uncertain conditions reported lower levels of trust, which is opposite to the initial hypothesis. Participants who conceptualized the robot as humanoid had higher levels of trust than those who conceptualized it as unembodied. There was a weak correlation between level of interaction with children and trust score, where participants who interacted with children less frequently had higher levels of trust.","Conceptual embodiment significantly impacts trust, with participants who conceptualized the robot as humanoid reporting higher levels of trust than those who conceptualized it as unembodied.","The robot provided an emotion label recommendation for each image. The human participant provided an initial emotion label and confidence rating, then a final label and confidence rating after seeing the robot's recommendation.",ANOVA; Spearman correlation; ANOVA,"The study used ANOVA to assess the impact of participant gender and parental status on trust scores and average final confidence. Spearman's rank correlation coefficient was used to assess the relationship between trust score and average final confidence, as well as the correlation between user-related factors and trust measures. MANOVA was used to determine the influence of conceptual embodiment, gender alignment, and certainty on the combined trust measures (trust score and average final confidence).",TRUE,Task-complexity; Task-environment,Task-complexity,Task-environment,"The researchers manipulated the level of certainty in the images, which directly impacts the difficulty of the emotion classification task. This is categorized as 'Task-complexity' because it changes the cognitive demands of the task. The study also manipulated the gender alignment between the participant and the image, which is categorized as 'Task-environment' because it changes the context of the task. The study found that task certainty (Task-complexity) significantly impacted trust, with participants in uncertain conditions reporting lower levels of trust. The study did not find that gender alignment (Task-environment) impacted trust, but it did impact average final confidence. The study also explored conceptual embodiment, but this was not a direct manipulation, rather a user-related factor that was measured. The study also explored participant gender, parental status, and level of interaction with children, but these were not manipulated, rather user-related factors that were measured.",10.1109/ARSO51874.2021.9542839,https://ieeexplore.ieee.org/document/9542839/,"Interactive robots are increasingly being used in social environments to provide humans with information, guidance and even recommendation. User trust plays an important role in the human decision-making process for whether to accept or reject a robot’s recommendation. Prior work in HRI has investigated various robot-related, task-related, and userrelated factors that inﬂuence human-robot trust. This work further examines task and user-related factors inﬂuencing user trust during a robot-assisted emotion classiﬁcation task when the embodiment of the robot is left for the user to conceptualize. The chosen task-related factors manipulate the level of certainty in the images to be labeled and the gender alignment between the individual in the image stimuli and the participant. We further consider participant gender, parental status, level of interaction with children, level of comfortability with robots and conceptual robot embodiment. An online between-subjects experiment was conducted with 241 participants. Experimental results show that task certainty and gender alignment along with level of comfortability with robots, level of interaction with children and conceptual embodiment all had signiﬁcant effects on human-robot trust. Notably, participants who described their conceptual embodiment of the robot with human-like characteristics reported higher levels of trust than those who did not. These ﬁndings provide insights that contribute to a greater understanding of factors inﬂuencing human-robot trust during a situated scenario involving robot recommendation."
"Calhoun, Christopher S.; Bobko, Philip; Gallimore, Jennie J.; Lyons, Joseph B.",Linking precursors of interpersonal trust to human-automation trust: An expanded typology and exploratory experiment,2019,1,40,40,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were trained on the task, then completed a short practice scenario. They then completed three blocks of 20 trials each, with varying levels of aid reliability. After each block, participants completed questionnaires assessing trust, ability, benevolence/integrity, transparency, and humanness.","Participants monitored a video feed from a UGV, classifying potential insurgents as 'insurgent' or 'non-insurgent'. They received recommendations from an automated aid and could revise their initial decision.",Unspecified,Unmanned Ground Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated UGV and automated aid through a computer interface.,simulation,The study used a simulated environment to represent the UGV task.,simulated,The robot was represented through a simulated interface.,shared control (fixed rules),"The automated aid provided recommendations based on fixed rules, and the human could accept or reject them.",Questionnaires; Custom Scales,Mayer and Davis' Trust/Trustworthiness Scales (1999),,Trust was measured using a 10-item self-report questionnaire after each block.,"parametric models (e.g., regression)","The study used path analysis to explore the relationships between transparency, humanness, ability, benevolence/integrity, and trust.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the transparency of the aid by providing or withholding information about its image analysis process, and manipulated humanness through the voice and emotional content of the aid's messages. Reliability of the aid was also manipulated across blocks.",Both transparency and humanness were positively correlated with trust. Ability and benevolence/integrity mediated the relationships between transparency/humanness and trust. The explanatory power of benevolence/integrity increased when the aid's reliability was lower.,"The study found that the explanatory power of benevolence/integrity increased when the aid's reliability was lower, suggesting that human-like intentionality matters more when the aid is less reliable. This is an unexpected result, as one might expect that ability would be the most important factor when reliability is low.","Perceived humanness and transparency of an automated aid significantly correlated with trust in that aid, and these relationships were mediated by perceptions of ability and benevolence/integrity.","The robot (UGV) moved autonomously along a path, stopping at potential insurgent locations. The human monitored the video feed, classified suspects, and could accept or reject the automated aid's recommendations.",ANOVA; Pearson correlation; partial correlations; Path analysis; Linear regression,"The study used ANOVA to check the manipulation of reliability across blocks. Correlations were used to examine the relationships between trust, ability, benevolence/integrity, transparency, and humanness. Partial correlations were used to examine the unique effects of benevolence/integrity on trust after controlling for ability. Path analysis, based on regression coefficients, was used to decompose the correlations between transparency/humanness and trust into direct and indirect effects, mediated by ability and benevolence/integrity.",TRUE,Robot-verbal-communication-content; Robot-verbal-communication-style; Robot-accuracy,Robot-verbal-communication-content; Robot-verbal-communication-style,Robot-accuracy,"The study manipulated the transparency of the aid by providing or withholding information about its image analysis process. This is categorized as 'Robot-verbal-communication-content' because it involves changing the information communicated to the user about the aid's analysis. The study also manipulated humanness through the voice and emotional content of the aid's messages, which is categorized as 'Robot-verbal-communication-style' because it changes how the information is communicated (tone, emotional content). The reliability of the aid was also manipulated across blocks, which is categorized as 'Robot-accuracy' because it directly impacts the aid's performance on the task. The study found that both transparency and humanness (communication content and style) were positively correlated with trust, while the manipulation of reliability (accuracy) did not directly impact trust, but rather influenced the explanatory power of other factors (benevolence/integrity).",10.1080/21515581.2019.1579730,https://www.tandfonline.com/doi/full/10.1080/21515581.2019.1579730,"This study provides an initial experimental investigation of the extent to which well-known precursors of interpersonal trust (ability, benevolence, integrity, or ABI) will manifest when assessing trust between a human and a non-human referent (e.g. an automated aid). An additional motivation was the metaanalytic ﬁnding that the ABI model only explains about half of the variation in interpersonal trust. Based on a review of interpersonal and automation trust literatures, two additional precursors to trust – transparency and humanness – were identiﬁed and studied as exogenous variables (with A, B, and I analysed as explanatory mediators of their relationships to trust). In our experimental task, users interacted with an automated aid in decision-making scenarios to identify suspected insurgents. Results indicated that perceived humanness of the aid signiﬁcantly correlated with trust in that aid (r = .364). This relationship was explained in part by perceptions of both ability and benevolence/integrity (unitweighted average) of the aid; the latter ﬁnding suggesting that human-like intentionality attributed to the aid was a factor in automation trust. Perceived transparency also signiﬁcantly correlated with trust (r = .464) although much of this relationship was explained by ability rather than benevolence/integrity. Aid reliability was also varied across the experiment. Interestingly, the explanatory power of benevolence/integrity increased when the aid’s reliability was lower, again suggesting human-like intentionality matters in automation trust models. Research and design considerations from these ﬁndings are noted."
"Calvo-Barajas, Natalia; Perugia, Giulia; Castellano, Ginevra",The Effects of Robot’s Facial Expressions on Children’s First Impressions of Trustworthiness,2020,1,155,129,26,"26 participants were excluded for reasons such as dropping the activity, having already encountered the robot, or incomplete questionnaires",Real-World Environment,mixed design,"Children were exposed to a Furhat robot displaying facial expressions of happiness and anger at different intensity levels. Each child saw two expressions, one on a female-like robot and one on a male-like robot. After each expression, they filled out a questionnaire.","Participants were asked to evaluate the robot in terms of trustworthiness, likability, and competence after seeing its facial expressions.",Furhat,Expressive Robots,Research; Social,Social,Trustworthiness Rating,passive observation,Participants observed the robot's facial expressions without direct interaction.,real-world,The study was conducted in a real-world setting with a physical robot.,physical,The robot was physically present during the experiment.,pre-programmed (non-adaptive),The robot displayed pre-programmed facial expressions without adapting to the participants.,Questionnaires,Godspeed Questionnaire,,Trust was measured using a modified version of the Godspeed questionnaire.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers manipulated the robot's facial expressions (emotion type and intensity) and gender-likeness to see how they affected trust, likability, and competence.","The study found that facial expressions of happiness increased the perceived likability and competence of the robot, but did not directly affect trust. Trust was indirectly influenced by likability and competence.","The study found that children preferred a robot that expressed low happiness over high happiness, which was an unexpected result. Also, the male-like robot was perceived as more likable when expressing high anger, while the female-like robot was perceived as less likable when expressing high anger.","Children's perception of trust in a robot is not directly influenced by its facial expressions, but rather mediated by their perceptions of the robot's likability and competence.","The robot displayed facial expressions of happiness and anger at different intensity levels. The human participants observed the robot's expressions and then completed a questionnaire to rate the robot's trustworthiness, likability, and competence.",ANOVA; Linear regression,"A 2x2x3 MANOVA was used to analyze the effects of Robot Gender (within-subjects), Emotion Type (between-subjects), and Intensity Level (between-subjects) on the dependent variables of trustworthiness, likability, and competence. Post-hoc tests with Bonferroni correction were used to further investigate significant main effects. Additionally, regression analyses were conducted to examine the predictive relationship between likability and competence (predictor variables) and trustworthiness (dependent variables).",TRUE,Robot-emotional-display; Robot-aesthetics,,Robot-emotional-display; Robot-aesthetics,"The study manipulated the robot's facial expressions (emotion type and intensity), which falls under 'Robot-emotional-display'. The study also manipulated the gender-likeness of the robot's appearance (male-like or female-like), which is categorized as 'Robot-aesthetics'. The results showed that while the manipulated factors influenced likability and competence, they did not directly impact trust. Therefore, 'Robot-emotional-display' and 'Robot-aesthetics' are listed as factors that did not directly impact trust.",10.1109/RO-MAN47096.2020.9223456,https://ieeexplore.ieee.org/document/9223456/,"Facial expressions of emotions inﬂuence the perception of robots in ﬁrst encounters. People can judge trustworthiness, likability, and aggressiveness in a few milliseconds by simply observing other individuals’ faces. While ﬁrst impressions have been extensively studied in adult-robot interaction, they have been addressed in child-robot interaction only rarely. This knowledge is crucial, as the ﬁrst impression children build of robots might inﬂuence their willingness to interact with them over extended periods of time, for example in applications where robots play the role of companions or tutors. The present study focuses on investigating the effects of facial expressions of emotions on children’s perceptions of trust towards robots during ﬁrst encounters. We constructed a set of facial expressions of happiness and anger varying in terms of intensity. We implemented these facial expressions onto a Furhat robot that was either male-like or female-like. 129 children were exposed to the robot’s expressions for a few seconds. We asked them to evaluate the robot in terms of trustworthiness, likability, and competence and investigated how emotion type, emotion intensity, and gender-likeness affected the perception of the robot. Results showed that a few seconds are enough for children to make a trait inference based on the robot’s emotion. We observed that emotion type, emotion intensity, and gender-likeness did not directly affect trust, but the perception of likability and competence of the robot served as facilitator to judge trustworthiness."
"Calvo, Natalia; Elgarf, Maha; Perugia, Giulia; Peters, Christopher; Castellano, Ginevra",Can a Social Robot Be Persuasive Without Losing Children's Trust?,2020,1,52,42,10,10 participants did not complete the questionnaires,Educational Setting,between-subjects,Children played a storytelling game with a Furhat robot in either a persuasive or neutral condition. They then completed a questionnaire.,Children created a story character with the help of a robot.,Furhat,Expressive Robots,Educational; Research; Social,Social,Persuasion,minimal interaction,Children interacted with the robot through a touchscreen interface and verbal instructions.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,A physical Furhat robot was used in the study.,wizard of oz (directly controlled),The robot was remotely operated using a Wizard of Oz technique.,Questionnaires,,,Trust was measured using a questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot used either high-controlling or low-controlling language to influence children's choices, which was intended to affect their trust.",The use of high-controlling language did not affect children's perceived trust towards the robot.,Children perceived the robot more often as a stranger in the persuasive condition compared to the neutral condition. The robot's role as a stranger or peer may have mitigated reactance to high-controlling language.,A persuasive social robot using explicit language can influence children's behavior without negatively affecting their perceived trust.,"The robot guided children through a storytelling game, attempting to influence their choices in the persuasive condition. Children made choices on a touchscreen to customize a character.",t-test,"An independent t-test was performed to compare the means of the dependent variables (trust in robot's advice, trust in robot's goodness, robot's likability, and enjoyment of the activity) between the two experimental conditions (Persuasive vs. Neutral). The purpose was to determine if the type of guidance (persuasive or neutral) had a significant effect on these variables.",TRUE,Robot-verbal-communication-style,,Robot-verbal-communication-style,"The study manipulated the robot's language style by using either high-controlling or low-controlling language. This is described in the 'METHODS 2.1 Design and Measures' section: 'In the Persuasive condition, the robot tried to influence the user's choices using high-controlling language... In the Neutral condition, the robot followed up on the user's choices using low-controlling language.' This manipulation directly relates to how the robot communicated, specifically the assertiveness and directness of its language, which falls under 'Robot-verbal-communication-style'. The results section states: 'Results revealed that the type of guidance did not affect trustworthiness in terms of robot's advice... nor in terms of robot's goodness... This suggests that the use of high-controlling language did not affect the children's perceived trustworthiness'. Therefore, 'Robot-verbal-communication-style' is the manipulated factor that did not impact trust.",10.1145/3371382.3378272,https://dl.acm.org/doi/10.1145/3371382.3378272,"Social robots can be used to motivate children to engage in learning activities in education. In such contexts, they might need to persuade children to achieve specific learning goals. We conducted an exploratory study with 42 children in a museum setting. Children were asked to play an interactive storytelling game on a touchscreen. A Furhat robot guided them through the steps of creating the character of a story in two conditions. In one condition, the robot tried to influence children’s choices using high-controlling language. In the other, the robot left children free to choose and used a low-controlling language. Participants in the persuasive condition generally followed the indications of the robot. Interestingly, the use of high-controlling language did not affect children’s perceived trust towards the robot. We discuss the important implications that these results may have when designing children-robot interactions."
"Cameron, David; de Saille, Stevienna; Collins, Emily C.; Aitken, Jonathan M.; Cheung, Hugo; Chua, Adriel; Loh, Ee Jing; Law, James","The effect of social-cognitive recovery strategies on likability, capability and trust in social robots",2021,1,549,326,223,"138 participants did not answer any questions, 32 participants had already seen the HRI-scenario video, 26 participants monitored play duration of the video being under 60 seconds, 27 participants did not pay attention to the HRI video scenario",Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of four video conditions showing a robot making a navigational error and then either apologizing, explaining the error, both, or neither. Participants then completed questionnaires assessing likability, capability, trust, and intention to use the robot.",Participants watched a video of a robot guiding a person and then rated the robot on several scales.,Pioneer LX,Mobile Robots,Research; Social,Social,Trustworthiness Rating,passive observation,Participants passively observed a video of a human-robot interaction.,media,Participants watched a video of a robot interaction.,physical,The robot was a physical robot shown in a video.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Custom Scales; Questionnaires,Godspeed Questionnaire; Jian et al. Trust Scale,,Trust was assessed using questionnaires and a custom scale derived from the Jian et al. Trust Scale.,"parametric models (e.g., regression)",Linear regression was used to explore the relationship between trust and other variables.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's behavior was manipulated by having it either apologize for an error, explain the error, both, or neither. The task was framed as a navigation task.","Apologies increased likability and intention to use, but decreased perceived capability. Explanations increased perceived capability. Trust was influenced by both apology and explanation conditions.","The study found that apologies increased likability and intention to use, but decreased perceived capability, which aligns with social cognitive models. The PCA revealed three components of trust: Performance, Integrity, and Deceit.","A robot's apology for an error supports its likability and intention to use, while explanations support perceptions of capability, but apologies reduce perceptions of capability.","The robot guides a person to a destination, makes a navigational error, and then either apologizes, explains the error, both, or neither. The human participant watches the video and then rates the robot.",ANOVA; Principal component analysis; Linear regression,"The study used ANOVA to examine the main and interaction effects of apology and competency statements on likability and capability ratings. Principal Component Analysis (PCA) was used to identify the underlying dimensions of trust based on participant ratings of various trust-related terms. Finally, multiple linear regression was used to explore the relationship between likability, capability, trust components, and intention to use the robot.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the robot's verbal communication content by having it either apologize for an error, explain the error, both, or neither. This manipulation directly influenced how participants perceived the robot's trustworthiness. The paper states, 'The four conditions comprised videos of an HRI scenario with the presence or absence of key statements regarding (i) the robot identifying how to correct its mistake, and (ii) the robot apologizing for making a mistake.' This clearly indicates a manipulation of the content of the robot's verbal communication. The results showed that the apology and explanation conditions impacted trust, as the apology decreased perceived capability and the explanation increased it. Therefore, 'Robot-verbal-communication-content' is the most appropriate category for both the manipulated factor and the factor that impacted trust. There were no other factors manipulated in the study.",10.1016/j.chb.2020.106561,https://linkinghub.elsevier.com/retrieve/pii/S0747563220303101,"As robots become more prevalent, particularly in complex public and domestic settings, they will be increasingly challenged by dynamic situations that could result in performance errors. Such errors can have a harmful impact on a user’s trust and confidence in the technology, potentially reducing use and preventing full realization of its benefits. A potential countermeasure, based on social psychological concepts of trust, is for robots to demonstrate self-awareness and ownership of their mistakes to mitigate the impact of errors and increase users’ affinity towards the robot. We describe an experiment examining 326 people’s perceptions of a mobile guide robot that employs synthetic social behaviours to elicit trust in its use after error. We find that a robot that identifies its mistake, and communicates its intention to rectify the situation, is considered by observers to be more capable than one that simply apologizes for its mistake. However, the latter is considered more likeable and, uniquely, increases people’s intention to use the robot. These outcomes highlight that the complex and multifaceted nature of trust in human–robot interaction may extend beyond established approaches considering robots’ capability in performance and indicate that social cognitive models are valuable in developing trustworthy synthetic social agents."
"Campagna, Giulio; Dadgostar, Mahed; Chrysostomou, Dimitrios; Rehm, Matthias",A Data-Driven Approach Utilizing Body Motion Data for Trust Evaluation in Industrial Human-Robot Collaboration,2024,1,20,20,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants wore a motion tracking suit and performed a chemical mixing task with a robot in two conditions: low and high performance. The robot handed a beaker to the human, then poured a chemical into it. Body motion data was collected during the task.",Participants interacted with a robot that handed them a beaker and then poured a chemical into it.,UR10,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Passing,minimal interaction,Participants interacted with the robot in a controlled setting with verbal instructions and physical presence of the robot.,real-world,The interaction took place in a real-world lab setting with a physical robot.,physical,The study used a physical robot for the interaction.,pre-programmed (non-adaptive),The robot followed pre-programmed trajectories without adapting to the user.,Behavioral Measures,,Video Data; robot data,Trust was assessed using body motion data and video analysis.,"deep learning (e.g., neural networks, reinforcement learning)",Deep learning models were used to classify trust levels based on motion data.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's performance was manipulated to be either low or high, with low performance including uncomfortable and potentially hazardous actions, and high performance including smooth and efficient actions. This was intended to influence the user's trust in the robot.","The study found that low robot performance led to lower trust, while high performance led to higher trust.","The study found that deep learning models outperformed machine learning models in classifying trust levels based on body motion data. The GRU model with the Adam optimizer achieved the highest accuracy for the Handing task, while the 1D-CNN model with the SGD optimizer achieved the highest accuracy for the Pouring task.","Body motion data can be used to assess trust levels in real-time, with deep learning models achieving high accuracy in classifying trust based on human motion during a human-robot collaborative task.","The robot handed a beaker to the human and then poured a chemical into it. The human's role was to hold the beaker and remain still unless for safety reasons, while their body motion was tracked to infer trust levels.",,"The study primarily used machine learning and deep learning models for classification, not traditional statistical tests. The models included Random Forest, XGBoost, LightGBM, LSTM, GRU, and 1D-CNN. These models were used to classify trust levels (high or low) based on body motion data. The performance of these models was evaluated using accuracy metrics. Hyperparameter tuning was performed using Grid Search Cross-Validation for machine learning models. The deep learning models were trained using Adam and SGD optimizers. The study focused on model performance rather than traditional statistical hypothesis testing.",TRUE,Robot-accuracy,Robot-accuracy,,"The study explicitly manipulated the robot's performance to be either low or high. In the low-performance condition, the robot's actions were described as 'uncomfortable and non-ergonomic' during the handing phase and 'potentially hazardous' during the pouring phase, creating a situation where the robot might pour the chemical onto the human's hand. In the high-performance condition, the robot's actions were described as 'efficiently handing and position[ing] the beaker as required' and 'smoothly' pouring the chemical without issues. This manipulation of the robot's actions directly influenced the task performance and was intended to impact the user's trust, thus it is classified as 'Robot-accuracy'. The paper states that 'In a prior study, we demonstrated that trust in a human operator is affected by both low and high performance of the robot, resulting in correspondingly diminished or elevated trust ratings [17]. The current data collection process was built upon these findings, aiming to replicate the scenario for automatically labeling behavioral data.' This confirms that the manipulation of robot performance was intended to impact trust. The results section also confirms that the manipulation of robot performance impacted trust, as the models were trained to classify high and low trust based on the robot's performance.",,,"Industry 5.0 signiﬁes a transformative era where humans and robots collaborate closely, leading to advancements in manufacturing efﬁciency and personalization. In light of this, it becomes essential to assess the robot’s trustworthiness to ensure a secure environment and equitable workload distribution. The majority of trust assessments hinge on post-hoc questionnaires for the extent of trust experienced during the interaction. A data-driven approach is required to promptly assess trust levels in real-time, allowing for the adjustment of robot behavior to align with human needs. The paper proposes a chemical industry scenario where a robot assisted a human in the process of mixing chemicals. Several machine learning models, including deep learning, were developed using body motion data to categorize the level of trust exhibited by the human operator. The models achieve an accuracy exceeding 90%. The results clearly show the feasibility of data-driven trust assessment."
"Campagna, Giulio; Chrysostomou, Dimitrios; Rehm, Matthias",Analysis of Facial Features for Trust Evaluation in Industrial Human-Robot Collaboration*,2024,1,20,20,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants wore protective equipment and were positioned in front of a robot. They completed a task four times, twice with high-performance and twice with low-performance robot behavior. Facial expressions were recorded during the task.",Participants observed a robot handing over a beaker and pouring chemicals into another beaker held by the participant.,UR10,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Passing,direct-contact interaction,Participants directly interacted with the robot during the task.,real-world,The interaction took place in a real-world lab setting with a physical robot.,physical,The robot was a physical industrial robot arm.,pre-programmed (non-adaptive),"The robot followed pre-programmed paths, with no adaptation to the user.",Behavioral Measures; Real-time Trust Measures,,Video Data,Trust was assessed using real-time facial expression analysis.,"deep learning (e.g., neural networks, reinforcement learning)",A 2D-CNN was used to model trust based on facial expressions.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's performance was manipulated to be either high or low, with low performance including hazardous trajectories and uncomfortable proximity to the user, which was intended to influence trust.","The study found that facial expressions correlated with the robot's performance, with higher trust associated with high-performance and lower trust with low-performance.","The study found that not all participants showed maximum expressiveness in their facial expressions, which may be due to the lab environment or cultural differences.","Facial expressions can be used to assess trust levels in real-time during human-robot interaction, with a 78.61% accuracy for the handover task and 73.35% for the pouring task using a 2D-CNN.","The robot delivered a beaker to the human, and then poured chemicals into a beaker held by the human. The human's role was to receive the beakers and observe the robot's actions.",,"The study used a 2D-CNN model to classify facial expressions into high and low trust categories based on the robot's performance. The model's performance was evaluated using accuracy, confusion matrices, and ROC curves with AUC. No traditional statistical tests like t-tests or ANOVA were explicitly mentioned.",TRUE,Robot-accuracy,Robot-accuracy,,"The study explicitly manipulated the robot's performance to be either high or low. High performance involved seamless precision and well-planned trajectories, while low performance included hazardous trajectories, coming too close to the human, and giving the impression of pouring chemicals onto the operator's hand. This manipulation of the robot's actions directly influenced the task performance metrics, specifically the safety and comfort of the interaction, which is why 'Robot-accuracy' is the most appropriate category. The paper states that the robot's performance (high vs. low) directly impacted the participants' trust levels, as evidenced by their facial expressions. Therefore, 'Robot-accuracy' is also listed as a factor that impacted trust. There were no other factors manipulated, and no factors that were manipulated that did not impact trust.",10.1109/ARSO60199.2024.10557748,https://ieeexplore.ieee.org/document/10557748/,"The advent of Industry 5.0 marks a signiﬁcant transition towards a collaborative partnership between humans and robots, exploiting their respective capabilities and features to enhance the manufacturing process. This increased cooperation necessitates a secure environment and, in this context, trust becomes a pivotal factor inﬂuencing the quality of humanrobot interactions. To ensure safety and workload balance, it is essential to have a reliable and timely measure of trust in robots. This study explores the use of facial features to identify potential correlations with human trust levels. To this purpose, a chemical industry scenario was developed where a cobot assisted the human handing over a beaker and pouring chemicals. The analysis employed Deep Learning models, speciﬁcally Convolutional Neural Networks (CNNs), to explore the relationship between facial expressions and trust levels. The results of the investigation revealed an accuracy rate of 78.61% for the handing task, and an accuracy of 73.35% for the pouring task. Nevertheless, the ﬁndings highlight the importance of implementing sensor fusion algorithms to improve the accuracy and robustness of trust evaluation towards robots."
"Campagna, Giulio; Chrysostomou, Dimitrios; Rehm, Matthias",Investigating Electrodermal Activity for Trust Assessment in Industrial Human-Robot Collaboration <sup>*</sup>,2024,1,20,20,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a handover task with a robot in two conditions (high and low performance), with each condition repeated twice. Skin conductance data was collected during the task.",Participants received a beaker from a robot in a simulated chemical industry setting.,UR10,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Passing,direct-contact interaction,Participants physically interacted with the robot during the handover task.,real-world,The interaction took place in a real-world lab setting with a physical robot.,physical,The robot was a physical industrial robot arm.,pre-programmed (non-adaptive),The robot followed pre-configured paths and grasping actions without adapting to the user.,Physiological Measures,,Physiological Signals,Trust was assessed using skin conductance response (SCR) data.,"deep learning (e.g., neural networks, reinforcement learning)","Machine learning algorithms (Random Forest, XGBoost, LightGBM, Voting Classifier) were used to classify trust levels based on SCR data.",Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's performance was manipulated to be either high or low, which was intended to influence the participants' trust levels.","The study found that lower robot performance led to lower trust levels, as indicated by changes in SCR values.",The study found that XGBoost performed slightly better than other machine learning models in classifying trust levels based on SCR data. The Voting Classifier was used to mitigate overfitting.,"Skin conductance response (SCR) can be used as a physiological indicator of trust levels during human-robot interaction, with machine learning models achieving moderate accuracy in classifying trust levels.",The robot handed a beaker to the human participant. The human participant received the beaker from the robot.,,"The study primarily used machine learning algorithms (Random Forest, XGBoost, LightGBM, and a Voting Classifier) to classify trust levels based on skin conductance response (SCR) data. While these algorithms involve statistical calculations, the paper does not explicitly mention any traditional statistical tests like t-tests or ANOVA. The machine learning models were used to predict trust levels (high or low) based on the extracted features from the SCR data. The performance of these models was evaluated using metrics like accuracy and AUC.",TRUE,Robot-accuracy,Robot-accuracy,,"The study explicitly manipulated the robot's performance to be either high or low. This manipulation directly influenced the robot's success in completing the task of handing over the beaker, which is a direct measure of 'Robot-accuracy'. The paper states, 'when the robot exhibited high-performance capabilities, completing tasks without malfunctions and minimizing the risk of collisions or harm from chemicals, participants displayed higher levels of trust. Drawing from these findings, the current study sought to replicate the same scenario while manipulating the robot's performance, resulting in two distinct performance modalities: high performance and the other low performance.' This clearly indicates that the robot's ability to perform the task correctly (or not) was the manipulated factor. The study found that the manipulation of robot performance (high vs. low) directly impacted trust levels, as indicated by changes in SCR values. Therefore, 'Robot-accuracy' is the factor that impacted trust. There were no other factors manipulated, and therefore no factors that did not impact trust.",10.1109/UR61395.2024.10597471,https://ieeexplore.ieee.org/document/10597471/,"In the Industry 5.0 framework, due to the close collaboration between humans and robots, providing a safe environment and balance workload becomes an essential requirement. In this context, evaluating the trustworthiness of robots from a human-centric perspective is essential as trust impacts the interaction in human-robot collaborations. Numerous researchers in the literature have delved into physiological responses as indicators of user trust in robots. In this research endeavor, multiple machine learning models were employed, leveraging skin conductance response (SCR) to classify the trust level of the human operator. A chemical industry scenario was developed, where a collaborative robot supported a human operator by handing over a beaker used for the pouring of chemicals. The machine learning models achieved a moderate accuracy rate of 68.99% and AUC of 0.73 for the handover task. Nonetheless, this study underscores the importance of sensor fusion techniques to improve the accuracy of trust assessment within the context of human-robot collaborations."
"Campagna, Giulio; Lagomarsino, Marta; Lorenzini, Marta; Chrysostomou, Dimitrios; Rehm, Matthias; Ajoudani, Arash",Promoting Trust in Industrial Human-Robot Collaboration Through Preference-Based Optimization,2024,1,14,14,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a collaborative task with a robot, first using a Preference-Based Optimization (PBO) method to personalize robot parameters, then using an Individual Selection (IS) method. They then completed the task with both sets of parameters and filled out questionnaires.",Participants collaborated with a robot to pour a chemical from one beaker to another.,Franka Emika Panda,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Passing,direct-contact interaction,Participants physically interacted with the robot during the task.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The robot was a physical robot present in the real-world environment.,shared control (adaptive),The robot adapted its behavior based on the user's preferences using PBO.,Questionnaires; Custom Scales,Schaefer's Trust Questionnaire/Scale; Charalambous's Trust Questionnaire,,Trust was assessed using standard and custom questionnaires.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's velocity, separation distance, and vertical proximity were manipulated using PBO and IS methods to influence user trust.","PBO-based parameters enhanced trust for novice users, while experienced users showed similar trust levels with both PBO and IS parameters.","Novice users showed a greater increase in trust when using PBO-based parameters compared to IS-based parameters, while experienced users did not show a significant difference. There was also a notable disparity between PBO-based and IS-based parameters for almost half of the subjects.","The Preference-Based Optimization (PBO) algorithm effectively personalized robot interaction parameters to enhance trust, particularly for novice users.","The robot moved a beaker containing a chemical to a location where the human was holding another beaker, and then poured the chemical into the human's beaker. The human moved the empty beaker to a designated location and held it while the robot poured the chemical.",Shapiro-Wilk; t-test; Wilcoxon signed-rank test,"The Shapiro-Wilk test was used to check the normality of the questionnaire data. Paired t-tests were performed to compare the perceived trust reported in the questionnaire across testing conditions (PBO-based vs IS-based parameters) when normality was confirmed. Otherwise, the non-parametric Wilcoxon signed-rank test was used to evaluate any significant differences.",TRUE,Robot-task-strategy; Robot-adaptability,Robot-task-strategy; Robot-adaptability,,"The study manipulated the robot's task strategy by altering the robot's velocity profile, human-robot separation distance, and vertical proximity to the user's head using both Preference-Based Optimization (PBO) and Individual Selection (IS) methods. These parameters directly influence how the robot performs the task, thus falling under 'Robot-task-strategy'. The PBO method also introduces 'Robot-adaptability' as the robot's behavior adapts to user preferences through iterative feedback. The study found that both the robot's task strategy (through the specific parameters) and its adaptability (through PBO) impacted trust, particularly for novice users. The PBO-based parameters enhanced trust for novice users, while experienced users showed similar trust levels with both PBO and IS parameters. There were no factors that were explicitly manipulated that did not impact trust.",10.1109/LRA.2024.3455792,https://ieeexplore.ieee.org/document/10669204/,"This letter proposes a novel theoretical framework for promoting trust in human-robot collaboration (HRC). The framework exploits Preference-Based Optimization (PBO) and focuses on three key interaction parameters: robot velocity proﬁle, human-robot separation distance, and vertical proximity to the user’s head. By iteratively reﬁning these parameters based on qualitative feedback from human collaborators, the system dynamically adapts robot trajectories. This personalization aims to enhance users’ conﬁdence in the robot’s actions and foster a more trusting collaborative environment. In our user study with fourteen participants, we simulated a chemical industrial scenario for the HRC task. Results suggest that the framework effectively promotes human operator conﬁdence in the robot assistant, particularly for individuals with limited prior experience in robotics."
"Campagna, Giulio; Rehm, Matthias",Trust Assessment with EEG Signals in Social Human-Robot Interaction,2024,1,21,21,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants played a guessing game with a robot across five sections, each with a different trust strategy. EEG data was collected throughout the interaction, and the MDMT questionnaire was administered between sections and after the last section.",Participants guessed a word presented by the robot through miming or vocal description.,EZ-robot JD Humanoid 1,Humanoid Robots,Research; Social,Game,Cooperative Game,minimal interaction,Participants interacted with the robot in a controlled setting with verbal instructions and a game.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,The study used a physical humanoid robot.,wizard of oz (directly controlled),The robot was manually controlled by a human operator throughout the experiment.,Multidimensional Measures; Physiological Measures,Multi-Dimensional Measure of Trust (MDMT),Physiological Signals,Trust was assessed using the MDMT questionnaire and EEG signals.,"parametric models (e.g., regression)","Machine learning models (SVM, kNN, Random Forest) were used to classify trust levels based on EEG features.",Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's behavior was manipulated across different sections to build, dampen, violate, and regain trust. The robot's performance was altered to introduce errors and technical limitations, and the robot's behavior was changed to establish a relationship with the participant.","Trust decreased significantly after the robot's deliberate malfunction in section 4, demonstrating the successful manipulation of trust levels.","The study found a statistically significant difference in trust scores between sections 3 and 4, indicating that the robot's performance had a direct impact on the participants' trust levels. The Random Forest model achieved the highest accuracy in classifying trust levels based on EEG data.","EEG signals can be used to classify trust levels in human-robot interaction, with a Random Forest model achieving 72.64% accuracy.","The robot either mimed or vocally described a word, and the human participant guessed the word. The robot's behavior was manipulated across different sections to influence trust.",Shapiro-Wilk; Wilcoxon signed-rank test,"A Shapiro-Wilk test was used to check if the difference in average trust scores between sections 3 and 4 followed a normal distribution. Since the data was not normally distributed, a Wilcoxon Signed-Rank Test was performed to compare the trust scores between section 3 and section 4 to determine if there was a statistically significant difference due to the trust violation manipulation.",TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-accuracy,Robot-verbal-communication-content,"The study manipulated the robot's behavior across five sections. In the first two sections, the robot aimed to build trust. In section 2, the robot made comments to establish a relationship, which is categorized as 'Robot-verbal-communication-content' because it involves the content of the robot's speech. In section 3, the robot introduced technical limitations, which is also categorized as 'Robot-verbal-communication-content' because it involves the content of the robot's speech. Section 4 involved a deliberate malfunction of the robot, which is categorized as 'Robot-accuracy' because it directly impacted the robot's performance on the task. Section 5 was used to regain trust. The paper explicitly states that the trust scores were significantly different between sections 3 and 4, indicating that the robot's deliberate malfunction (Robot-accuracy) impacted trust. While the robot's comments and explanations in sections 2 and 3 were intended to influence trust, the statistical analysis only showed a significant impact of the robot's malfunction on trust, thus 'Robot-verbal-communication-content' was not found to have a significant impact on trust.",,https://link.springer.com/10.1007/978-981-99-8715-3_4,"The role of trust in human-robot interaction (HRI) is becoming increasingly important for effective collaboration. Insufficient trust may result in disuse, regardless of the robot’s capabilities, whereas excessive trust can lead to safety issues. While most studies of trust in HRI are based on questionnaires, in this work it is explored how participants’ trust levels can be recognized based on electroencephalogram (EEG) signals. A social scenario was developed where the participants played a guessing game with a robot. Data collection was carried out with subsequent statistical analysis and selection of features as input for different machine learning models. Based on the highest achieved accuracy of 72.64%, the findings indicate the existence of a correlation between trust levels and the EEG data, thus offering a promising avenue for real-time trust assessment during interactions, reducing the reliance on retrospective questionnaires."
"Campanozzi, Laura Leondina; Guglielmelli, Eugenio; Cella, Eleonora; Ghilardi, Giampaolo; Michilli, Mirta; Molina, Alfonso; Ciccozzi, Massimo; Tambone, Vittoadolfo",Building Trust in Social Robotics: A Pilot Survey,2019,1,1640,1213,427,ambiguities and/or missing answers,Survey/Interview,,"Participants completed an online questionnaire with three sections: general trust, attitudes toward robots, and robot dependability. The questionnaire included Likert scale questions and hypothetical scenarios.","Participants answered questions about their general trust in others, their attitudes toward robots, and their preferences for robots in various roles and with different appearances.",Unspecified,Other,Other,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the robot or interaction scenarios.,media,The interaction was based on written descriptions and scenarios.,hypothetical,The robots were described in text without any visual representation.,not autonomous,The robot's actions were described hypothetically without any real autonomy.,Questionnaires,Interpersonal Trust Scale/Questionnaire,,Trust was measured using a questionnaire.,no modeling,Trust was not modeled computationally.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study assessed trust without intentionally altering any specific factors.,,"The study found a tendency to trust machines more than humans, but also that people are more likely to show positive behavior when trusted. There was a significant gender difference in trust in robots, with males trusting more than females. There was a preference for robots in technical roles over relational roles, and a general discomfort with anthropomorphic robots except in relational roles.","The study found a correlation between human-human trust and human-robot trust, with a tendency to trust machines more than humans, and that people are more likely to show positive behavior when trusted.","The robot was described in hypothetical scenarios, and the human participant answered questions about their trust in robots and their preferences for robot characteristics and roles.",odds ratios; Chi-squared; Fisher's exact test,"Descriptive statistical analysis was performed to summarize the results. Odds ratios and their 95% confidence intervals were calculated to evaluate associations among different variables, using reference answers for comparisons. Chi square and Fisher tests were used to confirm statistical significance, with p values < 0.05 considered statistically significant. These tests were used to examine relationships between survey responses, such as the relationship between a positive view of robots and other variables like personal interest in technology, robot use, and concerns about job displacement. They also examined the relationship between trust in humans and attitudes towards robots.",FALSE,,,,"The study did not manipulate any factors. It was a survey-based study that assessed participants' pre-existing attitudes and beliefs about trust in humans and robots. The study explored correlations between general trust in humans and trust in robots, as well as preferences for robot roles and appearances. There were no experimental manipulations of any kind. The study was observational and correlational, not experimental.",10.1109/MTS.2019.2948440,https://ieeexplore.ieee.org/document/8924732/,
"Carlson, Michelle S; Desai, Munjal; Drury, Jill L; Kwak, Hyangshim; Yanco, Holly A",Identifying Factors that Influence Trust in Automated Cars and Medical Diagnosis Systems,2014,1,800,737,63,63 participants were excluded for answering one or more test questions incorrectly,Online Crowdsourcing,between-subjects,"Participants completed an online survey with different scenarios for automated cars and medical diagnosis systems, varying safety criticality and brand recognizability. They answered questions about factors influencing their trust on a 7-point Likert scale.",Participants rated the influence of various factors on their trust in automated systems in different scenarios.,Unspecified,Autonomous Vehicles; Other: Medical diagnosis agent,Care; Other: Automated driving,Evaluation,Survey/Questionnaire Completion,passive observation,Participants read text-based scenarios and answered questions.,media,The interaction was based on text descriptions of scenarios.,hypothetical,The robots were described in text without any visual representation.,fully autonomous (limited adaptation),"The automated systems were described as fully autonomous, but with limited adaptation.",Questionnaires,,,Trust was measured using a 7-point Likert scale questionnaire.,no modeling,"The study used descriptive statistics to analyze the data, but did not model trust.",Observational & Survey Studies,Quantitative Surveys,Direct Manipulation,"The study manipulated the safety criticality of the situation and the brand of the automated system to influence trust. The brand was manipulated by stating the system was created by a well-known company or a small startup, and safety criticality was manipulated by describing the situation as high or low risk.","Trust was higher for systems from well-known brands and in non-safety critical situations. The study found that factors such as past performance, reliability, and the ability to stay up-to-date were important for trust.","The study found that brand recognition significantly influenced trust, with participants showing more trust in systems from well-known companies. There was also a difference in the importance of understanding the system between the automotive and medical domains, with participants relying more on themselves in the automotive domain and on the doctor in the medical domain.","The study identified key factors influencing trust in automated cars and medical diagnosis systems, including past performance, reliability, and brand recognition, and found that these factors vary across domains.",The human participant read a description of a scenario involving an automated car or medical diagnosis system and then rated the influence of various factors on their trust in the system.,t-test,"The study used t-tests to compare the means of trust ratings between different conditions, specifically focusing on the impact of brand recognition (well-known vs. startup company) on trust. The t-tests were used to determine if the differences in trust ratings were statistically significant.",TRUE,Task-complexity; Robot-aesthetics,Task-complexity; Robot-aesthetics,,"The study manipulated the safety criticality of the situation, which is categorized as 'Task-complexity' because it changes the cognitive demands and perceived risk associated with the task. Specifically, scenarios were described as either high-speed/high-traffic (safety critical) or low-speed/low-traffic (non-safety critical) for the automotive domain, and involving cancer diagnosis (safety critical) or common cold diagnosis (non-safety critical) for the medical domain. The study also manipulated the brand of the automated system, which is categorized as 'Robot-aesthetics' because it influences the perceived quality and trustworthiness of the system based on its association with a well-known or unknown brand. The paper states, 'In addition to the severity of the situation, we wanted to see whether the brand of the automated machine affected people's trust level as well.' The results showed that both safety criticality and brand recognition significantly influenced trust, as stated in the paper: 'We found a significant difference in people's trust of the system based upon whether the system was made by a well-known company...vs. a ""small, startup company.""' and 'Trust was higher for systems from well-known brands and in non-safety critical situations.' Therefore, both 'Task-complexity' and 'Robot-aesthetics' are listed as factors that impacted trust. There were no factors that were explicitly manipulated that did not impact trust.",,,"Our research goals are to understand and model the factors that affect trust in automation across a variety of application domains. For the initial surveys described in this paper, we selected two domains: automotive and medical. Specifically, we focused on driverless cars (e.g., Google Cars) and automated medical diagnoses (e.g., IBM’s Watson). There were two dimensions for each survey: the safety criticality of the situation in which the system was being used and name-brand recognizability. We designed the surveys and administered them electronically, using Survey Monkey and Amazon’s Mechanical Turk. We then performed statistical analyses of the survey results to discover common factors across the domains, domain-specific factors, and implications of safety criticality and brand recognizability on trust factors. We found commonalities as well as dissimilarities in factors between the two domains, suggesting the possibility of creating a core model of trust that could be modified for individual domains. The results of our research will allow for the creation of design guidelines for autonomous systems that will be better accepted and used by target populations."
"Cassady, Jacob T.; Robinson, Chris; Popa, Dan O.",Increasing user trust in a fetching robot using explainable AI in a traded control paradigm,2020,1,9,9,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed two tasks (homogeneous and heterogeneous box collection) with a mobile robot under traded control, with transparency level (TL) 2 and TL0, counterbalanced across test and control groups. They were given a description of the robot states and watched the robot collect blocks at TL2 for one minute before the experiment. They also manually controlled the robot to collect one block before the experiment.","Participants were tasked with collecting colored blocks using a mobile robot under traded control, aiming to maximize points by placing the correct blocks in a designated home base. The task was performed with two levels of transparency (TL2 and TL0).",Unspecified,Mobile Robots; Mobile Manipulators,Research,Manipulation,Sorting/Arranging,minimal interaction,"Participants interacted with the robot through a keyboard interface, with the robot physically present in the environment.",real-world,The interaction took place in a real-world setting with a physical robot.,physical,The robot was a physical entity present in the experimental environment.,shared control (fixed rules),The robot operated autonomously but the human could take control at any time.,Questionnaires; Custom Scales,,Performance Metrics,Trust was measured using a subjective rating scale and performance metrics.,no modeling,Trust was not modeled computationally; descriptive statistics were used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers manipulated the level of transparency (TL2 vs TL0) by providing or withholding text messages about the robot's state and reasoning, and also manipulated task complexity by using homogeneous and heterogeneous box collection tasks. This was intended to influence trust by providing more or less information about the robot's actions.","Increasing transparency (TL2) led to a small but significant improvement in subjective trust, with greater improvements in trust occurring when there is a need for the human operator to get vital information from the mobile robot. Mistrust was easier to gain than trust.","Users showed a preference for autonomous mode when at TL0, and mistrust was easier to gain than trust. The magnitude of trust lost was greater in the heterogeneous task, indicating that trust is more susceptible to information exchange in complex tasks. There was no statistically significant correlation between subjective trust and time spent in autonomous mode.","Increasing agent transparency in a traded control paradigm improves the level of trust given to the mobile robot from a human operator, and trust is easier to lose than it is to gain.","The robot autonomously navigates the arena, identifies and collects colored blocks, and delivers them to a home base. The human operator can take control of the robot at any time using a keyboard interface to manually navigate and manipulate the robot.",,"The paper mentions that there was no statistically significant correlation between subjective trust and time spent in autonomous mode. However, no specific statistical tests are named. The analysis primarily relies on descriptive statistics and comparisons of means and proportions. The paper also mentions that increases in agent transparency levels using text messages lead to small but significant improvement in subjective accounts of trust, but does not specify the statistical test used to determine significance.",TRUE,Robot-verbal-communication-content; Task-complexity; Robot-autonomy,Robot-verbal-communication-content; Task-complexity,Robot-autonomy,"The study manipulated 'Robot-verbal-communication-content' by varying the level of transparency (TL2 vs TL0), where TL2 provided text messages about the robot's state and reasoning, while TL0 provided no such information. This is explicitly stated in the paper: 'To achieve Level 2 of Endsley's SAT model, the human operator was able to view text messages from their mobile robot counterpart. These text messages conveyed information such as the robot's current and past states. Additionally, it gave reasoning for why it was changing states...Users completing the tasks at TL0 were not told what color object they were attempting to collect.' The study also manipulated 'Task-complexity' by using homogeneous and heterogeneous box collection tasks. The paper states: 'Our experiments were divided into two differing classes: homogeneous and heterogeneous boxes. These two tasks enabled us to explore the effects of transparent control on differing levels of task complexity'. 'Robot-autonomy' was also manipulated as the robot started in autonomous mode, but the human could take control at any time, which is described in the paper: 'The mobile robot began in autonomous mode in both experiments, but the human operator could take control of the mobile robot at any point.' The results showed that 'Robot-verbal-communication-content' and 'Task-complexity' impacted trust, as stated in the paper: 'increases in agent transparency levels using text messages lead to small but significant improvement in subjective accounts of trust, with greater improvements in trust occurring when there is a need for the human operator to get vital information from the mobile robot' and 'the average magnitude of trust lost in the test group of the heterogeneous experiment...is more than twice as much as the average lost in the test group of the homogeneous experiment. This illustrates that as task complexity increases, the level of user trust in the autonomous systems becomes more susceptible to the impact of information exchange.' However, the paper explicitly states that there was no statistically significant correlation between subjective trust and time spent in autonomous mode, indicating that the manipulation of 'Robot-autonomy' did not directly impact trust levels: 'However, neither the homogeneous nor the heterogeneous experiments showed positive correlations of statistical significance between the subjective trust level and the time spent in autonomous mode.'",10.1145/3389189.3393740,https://dl.acm.org/doi/10.1145/3389189.3393740,"Recently, there has been an increase use of collaborative robots in manufacturing, healthcare, military, and personal use scenarios. Such robots operate under shared or traded control paradigms with their human operators or users. Therefore, it is important to understand how to address and improve issues of trust between the humans and collaborative robots. In this paper, we investigate the impact of robotic agent transparency to their subjective trust level by a human operator. Several experiments were conceived with the help of a fetching mobile robot under traded control, and data such as subjective trust level was collected during experimentation. Results indicate that trust is easier to lose than it is to gain. Furthermore, results also indicate that agent transparency’s effect on operator trust is more significant in tasks of increasing complexity."
"Castellano, Andrea; Fossanetti, Massimo; Landini, Elisa; Tango, Fabio; Montanari, Roberto",Automation as Driver Companion: Findings of AutoMate Project,2020,1,20,20,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants drove a simulated car in two conditions: a baseline fully automated mode and a TeamMate mode where the car requested driver input at roundabouts. The order of conditions was not specified. The time to enter the roundabout, trust, acceptance, and workload were measured.","Participants drove a simulated car through a peri-urban scenario with roundabouts, providing input to the automated system when requested in the TeamMate condition.",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the simulated vehicle by providing input when requested.,simulation,The interaction took place in a driving simulator.,simulated,The robot was a simulated vehicle in a driving simulator.,shared control (fixed rules),The robot operated autonomously but requested human input at specific points based on pre-defined rules.,Questionnaires; Custom Scales,,,Trust was measured using a custom scale and questionnaires.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the level of automation by having the car either fully automated or requesting driver input at roundabouts, which influenced the task difficulty and the driver's role.","The TeamMate system increased trust compared to the baseline, although predictability was lower.","The TeamMate system was considered more reliable and clear, but less predictable than the baseline. The users appreciated the increased efficiency of the TeamMate system.","The cooperative driving concept implemented in the TeamMate system increased trust and acceptance compared to a fully automated baseline, while also improving efficiency and balancing workload.","The robot (simulated car) drove autonomously, and in the TeamMate condition, requested the human driver to provide input at roundabouts. The human driver monitored the driving and provided input when requested.",,"The paper reports descriptive statistics such as means and differences in means for time to enter the roundabout, trust, acceptance, and workload. However, no specific statistical tests (e.g., t-tests, ANOVAs) are explicitly mentioned. The results are presented as differences in scores between the baseline and TeamMate conditions, but the statistical significance of these differences is not reported.",TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,Task-complexity,"The study manipulated the level of robot autonomy by having the car either fully automated (baseline) or requesting driver input at roundabouts (TeamMate condition). This change in autonomy directly impacted the driver's role and the level of control they had over the vehicle. The task complexity was also implicitly manipulated, as the TeamMate condition required the driver to make decisions and provide input, which increased the cognitive load compared to the fully automated baseline. The paper states that the TeamMate system increased trust compared to the baseline, indicating that the change in autonomy was a key factor influencing trust. While the task complexity was manipulated, the paper does not explicitly state that this change in complexity directly impacted trust, but rather that the cooperative driving concept implemented in the TeamMate system increased trust and acceptance. Therefore, the change in autonomy is the factor that impacted trust, while the change in task complexity was a consequence of the manipulation of autonomy.",,http://link.springer.com/10.1007/978-3-030-39512-4_159,"Driving automation is radically changing the role of the driver. The proliferation of driving assistance systems is increasingly transforming driver’s tasks from vehicle control operations to supervising activities. However, the process of turning the driver into a passenger is far from being accomplished. This paper describes an innovative interaction approach developed in the framework of EU funded project AutoMate. The overarching aim of AutoMate is to build a “TeamMate System”, in which the human and the automation cooperate with each other to achieve a safe, pleasant and efﬁcient driving. Through an effective interaction, and by sharing perception, decision and action, they can negotiate speciﬁc behaviors and maneuvers in order to build a team based on trust. In order to measure the effectiveness of this concept, a driving simulator experiment has been conducted: ﬁndings suggest that the concept of Human-Machine Team can increase the trust in automation and improve the efﬁciency in speciﬁc conditions."
"Centeio Jorge, Carolina; Bouman, Nikki H.; Jonker, Catholijn M.; Tielman, Myrthe L.",Exploring the effect of automation failure on the human’s trustworthiness in human-agent teamwork,2023,1,54,54,0,No participants were excluded,Online Crowdsourcing,mixed design,"Participants completed a tutorial, then played a collaborative game with a virtual robot twice, with a questionnaire after each game. The experimental group experienced automation failure in the second game.",Participants collaborated with a virtual robot to move boxes to a dropzone in a 2D grid-world.,Unspecified,Mobile Robots,Research,Game,Cooperative Game,minimal interaction,Participants interacted with a virtual robot in a simulated environment.,simulation,The interaction took place in a 2D simulated environment.,simulated,The robot was a virtual representation in the simulation.,shared control (fixed rules),"The robot operated autonomously but followed fixed rules, responding to the human's actions.",Behavioral Measures; Questionnaires; Custom Scales,Propensity to Trust Scales,Performance Metrics,"Trust was measured using questionnaires, custom scales, and behavioral measures.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's performance was manipulated to include failures in the second game, which was intended to decrease trust and trustworthiness.","Automation failure decreased trust and trustworthiness, with a large effect on trust and a small effect on trustworthiness.","Participants generally disliked breaking boxes, even when it was strategically advantageous. There was a notable shift in strategy in the experimental group after the robot failure, with participants opting to carry medium boxes alone and deliver boxes in a random order.","Automation failure negatively affects the human's trustworthiness in human-automation teamwork, as well as the human's trust in the automation.","The robot autonomously navigates the environment, identifies boxes, and asks for help when needed. The human controls their avatar, moves boxes, and responds to the robot's requests for help.",ANOVA; Pearson correlation,"The study used a robust 2x2 mixed ANOVA to assess the statistical significance of the scenario's effects on various measurements, including subjective trust, trustworthiness, and liking scores, as well as objective measures like interaction with the robot. A Linear Pearson correlation test was used to assess the relationship between trust and trustworthiness scores.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the robot's performance by introducing failures in the second game for the experimental group. This manipulation directly affected the robot's ability to perform the task correctly, such as breaking boxes, placing them in the wrong location, or picking up boxes out of order. This falls under the 'Robot-accuracy' category because it directly impacts the robot's success rate and reliability in completing the task. The paper explicitly states, 'Overall, the robot breaks eight boxes during the game (two light, four medium, two heavy). Four boxes are delivered in the wrong place... Lastly, three boxes are collected out of order.' The results show that this manipulation of robot accuracy significantly impacted the human's trust in the robot, as well as the human's trustworthiness towards the robot. The paper states, 'The results show a large effect size of automation failure in the trust score... This is in line with previous research... We see that, in this human-automation collaborative setting, a change in trustworthiness of the automation affects the trust that the human has in the automation.' Therefore, 'Robot-accuracy' is the factor that impacted trust. There were no other factors manipulated, and therefore no factors that did not impact trust.",10.3389/frobt.2023.1143723,https://www.frontiersin.org/articles/10.3389/frobt.2023.1143723/full,"Introduction:               Collaboration in teams composed of both humans and automation has an interdependent nature, which demands calibrated trust among all the team members. For building suitable autonomous teammates, we need to study how trust and trustworthiness function in such teams. In particular, automation occasionally fails to do its job, which leads to a decrease in a human’s trust. Research has found interesting effects of such a reduction of trust on the human’s trustworthiness, i.e., human characteristics that make them more or less reliable. This paper investigates how automation failure in a human-automation collaborative scenario affects the human’s trust in the automation, as well as a human’s trustworthiness towards the automation.                                         Methods:               We present a 2 × 2 mixed design experiment in which the participants perform a simulated task in a 2D grid-world, collaborating with an automation in a “moving-out” scenario. During the experiment, we measure the participants’ trustworthiness, trust, and liking regarding the automation, both subjectively and objectively.                                         Results:               Our results show that automation failure negatively affects the human’s trustworthiness, as well as their trust in and liking of the automation.                                         Discussion:               Learning the effects of automation failure in trust and trustworthiness can contribute to a better understanding of the nature and dynamics of trust in these teams and improving human-automation teamwork."
"Chadalavada, Ravi Teja; Andreasson, Henrik; Krug, Robert; Lilienthal, Achim J",Empirical evaluation of human trust in an expressive mobile robot,2016,1,14,14,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed three tasks in two pilot experiments: human-robot encounter, human-robot with projection encounter, and human-human encounter. The order of tasks was human-human, human-robot, and human-robot with projection. After all tasks, participants rated their experience on a Likert scale.","Participants experienced encounters with a robot and a human in a corridor and at a junction crossing, and rated their experience.",Unspecified,Mobile Robots; Unmanned Ground Vehicles,Research,Navigation,Path Following,minimal interaction,Participants interacted with the robot in a real-world setting with verbal instructions.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The robot was physically present during the interaction.,shared control (fixed rules),The robot slowed down or stopped based on pre-defined rules when a human entered certain areas.,Questionnaires,,,Trust was assessed using a Likert scale questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's behavior was manipulated by adding a projection system that communicated its intentions, and the interaction medium was changed by adding the projection.","The projection system increased ratings for communication, predictability, and transparency, suggesting an increase in trust. Reliability ratings were lower, indicating some hesitation towards the robot.","The ratings for communication, predictability, and transparency with the projection system exceeded those of human-human interactions in one of the pilot experiments. Reliability ratings were lower than human-human interactions, indicating a remaining hesitation towards the robot.","The use of a Spatial Augmented Reality (SAR) module to communicate a mobile robot's intentions increased human trust, with ratings approaching those of human-human interactions.","The robot moved along a path, slowing down or stopping when a human entered certain areas. The human walked through the same area, experiencing encounters with the robot and another human, and then rated their experience.",ANOVA; Tukey HSD,"A one-way ANOVA was used to determine if there was a statistically significant difference between the three groups (Human-Robot encounter, Human-Robot with projection encounter, and Human-Human encounter) across the Likert scale ratings. Tukey's HSD test was then used as a post-hoc test to determine which specific group means were significantly different from each other. The analysis aimed to show the impact of the projection system on the robot's perceived attributes and how it compares to human-human interactions.",TRUE,Robot-verbal-communication-content; Robot-interface-design,Robot-verbal-communication-content; Robot-interface-design,,"The study manipulated the robot's communication by adding a Spatial Augmented Reality (SAR) projection system that displayed the robot's intended path and safety zones (green and red areas). This falls under 'Robot-verbal-communication-content' because the projection communicated the robot's intentions and future actions, which is a form of information content. The SAR module itself, which projects the information, is a change to the robot's interface, thus 'Robot-interface-design'. The results showed that the presence of the projection system increased ratings for communication, predictability, and transparency, indicating that both the content of the communication and the interface design impacted trust. There were no factors that were manipulated that did not impact trust.",,,A mobile robot communicating its intentions using Spatial Augmented Reality (SAR) on the shared floor space makes humans feel safer and more comfortable around the robot. Our previous work [1] and several other works established this fact. We built upon that work by adding an adaptable information and control to the SAR module. An empirical study about how a mobile robot builds trust in humans by communicating its intentions was conducted. A novel way of evaluating that trust is presented and experimentally shown that adaption in SAR module lead to natural interaction and the new evaluation system helped us discover that the comfort levels between human-robot interactions approached those of human-human interactions.
"Chauhan, Hardik; Pakbaz, Ali; Jang, Youjin; Jeong, Inbae",Analyzing Trust Dynamics in Human–Robot Collaboration through Psychophysiological Responses in an Immersive Virtual Construction Environment,2024,1,11,11,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a bricklaying task in a virtual reality environment, interacting with a robot assistant. They experienced three levels of interaction in both open and closed workspace environments. Psychophysiological data and trust scores were collected.","Participants performed a bricklaying task in a virtual environment, instructing a robot to bring tools and bricks.",Unspecified,Mobile Manipulators; Unmanned Ground Vehicles; Collaborative Robots,Industrial; Research,Manipulation,Object Passing,minimal interaction,"Participants interacted with a robot in a virtual environment, giving verbal instructions.",simulation,Participants used a VR headset to experience the virtual construction environment.,simulated,The robot was a virtual representation in the VR environment.,shared control (fixed rules),The robot followed pre-programmed actions based on participant commands.,Questionnaires; Physiological Measures; Real-time Trust Measures,Trust Perception Scale - HRI,Physiological Signals,"Trust was measured using questionnaires, real-time trust scores, and physiological data.","parametric models (e.g., regression)","The study used statistical analysis such as t-tests, ANOVA, and correlation analysis to examine the relationship between robot factors and trust measures.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated workspace environment, level of interaction, robot speed, proximity, and angle of approach to influence trust.","Workspace environment and level of interaction significantly affected trust. Open workspaces and higher levels of interaction were associated with higher trust. Robot speed, proximity, and angle of approach had varying impacts on trust depending on the interaction level and workspace environment.","EDA was the most sensitive psychophysiological measure to variations in robot factors. There was a positive correlation between proximity and perceived trust. The effect of speed, proximity, and angle of approach were dependent on the level of interaction and workspace environment. There was a conflict in the results, where higher levels of interaction were associated with higher trust scores, but also with higher EDA in the closed workspace, which is typically associated with lower trust.","Workspace environment and level of interaction were the most significant robot factors affecting human trust, with EDA exhibiting the most sensitivity to variations in robot factors.",The robot moved to pick up tools and bricks and either placed them on the floor or directly into the participant's hand. The human participant instructed the robot to perform these actions.,t-test; ANOVA; Spearman correlation; Moderation analysis; Kolmogorov-Smirnov,The study used the Kolmogorov-Smirnov test to check for normality of the data. Spearman's rank correlation was used to examine the relationships between robot factors and human trust measures. T-tests were used to compare the means of human trust measures between open and closed workspace environments. Two-way ANOVA was used to examine the effects of workspace environment and level of interaction on human trust measures. Post-hoc t-tests were used to further analyze differences between interaction levels. Moderation analysis was used to assess whether one factor influenced the relationship between two other factors. Correlation analysis was used to examine the relationship between psychophysiological responses and trust scores.,TRUE,Task-environment; Task-complexity; Robot-task-strategy,Task-environment; Task-complexity; Robot-task-strategy,,"The researchers manipulated the workspace environment by having participants perform the task in both open and closed virtual environments. This is categorized as 'Task-environment' because it directly changes the working conditions. The level of interaction was also manipulated, with low, moderate, and high levels of interaction, which is categorized as 'Task-complexity' because it changes the cognitive demands and effort required from the participant. The robot's speed, proximity, and angle of approach were also manipulated, which is categorized as 'Robot-task-strategy' because these changes affect how the robot completes the task without directly influencing the success rate of the task itself. The results indicated that workspace environment and level of interaction significantly affected trust, as well as robot speed, proximity, and angle of approach, which were all part of the robot's task strategy. Therefore, all three manipulated factors impacted trust. There were no factors that were manipulated that did not impact trust.",10.1061/JCCEE5.CPENG-5692,https://ascelibrary.org/doi/10.1061/JCCEE5.CPENG-5692,"Human–robot collaboration (HRC) has emerged as a promising frontier within the construction industry, offering the potential to enhance productivity, safety, and efficiency. The effectiveness of HRC critically depends on the degree of trust that workers place in their robots, and establishing a seamless level of trust in robots is essential to realize the full benefits of HRC. Despite the extensive exploration of trust dynamics in various industries, there is a notable research gap with regard to trust in construction robots, which possess distinctive characteristics in terms of appearance, capabilities, and interaction compared to robots in other sectors. Therefore, in this study, we analyzed trust dynamics within the context of HRC during construction tasks. Both subjective survey data and objective psychophysiological data—including heart rate variability (HRV), electrodermal activity (EDA), and electroencephalogram (EEG)-based emotional valence and arousal—were employed as human trust measures. We conducted experiments for bricklaying tasks in an immersive virtual construction environment and analyzed multifaceted robot factors—including workspace environment, level of interaction, and robot speed, proximity, and angle of approach—and their relationships with human trust measures using statistical analysis, such as t-test, two-way ANOVA, Spearman’s rank correlation, and moderation analysis. The results indicated that workspace environment and level of interaction were the most significant robot factors affecting human trust. EDA exhibited the most sensitivity to variations in robot factors. It was also observed that the effect of speed, proximity, and angle of approach were also dependent on level of interaction and type of workspace environment. There was a significant positive correlation between proximity and perceived trust. The findings of this study contribute to the optimization of robot design and interaction protocols for construction tasks, fostering greater worker trust, and enhancing project productivity and efficiency. DOI: 10.1061/JCCEE5.CPENG-5692. © 2024 American Society of Civil Engineers."
"Chauhan, Hardik; Jang, Youjin; Jeong, Inbae",Predicting human trust in human-robot collaborations using machine learning and psychophysiological responses,2024,1,11,11,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants completed a brick-laying task in a virtual reality environment with a robot assistant, in both open and closed workspaces, and at three levels of interaction; psychophysiological data and trust scores were collected.",Participants completed a brick-wall building task with a construction robot that collaborated by bringing bricks and tools.,Unspecified,Mobile Manipulators; Unmanned Ground Vehicles,Industrial; Research,Manipulation,Object Passing,minimal interaction,Participants interacted with the robot in a virtual environment.,simulation,Participants were immersed in a virtual reality environment.,simulated,The robot was a virtual representation in the VR environment.,shared control (fixed rules),"The robot followed pre-set rules for its actions, responding to participant commands.",Questionnaires; Physiological Measures,Trust Perception Scale - HRI,Physiological Signals,Trust was measured using questionnaires and physiological data.,"parametric models (e.g., regression)",Machine learning models were used to predict trust based on physiological data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's level of interaction, the workspace environment, and the robot's speed, proximity, and angle of approach were manipulated to influence trust.","Trust scores varied based on the level of interaction, workspace environment, and robot behavior, with EDA and ST being significant predictors.","The study found that EDA and ST were significant standalone variables for trust prediction, and that XG Boost and Random Forest models displayed superior predictive accuracy.","Machine learning models, particularly XG Boost and Random Forest, can accurately predict human trust in construction robots using psychophysiological data, with EDA and ST being key predictors.","The robot brought bricks and tools to the participant, and the participant used hand controllers to build a wall.",Spearman correlation; recursive feature elimination (rfe); Cross-validation,"Spearman's correlation was used to analyze the relationships between psychophysiological variables and trust scores, as the data was non-normally distributed. Recursive Feature Elimination (RFE) was employed to rank and select the most influential features for predicting trust scores. K-fold cross-validation with k=5 was used to assess the robustness and generalization of the machine learning models.",TRUE,Robot-task-strategy; Task-environment; Robot-autonomy,Robot-task-strategy; Task-environment,Robot-autonomy,"The study manipulated the robot's task strategy by varying the level of interaction (low, moderate, high) which influenced how the robot approached and delivered bricks to the participant. This is described in the 'Experimental procedure' section: 'In the low-level interaction task, there was no direct interaction; the robot merely moved around the participant. For the moderate-level interaction task, participants commanded the robot to retrieve a brick... In contrast, the high-level interaction task involved the participant commanding the robot to bring a brick, and the robot retrieved the brick from storage and passed it directly to the participant's hand.' The task environment was manipulated by having participants complete the task in both open and closed workspaces, as stated in the 'Introduction' section: 'In the presented study, a virtual construction environment was created in UNITY platform, with two working conditions: open workspace and closed workspace.' The robot's autonomy was manipulated by having the robot follow pre-set rules for its actions, responding to participant commands, as described in the 'Introduction' section: 'In both conditions, the participants completed brick-wall building tasks with a construction robot which collaborated with them by bringing bricks and tools.' The 'Experimental procedure' section also mentions that 'the robot would approach the participant, and the participant would issue a command, initiating the interaction.' The study found that the level of interaction (Robot-task-strategy) and the workspace environment (Task-environment) impacted trust, as stated in the 'Abstract' and 'Discussion' sections, where it is mentioned that 'Results indicated that electrodermal activity and skin temperature were two significant standalone variables for trust prediction' and 'The Spearman's correlation analysis showed that trust scores had a significant relationship with all the psychophysiological signals studied, with EDA and ST having the highest positive correlation values.' The study did not find that the robot's autonomy level impacted trust, as the robot's actions were always based on pre-set rules and participant commands, and this was not a factor that was varied to measure its impact on trust.",10.1016/j.aei.2024.102720,https://linkinghub.elsevier.com/retrieve/pii/S1474034624003689,"In the ever-evolving construction industry, grappling with challenges such as labor shortages and workplace hazards, human-robot collaboration (HRC) has emerged as a transformative solution. However, the industry faces hurdles in comprehending the intricacies of trust dynamics within the domain of HRC. It exerts considerable influence on both productivity and safety within the construction sector. To address this issue, the paper proposes machine learning-based models to predict and enhance human trust in construction robots using psychophysiological data. Through a virtual reality bricklaying task across varied construction settings, this study collected psychophysiological data from participants and predicted trust score. Results indicated that electrodermal activity and skin temperature were two significant standalone variables for trust prediction. With similar R squared value of 0.98, the XG boost, and random forest models displayed superior predictive accuracy, with minor standard deviations of 0.003 and 0.004, respectively. This study contributes valuable insights into trust dynamics, paving the way for more dependable and secure HRC in construction, optimizing workflows and ensuring industry-wide advancements."
"Chavaillaz, Alain; Schwaninger, Adrian; Michel, Stefan; Sauer, Juergen","Expertise, Automation and Trust in X-Ray Screening of Cabin Baggage",2019,1,61,60,1,One novice was excluded due to a poor detection performance in the main test,Controlled Lab Environment,between-subjects,"Participants completed a pre-test and a main test. In the main test, half of the participants were assisted by a diagnostic aid with three levels of automation, while the other half completed the task without support. Participants were instructed to indicate whether a prohibited object was present in X-ray images of baggage.",Participants were asked to identify whether a prohibited item (gun or knife) was present in X-ray images of baggage.,Unspecified,Other,Research,Evaluation,Image Analysis,minimal interaction,Participants interacted with a diagnostic aid through a computer interface.,simulation,Participants viewed X-ray images on a computer screen.,simulated,The diagnostic aid was a simulated system displayed on a computer screen.,shared control (fixed rules),"The diagnostic aid provided cues based on pre-set rules, with participants able to choose the level of assistance.",Behavioral Measures; Questionnaires,Checklist for Trust between People and Automation,,Trust was measured using a questionnaire and behavioral measures of compliance and reliance.,no modeling,Trust was not modeled computationally; only descriptive statistics were used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the presence and level of automation of the diagnostic aid, which influenced the task difficulty and the robot's performance.","Experts showed higher compliance and reliance on the diagnostic aid than novices, but there was no difference in perceived trust ratings between the two groups. Compliance was correlated with trust for novices, but not for experts.",Experts showed higher compliance and reliance on the diagnostic aid despite no difference in perceived trust ratings. Novices benefited more from the diagnostic aid than experts.,"Novices benefited more from the diagnostic aid than experts, and experts used the diagnostic aid as a 'back-up' to confirm their decisions, while novices used it as a guide.","The diagnostic aid provided cues about the presence of prohibited items in X-ray images, and participants had to decide whether a prohibited item was present and mark its location.",ANOVA; t-test; Pearson correlation,"The study used a 2-way ANOVA to analyze the effects of expertise (expert vs. novice) and diagnostic aid (present vs. absent) on various dependent variables such as detection performance, criterion, target localization, self-confidence, and subjective workload. Independent samples t-tests were used to compare experts and novices on measures collected only when the diagnostic aid was present, such as median LOA, number of LOA switches, trust perception, compliance, and reliance. A correlation analysis was conducted to investigate the relationships between perceived trust in automation and other relevant measures like compliance, reliance, and experience, separately for experts and novices.",TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,,"The study manipulated the level of automation of the diagnostic aid (DA), which directly corresponds to 'Robot-autonomy'. The DA had three levels: (1) no support, (2) a cue indicating the presence of a potential target without locating it, and (3) a cue indicating the presence of a potential target by surrounding it with a red frame. This manipulation directly changes the level of decision authority given to the system, thus fitting the 'Robot-autonomy' category. The presence of the diagnostic aid also influenced the task difficulty, making it easier for novices, which corresponds to 'Task-complexity'. The study found that the level of automation (Robot-autonomy) impacted trust behaviors (compliance and reliance), with experts showing higher compliance and reliance than novices. There was no manipulation of any other factors from the list provided. The study did not find any factors that did not impact trust.",10.3389/fpsyg.2019.00256,https://www.frontiersin.org/article/10.3389/fpsyg.2019.00256/full,"X-ray screening of passenger baggage is a key component in aviation security. The current study investigated how experts and novices performed in an X-ray baggage screening task while being assisted by an adaptable diagnostic aid. Furthermore, it examined how both groups operated and trusted this automated system. 30 experts (certiﬁed screeners) and 31 novices (students) had to indicate whether a target item (either a knife or a gun) was present in a series of X-ray images of cabin baggage. Half of the participants could choose between three different support levels of the diagnostic aid (DA): (1) no support, (2) a cue indicating the presence of a potential target without locating it, or (3) a cue indicating the presence of a potential target by surrounding it with a red frame. As expected, experts achieved higher detection performance (d’), were more self-conﬁdent and felt more competent in achieving the task than novices. Furthermore, experts experienced less time pressure and fatigue. Although both groups used the DA in a comparable way (in terms of support level used and frequency of level switches), results showed a performance increase for novices working with the DA compared to novices without support. This beneﬁt of DA was not observed for experts. Interestingly, despite no difference in perceived trust ratings, experts were more compliant (i.e., following DA recommendations when it indicated the presence of a target) and reliant (i.e., following DA recommendations when it indicated the absence of a target) than novices. Altogether, the results of the present study suggested that novices beneﬁted more from a DA than experts. Furthermore, compliance and reliance on DA seemed to depend on expertise with the task. Since experts should be better at assessing the reliability of the DA than novices, they may have used the DA as ‘back-up’ to conﬁrm their decisions based on expertise (conﬁrmatory function), while novices may have used it as a guide to base their decisions on (support function). Finally, trust towards a DA was associated with the degree to which participants found the DA useful."
"Che, Yuhang; Okamura, Allison M.; Sadigh, Dorsa",Efficient and Trustworthy Social Navigation via Explicit and Implicit Robot–Human Communication,2020,1,12,12,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a navigation task with a robot in three different communication modes (baseline, implicit only, explicit + implicit). Each session consisted of 20 trials, with the first 10 trials treated as training. Participants were asked to remember and answer arithmetic questions at target locations to distract them from focusing solely on the robot. After each session, participants rated their trust in the robot.",Participants were asked to walk between two target locations while avoiding collision with a mobile robot.,TurtleBot,Mobile Robots,Research,Navigation,Path Following,minimal interaction,Participants interacted with the robot in a real-world setting with verbal instructions but no physical touch.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The robot was a physical mobile robot present in the real-world environment.,shared control (adaptive),The robot adapted its behavior based on a model that predicted the user's movement.,Questionnaires,,Video Data,Trust was measured using a post-experiment questionnaire and video data was collected.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's communication mode (baseline, implicit only, explicit + implicit) was manipulated, which influenced the robot's behavior, autonomy level, and the task difficulty for the human. The robot's behavior was also influenced by the task priority (robot or human priority), which influenced human expectations.","Trust was highest when the robot used both implicit and explicit communication, followed by implicit only, and lowest in the baseline condition.","Users were more confused in the implicit only condition, especially in robot priority trials, and acted more conservatively in the baseline condition. Users adapted to the robot's communication mode in a few trials.","The study found that combining implicit and explicit communication in robot social navigation leads to better understanding of the robot's intent, reduced user effort, and increased user trust.","The robot moved between target positions, communicating its intent through motion and haptic feedback. The human walked between two target locations, avoiding collision with the robot and answering arithmetic questions.",ANOVA; Tukey HSD,"A one-way repeated measures ANOVA was used to analyze the effect of communication mode on the percentage of trials where users passed in front of the robot, and on the average path length. Post hoc analysis with Tukey HSD was used to determine pairwise differences between the communication modes for these measures. A one-way repeated measures ANOVA was also used to analyze the effect of communication mode on the user's trust rating, with post hoc Tukey HSD analysis to determine pairwise differences.",TRUE,Robot-nonverbal-communication; Robot-verbal-communication-content,Robot-nonverbal-communication; Robot-verbal-communication-content,,"The study manipulated the robot's communication mode, which included both implicit (robot motion) and explicit (haptic feedback) communication. The 'Robot-nonverbal-communication' category was chosen because the robot's motion (speed and direction changes) was intentionally altered to convey its intent, which is a form of nonverbal communication. The 'Robot-verbal-communication-content' category was chosen because the haptic feedback was used to explicitly communicate the robot's intent (human priority or robot priority), which is a form of conveying information. The results showed that both the robot's motion and the explicit haptic feedback impacted the user's trust in the robot. Specifically, the combination of implicit and explicit communication led to the highest trust ratings. The study explicitly states that the robot's communication mode (baseline, implicit only, explicit + implicit) was manipulated, which influenced the robot's behavior, autonomy level, and the task difficulty for the human. The robot's behavior was also influenced by the task priority (robot or human priority), which influenced human expectations. The study found that combining implicit and explicit communication in robot social navigation leads to better understanding of the robot's intent, reduced user effort, and increased user trust. Therefore, both factors were found to impact trust.",10.1109/TRO.2020.2964824,https://ieeexplore.ieee.org/document/8967120/,"In this article, we present a planning framework that uses a combination of implicit (robot motion) and explicit (visual/audio/haptic feedback) communication during mobile robot navigation. First, we developed a model that approximates both continuous movements and discrete behavior modes in human navigation, considering the effects of implicit and explicit communication on human decision-making. The model approximates the human as an optimal agent, with a reward function obtained through inverse reinforcement learning. Second, a planner uses this model to generate communicative actions that maximize the robot’s transparency and efﬁciency. We implemented the planner on a mobile robot, using a wearable haptic device for explicit communication. In a user study of an indoor human–robot pair orthogonal crossing situation, the robot is able to actively communicate its intent to users in order to avoid collisions and facilitate efﬁcient trajectories. Results show that the planner generated plans that are easier to understand, reduce users‘ effort, and increase users’ trust of the robot, compared to simply performing collision avoidance. The key contribution of this article is the integration and analysis of explicit communication (together with implicit communication) for social navigation."
"Chen, Shenghui; Boggess, Kayla; Feng, Lu",Towards Transparent Robotic Planning via Contrastive Explanations,2020,1,100,100,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants evaluated different types of explanations for robot routes on a grid map, rating their understanding and trust, and choosing their preferred explanation.",Participants were asked to evaluate different explanations for a robot's route on a grid map.,Unspecified,Mobile Robots,Research,Evaluation,Rating,passive observation,Participants only read about the robot's actions and explanations.,media,The interaction was based on text descriptions of the robot's actions.,hypothetical,"The robot was only described in text, without any visual representation.",pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires,,,Trust was measured using a 5-point Likert scale questionnaire.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The type of explanation provided for the robot's actions was manipulated, including naive, responsibility, constrictive, selective, and contrastive explanations, which influenced how the robot's behavior was presented.","Responsibility explanations significantly increased user trust, while constrictive explanations had a smaller effect, and selective explanations did not increase trust. Contrastive explanations, combining all factors, did not increase trust as much as responsibility explanations alone.","The study found that while contrastive explanations decreased cognitive burden, they did not increase user understanding or trust as much as expected, possibly due to the selective factor. Users preferred explanations with some type of contrastive justification over naive explanations, but did not prefer selective explanations.",The use of responsibility in explanations significantly increases user trust in robotic systems.,"The robot navigates a grid map, and the human evaluates different explanations for the robot's route.",ANOVA; ANOVA,The study used two One-Way ANOVA tests. The first ANOVA was used to analyze the statistical differences in user understanding across different explanation types. The second ANOVA was used to analyze the statistical differences in the average time users spent processing different explanation types. Both tests aimed to determine if the different explanation types had a significant impact on user understanding and processing time.,TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the content of the robot's explanations by varying the inclusion of responsibility, constrictiveness, and selectiveness factors. The paper states, 'Each route was presented with 7 different explanations about the robotic actions taken within it.' These explanations varied in the information they provided, such as 'We move east at grid 10' (naive), 'We move east at grid 10 because it leads to the shortest route' (responsibility), and 'First, we move south instead of other directions at critical grid 5 because it leads to the shortest and most flexible future route' (contrastive). The manipulation of these factors directly altered the content of the robot's verbal communication, thus 'Robot-verbal-communication-content' is the most appropriate category. The results showed that explanations including responsibility significantly increased user trust, while constrictive explanations had a smaller effect, and selective explanations did not increase trust. Therefore, 'Robot-verbal-communication-content' is also the factor that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1109/IROS45743.2020.9341773,https://ieeexplore.ieee.org/document/9341773/,"Providing explanations of chosen robotic actions can help to increase the transparency of robotic planning and improve users’ trust. Social sciences suggest that the best explanations are contrastive, explaining not just why one action is taken, but why one action is taken instead of another. We formalize the notion of contrastive explanations for robotic planning policies based on Markov decision processes, drawing on insights from the social sciences. We present methods for the automated generation of contrastive explanations with three key factors: selectiveness, constrictiveness and responsibility. The results of a user study with 100 participants on the Amazon Mechanical Turk platform show that our generated contrastive explanations can help to increase users’ understanding and trust of robotic planning policies, while reducing users’ cognitive burden."
"Chen, Min; Nikolaidis, Stefanos; Soh, Harold; Hsu, David; Srinivasa, Siddhartha",Trust-Aware Decision Making for Human-Robot Collaboration: Model Learning and Planning,2020,3,231,222,9,9 data points were removed because the participants failed on the attention check question or their data were incomplete,Online Crowdsourcing,,"Participants performed an online table clearing task with a robot, were informed of the reward function, reported initial trust using Muir's questionnaire, watched videos of the robot attempting to pick up objects, chose to intervene or stay put, watched a video of the outcome, and reported updated trust.",Participants watched a video of a robot attempting to pick up objects on a table and chose to intervene or stay put.,Unspecified,Mobile Manipulators,Research,Manipulation,Cleaning,passive observation,Participants observed videos of the robot performing the task.,media,Participants watched videos of the robot interacting with objects.,simulated,The robot was represented through videos.,pre-programmed (non-adaptive),The robot followed pre-specified policies.,Questionnaires,Muir's Trust Questionnaire,Performance Metrics,Trust was measured using a questionnaire and performance metrics.,POMDP,Trust was modeled using a POMDP framework.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's policy was manipulated to either prioritize trust building or ignore it, influencing the human's perception of the robot's performance and the task difficulty.","Trust increased when the robot successfully picked up objects, and decreased when the robot failed or the human intervened.","The trust-based human behavioral model fit the data better than the trust-free model, indicating that human behavior is influenced by trust.","Human trust is affected by robot performance, and human trust affects human behaviors.","The robot attempted to pick up objects from a table, and the human could choose to intervene or let the robot complete the task.",,No statistical tests were explicitly mentioned in the description of this study. The analysis focused on model fitting and comparison of log-likelihood values between trust-based and trust-free human behavioral models.,TRUE,Robot-task-strategy,Robot-task-strategy,,"The study manipulated the robot's policy to either prioritize trust building or ignore it. This manipulation directly influenced the order in which the robot attempted to pick up objects, which is a change in the task completion strategy. The paper states, 'To focus the learning on a few interesting robot policies (i.e., picking up the glass in the beginning vs. in the end), while still covering a large space of policies, we split the data collection process so that in one half of the trials the robot randomly chose a policy out of a set of pre-specified policies, whereas in the other half the robot followed a random policy.' This shows that the robot's task strategy was manipulated. The results showed that the trust-based human behavioral model fit the data better than the trust-free model, indicating that human behavior is influenced by trust, which was affected by the robot's task strategy. Therefore, 'Robot-task-strategy' is the factor that impacted trust.",10.1145/3359616,https://dl.acm.org/doi/10.1145/3359616,"Trust in autonomy is essential for effective human-robot collaboration and user adoption of autonomous systems such as robot assistants. This article introduces a computational model that integrates trust into robot decision making. Specifically, we learn from data a partially observable Markov decision process (POMDP) with human trust as a latent variable. The trust-POMDP model provides a principled approach for the robot to (i) infer the trust of a human teammate through interaction, (ii) reason about the effect of its own actions on human trust, and (iii) choose actions that maximize team performance over the long term. We validated the model through human subject experiments on a table clearing task in simulation (201 participants) and with a real robot (20 participants). In our studies, the robot builds human trust by manipulating low-risk objects first. Interestingly, the robot sometimes fails intentionally to modulate human trust and achieve the best team performance. These results show that the trust-POMDP calibrates trust to improve human-robot team performance over the long term. Further, they highlight that maximizing trust alone does not always lead to the best performance."
"Chen, Min; Nikolaidis, Stefanos; Soh, Harold; Hsu, David; Srinivasa, Siddhartha",Trust-Aware Decision Making for Human-Robot Collaboration: Model Learning and Planning,2020,3,208,201,7,7 participants failed on the attention check question or their data were incomplete,Online Crowdsourcing,between-subjects,"Participants performed an online table clearing task with a robot, following either a trust-POMDP or myopic policy, and reported their trust using Muir's questionnaire.",Participants watched a video of a robot attempting to pick up objects on a table and chose to intervene or stay put.,Unspecified,Mobile Manipulators,Research,Manipulation,Cleaning,passive observation,Participants observed videos of the robot performing the task.,media,Participants watched videos of the robot interacting with objects.,simulated,The robot was represented through videos.,pre-programmed (non-adaptive),The robot followed pre-specified policies.,Questionnaires,Muir's Trust Questionnaire,Performance Metrics,Trust was measured using a questionnaire and performance metrics.,POMDP,Trust was modeled using a POMDP framework.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's policy was manipulated to either prioritize trust building (trust-POMDP) or ignore it (myopic), influencing the human's perception of the robot's performance and the task difficulty.",The trust-POMDP policy resulted in lower intervention rates and higher team performance compared to the myopic policy.,"The trust-POMDP robot outperformed the myopic robot, and the intervention rate was lower in the trust-POMDP condition.",The trust-POMDP robot policy resulted in better team performance than a policy that did not account for human trust.,"The robot attempted to pick up objects from a table, and the human could choose to intervene or let the robot complete the task.",ANOVA,A one-way ANOVA test was used to compare the accumulated rewards between the trust-POMDP and myopic conditions. The effect size (Cohen's d) was also computed to quantify the difference between the means of accumulated rewards in the two conditions.,TRUE,Robot-task-strategy,Robot-task-strategy,,"The study manipulated the robot's policy by having it follow either a trust-POMDP or a myopic policy. This manipulation directly influenced the order in which the robot attempted to pick up objects, which is a change in the task completion strategy. The paper states, 'In the trust-POMDP condition, the robot uses human trust as a means to optimize the long-term team performance. It follows the policy computed from the trust-POMDP... In the myopic condition, the robot ignores human trust. It follows a myopic policy...' This shows that the robot's task strategy was manipulated. The results showed that the trust-POMDP policy resulted in lower intervention rates and higher team performance compared to the myopic policy, indicating that the robot's task strategy impacted trust. Therefore, 'Robot-task-strategy' is the factor that impacted trust.",10.1145/3359616,https://dl.acm.org/doi/10.1145/3359616,"Trust in autonomy is essential for effective human-robot collaboration and user adoption of autonomous systems such as robot assistants. This article introduces a computational model that integrates trust into robot decision making. Specifically, we learn from data a partially observable Markov decision process (POMDP) with human trust as a latent variable. The trust-POMDP model provides a principled approach for the robot to (i) infer the trust of a human teammate through interaction, (ii) reason about the effect of its own actions on human trust, and (iii) choose actions that maximize team performance over the long term. We validated the model through human subject experiments on a table clearing task in simulation (201 participants) and with a real robot (20 participants). In our studies, the robot builds human trust by manipulating low-risk objects first. Interestingly, the robot sometimes fails intentionally to modulate human trust and achieve the best team performance. These results show that the trust-POMDP calibrates trust to improve human-robot team performance over the long term. Further, they highlight that maximizing trust alone does not always lead to the best performance."
"Chen, Min; Nikolaidis, Stefanos; Soh, Harold; Hsu, David; Srinivasa, Siddhartha",Trust-Aware Decision Making for Human-Robot Collaboration: Model Learning and Planning,2020,3,20,20,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants performed a table clearing task with a real robot, following either a trust-POMDP or myopic policy, and reported their trust using Muir's questionnaire.","Participants interacted with a real robot to clear objects off a table, choosing to intervene or stay put.",Unspecified,Mobile Manipulators,Research,Manipulation,Cleaning,direct-contact interaction,Participants physically interacted with a real robot.,real-world,Participants interacted with a real robot in a lab setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot followed pre-specified policies.,Questionnaires,Muir's Trust Questionnaire,Performance Metrics,Trust was measured using a questionnaire and performance metrics.,POMDP,Trust was modeled using a POMDP framework.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's policy was manipulated to either prioritize trust building (trust-POMDP) or ignore it (myopic), influencing the human's perception of the robot's performance and the task difficulty.",The trust-POMDP policy resulted in lower intervention rates and higher team performance compared to the myopic policy.,"The trust-POMDP robot outperformed the myopic robot, and the intervention rate was lower in the trust-POMDP condition. The effect size was larger in the real-robot experiment than the online experiment.",The trust-POMDP robot policy resulted in better team performance than a policy that did not account for human trust.,"The robot attempted to pick up objects from a table, and the human could choose to intervene or let the robot complete the task.",ANOVA,A one-way ANOVA test was used to compare the accumulated rewards between the trust-POMDP and myopic conditions. The effect size (Cohen's d) was also computed to quantify the difference between the means of accumulated rewards in the two conditions.,TRUE,Robot-task-strategy,Robot-task-strategy,,"The study manipulated the robot's policy by having it follow either a trust-POMDP or a myopic policy. This manipulation directly influenced the order in which the robot attempted to pick up objects, which is a change in the task completion strategy. The paper states, 'In the real-robot experiment, we followed the same robot policies, model parameters, and procedures as the online AMT experiment...'. This means the robot's task strategy was manipulated in the same way as the previous study. The results showed that the trust-POMDP policy resulted in lower intervention rates and higher team performance compared to the myopic policy, indicating that the robot's task strategy impacted trust. Therefore, 'Robot-task-strategy' is the factor that impacted trust.",10.1145/3359616,https://dl.acm.org/doi/10.1145/3359616,"Trust in autonomy is essential for effective human-robot collaboration and user adoption of autonomous systems such as robot assistants. This article introduces a computational model that integrates trust into robot decision making. Specifically, we learn from data a partially observable Markov decision process (POMDP) with human trust as a latent variable. The trust-POMDP model provides a principled approach for the robot to (i) infer the trust of a human teammate through interaction, (ii) reason about the effect of its own actions on human trust, and (iii) choose actions that maximize team performance over the long term. We validated the model through human subject experiments on a table clearing task in simulation (201 participants) and with a real robot (20 participants). In our studies, the robot builds human trust by manipulating low-risk objects first. Interestingly, the robot sometimes fails intentionally to modulate human trust and achieve the best team performance. These results show that the trust-POMDP calibrates trust to improve human-robot team performance over the long term. Further, they highlight that maximizing trust alone does not always lead to the best performance."
"Chen, Na; Zu, Yao; Song, Jing",Research on the influence and mechanism of human–vehicle moral matching on trust in autonomous vehicles,2023,1,227,200,27,27 participants were excluded due to short answer time and inconsistent scores,Online Crowdsourcing,between-subjects,"Participants completed an informed consent form, watched an introductory video about autonomous vehicles, took a one-minute break, watched a 'trolley problem' video showing either a deontological or utilitarian vehicle, answered questions about the vehicle's moral type, and completed a questionnaire.",Participants watched a video of an autonomous vehicle making a moral decision in a trolley problem scenario and then completed a questionnaire.,Unspecified,Autonomous Vehicles,Other: The autonomous vehicle was used to make moral decisions in a simulated driving scenario.,Evaluation,Survey/Questionnaire Completion,passive observation,Participants watched a video of the autonomous vehicle.,media,Participants watched a video of a simulated driving scenario.,simulated,The robot was represented in a video.,pre-programmed (non-adaptive),The autonomous vehicle followed a pre-programmed decision in the video.,Questionnaires; Custom Scales,,,Trust was measured using a custom questionnaire.,"parametric models (e.g., regression)",Multiple linear regression was used to test the hypotheses.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The moral type of the autonomous vehicle was manipulated by showing different trolley problem scenarios, influencing participants' expectations and perceptions of the vehicle's behavior.","Utilitarian individuals showed greater trust in autonomous vehicles than deontological individuals. Perceived value positively impacted trust, while perceived risk negatively impacted trust. Heterogeneous moral matching (utilitarian people, deontological vehicles) resulted in higher trust than homogeneous matching.","Heterogeneous moral matching (utilitarian people, deontological vehicles) resulted in higher trust than homogeneous matching, which is inconsistent with the theory of shared values but consistent with the hypothesis of individual selfish preferences.","People's moral type affects their trust in autonomous vehicles, with utilitarian individuals showing greater trust. Heterogeneous moral matching between humans and vehicles leads to higher trust than homogeneous matching.","The robot (autonomous vehicle) made a decision in a trolley problem scenario, choosing to either save more lives at the cost of one or to not intervene and let the original outcome occur. The human participant watched the video and then completed a questionnaire.",ANOVA; Pearson correlation; Linear regression,"The study used ANOVA to check for differences in demographic variables across groups. Correlation tests were used to examine the relationships between utilitarianism, deontology, perceived risk, perceived value, and trust. Multiple linear regression was used to test the hypotheses, examining the direct effect of human moral type on trust, the mediating roles of perceived value and perceived risk, and the moderating effect of vehicle moral type.",TRUE,Robot-morality,Robot-morality,,"The study manipulated the moral type of the autonomous vehicle by showing different trolley problem scenarios where the vehicle either chose to save more lives at the cost of one (utilitarian) or not intervene and let the original outcome occur (deontological). This directly changes the robot's actions based on a moral framework, which fits the 'Robot-morality' category. The results showed that the moral type of the vehicle impacted trust, as heterogeneous moral matching (utilitarian people, deontological vehicles) resulted in higher trust than homogeneous matching. Therefore, 'Robot-morality' is also listed as a factor that impacted trust. There were no other factors manipulated, so there are no factors that did not impact trust.",10.3389/fpsyg.2023.1071872,https://www.frontiersin.org/articles/10.3389/fpsyg.2023.1071872/full,"Introduction               Autonomous vehicles can have social attributes and make ethical decisions during driving. In this study, we investigated the impact of human-vehicle moral matching on trust in autonomous vehicles and its mechanism.                                         Methods               A 2*2 experiment involving 200 participants was conducted.                                         Results               The results of the data analysis show that utilitarian moral individuals have greater trust than deontological moral individuals. Perceived value and perceived risk play a double-edged role in people’s trust in autonomous vehicles. People’s moral type has a positive impact on trust through perceived value and a negative impact through perceived risk. Vehicle moral type moderates the impact of human moral type on trust through perceived value and perceived risk.                                         Discussion               The conclusion shows that heterogeneous moral matching (people are utilitarian, vehicles are deontology) has a more positive effect on trust than homogenous moral matching (both people and vehicles are deontology or utilitarian), which is consistent with the assumption of selfish preferences of individuals. The results of this study provide theoretical expansion for the fields related to human-vehicle interaction and AI social attributes and provide exploratory suggestions for the functional design of autonomous vehicles."
"Chi, Oscar Hengxuan; Chi, Christina G.; Gursoy, Dogan; Nunkoo, Robin",Customers’ acceptance of artificially intelligent service robots: The influence of trust and culture,2023,1,986,986,0,No participants were excluded,Online Crowdsourcing,,Participants read a short scenario of AI robot service and completed a survey.,Participants were asked to imagine interacting with an AI robot in a hospitality setting and then complete a survey.,Unspecified,Service and Assistive Robots,Social,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read a scenario about the robot interaction.,media,The interaction was based on a written scenario.,hypothetical,The robot was only described in a text scenario.,not autonomous,The robot's actions were not described as autonomous.,Questionnaires,,,Trust was measured using a questionnaire.,"parametric models (e.g., regression)",The study used SEM to model the relationships between trust and other variables.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study did not manipulate any specific factors related to trust; it measured trust based on a hypothetical scenario.,"The study did not manipulate trust, but measured it as an outcome variable.","The study found that trust is a significant factor influencing the intention to use AI robots, and that cultural dimensions moderate this relationship. The study also found that uncertainty avoidance is a stronger driver of social influence than collectivism.","Trust in interaction with AI robots is a significant higher-order construct that influences the intention of use, and is moderated by cultural factors.",Participants read a scenario about interacting with an AI robot in a hospitality setting and then completed a survey about their perceptions and intentions.,Multilevel Model; Partial least squares; independent t-tests; Partial least squares; Bootstrapping,"The study used Confirmatory Factor Analysis (CFA) to assess the measurement model, followed by Structural Equation Modeling (SEM) to test the relationships between variables in the proposed model. Independent t-tests were used to compare cultural dimensions between the U.S. and China. Partial Least Squares Structural Equation Modeling (PLS-SEM) with a bootstrapping method was employed to analyze the moderation effects of cultural dimensions on the relationships between variables. The interaction terms were created at the indicator level and were examined through the test of standardized path coefficients (β).",FALSE,,,,"The study did not manipulate any factors related to the robot or the task. Participants read a scenario and completed a survey. The study measured the impact of various factors (performance expectancy, effort expectancy, hedonic motivation, social influence, anthropomorphism, and cultural dimensions) on trust and intention to use AI robots, but these were not experimentally manipulated. The study is observational and survey-based, not experimental.",10.1016/j.ijinfomgt.2023.102623,https://linkinghub.elsevier.com/retrieve/pii/S026840122300004X,"This study addresses two critical research gaps in human-robot interaction (HRI): the limited systematic research on the role of trust in customers’ acceptance of artificially intelligent (AI) robots; and the lack of understanding of robot acceptance under different cultural backgrounds. Drawing on the AIDUA framework, this study examines the impacts of trust and moderating effects of both national (the U.S. and China) and individual culture on customers’ intentions to use AI robots in hospitality services by developing a theoretical model. The model is tested on data collected using online data collection platforms from 491 U.S. and 495 Chinese respondents. PLSSEM and the bootstrapping method were used to test the hypothesized relationships and analyze the moderating effects of culture, respectively. The findings suggest that trust in interaction with AI robots is a significant higherorder construct that influences the intention of use. Furthermore, uncertainty avoidance, long-term orientation, and power distance have been found to exhibit significant moderation effects. The results of this study extend the theoretical frameworks in HRI and provide detailed guidance to promote AI robot applications across different cultures."
"Chi, Vivienne Bihe; Malle, Bertram F.",People Dynamically Update Trust When Interactively Teaching Robots,2023,1,220,220,0,No participants were excluded,Online Crowdsourcing,mixed design,"Participants acted as teachers for a simulated robot on a smartphone, making decisions on how to teach the robot across 15 trials. Trust was measured at three levels: momentary trust feelings, perceptions of trustworthiness, and intended reliance. The robot's learning rate and task difficulty were manipulated.","Participants taught a simulated robot how to act appropriately in healthcare tasks, choosing between instructing the robot or evaluating its proposed action.",Unspecified,Service and Assistive Robots,Care; Educational; Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated robot through a smartphone interface.,simulation,The interaction was conducted through a smartphone-based simulation.,simulated,The robot was a simulated agent presented through a smartphone interface.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user's input.,Questionnaires; Multidimensional Measures; Real-time Trust Measures,Multi-Dimensional Measure of Trust (MDMT),Performance Metrics,"Trust was measured using questionnaires, a multi-dimensional scale, and real-time trust ratings, along with performance metrics.","parametric models (e.g., regression)",Mixed-effects models were used to predict trust based on various factors.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's learning rate and task difficulty were directly manipulated to observe their effect on trust. The robot's performance was pre-programmed to improve at different rates.,Trust increased with robot performance and was higher for fast-learning robots. Task difficulty negatively impacted trust. The robot's learning rate had a significant impact on trust.,"Initial trust feelings showed a dip before recovery, especially for the slow-learning robot. The MDMT showed that the ethical and transparent dimensions had more 'Does Not Fit' responses than the competent and reliable dimensions.","People dynamically update their trust in robots based on the robot's performance, task difficulty, and cumulative learning, and this trust is influenced by the teacher's active involvement.","The robot presented a healthcare task, and the human teacher decided whether to instruct the robot or evaluate its proposed action. The robot's performance was pre-programmed to improve over time.",Mixed-effects model; ANOVA,"The study used mixed-effects models to analyze trial-by-trial trust feelings, perceptions of trustworthiness (across four dimensions), and changes in intended reliance. These models included participant as a random effect and various fixed effects such as robot learning rate, task difficulty, and robot performance. ANOVA was used to analyze the distribution of 'Does Not Fit' responses across the four trust dimensions of the MDMT.",TRUE,Robot-accuracy; Task-complexity; Robot-adaptability,Robot-accuracy; Task-complexity; Robot-adaptability,,"The study manipulated the robot's learning rate, which directly impacts its performance and is categorized as 'Robot-accuracy'. The robot's performance was pre-programmed to improve at different rates (fast vs. slow), which is a manipulation of its learning capability, categorized as 'Robot-adaptability'. The study also manipulated the difficulty of the tasks, which is categorized as 'Task-complexity'. The paper states, 'Performance was also manipulated between subjects, such that half of participants encountered a robot with slow performance improvement and the other half, a robot with fast performance improvement (labeled robot learning rate).' and 'The robot's tasks were initially designated as easy, medium, or hard and were presented in this fixed order, from easy to hard, on each training day.' The results showed that trust was influenced by the robot's performance, task difficulty, and the robot's learning rate. The paper states, 'The analysis revealed that, first, people's feelings of trust in the robot on the immediately upcoming task was strongly influenced by a robot's level of appropriate performance observed on the previous task... Furthermore, the task difficulty of the upcoming task mattered as well: The more difficult this task, the less people trusted the robot to perform well on it... However, learning rate interacted with cumulative performance: The more evidence of the robot's learning accumulated, the more people built stronger feelings of trust in the fast-learning robot than in the slow-learning robot'. Therefore, all three manipulated factors impacted trust. There were no factors that were manipulated that did not impact trust.",10.1145/3568162.3576962,https://dl.acm.org/doi/10.1145/3568162.3576962,"Human-robot trust research often measures people’s trust in robots in individual scenarios. However, humans may update their trust dynamically as they continuously interact with a robot. In a wellpowered study (𝑛 = 220), we investigate the trust updating process across a 15-trial interaction. In a novel paradigm, participants act in the role of teacher to a simulated robot on a smartphone-based platform, and we assess trust at multiple levels (momentary trust feelings, perceptions of trustworthiness, and intended reliance). Results reveal that people are highly sensitive to the robot’s learning progress trial by trial: they take into account both previous-task performance, current-task difficulty, and cumulative learning across training. More integrative perceptions of robot trustworthiness steadily grow as people gather more evidence from observing robot performance, especially of faster-learning robots. Intended reliance on the robot in novel tasks increased only for faster-learning robots."
"Chi, Vivienne Bihe; Malle, Bertram F.","Interactive Human-Robot Teaching Recovers and Builds Trust, Even With Imperfect Learners",2024,2,221,221,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were assigned to either an interactive teacher or supervisor role and trained a virtual robot through a chat app, expressing trust before each of 15 tasks.",Participants trained a virtual robot to act appropriately in healthcare tasks.,Unspecified,Service and Assistive Robots,Care; Educational; Research,Supervision,Monitoring,minimal interaction,Participants interacted with a virtual robot through a chat interface.,simulation,The interaction was through a simulated chat interface with a virtual robot.,simulated,The robot was a virtual agent presented through a chat interface.,wizard of oz (directly controlled),The robot's actions were controlled by the experimenter.,Custom Scales; Real-time Trust Measures,,Performance Metrics,Trust was measured using a slider scale after each task and through a multi-item measure after each block of tasks.,"parametric models (e.g., regression)",Trust trajectories were predicted using mixed-effects models.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"Participants were assigned to either an interactive teacher or supervisor role, and the robot's performance improved over time.",Interactive teachers showed higher trust and were more resilient to initial trust loss compared to supervisors.,Supervisors experienced a more drastic initial trust loss but recovered to similar or slightly higher trust levels than interactive teachers by the end of the training.,"Interactive teaching mitigated early trust loss compared to supervision, but supervisors recovered trust and reached similar or slightly higher levels by the end of the training.","The robot described its environment, goal, and planned action, and the human either instructed the robot, evaluated its plan, or observed its plan.",Mixed-effects model; polynomial interaction contrasts; t-test; Linear regression,"The study used mixed-effects models to analyze trial-by-trial trust ratings, with participant as a random effect and factors like interactivity, task difficulty, and robot performance as fixed effects. Polynomial interaction contrasts were used to analyze trust trajectories across training days, comparing interactive teachers and supervisors. A t-test was used to compare self-attribution scores between the two groups. Multiple regression was used to predict self-attributed contributions to the robot's improvement.",TRUE,Robot-autonomy; Robot-accuracy,Robot-autonomy; Robot-accuracy,,"The study manipulated 'Robot-autonomy' by assigning participants to either an interactive teacher role (where they could instruct or evaluate the robot's actions) or a supervisor role (where they only observed the robot's actions). This is a manipulation of the level of control the human has over the robot's actions, which is a form of autonomy. The study also manipulated 'Robot-accuracy' by having the robot's performance improve over the course of the training session, starting with flawed actions and progressing to near-perfect performance. This is explicitly stated in the paper: 'The robot was engaged in learning norm-appropriate behavior in a healthcare setting and improved from mistake-prone to near-fawless performance.' Both of these manipulations were found to impact trust. The paper states that 'interactive teachers felt overall higher trust than did supervisors' and that 'people's feelings of trust in the robot increased considerably in response to the robot's current performance'. Therefore, both 'Robot-autonomy' and 'Robot-accuracy' impacted trust. There were no factors that were manipulated that did not impact trust.",10.1145/3610977.3634977,https://dl.acm.org/doi/10.1145/3610977.3634977,"Building and maintaining trust is critically important for continued human-robot teaching and the prospect of robots learning social skills from natural environments. Whereas previous work often explored strategies to reduce system errors, mitigate trust loss, or enhance learning by interactive teaching, few studies have investigated the possible benefts of fully engaged, interactive teaching on human trust. Motivated by a pair of discrepant previous investigations, the present studies for the frst time directly tested the causal impact of interactivity on the loss and recovery of trust in a human-robot social skills training context. Building on a previously developed experimental paradigm, we randomly assigned participants to one of two modes of interaction: interactive teacher vs. supervisor of an experimentally controlled virtual robot. The robot was engaged in learning norm-appropriate behavior in a healthcare setting and improved from mistake-prone to near-fawless performance. Participants indicated their changing trust during the 15-trial training session and how much they attributed the robot’s improvement to their own training contributions. Interactive teachers were more resilient to initial trust loss, showed increased trust in the robot’s performance on additional tasks, and attributed more of the robot’s improvement to themselves than did supervisors, even when the robots were slow learners."
"Chi, Vivienne Bihe; Malle, Bertram F.","Interactive Human-Robot Teaching Recovers and Builds Trust, Even With Imperfect Learners",2024,2,258,247,11,11 participants were excluded following the preregistered criteria,Online Crowdsourcing,between-subjects,"Participants were assigned to either an interactive teacher or supervisor role and trained a virtual robot with either a slow or fast learning rate, expressing trust before each of 15 tasks.",Participants trained a virtual robot to act appropriately in healthcare tasks.,Unspecified,Service and Assistive Robots,Care; Educational; Research,Supervision,Monitoring,minimal interaction,Participants interacted with a virtual robot through a chat interface.,simulation,The interaction was through a simulated chat interface with a virtual robot.,simulated,The robot was a virtual agent presented through a chat interface.,wizard of oz (directly controlled),The robot's actions were controlled by the experimenter.,Custom Scales; Real-time Trust Measures,,Performance Metrics,Trust was measured using a slider scale after each task and through a multi-item measure after each block of tasks.,"parametric models (e.g., regression)",Trust trajectories were predicted using mixed-effects models.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"Participants were assigned to either an interactive teacher or supervisor role, and the robot's learning rate was manipulated to be either slow or fast.","Interactive teachers were more resilient to early trust loss, and supervisors of slow-learning robots showed less increase in future trust.","Supervisors of slow-learning robots did not show an increase in future trust after training, unlike supervisors of fast-learning robots.","Interactive teaching mitigated early trust loss even with a slow-learning robot, and supervisors of slow-learning robots showed less increase in future trust.","The robot described its environment, goal, and planned action, and the human either instructed the robot, evaluated its plan, or observed its plan.",Mixed-effects model; polynomial contrasts; t-test; Linear regression; Mediation analysis,"The study employed mixed-effects models to analyze trial-by-trial trust ratings, incorporating robot learning rate as an additional fixed effect. Polynomial contrasts were used to analyze trust trajectories across training days, comparing different learning rates and interaction modes. A t-test was used to compare self-attribution scores between groups. Multiple regression was used to predict self-attributed contributions to the robot's improvement. Mediation analyses were used to examine the role of perceived improvement in the relationship between interactive teaching and self-attributed contributions.",TRUE,Robot-autonomy; Robot-accuracy; Robot-adaptability,Robot-autonomy; Robot-accuracy; Robot-adaptability,,"Similar to Study 1, 'Robot-autonomy' was manipulated by assigning participants to either an interactive teacher or supervisor role. 'Robot-accuracy' was manipulated by having the robot's performance improve over time. Additionally, 'Robot-adaptability' was manipulated by varying the robot's learning rate (slow vs. fast). The paper states: 'In Study 2, we experimentally manipulated not only the teaching interactivity but also the robot's learning rate (slow vs. fast performance improvement)'. The paper also states that 'interactive teachers expressed greater feelings of trust than did supervisors, regardless of the robot learning rate' and that 'participants' trust feelings were responsive to the robot's task-by-task performance as well as its cumulative performance'. Furthermore, the paper states that 'Trust increased after the training session more for the fast-learning than the slow-learning robot'. Therefore, all three manipulated factors impacted trust. There were no factors that were manipulated that did not impact trust.",10.1145/3610977.3634977,https://dl.acm.org/doi/10.1145/3610977.3634977,"Building and maintaining trust is critically important for continued human-robot teaching and the prospect of robots learning social skills from natural environments. Whereas previous work often explored strategies to reduce system errors, mitigate trust loss, or enhance learning by interactive teaching, few studies have investigated the possible benefts of fully engaged, interactive teaching on human trust. Motivated by a pair of discrepant previous investigations, the present studies for the frst time directly tested the causal impact of interactivity on the loss and recovery of trust in a human-robot social skills training context. Building on a previously developed experimental paradigm, we randomly assigned participants to one of two modes of interaction: interactive teacher vs. supervisor of an experimentally controlled virtual robot. The robot was engaged in learning norm-appropriate behavior in a healthcare setting and improved from mistake-prone to near-fawless performance. Participants indicated their changing trust during the 15-trial training session and how much they attributed the robot’s improvement to their own training contributions. Interactive teachers were more resilient to initial trust loss, showed increased trust in the robot’s performance on additional tasks, and attributed more of the robot’s improvement to themselves than did supervisors, even when the robots were slow learners."
"Chien, Shih-Yi; Lewis, Michael; Mehrotra, Siddharth; Sycara, Katia",Imperfect Automation in Scheduling Operator Attention on Control of Multi-Robots,2013,1,48,48,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were trained on robot control, then completed a foraging task with either 3 or 6 robots under different reliability and queue conditions, and completed workload and trust questionnaires after each task load condition.","Participants performed a foraging task in a simulated environment, identifying and marking victims while managing robot failures.",Pioneer P3-AT,Mobile Robots; UGVs,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the robots through a simulation interface.,simulation,The interaction took place in a simulated environment.,simulated,The robots were represented in a simulation.,shared control (fixed rules),"The robots followed pre-defined paths and reported failures, but the human operator had to resolve the failures.",Questionnaires,Jian et al. Trust Scale; NASA Task Load Index (NASA-TLX),Performance Metrics,Trust was measured using a modified trust questionnaire and performance metrics.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The reliability of the robot alarms and the queue type were manipulated to influence the operator's workload and trust. The task load was also manipulated by changing the number of robots.,"Trust was higher in the SJF-queue condition when reliability was high, and higher in the Open-queue condition when reliability was low. An inverse relationship between workload and trust was observed.",An inverse relationship was observed between experienced workload and rated trust. The results also showed that simply increasing the system reliability might not effectively contribute to the overall performance or the participants' trust in automation.,The study found that the relationship between trust and workload is complex and that simply increasing system reliability does not necessarily increase trust. The type of queueing discipline also affected trust and workload.,"The robots autonomously followed pre-defined paths and reported failures. The human operator monitored the robots, identified victims, and resolved robot failures by teleoperating the robot to the next waypoint.",ANOVA,"A mixed-model ANOVA was used to analyze the effects of Reliability Level (high-90% vs. low-50%), Queue Type (Open-queue vs. SJF-queue) as between-subject factors, and Task Load (high-6 robots vs. low-3 robots) as a within-subject factor on various dependent variables. These variables included task performance measurements (victim detection, distance traveled, failures resolved) and post-experiment self-rating surveys (NASA-TLX workload and trust questionnaire). The ANOVA was used to determine significant differences among the experimental conditions.",TRUE,Robot-accuracy; Robot-interface-design; Task-complexity,Robot-accuracy; Robot-interface-design,Task-complexity,"The study manipulated the reliability of the robot alarms, which directly impacts the accuracy of the robot's ability to report failures, thus influencing 'Robot-accuracy'. The study also manipulated the queue type (Open-queue vs. SJF-queue), which is a change to the interface design that affects how alarms are presented to the user, thus influencing 'Robot-interface-design'. The number of robots (3 vs. 6) was also manipulated, which directly impacts the cognitive load and the amount of attention required from the operator, thus influencing 'Task-complexity'. The results showed that the reliability of the alarms and the queue type had an impact on trust, with higher trust in the SJF-queue condition when reliability was high and higher trust in the Open-queue condition when reliability was low. The task load (number of robots) did not have a direct impact on trust, although it did influence workload. Therefore, 'Robot-accuracy' and 'Robot-interface-design' are listed as factors that impacted trust, while 'Task-complexity' is listed as a factor that did not impact trust.",10.1177/1541931213571260,http://journals.sagepub.com/doi/10.1177/1541931213571260,"An operator's workload increases substantially when the operator must control multiple robots and continually shift attention from robot to robot. As the number of robots increases, the amount of time an operator can spend operating any particular robot decreases, which leads to inevitable changes in the robot’s performance. If the robots could self-report encountered faults, the operator could conserve cognitive resources to spend on reasoning about more complex situations. In the reported experiment, participants performed foraging tasks while assisted by an alarmed system, either Open-queue in which all alarms are displayed or SJF-queue (shortest-job-first), whose reliability level was high (90%) or low (50%) under different task load (3-robots vs. 6-robots). The results showed that simply increasing the system reliability might not effectively contribute to the overall performance or the participants’ trust in automation. An inverse relationship was observed between experienced workload and rated trust which also amplified the effects of imperfect automation."
"Chien, Shih-Yi; Lewis, Michael; Semnani-Azad, Zhaleh; Sycara, Katia",An Empirical Model of Cultural Factors on Trust in Automation,2014,1,45,65,0,No participants were excluded,Online Crowdsourcing,,"Participants were recruited from the University of Pittsburgh community to identify the referents of the selected items. Then, participants were recruited from Amazon Mechanical Turk to complete the trust instrument, which consisted of 110 items for measuring attitudes toward automation in general and specific use of automation. Exploratory factor analysis was used to determine the dimensionality of the data.","Participants rated their trust in automation using a 5-point Likert scale. They were asked to consider both general attitudes toward automation and specific instances of automation, such as a GPS navigation system.",Unspecified,Mobile Robots,Other: Navigation,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the automation and rated their trust.,media,The interaction was based on text descriptions of automation.,hypothetical,The robot was only described hypothetically.,not autonomous,The automation was described but did not perform any actions.,Questionnaires,Culture-Technology Fit (CTF); Empirically Derived (ED); Human-Computer Trust (HCT); International Comparison of Technology Adoption (ICTA); Online Trust Belief (OTB); SHAPE Automation Trust Index (SATI); Technological Adoptiveness Scale (TAS); Trust in Specific Technology (TIST),,Trust was measured using a questionnaire.,no modeling,No computational model of trust was developed.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study did not manipulate any factors related to trust; it focused on developing a trust assessment instrument.,,The study identified 70 items addressing automation in general and 40 items involving judgments about particular instances of automation.,"The study developed a psychometrically grounded measure of trust in automation, comprising 42 items categorized into three main constructs (performance expectancy, process transparency, and purpose influence) and three types of moderators (cultural-technological contexts, individual, and cultural differences).","Participants completed a survey rating their trust in automation, considering both general attitudes and specific instances of automation.",Factor analysis; Principal component analysis,"Exploratory factor analysis (EFA) was used to determine the dimensionality of the data. A principal components factor analysis with varimax rotation was performed to examine the number of factors produced. These analyses were used to refine the initial set of 110 items for measuring trust in automation, resulting in a reduced set of 42 items.",FALSE,,,,"The study did not manipulate any factors. The study focused on developing a psychometrically grounded measure of trust in automation. Participants were asked to rate their trust in automation based on a questionnaire. There were no intentional changes to any factors related to the automation or the task itself. The study aimed to identify the dimensions of trust in automation through factor analysis of survey responses, not to test the impact of any manipulated variables on trust.",10.1177/1541931214581181,http://journals.sagepub.com/doi/10.1177/1541931214581181,"Trust is conceived to be an important factor mediating an individual’s reliance on automation. Studies have shown individual and cultural differences as well as tasking context significantly affect an individual’s development of trust behaviors. This paper reports preliminary progress in developing a psychometrically grounded subjective measure of trust in automation. A total of 110 items from 8 existing instruments were considered for inclusion in this instrument using Amazon Mechanical Turk to supply samples. Exploratory factor analysis was performed to determine the dimensionality of the data, with 42 items selected for continued refinement. Our proposed model comprises 3 main constructs (performance expectancy, process transparency, and purpose influence) along with 3 types of moderators (culturaltechnological contexts, individual, and cultural differences)."
"Chien, Shih-Yi; Lewis, Michael; Sycara, Katia; Liu, Jyi-Shane; Kumru, Asiye",The Effect of Culture on Trust in Automation: Reliability and Workload,2018,1,360,360,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants provided demographic data, completed questionnaires, and then completed a training session. They then performed the target classification tasks in two 10-minute experimental sessions with different task load conditions, completing trust questionnaires and NASA-TLX after each session. Conditions were counterbalanced for reliability.",Participants were tasked to identify and attack hostile targets (payload task) while monitoring and rerouting the UAVs' paths if necessary (navigation tasks).,Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulation of UAVs through a computer interface.,simulation,Participants interacted with a simulated environment of UAVs.,simulated,The robots were represented as simulated UAVs in a computer interface.,shared control (fixed rules),"The UAVs followed pre-planned paths, but the human could reroute them and had to identify targets.",Behavioral Measures; Custom Scales; Multidimensional Measures; Questionnaires,NASA Task Load Index (NASA-TLX); Big Five Inventory Scale; Propensity to Trust Scales,Performance Metrics,"Trust was measured using questionnaires, behavioral measures, and performance metrics.","parametric models (e.g., regression)",The study used ANOVA to analyze the effects of the manipulations on trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the target finder and the task load were directly manipulated to influence trust. Reliability was manipulated by changing the accuracy of the target finder, and task load was manipulated by changing the speed of the UAVs.","Increased reliability led to higher trust ratings, while increased task load led to higher trust ratings for Taiwanese and Turkish participants but not for U.S. participants. The U.S. participants showed a positive correlation between perceived workload and trust.",Turkish participants showed lower initial trust and less effective trust calibration compared to U.S. and Taiwanese participants. The Turkish participants also checked less frequently despite reporting lower trust. The study also found that operators from all three cultures were more likely to over-trust the automation on the trial following its first error than they were for the full session.,"The study found that cultural factors significantly influence trust in automation, with Turkish participants exhibiting lower initial trust and less effective trust calibration compared to U.S. and Taiwanese participants. The study also found that increased reliability led to higher trust ratings, while increased task load led to higher trust ratings for Taiwanese and Turkish participants but not for U.S. participants.","The robot (simulated UAV) autonomously navigated to targets. The human participant monitored the UAVs, identified targets using an automated target finder, and made decisions to attack or not attack the targets.",ANOVA; t-test,"The study used mixed-model ANOVA to analyze the effects of reliability (high vs. low), culture (U.S., Taiwan, and Turkey), and task load (high vs. low) on trust, performance, and behavioral measures. Post-hoc comparisons were conducted using t-tests with Bonferroni correction to examine specific differences between groups and conditions. The ANOVA was used to determine the main effects and interactions of the manipulated variables on the dependent variables, while t-tests were used for pairwise comparisons to identify where significant differences occurred.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy; Task-complexity,,"The study manipulated the reliability of the target finder, which directly impacts the accuracy of the robot's performance in identifying targets. This falls under 'Robot-accuracy' because it changes the success rate of the robot's actions. The study also manipulated the speed of the UAVs, which changes the frequency of payload requests and thus the cognitive demands on the participants, which is categorized as 'Task-complexity'. Both of these manipulations were found to impact trust levels. Specifically, increased reliability led to higher trust, and increased task load led to higher trust for some cultural groups.",10.1145/3230736,https://dl.acm.org/doi/10.1145/3230736,"Trust in automation has become a topic of intensive study since the late 1990s and is of increasing importance with the advent of intelligent interacting systems. While the earliest trust experiments involved human interventions to correct failures/errors in automated control systems, a majority of subsequent studies have investigated information acquisition and analysis decision aiding tasks such as target detection for which automation reliability is more easily manipulated. Despite the high level of international dependence on automation in industry, almost all current studies have employed Western samples primarily from the U.S. The present study addresses these gaps by running a large sample experiment in three (U.S., Taiwan, and Turkey) diverse cultures using a “trust sensitive task” consisting of both automated control and target detection subtasks. This article presents results for the target detection subtask for which reliability and task load were manipulated. The current experiments allow us to determine whether reported effects are universal or specific to Western culture, vary in baseline or magnitude, or differ across cultures. Results generally confirm consistent effects of manipulations across the three cultures as well as cultural differences in initial trust and variation in effects of manipulations consistent with 10 cultural hypotheses based on Hofstede’s Cultural Dimensions and Leung and Cohen’s theory of Cultural Syndromes. These results provide critical implications and insights for correct trust calibration and to enhance human trust in intelligent automation systems across cultures. Additionally, our results would be useful in designing intelligent systems for users of different cultures. Our article presents the following contributions: First, to the best of our knowledge, this is the first set of studies that deal with cultural factors across all the cultural syndromes identified in the literature by comparing trust in the Honor, Face, Dignity cultures. Second, this is the first set of studies that uses a validated cross-cultural trust measure for measuring trust in automation. Third, our experiments are the first to study the dynamics of trust across cultures."
"Chien, S.; Lewis, M.; Sycara, K.; Kumru, A.; Liu, J.","Influence of Culture, Transparency, Trust, and Degree of Automation on Automation Use",2020,1,360,360,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants provided demographic data and completed a dispositional trust scale. They were then randomly assigned to a display condition and completed a training tutorial. Participants then completed two 10-minute experimental sessions with different task loads, followed by workload and trust measures.",Participants managed UAV paths to avoid conflicts and hazards while also identifying and attacking hostile targets.,Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulation of UAVs through a computer interface.,simulation,Participants interacted with a simulated environment of UAVs.,simulated,The robots were represented as simulated UAVs on a computer screen.,shared control (fixed rules),"The UAVs followed pre-programmed paths, but the human could intervene to reroute them.",Questionnaires; Custom Scales,NASA Task Load Index (NASA-TLX); Disposition to Trust Questionnaire,Performance Metrics,Trust was measured using questionnaires and performance metrics.,"parametric models (e.g., regression)",The study used ANOVA to analyze the effects of different factors on trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the level of automation and transparency of the path planner, as well as task load, to see how these factors affected trust.","Trust was higher in transparent conditions (HL and PRHL) compared to the opaque condition (PR). The level of trust varied across cultures, with Turkish participants showing lower trust in the planner even when its actions were visible.","The study found that Taiwanese participants exhibited automation bias, relying on the opaque automation more than participants from other cultures. Also, V-H damage was higher in highlighting conditions, which was counterintuitive.","Transparency is a key factor in trust and automation dependence, and the role of trust is more important when information about automation behavior is incomplete. Cultural differences also play a significant role in trust and compliance with automation.","The human participant monitored a map display and rerouted UAVs to avoid collisions and hazards. The automated system either highlighted potential conflicts, proposed new paths, or both. The human could accept or reject the proposed paths.",ANOVA; t-test,"The study used a mixed-model ANOVA to analyze the effects of DOA/transparency (control, PR, HL, PRHL), countries (U.S., TW, and TK), and taskload (high vs. low) on various dependent variables such as trust, V-V damage, V-H damage, and compliance. T-tests were used for post-hoc analysis to compare specific group differences within the ANOVA results, such as differences in dispositional trust between countries and differences in trust ratings between conditions.",TRUE,Robot-verbal-communication-content; Robot-autonomy; Task-complexity,Robot-verbal-communication-content; Robot-autonomy,Task-complexity,"The study manipulated the transparency of the path planner by varying the information provided to the user about the planner's actions. This is categorized as 'Robot-verbal-communication-content' because the manipulation involves changing what information is communicated to the user about the system's decision-making process (e.g., highlighting conflicts, proposing paths). The study also manipulated the degree of automation by varying whether the system only highlighted conflicts or also proposed new paths, which is categorized as 'Robot-autonomy' because it changes the level of decision-making authority given to the automated system. The study also manipulated task load by changing the speed of the UAVs, which is categorized as 'Task-complexity' because it changes the cognitive demands on the user. The results showed that transparency and degree of automation impacted trust, with higher trust in transparent conditions and lower trust in opaque conditions. Task load did not have a significant impact on trust, although it did impact performance metrics. Therefore, 'Robot-verbal-communication-content' and 'Robot-autonomy' are listed as factors that impacted trust, and 'Task-complexity' is listed as a factor that did not impact trust.",10.1109/THMS.2019.2931755,,"The reported study compares groups of 120 participants each, from the United States (U.S.), Taiwan (TW), and Turkey (TK), interacting with versions of an automated path planner that vary in transparency and degree of automation. The nationalities were selected in accordance with the theory of cultural syndromes as representatives of Dignity (U.S.), Face (TW), and Honor (TK) cultures, and were predicted to differ in readiness to trust automation, degree of transparency required to use automation, and willingness to use systems with high degrees of automation. Three experimental conditions were tested. In the first, highlight, path conflicts were highlighted leaving rerouting to the participant. In the second, replanner made requests for permission to reroute when a path conflict was detected. The third combined condition increased transparency of the replanner by combining highlighting with rerouting to make the conflict on which decision was based visible to the user. A novel framework relating transparency, stages of automation, and trust in automation is proposed in which transparency plays a primary role in decisions to use automation but is supplemented by trust where there is insufficient information otherwise. Hypothesized cultural effects and framework predictions were confirmed."
"Chien, Shih-Yi; Lin, Yi-Ling; Chang, Bu-Fang",The Effects of Intimacy and Proactivity on Trust in Human-Humanoid Robot Interaction,2022,1,280,215,65,65 responses were removed because the task completion time was shorter than the sum of all corresponding videos,Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of four experimental conditions (proactive/intimate, proactive/impassive, reactive/intimate, reactive/impassive). They were asked to select a birthday gift for a simulated customer on an e-commerce website with the assistance of a robot salesperson. The robot's responses were presented as streamed video. Participants completed a questionnaire at the end of the experiment.","Participants were asked to select a birthday gift for a simulated customer on an e-commerce website, with the robot acting as a salesperson offering assistance.",Pepper,Humanoid Robots; Expressive Robots,Social; Research,Social,Social Guidance/Coaching,minimal interaction,Participants watched videos of the robot and interacted with a website.,media,Participants watched videos of the robot interacting with a simulated e-commerce website.,physical,The robot was physically present in the videos shown to participants.,pre-programmed (non-adaptive),The robot followed a pre-set script and did not adapt to user input.,Questionnaires; Multidimensional Measures,,,Trust was measured using questionnaires that assessed cognition-based and affect-based trust.,no modeling,The study did not use computational models of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's social manner (proactive vs. reactive) and expressive behavior (intimate vs. impassive) were manipulated to influence trust. The task was framed as a shopping experience.,Proactive and intimate robot behavior increased both cognition-based and affect-based trust. Proactive behavior also increased future intention to use.,"The study found that proactive behavior had a stronger impact on trust and future intention to use than intimate behavior. The interaction effect between proactive and intimate behavior was only significant for perceived intimacy, not for trust.",A robot's proactive manner and intimate behavior significantly increase user trust and intention to use the robot's services.,"The robot acted as a salesperson, providing product recommendations and assistance. The human participant selected a gift for a simulated customer on an e-commerce website.",t-test; ANOVA,"The study used t-tests to compare the means of different conditions in the preliminary study to validate the experimental manipulations. ANOVA was used in the formal study to analyze the main effects and interactions of the manipulated variables (social manner and expressive behavior) on perceived proactivity, intimacy, cognition-based trust, affect-based trust, and future intention to use. Post-hoc tests with Bonferroni correction were applied to identify significant differences between specific conditions.",TRUE,Robot-social-attitude; Robot-social-timing; Robot-verbal-communication-content; Robot-nonverbal-communication,Robot-social-attitude; Robot-social-timing; Robot-verbal-communication-content,Robot-nonverbal-communication,"The study manipulated the robot's social manner (proactive vs. reactive) and expressive behavior (intimate vs. impassive). 'Robot-social-attitude' was chosen because the intimate vs. impassive manipulation directly relates to the robot's perceived warmth and friendliness, which is a core aspect of social attitude. The 'Robot-social-timing' factor was chosen because the proactive vs. reactive manipulation directly relates to the timing of the robot's actions and responses during the interaction. 'Robot-verbal-communication-content' was chosen because the intimate condition included the robot sharing personal advice and using 'I' statements, which changes the content of the robot's communication. 'Robot-nonverbal-communication' was chosen because the intimate condition included changes to the robot's gaze and gestures, which are nonverbal cues. The results showed that the robot's social manner (proactive vs. reactive) and expressive behavior (intimate vs. impassive) significantly influenced trust. Specifically, proactive behavior and intimate verbal content increased both cognition-based and affect-based trust. The nonverbal cues were manipulated, but the results did not show a significant impact on trust, so it was included in the 'factors_that_did_not_impact_trust' category.",10.1007/s10796-022-10324-y,https://link.springer.com/10.1007/s10796-022-10324-y,"Social humanoid robots (SHRs) have been widely applied in diverse contexts to enhance human–robot interaction by imitating humanlike behavior. Previous studies have utilized a variety of design features to explore the influence of human–robot relationships, but the robot’s communication scheme when providing assistance during the interaction is rarely discussed. The purpose of this study is to investigate which SHR communication approaches are more favorable for users, where different levels of social manner (proactive vs. reactive) and types of expressive behavior (intimate vs. impassive) are developed and empirically validated. A total of 273 participants were recruited for our user studies, and two online survey sessions were conducted to simulate an online shopping experience. During the experiments, an SHR (the Pepper robot) was used to provide the associated services to the participants (such as providing recommendations or subjective opinions regarding a chosen product). The preliminary study confirmed that the manipulations designed for each experimental condition were valid. In the formal study, the results revealed strong evidence that both the SHR’s social manner and its expressive behavior significantly influence participant perceptions of robots and the resultant HRI experience."
"Chiou, Manolis; McCabe, Faye; Grigoriou, Markella; Stolkin, Rustam","Trust, Shared Understanding and Locus of Control in Mixed-Initiative Robotic Systems",2021,1,20,20,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were trained on the system, then completed 5 trials of a navigation task with a secondary cognitive task. They completed trust questionnaires after each trial and a workload questionnaire at the end.","Participants controlled a simulated robot in a navigation task, searching for a victim, while also completing a secondary mental rotation task.",Husky,Mobile Robots,Research,Navigation,Path Following,minimal interaction,Participants interacted with the robot through a simulation interface.,simulation,The interaction took place in a simulated environment.,simulated,The robot was a virtual representation in a simulation.,shared control (fixed rules),"The robot switched between teleoperation and autonomous navigation based on fixed rules in the MI condition, and the human switched in the HI condition.",Custom Scales; Questionnaires,,Performance Metrics,Trust was measured using a custom questionnaire and performance metrics.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The study manipulated the level of autonomy (HI vs MI) and introduced performance degradation factors (secondary task and noise) to influence trust.,"Trust increased over time in the MI condition, but not significantly in the HI condition. The MI system was trusted more than the HI system.","Trust and shared understanding increased over time in the MI condition, but not in the HI condition. There was a dip in trust in trial 4 in the MI condition, which was quickly reconciled in trial 5. Operators with high internal LoC trusted the robot less in the HI condition.","Operators learned to trust the MI robotic system over time, and their understanding of the system improved with more interactions.","The robot autonomously navigated to waypoints or was teleoperated by the human. The human controlled the robot's movement and switched between autonomy and teleoperation, while also completing a secondary mental rotation task.",Mann-Whitney U; Wilcoxon rank sum; Spearman correlation,"The study used the Mann-Whitney U test for between-group comparisons (HI vs MI conditions). The Wilcoxon signed ranks test was used for within-subject comparisons (different trials within the same condition). Spearman's rank correlation was used to assess correlations between different metrics. All tests were two-tailed, and a p-value less than 0.05 was considered statistically significant.",TRUE,Robot-autonomy; Task-complexity; Task-constraints,Robot-autonomy,Task-complexity; Task-constraints,"The study manipulated the level of autonomy by having two conditions: Human-Initiative (HI) and Mixed-Initiative (MI). In HI, the human operator was solely responsible for switching between teleoperation and autonomous navigation. In MI, both the operator and the robot's AI could initiate switches. This is a direct manipulation of 'Robot-autonomy' as it changes the decision authority. The study also included a secondary mental rotation task, which increased the 'Task-complexity' by adding a cognitive load. Additionally, the study introduced performance degradation factors such as noise to the laser scanner and a time limit for the secondary task, which are classified as 'Task-constraints'. The results showed that trust increased over time in the MI condition, but not significantly in the HI condition, indicating that 'Robot-autonomy' impacted trust. The study did not find any evidence that the secondary task or the performance degradation factors directly impacted trust levels, therefore 'Task-complexity' and 'Task-constraints' are classified as factors that did not impact trust.",10.1109/RO-MAN50785.2021.9515476,https://ieeexplore.ieee.org/document/9515476/,"This paper investigates how trust, shared understanding between a human operator and a robot, and the Locus of Control (LoC) personality trait, evolve and affect Human-Robot Interaction (HRI) in mixed-initiative robotic systems. As such systems become more advanced and able to instigate actions alongside human operators, there is a shift from robots being perceived as a tool to being a teammate. Hence, the team-oriented human factors investigated in this paper (i.e. trust, shared understanding, and LoC) can play a crucial role in efﬁcient HRI. Here, we present the results from an experiment inspired by a disaster response scenario in which operators remotely controlled a mobile robot in navigation tasks, with either human-initiative or mixedinitiative control, switching dynamically between two different levels of autonomy: teleoperation and autonomous navigation. Evidence suggests that operators trusted and developed an understanding of the robotic systems, especially in mixedinitiative control, where trust and understanding increased over time, as operators became more familiar with the system and more capable of performing the task. Lastly, evidence and insights are presented on how LoC affects HRI."
"Chita-Tegmark, Meia; Law, Theresa; Rabb, Nicholas; Scheutz, Matthias",Can You Trust Your Trust Measure?,2021,2,78,78,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants watched a video vignette of a robot in a fire evacuation scenario, then rated items from trust questionnaires, including N/A options, and explained their choices.",Participants rated the applicability of trust questionnaire items to a robot in a fire evacuation scenario.,Unspecified,Mobile Robots,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants observed a video of a robot.,media,Participants watched a video of a robot interaction.,hypothetical,Participants saw a static image of a physical robot in a video.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires,Reliance Intention Scale; Trust Perception Scale - HRI; Multi-Dimensional Measure of Trust (MDMT),,Trust was assessed using multiple questionnaires.,no modeling,No computational modeling of trust was performed.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,The study assessed trust without intentionally altering any specific factors.,,Participants frequently rated items from the MDMT-Sincere & Ethical subscale as 'Non-applicable to robots in general'.,"People rate some trust questionnaire items as non-applicable to robots, especially those related to social dimensions of trust.",Participants watched a video of a robot guiding a person during a fire evacuation and then rated the applicability of trust questionnaire items.,ANOVA,"An ANOVA was used to compare the proportion of 'Non-applicable to robots in general' ratings across the different trust measures (RIS, TPS-HRI, MDMT-Capable & Reliable, and MDMT-Sincere & Ethical). Another ANOVA was used to compare the proportion of 'Non-applicable to this robot' ratings across the same measures. The purpose was to determine if there were significant differences in how participants perceived the applicability of items in different questionnaires.",FALSE,,,,"Study 1 did not manipulate any factors. The study was designed to assess the applicability of trust questionnaire items to a robot in a fire evacuation scenario. There was no intentional manipulation of any variables to observe their impact on trust. Participants were simply asked to rate the applicability of items, and the study examined the frequency of 'N/A' ratings across different questionnaires. Therefore, no factors were manipulated, and no factors impacted or did not impact trust.",10.1145/3434073.3444677,https://dl.acm.org/doi/10.1145/3434073.3444677,"Trust in human-robot interactions (HRI) is measured in two main ways: through subjective questionnaires and through behavioral tasks. To optimize measurements of trust through questionnaires, the field of HRI faces two challenges: the development of standardized measures that apply to a variety of robots with different capabilities, and the exploration of social and relational dimensions of trust in robots (e.g., benevolence). In this paper we look at how different trust questionnaires [18, 30, 35] fare given these challenges that pull in different directions (being general vs. being exploratory) by studying whether people think the items in these questionnaires are applicable to different kinds of robots and interactions. In Study 1 we show that after being presented with a robot (non-humanoid) and an interaction scenario (fire evacuation), participants rated multiple questionnaire items such as “This robot is principled” as “Non-applicable to robots in general” or “Non-applicable to this robot.” In Study 2 we show that the frequency of these ratings change (indeed, even for items rated as N/A to robots in general) when a new scenario is presented (game playing with a humanoid robot). Finally, while overall trust scores remained robust to N/A ratings, our results revealed potential fallacies in the way these scores are commonly interpreted. We conclude with recommendations for the development, use and results-reporting of trust questionnaires for future studies, as well as theoretical implications for the field of HRI."
"Chita-Tegmark, Meia; Law, Theresa; Rabb, Nicholas; Scheutz, Matthias",Can You Trust Your Trust Measure?,2021,2,126,126,0,No participants were excluded,Online Crowdsourcing,mixed design,Participants were divided into conditions based on the vignette they saw (evacuation or game-playing) and whether they had N/A options or were forced to rate each item. All participants answered all four questionnaires.,Participants rated trust questionnaire items after watching a video vignette of a robot in either a fire evacuation or game-playing scenario.,Unspecified,Mobile Robots; Humanoid Robots,Research; Social,Evaluation,Survey/Questionnaire Completion,passive observation,Participants observed a video of a robot.,media,Participants watched a video of a robot interaction.,hypothetical,Participants saw a static image of a physical robot in a video.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires,Reliance Intention Scale; Trust Perception Scale - HRI; Multi-Dimensional Measure of Trust (MDMT),,Trust was assessed using multiple questionnaires.,no modeling,No computational modeling of trust was performed.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the interaction scenario (fire evacuation vs. game-playing) and the robot's behavior (e.g., the robot's actions in the game-playing scenario) to see how it affected the applicability of trust measures.","The study found that the applicability of trust measures changed depending on the interaction scenario, with social scenarios leading to fewer 'N/A' ratings.","The proportion of 'N/A to robots in general' ratings decreased for the MDMT-Sincere & Ethical subscale when participants viewed the game-playing scenario, suggesting that social context influences perceptions of robot applicability.","People's mental models of robots are not stable across all scenarios, and they shift when presented with robot interactions that could be construed as social.",Participants watched a video of a robot in either a fire evacuation or game-playing scenario and then rated the applicability of trust questionnaire items.,ANOVA; t-test; Linear regression; Logistic regression,"A mixed ANOVA was used to analyze the proportion of 'N/A to robots in general' ratings, with vignette (evacuation vs. game-playing) as a between-subjects factor and measure (RIS, TPS-HRI, MDMT-Capable & Reliable, MDMT-Sincere & Ethical) as a within-subjects factor. A similar ANOVA was used for 'N/A to this robot' ratings. Paired t-tests were used to compare overall trust ratings between forced-choice and N/A conditions for each measure and scenario. Nested regression models were used to examine the relationship between individual item ratings, applicability scores, and forced-choice conditions. Nested logistic regressions were used to analyze the likelihood of choosing 'Neither disagree nor agree' or 0% ratings based on item applicability and choice conditions.",TRUE,Teaming; Robot-verbal-communication-content,Teaming; Robot-verbal-communication-content,,"In Study 2, the researchers manipulated the interaction scenario by having participants watch either a fire evacuation scenario or a game-playing scenario. This manipulation is categorized as 'Teaming' because the game-playing scenario involved a competitive interaction between the participant and the robot, while the evacuation scenario did not involve any direct interaction. Additionally, the game-playing scenario included the robot using verbal communication, which is categorized as 'Robot-verbal-communication-content'. The robot's dialogue included a promise not to immobilize the participant, followed by a violation of that promise and an apology. The study found that the interaction scenario (and thus the teaming and communication) impacted the proportion of 'N/A to robots in general' ratings, particularly for the MDMT-Sincere & Ethical subscale. The game-playing scenario, which included social interaction and communication, led to fewer 'N/A' ratings, indicating that these factors impacted how participants perceived the applicability of trust measures. No factors were found to not impact trust.",10.1145/3434073.3444677,https://dl.acm.org/doi/10.1145/3434073.3444677,"Trust in human-robot interactions (HRI) is measured in two main ways: through subjective questionnaires and through behavioral tasks. To optimize measurements of trust through questionnaires, the field of HRI faces two challenges: the development of standardized measures that apply to a variety of robots with different capabilities, and the exploration of social and relational dimensions of trust in robots (e.g., benevolence). In this paper we look at how different trust questionnaires [18, 30, 35] fare given these challenges that pull in different directions (being general vs. being exploratory) by studying whether people think the items in these questionnaires are applicable to different kinds of robots and interactions. In Study 1 we show that after being presented with a robot (non-humanoid) and an interaction scenario (fire evacuation), participants rated multiple questionnaire items such as “This robot is principled” as “Non-applicable to robots in general” or “Non-applicable to this robot.” In Study 2 we show that the frequency of these ratings change (indeed, even for items rated as N/A to robots in general) when a new scenario is presented (game playing with a humanoid robot). Finally, while overall trust scores remained robust to N/A ratings, our results revealed potential fallacies in the way these scores are commonly interpreted. We conclude with recommendations for the development, use and results-reporting of trust questionnaires for future studies, as well as theoretical implications for the field of HRI."
"Choi, Jong Kyu; Ji, Yong Gu",Investigating the Importance of Trust on Adopting an Autonomous Vehicle,2015,1,635,552,83,83 respondents were excluded due to missing data in any of the survey's items,Online Crowdsourcing,,"Participants completed an online questionnaire with 30 items on 10 constructs, including demographic information.",Participants responded to a survey about their attitudes towards autonomous vehicles.,Unspecified,Autonomous Vehicles,,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the autonomous vehicle.,media,The interaction was based on written descriptions of autonomous vehicles.,hypothetical,The robot was described hypothetically without any visual representation.,not autonomous,The autonomous vehicle was described hypothetically without any real autonomy.,Questionnaires,,,Trust was measured using a questionnaire.,"parametric models (e.g., regression)",The study used PLS analysis to model the relationships between constructs.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,"The study did not manipulate any factors, but measured existing attitudes and beliefs.",,"The study found that perceived usefulness and trust are major determinants of intention to use autonomous vehicles. Perceived risk was not a significant factor to predict behavioral intention, which is contrary to other studies' findings.","Trust is a major construct for predicting the adoption of autonomous vehicles, and it is influenced by system transparency, technical competence, and situation management.","Participants completed a survey about their attitudes towards autonomous vehicles, without any direct interaction with a physical or simulated robot.",pls analysis; Bootstrapping; t-test,The study used Partial Least Squares (PLS) analysis to test the proposed research model and the relationships between the constructs. A bootstrapping resampling method with 500 subsamples was used to estimate path significance. A t-test was conducted to test the significance of path coefficients based on a significance level of .05.,FALSE,,,,"The study did not manipulate any factors. Participants completed a survey about their attitudes towards autonomous vehicles. The study measured existing attitudes and beliefs, but did not introduce any experimental manipulations. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust due to manipulation.",10.1080/10447318.2015.1070549,http://www.tandfonline.com/doi/full/10.1080/10447318.2015.1070549,"The objective of this study is to examine the user’s adoption aspects of autonomous vehicle, as well as to investigate what factors drive people to trust an autonomous vehicle. A model explaining the impact of different factors on autonomous vehicles’ intention is developed based on the technology acceptance model and trust theory. A survey of 552 drivers was conducted and the results were analyzed using partial least squares. The results demonstrated that perceived usefulness and trust are major important determinants of intention to use autonomous vehicles. The results also show that three constructs—system transparency, technical competence, and situation management—have a positive effect on trust. The study identified that trust has a negative effect on perceived risk. Among the driving-related personality traits, locus of control has significant effects on behavioral intention, whereas sensation seeking did not. This study investigated that the developed model explains the factors that influence the acceptance of autonomous vehicle. The results of this study provide evidence on the importance of trust in the user’s acceptance of an autonomous vehicle."
"Choo, Sanghyun; Sanders, Nathan; Kim, Nayoung; Kim, Wonjoon; Nam, Chang S.; Fitts, Edward P.",Detecting Human Trust Calibration in Automation: A Deep Learning Approach,2019,1,8,8,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were trained on the AF-MATB, then completed three blocks of four observation conditions and two active conditions. After each observation condition, participants reported their perceived trust level.",Participants observed an automated system performing a monitoring task and reported their trust level after each observation period.,Unspecified,Other,Research,Supervision,Monitoring,passive observation,Participants observed the automated system without direct interaction.,simulation,Participants interacted with a simulated automated system.,simulated,The robot was represented as a simulated system on a screen.,pre-programmed (non-adaptive),The automated system followed a pre-programmed sequence of actions.,Questionnaires,,Physiological Signals,Trust was measured using a questionnaire and EEG signals were collected for modeling.,"deep learning (e.g., neural networks, reinforcement learning)",A CNN-based deep learning framework was used to model trust calibration.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the automated system was manipulated (low or high), and the credibility of the automation was also manipulated (novice or expert), which influenced participants' expectations.",The study aimed to detect changes in trust calibration based on the reliability and credibility of the automation.,,"The study proposed a CNN-based framework to detect human trust calibration in automation using EEG data, and showed that it had a higher accuracy than other methods.","The automated system performed a system monitoring task, and the human participant observed the system's performance and reported their trust level.",,"No statistical tests were explicitly mentioned in the paper. The study focused on developing and validating a CNN-based deep learning framework for detecting trust calibration using EEG data. The performance of the CNN was compared against MLP and DNN models, but no specific statistical tests were reported for this comparison.",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-accuracy; Robot-verbal-communication-content,,"The study manipulated the reliability of the automated system (low 60% or high 80%), which directly impacts the accuracy of the system, thus 'Robot-accuracy'. The study also manipulated the credibility of the automation by labeling it as either 'novice' or 'expert', which influences the participants' expectations and is a form of communication about the system's capabilities, thus 'Robot-verbal-communication-content'. The paper states that the study aimed to detect changes in trust calibration based on the reliability and credibility of the automation, indicating that both factors were expected to impact trust. The results section does not explicitly state that either of these factors did not impact trust, so they are both listed under 'factors_that_impacted_trust'.",10.1177/1071181319631298,http://journals.sagepub.com/doi/10.1177/1071181319631298,
"Choo, Sanghyun; Nam, Chang",Detecting Human Trust Calibration in Automation: A Convolutional Neural Network Approach,2022,1,13,13,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed two training sessions, followed by three experimental blocks. Each block included two active trials and four observational trials. Participants answered a trust questionnaire before the experiment and after each trial session.",Participants monitored four dials on a screen and responded to automation failures by pressing a button.,Unspecified,Other,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated system by monitoring dials and pressing buttons.,simulation,The interaction was through a computer-based simulation of a system monitoring task.,simulated,The robot was represented as a simulated system on a computer screen.,pre-programmed (non-adaptive),The automation system had pre-programmed behaviors for detecting and correcting malfunctions.,Questionnaires; Physiological Measures,,Physiological Signals,Trust was measured using a questionnaire and EEG signals.,"deep learning (e.g., neural networks, reinforcement learning)",A convolutional neural network was used to model trust based on EEG data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the automation system and the credibility of the algorithm were manipulated to influence trust. Reliability was changed by varying the rate of automation successes and failures, and credibility was changed by providing cover stories about the algorithm's developers.","Trust increased with high reliability and decreased with low reliability. Initial trust was higher with high credibility, but this effect diminished over time. Trust calibration occurred when actual automation performance changed.","The initial trust level was higher in the high credibility condition, but trust was primarily influenced by actual automation performance (reliability) over time. Trust calibration occurred when the actual automation performance was decreased or increased regardless of credibility levels.",A CNN-based framework using EEG data can effectively estimate trust levels and detect trust calibration in automation.,The human participant monitored dials on a screen and pressed buttons when automation failures were observed. The automation system was designed to detect and correct malfunctions.,ANOVA; tukey test,"A 2 (reliability: High, Low) x 2 (credibility: High, Low) x 4 (blocks: 0, 1, 2, 3) repeated measures ANOVA was conducted to analyze the subjective trust levels based on reliability, credibility, and time blocks. Post-hoc Tukey tests were used to further examine significant main effects and interactions identified by the ANOVA, specifically comparing trust ratings across different conditions and time blocks.",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-accuracy,Robot-verbal-communication-content,"The study manipulated the reliability of the automation system by varying the rate of automation successes and failures, which directly impacts the accuracy of the robot's performance, thus 'Robot-accuracy'. The study also manipulated the credibility of the algorithm by providing cover stories about the algorithm's developers, which is a form of verbal communication about the system, thus 'Robot-verbal-communication-content'. The results showed that trust was primarily influenced by the actual automation performance (reliability), indicating that 'Robot-accuracy' impacted trust. While initial trust was higher with high credibility, this effect diminished over time, suggesting that 'Robot-verbal-communication-content' did not have a lasting impact on trust.",10.1109/THMS.2021.3137015,https://ieeexplore.ieee.org/document/9687123/,"There is a general lack of studies that are aimed at monitoring and detecting an operator’s trust calibration, even though detecting someone’s adjusted trust towards automation is essential to prevent misuse and disuse of automation. The goal of this article is to propose a convolutional neural network (CNN) based framework to estimate operators’ trust levels and detect their trust calibration in automation using image features of electroencephalogram (EEG) signals preserving temporal, spectral, and spatial information. Thirteen participants performed a set of automated Air Force multiattribute task battery tasks that differed in reliability (High/Low) and credibility (High/Low) levels. The proposed framework was compared with three machine learning methods—naïve bayes, support vector machine, multilayer perceptron—in terms of accuracy, sensitivity, and speciﬁcity of trust estimation and detection of trust calibration. Results of this article showed that the proposed framework had the highest performance of both trust estimation and detection of trust calibration in automation compared to the other comparison methods. This indicates that the proposed framework using the CNN classiﬁer with the image-based EEG features could be an applicable model for estimating multilevel trust and detecting trust calibration during human-automation interaction. Also, it can help to prevent disuse and misuse of automation by estimating operators’ trust levels and monitoring their trust calibration in automation."
"Christensen, Anders B. H.; Dam, Christian R.; Rasle, Corentin; Bauer, Jacob E.; Mohamed, Ramlo A.; Jensen, Lars Christian",Reducing Overtrust in Failing Robotic Systems,2019,1,31,22,9,"9 of the observations were discarded, leaving 22 useable observations",Controlled Lab Environment,between-subjects,Participants were blindfolded and guided through a maze by a robot's voice in a Wizard of Oz setup. They were given instructions corresponding to their position through a wireless headset. The experiment ended if they kept following the robot's instructions for more than 6 minutes or if they decided not to follow them.,Participants were guided through a maze by a robot's voice and had to decide whether to continue following the robot's instructions or not.,Nao,Humanoid Robots,Research,Navigation,Guiding,minimal interaction,Participants received verbal instructions from the robot while blindfolded.,real-world,Participants were physically present in a maze and believed they were interacting with a real robot.,physical,"The robot was physically present, but participants only interacted with it through its voice.",wizard of oz (directly controlled),The robot's voice was controlled by a human operator in a Wizard of Oz setup.,Behavioral Measures; Questionnaires,,Performance Metrics,Trust was assessed through a post-experiment questionnaire and by measuring whether participants continued to follow the robot's instructions.,no modeling,No computational model of trust was used in this study.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The study manipulated the robot's reliability by introducing vocal warnings about system errors or uncertainty, aiming to reduce overtrust.","The vocal warnings did not significantly reduce overtrust, as participants continued to follow the robot's instructions despite the warnings.","The vocal warnings had no significant effect on reducing overtrust, and only one participant mentioned the robot's error message as a reason for doubt. The percentage of people who stopped trusting the robots instructions during the experiment was higher in condition 2 than in condition 1 and 3, but this result was not statistically significant.",Vocal warnings of error from a robot guiding participants through a maze did not significantly reduce overtrust.,"The robot provided verbal instructions to guide blindfolded participants through a maze. The human participant's task was to follow the robot's instructions and navigate the maze, deciding whether to continue following the robot or not.",ANOVA; barletts test,A one-way ANOVA was performed to test if there were a significant difference between the amount of waypoints reached for the three conditions. Barletts test was used to test for equal variance before conducting the ANOVA.,TRUE,Robot-verbal-communication-content,,Robot-verbal-communication-content,"The study manipulated the content of the robot's verbal communication by introducing vocal warnings about system errors or uncertainty in conditions 2 and 3, while condition 1 had no warnings. This is a manipulation of the content of the robot's verbal communication, specifically warnings about the robot's reliability. The study found that these warnings did not significantly impact trust, as participants continued to follow the robot's instructions despite the warnings. Therefore, 'Robot-verbal-communication-content' is the manipulated factor, and it did not impact trust.",10.1109/HRI.2019.8673235,https://ieeexplore.ieee.org/document/8673235/,"In general, people tend to place too much trust in robotic systems, also in emergency situations. Our study attempts to discover ways of reducing this overtrust, by adding vocal warnings of error from a robot that guides participants blindfolded through a maze. The results indicate that the tested vocal warnings have no effect in reducing overtrust, but we encourage further testing of similar warnings to fully explore its potential effects."
"Christoforakos, Lara; Gallucci, Alessio; Surmava-Große, Tinatini; Ullrich, Daniel; Diefenbach, Sarah","Can Robots Earn Our Trust the Same Way Humans Do? A Systematic Exploration of Competence, Warmth, and Anthropomorphism as Determinants of Trust Development in HRI",2021,2,155,155,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of four experimental conditions and watched a video of a human-robot interaction, then completed a questionnaire.",Participants watched a video of a robot playing a shell game with a human and then answered questions about the robot.,Pepper,Humanoid Robots; Service and Assistive Robots,Research; Social,Game,Cooperative Game,passive observation,Participants watched a video of the robot interacting with a human.,media,Participants watched a video of the robot interacting with a human.,physical,The robot was a physical robot shown in a video.,wizard of oz (directly controlled),The robot's actions were remote-controlled by a human operator.,Questionnaires; Custom Scales,Human-Computer Trust Scale/Questionnaire (HCT/HCTM),,Trust was measured using questionnaires and custom scales.,"parametric models (e.g., regression)","The study used regression analysis to model the relationship between competence, anthropomorphism, and trust.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"Robot competence was manipulated by varying the robot's success in a shell game, and anthropomorphism was manipulated through verbal and non-verbal cues.",Increased robot competence led to higher anticipated trust and attributed trustworthiness. Anthropomorphism did not moderate the effect of competence on trust.,"Perceived anthropomorphism moderated the effect of perceived competence on attributed trustworthiness, but manipulated anthropomorphism did not moderate the effect of manipulated competence on trust.","Robot competence positively influences trust development in HRI, and this relationship is mediated by perceived competence.","The robot played a shell game with a human, guessing which shell hid an object. The human mixed the shells, and the robot made a guess. Participants watched a video of this interaction.",ANOVA; Mediation analysis; Moderation analysis,The study used one-way ANOVAs to check for differences in age and gender across experimental conditions and to verify the effectiveness of the competence and anthropomorphism manipulations. Two-way ANOVAs were used to test the effects of manipulated competence and anthropomorphism on anticipated trust and attributed trustworthiness. Mediated regression analysis was used to examine whether perceived competence mediated the relationship between manipulated competence and the two trust measures. Moderation analyses were conducted to explore the interaction effects of perceived competence and perceived anthropomorphism on trust.,TRUE,Robot-accuracy; Robot-nonverbal-communication; Robot-verbal-communication-style,Robot-accuracy,Robot-nonverbal-communication; Robot-verbal-communication-style,"The study manipulated robot competence by varying the robot's success in a shell game, which directly impacts the robot's accuracy in the task, thus 'Robot-accuracy'. The study also manipulated anthropomorphism through verbal and non-verbal cues. The verbal cues involved the robot speaking or not, which is a change in the style of communication, thus 'Robot-verbal-communication-style'. The non-verbal cues involved the robot making gestures or not, which is a change in physical movements, thus 'Robot-nonverbal-communication'. The results showed that manipulated robot competence (accuracy) had a significant positive effect on trust, while manipulated anthropomorphism (nonverbal and verbal communication style) did not moderate the effect of competence on trust. Therefore, 'Robot-accuracy' impacted trust, while 'Robot-nonverbal-communication' and 'Robot-verbal-communication-style' did not.",10.3389/frobt.2021.640444,https://www.frontiersin.org/articles/10.3389/frobt.2021.640444/full,"Robots increasingly act as our social counterparts in domains such as healthcare and retail. For these human-robot interactions (HRI) to be effective, a question arises on whether we trust robots the same way we trust humans. We investigated whether the determinants competence and warmth, known to influence interpersonal trust development, influence trust development in HRI, and what role anthropomorphism plays in this interrelation. In two online studies with 2 × 2 between-subjects design, we investigated the role of robot competence (Study 1) and robot warmth (Study 2) in trust development in HRI. Each study explored the role of robot anthropomorphism in the respective interrelation. Videos showing an HRI were used for manipulations of robot competence (through varying gameplay competence) and robot anthropomorphism (through verbal and non-verbal design cues and the robot's presentation within the study introduction) in Study 1 (               n               = 155) as well as robot warmth (through varying compatibility of intentions with the human player) and robot anthropomorphism (same as Study 1) in Study 2 (               n               = 157). Results show a positive effect of robot competence (Study 1) and robot warmth (Study 2) on trust development in robots regarding anticipated trust and attributed trustworthiness. Subjective perceptions of competence (Study 1) and warmth (Study 2) mediated the interrelations in question. Considering applied manipulations, robot anthropomorphism neither moderated interrelations of robot competence and trust (Study 1) nor robot warmth and trust (Study 2). Considering subjective perceptions, perceived anthropomorphism moderated the effect of perceived competence (Study 1) and perceived warmth (Study 2) on trust on an attributional level. Overall results support the importance of robot competence and warmth for trust development in HRI and imply transferability regarding determinants of trust development in interpersonal interaction to HRI. Results indicate a possible role of perceived anthropomorphism in these interrelations and support a combined consideration of these variables in future studies. Insights deepen the understanding of key variables and their interaction in trust dynamics in HRI and suggest possibly relevant design factors to enable appropriate trust levels and a resulting desirable HRI. Methodological and conceptual limitations underline benefits of a rather robot-specific approach for future research."
"Christoforakos, Lara; Gallucci, Alessio; Surmava-Große, Tinatini; Ullrich, Daniel; Diefenbach, Sarah","Can Robots Earn Our Trust the Same Way Humans Do? A Systematic Exploration of Competence, Warmth, and Anthropomorphism as Determinants of Trust Development in HRI",2021,2,157,157,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of four experimental conditions and watched a video of a human-robot interaction, then completed a questionnaire.",Participants watched a video of a robot consulting a human player in a shell game and then answered questions about the robot.,Pepper,Humanoid Robots; Service and Assistive Robots,Research; Social,Game,Cooperative Game,passive observation,Participants watched a video of the robot interacting with humans.,media,Participants watched a video of the robot interacting with humans.,physical,The robot was a physical robot shown in a video.,wizard of oz (directly controlled),The robot's actions were remote-controlled by a human operator.,Questionnaires; Custom Scales,Human-Computer Trust Scale/Questionnaire (HCT/HCTM),,Trust was measured using questionnaires and custom scales.,"parametric models (e.g., regression)","The study used regression analysis to model the relationship between warmth, anthropomorphism, and trust.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"Robot warmth was manipulated by varying the robot's intentions in a shell game, and anthropomorphism was manipulated through verbal and non-verbal cues.",Increased robot warmth led to higher anticipated trust and attributed trustworthiness. Anthropomorphism did not moderate the effect of warmth on trust.,"Perceived anthropomorphism moderated the effect of perceived warmth on attributed trustworthiness, but manipulated anthropomorphism did not moderate the effect of manipulated warmth on trust.","Robot warmth positively influences trust development in HRI, and this relationship is mediated by perceived warmth.","The robot consulted a human player in a shell game, either helping or misleading the player. Participants watched a video of this interaction.",ANOVA; Mediation analysis; Moderation analysis,The study used one-way ANOVAs to check for differences in age and gender across experimental conditions and to verify the effectiveness of the warmth and anthropomorphism manipulations. Two-way ANOVAs were used to test the effects of manipulated warmth and anthropomorphism on anticipated trust and attributed trustworthiness. Mediated regression analysis was used to examine whether perceived warmth mediated the relationship between manipulated warmth and the two trust measures. Moderation analyses were conducted to explore the interaction effects of perceived warmth and perceived anthropomorphism on trust.,TRUE,Robot-social-attitude; Robot-nonverbal-communication; Robot-verbal-communication-style,Robot-social-attitude,Robot-nonverbal-communication; Robot-verbal-communication-style,"The study manipulated robot warmth by varying the robot's intentions in a shell game, which is a change in the robot's social approach, thus 'Robot-social-attitude'. The study also manipulated anthropomorphism through verbal and non-verbal cues. The verbal cues involved the robot speaking or not, which is a change in the style of communication, thus 'Robot-verbal-communication-style'. The non-verbal cues involved the robot making gestures or not, which is a change in physical movements, thus 'Robot-nonverbal-communication'. The results showed that manipulated robot warmth (social attitude) had a significant positive effect on trust, while manipulated anthropomorphism (nonverbal and verbal communication style) did not moderate the effect of warmth on trust. Therefore, 'Robot-social-attitude' impacted trust, while 'Robot-nonverbal-communication' and 'Robot-verbal-communication-style' did not.",10.3389/frobt.2021.640444,https://www.frontiersin.org/articles/10.3389/frobt.2021.640444/full,"Robots increasingly act as our social counterparts in domains such as healthcare and retail. For these human-robot interactions (HRI) to be effective, a question arises on whether we trust robots the same way we trust humans. We investigated whether the determinants competence and warmth, known to influence interpersonal trust development, influence trust development in HRI, and what role anthropomorphism plays in this interrelation. In two online studies with 2 × 2 between-subjects design, we investigated the role of robot competence (Study 1) and robot warmth (Study 2) in trust development in HRI. Each study explored the role of robot anthropomorphism in the respective interrelation. Videos showing an HRI were used for manipulations of robot competence (through varying gameplay competence) and robot anthropomorphism (through verbal and non-verbal design cues and the robot's presentation within the study introduction) in Study 1 (               n               = 155) as well as robot warmth (through varying compatibility of intentions with the human player) and robot anthropomorphism (same as Study 1) in Study 2 (               n               = 157). Results show a positive effect of robot competence (Study 1) and robot warmth (Study 2) on trust development in robots regarding anticipated trust and attributed trustworthiness. Subjective perceptions of competence (Study 1) and warmth (Study 2) mediated the interrelations in question. Considering applied manipulations, robot anthropomorphism neither moderated interrelations of robot competence and trust (Study 1) nor robot warmth and trust (Study 2). Considering subjective perceptions, perceived anthropomorphism moderated the effect of perceived competence (Study 1) and perceived warmth (Study 2) on trust on an attributional level. Overall results support the importance of robot competence and warmth for trust development in HRI and imply transferability regarding determinants of trust development in interpersonal interaction to HRI. Results indicate a possible role of perceived anthropomorphism in these interrelations and support a combined consideration of these variables in future studies. Insights deepen the understanding of key variables and their interaction in trust dynamics in HRI and suggest possibly relevant design factors to enable appropriate trust levels and a resulting desirable HRI. Methodological and conceptual limitations underline benefits of a rather robot-specific approach for future research."
"Chung, Hyesun; Yang, X. Jessie",Trust Dynamics in Human-Autonomy Interaction: Uncover Associations between Trust Dynamics and Personal Characteristics,2024,1,130,130,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants completed a pre-experiment survey, then performed a simulated surveillance task with an automated threat detector, and finally completed a post-experiment survey.","Participants performed a simulated surveillance task, including a compensatory tracking task and a threat detection task, aided by an automated threat detector.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated automated threat detector through a computer interface.,simulation,The interaction was conducted in a simulated environment using a computer interface.,simulated,The robot was represented as a simulated automated threat detector.,pre-programmed (non-adaptive),The automated threat detector operated with a pre-set reliability level without adapting to the user.,Behavioral Measures; Custom Scales; Questionnaires; Real-time Trust Measures,,Performance Metrics,"Trust was measured using a visual analog scale after each trial, along with behavioral measures and post-experiment questionnaires.","parametric models (e.g., regression)",A Beta random variable model was used to predict temporal trust based on self-reported trust ratings.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the automated threat detector was manipulated across five different levels, influencing the robot's performance and perceived autonomy.","The study found that different trust dynamics emerged based on the reliability of the automated threat detector, with some participants showing consistently low trust (disbelievers) and others showing fluctuating trust (oscillators).","The study identified three distinct trust dynamics clusters: Bayesian decision makers, disbelievers, and oscillators, with the disbelievers showing high neuroticism and low performance expectancy, and the oscillators showing higher scores in masculinity, positive affect, extraversion and intellect. The study also found that the disbelievers were less likely to blindly follow the recommendations of the automated threat detector.","Personal characteristics significantly influence trust dynamics, with specific traits associated with distinct trust patterns such as disbelievers and oscillators.","The automated threat detector provided alerts for potential threats, and participants had to monitor the drone feeds and report threats while also performing a tracking task. The human's primary task was to monitor the system and make decisions based on the information provided by the automated threat detector.",ANOVA; K-means; decision tree model,"The study used k-means clustering to group participants based on their trust dynamics, using average logarithm trust and RMSE as input features. ANOVAs were then conducted to examine differences between the identified clusters in terms of personal characteristics, behaviors, performance, and post-experiment ratings. Post-hoc tests with Bonferroni corrections were used for significant ANOVA results. Finally, a decision tree model was developed to predict cluster membership based on personal characteristics.",TRUE,Robot-accuracy,Robot-accuracy,,"The study explicitly manipulated the reliability of the automated threat detector across five different levels (62%, 64%, 66%, 68%, and 70%). This manipulation directly affects the accuracy of the robot's performance in detecting threats, which is a key aspect of 'Robot-accuracy'. The paper states, 'The experimental data was collected under five different automation reliability levels: 62%, 64%, 66%, 68%, and 70%.' and 'The reliability of the automated threat detector was manipulated across five different levels, influencing the robot's performance and perceived autonomy.' The study found that different trust dynamics emerged based on the reliability of the automated threat detector, with some participants showing consistently low trust (disbelievers) and others showing fluctuating trust (oscillators). This indicates that the manipulation of 'Robot-accuracy' had a direct impact on trust levels. There were no other factors explicitly manipulated in the study that would fit into the other categories.",,http://arxiv.org/abs/2409.07406,"While personal characteristics influence people’s snapshot trust towards autonomous systems, their relationships with trust dynamics remain poorly understood. We conducted a human-subject experiment with 130 participants performing a simulated surveillance task aided by an automated threat detector. A comprehensive pre-experimental survey collected data on participants’ personal characteristics across 12 constructs and 28 dimensions. Based on data collected in the experiment, we clustered participants’ trust dynamics into three types and assessed differences among the three clusters in terms of personal characteristics, behaviors, performance, and postexperiment ratings. Participants were clustered into three groups, namely Bayesian decision makers, disbelievers, and oscillators. Results showed that the clusters differ significantly in seven personal characteristics: masculinity, positive affect, extraversion, neuroticism, intellect, performance expectancy, and high expectations. The disbelievers tend to have high neuroticism, and low performance expectancy. The oscillators tend to have higher scores in masculinity, positive affect, extraversion and intellect. We also found significant differences in the behaviors and postexperiment ratings among the three groups. The disbelievers are the least likely to blindly follow the recommendations made by the automated threat detector. Based on the significant personal characteristics, we developed a decision tree model to predict cluster types with an accuracy of 70%."
"Ciocirlan, Stefan-Dan; Agrigoroaie, Roxana; Tapus, Adriana",Human-Robot Team: Effects of Communication in Analyzing Trust,2019,1,71,71,0,No participants were excluded,Online Crowdsourcing,between-subjects,Participants were randomly assigned to one of three communication conditions and completed three tasks with a simulated robot. They could intervene in the robot's actions and completed questionnaires before and after the interaction.,"Participants interacted with a simulated robot to complete three tasks: identifying a plant in a room, identifying the type of room, and identifying objects in two rooms. Participants could intervene in the robot's actions.",TIAGo,Mobile Manipulators,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the robot through a simulated environment and could intervene in its actions.,media,Participants viewed video recordings of the robot performing tasks.,simulated,The robot was represented through video recordings.,shared control (fixed rules),The robot performed tasks autonomously but participants could intervene based on fixed rules.,Questionnaires,,Performance Metrics,Trust was measured using a questionnaire and performance metrics were collected for trust modeling.,"parametric models (e.g., regression)",The study used classifiers from scikit-learn to model trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the communication method (text and verbal interaction related to the task, text and verbal interaction not related to the task, or no communication) and the robot's performance by using different object recognition APIs, which influenced trust.","Text and verbal communication decreased trust loss when the robot performed poorly, and text and verbal communication related to the task increased trust at the end of the experiment. Communication did not increase trust gain when the robot performed well.","The study found that participants who were more willing to befriend the robot showed a greater trust decrease when the robot failed. Also, participants who initially had lower trust in robots showed a higher increase in trust when the robot performed well. There was a conflict in the results, as the study found that communication did not increase trust gain when the robot performed well, but also found that text and verbal communication related to the task increased trust at the end of the experiment.","Text and verbal communication, especially when related to the task, reduces trust loss when the robot performs poorly and increases trust at the end of the experiment.","The robot performed object recognition tasks in a simulated environment, and the human participant could monitor the robot's progress, intervene in its actions, and communicate with it through text and verbal interaction.",Shapiro-Wilk; ANOVA; Kruskal-Wallis; Mann-Whitney U,"The study used the Shapiro-Wilk test to check for normality of data. Depending on the normality of the data, either ANOVA or Kruskal-Wallis tests were used to compare means or medians across different conditions. Specifically, Kruskal-Wallis tests were used to compare trust changes across different communication conditions (NCC, FCC, ICC) when the robot performed well, when it performed poorly, and at the end of the experiment. Mann-Whitney rank tests were used for pairwise comparisons between conditions when significant differences were found with the Kruskal-Wallis test. These tests were used to validate or reject the study's hypotheses about the effect of communication on trust.",TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-verbal-communication-content; Robot-accuracy,,"The study manipulated the communication method between the robot and the participant, which falls under 'Robot-verbal-communication-content'. The communication conditions were: no communication (NCC), text and verbal communication related to the task (FCC), and text and verbal communication not related to the task (ICC). The study also manipulated the robot's performance by using different object recognition APIs, which influenced the success/failure rate of the robot on the task, thus falling under 'Robot-accuracy'. The results showed that both communication content and robot accuracy impacted trust. Specifically, text and verbal communication decreased trust loss when the robot performed poorly, and text and verbal communication related to the task increased trust at the end of the experiment. The study found that communication did not increase trust gain when the robot performed well, but this does not mean that communication did not impact trust, only that it did not increase trust gain in this specific scenario. Therefore, both 'Robot-verbal-communication-content' and 'Robot-accuracy' are listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1109/RO-MAN46459.2019.8956345,https://ieeexplore.ieee.org/document/8956345/,"Trust is related to the performance of human teams, making it a signiﬁcant characteristic, which needs to be analyzed inside human-robot teams. Trust was researched for a long time in other domains like social sciences, psychology, and economics. Building trust within a team is formed through common tasks and it depends on team performance and communication. By applying an online game based tasks for human-robot teams, the effects of three communication conditions ( communication without text and verbal interaction, communication with text and verbal interaction related/not related to the task) on trust are analyzed. Additionally, we found that the participants’ background is linked to the trust in the interaction with the robot. The results show that in a human-robot team the human trust will increase more over time when he/she is working with a robot that uses text and verbal interaction communication related to the task. They further suggest that human trust will decrease to a lower extent when the robot fails in doing the tasks if it uses text and verbal communication with the human."
"Cogurcu, Yunus Emre; Douthwaite, James A.; Maddock, Steve",A Comparative Study of Safety Zone Visualisations for Virtual and Physical Robot Arms Using Augmented Reality,2023,2,14,14,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a pick-and-place task with a real robot arm, using four different safety zone configurations, and then completed a questionnaire.","Participants performed a pick-and-place task, moving a block from one table to another, while interacting with a robot arm.",UR10,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Passing,direct-contact interaction,Participants physically interacted with the robot arm during the pick-and-place task.,real-world,Participants used AR to visualize safety zones around the real robot arm.,physical,"Participants interacted with a real, physical robot arm.",pre-programmed (non-adaptive),The robot arm followed a pre-programmed sequence of actions.,Questionnaires,,,Trust was assessed using a questionnaire.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Mixed Reality Studies,Direct Manipulation,"The study manipulated the appearance of the safety zone (shape, size, presence of cage bars) and the interaction medium (AR visualization of a real robot arm) to influence trust.","The use of virtual cage bars increased the perception of safety and trust, with large cylinders with cage bars being the most preferred configuration.","Participants preferred safety zones with virtual cage bars, which increased the perception of depth and made the safety volume clearer. Some users felt unsafe with large cuboids.",The addition of virtual cage bars to safety zone visualizations increased user perception of safety and trust when working with a real robot arm.,"The robot arm moved a block from one table to another, and the human moved the block to its final position. The human's primary task was to interact with the robot arm while avoiding the safety zone.",Mann-Whitney U,"The Mann-Whitney U test was used to compare the results of the real and virtual robot arm experiments for questions 1-4 and 6-8. This non-parametric test was chosen because the data did not meet the normality assumption. The test compared the medians of the two sets of results. The Likert scale scores were converted into numerical scores for each question for each experiment. For question 5, a comparison of means within participants was used, with separate results for each experiment.",TRUE,Robot-interface-design,Robot-interface-design,,"The study manipulated the visual appearance of the safety zone around the robot arm, specifically the shape (cuboid vs. cylinders), size (smaller vs. larger), and the presence of virtual cage bars. These manipulations directly relate to the visual interface presented to the user, thus falling under 'Robot-interface-design'. The results showed that the presence of virtual cage bars significantly impacted the user's perception of safety and trust, indicating that this aspect of the interface design influenced trust. The study did not find any manipulated factors that did not impact trust.",10.3390/computers12040075,https://www.mdpi.com/2073-431X/12/4/75,"The use of robot arms in various industrial settings has changed the way tasks are completed. However, safety concerns for both humans and robots in these collaborative environments remain a critical challenge. Traditional approaches to visualising safety zones, including physical barriers and warning signs, may not always be effective in dynamic environments or where multiple robots and humans are working simultaneously. Mixed reality technologies offer dynamic and intuitive visualisations of safety zones in real time, with the potential to overcome these limitations. In this study, we compare the effectiveness of safety zone visualisations in virtual and real robot arm environments using the Microsoft HoloLens 2. We tested our system with a collaborative pickand-place application that mimics a real manufacturing scenario in an industrial robot cell. We investigated the impact of safety zone shape, size, and appearance in this application. Visualisations that used virtual cage bars were found to be the most preferred safety zone conﬁguration for a real robot arm. However, the results for this aspect were mixed for a virtual robot arm experiment. These results raise the question of whether or not safety visualisations can initially be tested in a virtual scenario and the results transferred to a real robot arm scenario, which has implications for the testing of trust and safety in human–robot collaboration environments."
"Cogurcu, Yunus Emre; Douthwaite, James A.; Maddock, Steve",A Comparative Study of Safety Zone Visualisations for Virtual and Physical Robot Arms Using Augmented Reality,2023,2,14,14,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a pick-and-place task with a virtual robot arm, using four different safety zone configurations, and then completed a questionnaire.","Participants performed a pick-and-place task, moving a virtual block from one table to another, while interacting with a virtual robot arm.",UR10,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Passing,minimal interaction,Participants interacted with a virtual robot arm in a simulated environment.,simulation,Participants used AR to visualize safety zones around the virtual robot arm.,simulated,Participants interacted with a virtual representation of a robot arm.,pre-programmed (non-adaptive),The virtual robot arm followed a pre-programmed sequence of actions.,Questionnaires,,,Trust was assessed using a questionnaire.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Mixed Reality Studies,Direct Manipulation,"The study manipulated the appearance of the safety zone (shape, size, presence of cage bars) and the interaction medium (AR visualization of a virtual robot arm) to influence trust.","The use of virtual cage bars was preferred, but the preferred shape was less clear than in the real robot arm experiment. Large cuboids with cage bars and large cylinders with cage bars were the most preferred options.","Participants preferred safety zones with virtual cage bars, but the preferred shape was less clear than in the real robot arm experiment. Some participants found multiple cylinders to be a cluttered visual display.","The addition of virtual cage bars to safety zone visualizations was preferred, but the preferred shape was less clear than in the real robot arm experiment, suggesting that the virtual environment may impact user perception.","The virtual robot arm moved a virtual block from one table to another, and the human moved the virtual block to its final position. The human's primary task was to interact with the virtual robot arm while avoiding the safety zone.",Mann-Whitney U,"The Mann-Whitney U test was used to compare the results of the real and virtual robot arm experiments for questions 1-4 and 6-8. This non-parametric test was chosen because the data did not meet the normality assumption. The test compared the medians of the two sets of results. The Likert scale scores were converted into numerical scores for each question for each experiment. For question 5, a comparison of means within participants was used, with separate results for each experiment.",TRUE,Robot-interface-design,Robot-interface-design,,"Similar to the real robot arm experiment, this study manipulated the visual appearance of the safety zone around the virtual robot arm, including shape (cuboid vs. cylinders), size (smaller vs. larger), and the presence of virtual cage bars. These manipulations are related to the visual interface, thus falling under 'Robot-interface-design'. The results showed that the presence of virtual cage bars was preferred, although the preferred shape was less clear than in the real robot arm experiment, indicating that the interface design influenced trust. The study did not find any manipulated factors that did not impact trust.",10.3390/computers12040075,https://www.mdpi.com/2073-431X/12/4/75,"The use of robot arms in various industrial settings has changed the way tasks are completed. However, safety concerns for both humans and robots in these collaborative environments remain a critical challenge. Traditional approaches to visualising safety zones, including physical barriers and warning signs, may not always be effective in dynamic environments or where multiple robots and humans are working simultaneously. Mixed reality technologies offer dynamic and intuitive visualisations of safety zones in real time, with the potential to overcome these limitations. In this study, we compare the effectiveness of safety zone visualisations in virtual and real robot arm environments using the Microsoft HoloLens 2. We tested our system with a collaborative pickand-place application that mimics a real manufacturing scenario in an industrial robot cell. We investigated the impact of safety zone shape, size, and appearance in this application. Visualisations that used virtual cage bars were found to be the most preferred safety zone conﬁguration for a real robot arm. However, the results for this aspect were mixed for a virtual robot arm experiment. These results raise the question of whether or not safety visualisations can initially be tested in a virtual scenario and the results transferred to a real robot arm scenario, which has implications for the testing of trust and safety in human–robot collaboration environments."
"Cohen, Myke C.; Peel, Matthew A.; Scalia, Matthew J.; Willett, Matthew M.; Chiou, Erin K.; Gorman, Jamie C.; Cooke, Nancy J.","Anthropomorphism Moderates the Relationships of Dispositional, Perceptual, and Behavioral Trust in a Robot Teammate",2023,1,66,66,0,No participants were excluded,Online Crowdsourcing,mixed design,"Participants completed an initial questionnaire, watched a training video, completed a training mission, and then completed two experimental missions, followed by trust and anthropomorphism questionnaires.",Participants were tasked with directing a robot to search for survivors in a simulated urban search and rescue environment and then allocating medical resources based on the robot's recommendations.,Unspecified,Mobile Robots,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the robot through a simulated environment, issuing commands and receiving recommendations.",simulation,The interaction took place in a simulated environment using a game engine.,simulated,The robot was a virtual representation within the simulation.,wizard of oz (directly controlled),The robot's actions were controlled by a human experimenter using a Wizard of Oz technique.,Questionnaires; Behavioral Measures,Propensity to Trust Scales; Human-Robot Trust Scale/Questionnaire (HRTS/HRITS); Godspeed Questionnaire,Performance Metrics,"Trust was measured using questionnaires, and behavioral compliance with the robot's recommendations.","parametric models (e.g., regression)","Hierarchical regression models were used to analyze the relationships between trust, anthropomorphism, and compliance.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the style of robot communication (graphics, text, or both) and the presence of confidence indicators to influence how participants perceived and trusted the robot.","The study found that anthropomorphism moderated the relationship between perceptual and behavioral trust, and between dispositional and perceptual/behavioral trust, depending on the communication style and presence of confidence indicators. Specifically, the absence of confidence indicators led to higher reliance on perceived trustworthiness when communication was text-based.","The study found that the moderating effects of anthropomorphism on trust were only significant when the robot communicated through text, suggesting that language-based communication plays a key role in how people anthropomorphize and trust robots. Also, the study found that for participants who anthropomorphized less, more trustworthy perceptions resulted in lower compliance when recommendations were communicated textually.","Anthropomorphism moderates the relationships between dispositional, perceptual, and behavioral trust in a virtual robot teammate, particularly when the robot communicates through text and when confidence indicators are absent.","The robot navigated the simulated environment, detected survivors, and provided recommendations on medical resources. The human participant issued high-level navigation commands and decided whether to follow the robot's recommendations for resource allocation.",hierarchical regression; Simple slopes analysis,"The study used hierarchical regression to test the moderating effect of anthropomorphism on the relationships between perceptual and behavioral trust, dispositional and perceptual trust, and dispositional and behavioral trust. Simple slopes analyses were then conducted to further examine the significant moderations found in the regression analyses, specifically to understand the nature of the interaction effects at different levels of anthropomorphism.",TRUE,Robot-verbal-communication-style; Robot-interface-design,Robot-verbal-communication-style,Robot-interface-design,"The study manipulated the style of robot communication by presenting information either through graphics only, text only, or a combination of both. This is classified as 'Robot-verbal-communication-style' because it changes how the robot's recommendations and confidence were conveyed (e.g., textually vs. graphically). The presence or absence of confidence indicators (graphical bars and percentages) is classified as 'Robot-interface-design' because it is a change to the interactive elements of the interface. The study found that the communication style (text-based vs. graphical) impacted trust, as the moderating effects of anthropomorphism were only significant when the robot communicated through text. The presence or absence of confidence indicators did not directly impact trust, but rather interacted with the communication style to influence the relationship between anthropomorphism and trust. Therefore, 'Robot-verbal-communication-style' impacted trust, while 'Robot-interface-design' did not directly impact trust.",10.1177/21695067231196240,https://journals.sagepub.com/doi/10.1177/21695067231196240,"Trust plays a critical role in the success of human-robot teams (HRTs). While typically studied as a perceptual attitude, trust also encompasses individual dispositions and interactive behaviors like compliance. Anthropomorphism, the attribution of human-like qualities to robots, is a related phenomenon that designers often leverage to positively influence trust. However, the relationship of anthropomorphism to perceptual, dispositional, and behavioral trust is not fully understood. This study explores how anthropomorphism moderates these relationships in a virtual urban search and rescue HRT scenario. Our findings indicate that the moderating effects of anthropomorphism depend on how a robot’s recommendations and its confidence in them are communicated through text and graphical information. These results highlight the complexity of the relationships between anthropomorphism, trust, and the social conveyance of information in designing for safe and effective human-robot teaming."
"Cominelli, Lorenzo; Feri, Francesco; Garofalo, Roberto; Giannetti, Caterina; Meléndez-Jiménez, Miguel A.; Greco, Alberto; Nardelli, Mimma; Scilingo, Enzo Pasquale; Kirchkamp, Oliver",Promises and trust in human–robot interaction,2021,1,164,162,2,2 subjects were removed because they did not follow the experimental procedure correctly,Controlled Lab Environment,between-subjects,"Participants read instructions, wore a sensor, then interacted with a robot, computer, or human, and completed a questionnaire.","Participants played a trust game where they decided whether to trust a robot, computer, or human counterpart.",Unspecified,Humanoid Robots,Research; Social,Game,Economic Game,minimal interaction,"Participants interacted with a robot, computer, or human through verbal instructions and a game.",real-world,"Participants interacted with a physical robot, computer, or human in a real-world setting.",physical,"Participants interacted with a physical humanoid robot, a computer box, or a human.",fully autonomous (limited adaptation),"The robot made decisions based on its internal state, but its behavior was pre-programmed.",Behavioral Measures; Physiological Measures; Questionnaires,Affinity for Technological Interaction (ATI) Scale,Physiological Signals,"Trust was assessed using behavioral choices, physiological data, and questionnaires.",no modeling,The study did not use computational models of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the appearance of the counterpart (humanoid, computer, human), the message sent (promise or empty), and the perceived human-likeness of the counterpart, which influenced trust.","Trust increased when participants perceived the humanoid as more human-like, especially after receiving a promise. Trust was lower when interacting with a human compared to the robot or computer. Stronger psychophysiological reactions were associated with lower trust.","Participants trusted the humanoid robot more than the human counterpart, which is an unexpected result. The effect of a promise on trust was only significant when the counterpart was perceived as highly human-like. Participants with stronger psychophysiological reactions were less likely to trust the counterpart.","Human-likeness and promises increase trust in a humanoid robot, but only when the robot is perceived as very similar to a human. Stronger psychophysiological reactions were associated with lower trust.","The robot (or computer/human) sent a message (promise or empty) to the participant, who then decided whether to trust the counterpart in a trust game. The participant chose between 'In' or 'Out', and the counterpart chose between 'Roll' or 'Don't Roll'.",t-test; test for proportions; test with 500 data permutations; probit analysis,"The study used t-Student tests to compare means of continuous variables between groups, tests for proportions to compare frequencies of categorical variables, and tests with 500 data permutations to validate the results of the other tests. A probit analysis was also conducted to study the interaction between human-likeness and psychophysiological reaction on the probability of playing 'In'.",TRUE,Robot-aesthetics; Robot-verbal-communication-content,Robot-aesthetics; Robot-verbal-communication-content,,"The study manipulated the appearance of the counterpart (humanoid robot, computer box, or human), which falls under 'Robot-aesthetics'. The study also manipulated the message sent by the counterpart, which was either a promise to roll the dice or a generic message, which falls under 'Robot-verbal-communication-content'. The results showed that both the appearance of the counterpart and the message content impacted trust. Specifically, trust was higher when the counterpart was perceived as more human-like and when a promise was received, but only when the counterpart was perceived as highly human-like. The length of the message did not significantly affect the decisions to play 'In', so it is not included as a factor that impacted trust.",10.1038/s41598-021-88622-9,https://www.nature.com/articles/s41598-021-88622-9,"Abstract                            Understanding human trust in machine partners has become imperative due to the widespread use of intelligent machines in a variety of applications and contexts. The aim of this paper is to investigate whether human-beings trust a social robot—i.e. a               human-like               robot that embodies emotional states, empathy, and non-verbal communication—differently than other types of agents. To do so, we adapt the well-known economic trust-game proposed by Charness and Dufwenberg (2006) to assess whether receiving a promise from a robot increases human-trust in it. We find that receiving a promise from the robot increases the trust of the human in it, but only for individuals who perceive the robot very similar to a human-being. Importantly, we observe a similar pattern in choices when we replace the humanoid counterpart with a real human but not when it is replaced by a computer-box. Additionally, we investigate participants’ psychophysiological reaction in terms of cardiovascular and electrodermal activity. Our results highlight an increased psychophysiological arousal when the game is played with the social robot compared to the computer-box. Taken all together, these results strongly support the development of technologies enhancing the humanity of robots."
"Conlon, Nicholas; Szafir, Daniel; Ahmed, Nisar",Investigating the Effects of Robot Proficiency Self-Assessment on Trust and Performance,2022,1,155,155,0,No participants were excluded,Online Crowdsourcing,mixed design,"Participants were given a link to a web page with study information, a suitability survey, and a deeper overview of the study. They completed a quiz and a training round. Participants then completed groups of navigation tasks, with or without robot self-assessment reports. After each group, they completed a self-reported trust survey.","Participants were tasked with navigating a simulated robot through a grid world from a starting location to a goal location, using either manual or automatic control.",Unspecified,Unmanned Ground Vehicles,Research,Navigation,Path Following,minimal interaction,Participants interacted with a simulated robot through a user interface.,simulation,The interaction took place in a simulated grid world environment.,simulated,The robot was a simulated entity within the grid world environment.,shared control (fixed rules),"Participants could switch between manual and automatic control of the robot, with the robot following a pre-defined policy in automatic mode.",Behavioral Measures; Multidimensional Measures; Questionnaires,Multi-Dimensional Measure of Trust (MDMT),Performance Metrics; robot data,"Trust was assessed using a questionnaire, behavioral measures of control allocation, and performance metrics.",no modeling,Trust was not modeled computationally; the study used statistical analysis of the collected data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated robot performance (high or random) and the presence and accuracy of robot self-assessment reports (informed, random, or absent) to influence human trust and behavior.","Informed robot self-assessment reports led to small increases in self-reported trust, and high robot performance led to significant increases in self-reported trust. The presence of robot self-assessment reports also influenced the participants' choice of autonomy level.","The study found that participants were more likely to change to an appropriate level of autonomy based on the robot's self-assessment report, even if the report was inaccurate. This highlights the importance of accurate robot self-assessment. The study also found that participants were more likely to abort the task when a robot self-assessment was present, which may be due to the participant having an indication of task difficulty implicit in the robot's assessment.","Reporting a robot's a priori proficiency self-assessment, in the form of rewards based FaMSeC Outcome Assessment, resulted in a decrease in task failures and small improvements to the participants self-reported trust.","The robot autonomously navigates a grid world based on a pre-defined policy, or is manually controlled by the human participant. The human participant monitors the robot's progress and can switch between manual and automatic control, and can also abort the task.",contingency analysis; t-test; ANOVA; Tukey HSD,"The study used contingency analysis to examine the relationship between robot performance and task outcome, and between the presence of informed reports and task outcome. Independent t-tests were used to compare self-reported trust scores (MDMT Reliability and Capability) between high and random robot performance conditions, and between informed and absent report conditions. One-way ANOVAs were used to test the effect of robot self-assessment reports on control proportion, followed by Tukey's HSD post-hoc tests to identify specific differences between report conditions.",TRUE,Robot-accuracy; Robot-verbal-communication-content; Robot-autonomy,Robot-accuracy; Robot-verbal-communication-content,,"The study manipulated several factors. 'Robot-accuracy' was manipulated by having the robot perform at either a high level or a random level while in automatic control mode, directly impacting task performance. 'Robot-verbal-communication-content' was manipulated by providing the participants with robot self-assessment reports that were either informed (based on the robot's actual performance) or random, or absent. This directly changes the content of the robot's communication to the user. 'Robot-autonomy' was manipulated by allowing participants to switch between manual and automatic control of the robot, which changes the level of decision authority between the human and the robot. The study found that both 'Robot-accuracy' and 'Robot-verbal-communication-content' impacted trust. Specifically, high robot performance led to significant increases in self-reported trust, and informed robot self-assessment reports led to small increases in self-reported trust. The study did not find any factors that did not impact trust.",,http://arxiv.org/abs/2203.10407,"Human-robot teams will soon be expected to accomplish complex tasks in high-risk and uncertain environments. Here, the human may not necessarily be a robotics expert, but will need to establish a baseline understanding of the robot's abilities in order to appropriately utilize and rely on the robot. This willingness to rely, also known as trust, is based partly on the human's belief in the robot's proficiency at a given task. If trust is too high, the human may push the robot beyond its capabilities. If trust is too low, the human may not utilize it when they otherwise could have, wasting precious resources. In this work, we develop and execute an online human-subjects study to investigate how robot proficiency self-assessment reports based on Factorized Machine Self-Confidence affect operator trust and task performance in a grid world navigation task. Additionally we present and analyze a metric for trust level assessment, which measures the allocation of control between an operator and robot when the human teammate is free to switch between teleportation and autonomous control. Our results show that an a priori robot self-assessment report aligns operator trust with robot proficiency, and leads to performance improvements and small increases in self-reported trust."
"Correia, Filipa; Alves-Oliveira, Patricia; Maia, Nuno; Ribeiro, Tiago; Petisca, Sofia; Melo, Francisco S.; Paiva, Ana",Just follow the suit! Trust in human-robot interactions during card game playing,2016,2,60,60,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants played a card game with either a robot or a human partner, and completed questionnaires before and after the game to assess trust and affect.",Participants played the Sueca card game with a robot or human partner.,EMYS,Expressive Robots,Social; Research,Game,Cooperative Game,minimal interaction,Participants interacted with the robot through a touch table interface.,real-world,Participants played a card game with a physical robot in a lab setting.,physical,The robot was physically present during the interaction.,fully autonomous (limited adaptation),The robot played the game autonomously with a limited adaptation to the game state.,Questionnaires,Human-Robot Trust Scale/Questionnaire (HRTS/HRITS),,Trust was measured using a questionnaire before and after the game.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The study manipulated the type of partner (human or robot) and the participants' prior knowledge of their partner to influence trust.,"Trust increased for participants who had prior interaction with the robot, but not for those who had no prior interaction with the robot or with human partners.","Trust levels increased for participants who had prior interaction with the robot, but not for those who had no prior interaction with the robot or with human partners.","Trust in a robot partner increases with prior interaction, unlike trust in human partners, which remains stable.","The robot played the Sueca card game autonomously, while the human participants played as partners with the robot or with other human participants.",ANOVA,"A Mixed ANOVA statistical test was used to analyze the influence of partner type (human vs. robot) and partner knowledge (previous interaction vs. no previous interaction) on trust levels before and after the game. Additionally, Mixed ANOVA tests were used to analyze the changes in positive and negative affect before and after the game, and to compare these changes between human and robot partners.",TRUE,Teaming; Robot-social-attitude,Robot-social-attitude,Teaming,"The study manipulated the type of partner (human or robot), which is categorized as 'Teaming' because it changes whether the participant is collaborating with a human or a robot. The study also implicitly manipulated the 'Robot-social-attitude' by having some participants interact with the robot for the first time, while others had prior interaction. This prior interaction influenced how the robot was perceived, which is a social attitude. The results showed that prior interaction with the robot (a change in 'Robot-social-attitude') impacted trust levels, while the type of partner (human or robot, 'Teaming') did not have a direct impact on trust levels, but rather the prior knowledge of the partner did.",10.1109/ROMAN.2016.7745165,http://ieeexplore.ieee.org/document/7745165/,"Robots are currently being developed to enter our lives and interact with us in different tasks. For humans to be able to have a positive experience of interaction with such robots, they need to trust them to some degree. In this paper, we present the development and evaluation of a social robot that was created to play a card game with humans, playing the role of a partner and opponent. This type of activity is especially important, since our target group is elderly people - a population that often suffers from social isolation. Moreover, the card game scenario can lead to the development of interesting trust dynamics during the interaction, in which the human that partners with the robot needs to trust it in order to succeed and win the game. The design of the robot’s behavior and game dynamics was inspired in previous user-centered design studies in which elderly people played the same game. Our evaluation results show that the levels of trust differ according to the previous knowledge that players have of their partners. Thus, humans seem to signiﬁcantly increase their trust level towards a robot they already know, whilst maintaining the same level of trust in a human that they also previously knew. Henceforth, this paper shows that trust is a multifaceted construct that develops differently for humans and robots."
"Correia, Filipa; Alves-Oliveira, Patricia; Maia, Nuno; Ribeiro, Tiago; Petisca, Sofia; Melo, Francisco S.; Paiva, Ana",Just follow the suit! Trust in human-robot interactions during card game playing,2016,2,15,17,0,No participants were excluded,Real-World Environment,,Participants played a card game with a robot in a real-world tournament setting and answered a questionnaire about their experience.,Participants played the Sueca card game with a robot partner in a tournament.,EMYS,Expressive Robots,Social; Research,Game,Cooperative Game,minimal interaction,Participants interacted with the robot through a touch table interface.,real-world,Participants played a card game with a physical robot in a real-world tournament setting.,physical,The robot was physically present during the interaction.,fully autonomous (limited adaptation),The robot played the game autonomously with a limited adaptation to the game state.,Questionnaires,,,Trust was assessed using a questionnaire about the robot's performance.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,No Manipulation,"The study did not manipulate any specific factors, but assessed trust in a real-world setting.","Participants generally perceived the robot as playing well, but some noted issues with the game flow and card recognition.","Sueca champions were unwilling to play with the robot, fearing a loss of reputation.","The robot performed well in a real-world setting, but some usability issues were noted.","The robot played the Sueca card game autonomously, while the human participants played as partners with the robot.",,No statistical tests were explicitly mentioned in the description of this study. The results were based on descriptive statistics and qualitative analysis of questionnaire responses.,FALSE,,,,"This study did not explicitly manipulate any factors. It was an observational study in a real-world setting. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust due to manipulation.",10.1109/ROMAN.2016.7745165,http://ieeexplore.ieee.org/document/7745165/,"Robots are currently being developed to enter our lives and interact with us in different tasks. For humans to be able to have a positive experience of interaction with such robots, they need to trust them to some degree. In this paper, we present the development and evaluation of a social robot that was created to play a card game with humans, playing the role of a partner and opponent. This type of activity is especially important, since our target group is elderly people - a population that often suffers from social isolation. Moreover, the card game scenario can lead to the development of interesting trust dynamics during the interaction, in which the human that partners with the robot needs to trust it in order to succeed and win the game. The design of the robot’s behavior and game dynamics was inspired in previous user-centered design studies in which elderly people played the same game. Our evaluation results show that the levels of trust differ according to the previous knowledge that players have of their partners. Thus, humans seem to signiﬁcantly increase their trust level towards a robot they already know, whilst maintaining the same level of trust in a human that they also previously knew. Henceforth, this paper shows that trust is a multifaceted construct that develops differently for humans and robots."
"Correia, Filipa; Guerra, Carla; Mascarenhas, Samuel; Melo, Francisco S; Paiva, Ana",Exploring the Impact of Fault Justification in Human-Robot Trust,2018,1,107,97,10,"5 participants were excluded due to an unexpected technical failure in the system during the interaction with the robot, 1 participant was excluded given that he left the room to ask for support immediately after the robot's failure, 4 outliers regarding the trust levels were excluded",Controlled Lab Environment,between-subjects,"Participants completed an initial questionnaire, then played a Tangram game with a NAO robot on a touchscreen, and finally repeated the initial questionnaire.",Participants played a Tangram puzzle game collaboratively with a NAO robot.,Nao,Humanoid Robots; Expressive Robots,Social; Research,Game,Puzzle/Logic Game,minimal interaction,Participants interacted with the robot through a touchscreen interface.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical NAO robot.,fully autonomous (limited adaptation),"The robot's decisions and behaviors were fully autonomous, with limited adaptation.",Questionnaires,Schaefer's Trust Questionnaire/Scale,,Trust was measured using a 12-item subscale of the Human-Robot Trust Questionnaire.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's behavior was manipulated to include a technical failure, and the recovery strategy (justification or no justification) and the consequence of the failure (restart or continue) were also manipulated to influence trust.","A technical failure negatively impacted trust, but justifying the failure mitigated this negative impact when the task continued. When the task restarted, the justification did not mitigate the negative impact on trust.",The recovery strategy of justifying the failure was only effective in mitigating the negative impact of the failure when the consequence was less severe (when the task continued).,"A technical failure by a social robot negatively impacts trust, but justifying the failure can mitigate this impact when the consequence of the failure is not severe.",The robot and the participant took turns placing pieces in a Tangram puzzle. The robot provided verbal feedback and hints. The human interacted with the robot through a touchscreen interface.,cronbach's alpha; Shapiro-Wilk; Levene's test; Mann-Whitney U; ANOVA; ANOVA,"The study used Cronbach's alpha to assess the internal consistency of the trust scale, leading to the removal of two items. The Shapiro-Wilk test was used to check for normality of the trust data, and Levene's test was used to check for homogeneity of variances. Mann-Whitney tests were used to compare the impact of the failure on the task, to compare trust levels between different conditions, and to conduct simple effects analysis. A one-way ANOVA was used to analyze the overall effect of the robot failing on trust, and a factorial ANOVA was used to analyze the impact of recovery strategy and failure consequence on trust, after applying a rank transformation to the data.",TRUE,Robot-verbal-communication-content; Task-constraints,Robot-verbal-communication-content; Task-constraints,,"The study manipulated the robot's verbal communication by having it either justify a technical failure or remain silent, which directly alters the content of the robot's communication. This falls under 'Robot-verbal-communication-content'. The study also manipulated the consequence of the failure by either continuing the task or restarting it, which changes the constraints on the task by requiring participants to redo work, thus falling under 'Task-constraints'. The results showed that both the robot's justification (or lack thereof) and the consequence of the failure (restart or continue) impacted trust levels, as the justification was only effective in mitigating the negative impact of the failure when the task continued. Therefore, both 'Robot-verbal-communication-content' and 'Task-constraints' are listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",,,"With the growing interest on human-robot collaboration, the development of robotic partners that we can trust has to consider the impact of error situations. In particular, human-robot trust has been pointed as mainly affected by the performance of the robot and as such, we believe that in a collaborative setting, trust towards a robotic partner may be compromised after a faulty behaviour. This paper contributes to a user study exploring how a technical failure of an autonomous social robot affects trust during a collaborative scenario, where participants play the Tangram game in turns with the robot. More precisely, in a 2x2 (plus control) experiment we investigated 2 different recovery strategies, justify the failure or ignore the failure, after 2 different consequences of the failure, compromising or not the collaborative task. Overall, the results indicate that a faulty robot is perceived significantly less trustworthy. However, the recovery strategy of justifying the failure was able to mitigate the negative impact of the failure when the consequence was less severe. We also found an interaction effect between the two factors considered. These findings raise new implications for the development of reliable and trustworthy robots in human-robot collaboration."
"Croijmans, Ilja; Van Erp, Laura; Bakker, Annelie; Cramer, Lara; Heezen, Sophie; Van Mourik, Dana; Weaver, Sterre; Hortensius, Ruud",No Evidence for an Effect of the Smell of Hexanal on Trust in Human–Robot Interaction,2023,1,90,90,1,1 participant was excluded due to an error in the data file,Controlled Lab Environment,within-subjects,"Participants were exposed to four different smell conditions (Hexanal, Hexanal masked by Eugenol, Eugenol, and no smell) while completing a collaborative perceptual decision-making task with a robot. After each condition, participants completed the Reliance Intention Scale and the Mood Arousal Scale.","Participants completed a collaborative perceptual decision-making task where they had to identify a specific shape and color in a visual display. After giving their initial answer, they were presented with the robot's answer and could change their answer.",Vector,Expressive Robots,Research; Social,Game,Cooperative Game,minimal interaction,Participants interacted with the robot through a visual task and were able to pet the robot briefly.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot's answers were pre-determined and did not adapt to the participant's behavior.,Behavioral Measures; Questionnaires,Reliance Intention Scale,Performance Metrics,Trust was measured using a questionnaire and a behavioral measure of how often participants changed their answers to match the robot's.,"parametric models (e.g., regression)",The study used a generalized mixed-effects logistic regression model to test the impact of smell on trust decisions.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers manipulated the smell presented to participants during the task, and framed the task as a collaboration with the robot.",The study found no significant effect of the smell manipulation on trust in the robot.,"The study did not find the expected increase in trust due to hexanal, contrasting with previous findings in human-human interaction. There was a difference in pleasantness ratings for the smells in sample 2, but not in sample 1.","The study found no evidence that the smell of hexanal increases trust in human-robot interaction, unlike its effect in human-human interactions.","The robot provided answers to a visual task, and the human participant could choose to change their answer to match the robot's. The human also rated the smell and completed questionnaires.",ANOVA; generalized mixed-effects logistic regression model; ANOVA,"The study used repeated-measures ANOVAs to compare the effects of different odour conditions on mood, arousal, pleasantness, intensity, and familiarity ratings. A generalized mixed-effects logistic regression model was used to test the impact of smell on trial-by-trial trust decisions (number of times participants changed their answers to the robot's answer), with contrast coding for smell conditions. A linear model was fitted to test the effect of smell on general levels of trust, also using contrast coding for smell conditions. Bayesian posterior distributions were also reported as additional information.",TRUE,Task-environment,,Task-environment,"The study manipulated the smell presented to participants during the task, which is categorized as 'Task-environment' because it alters the sensory conditions of the task environment. The paper states, 'Participants were exposed to four different smell conditions (Hexanal, Hexanal masked by Eugenol, Eugenol, and no smell) while completing a collaborative perceptual decision-making task with a robot.' The study found no significant effect of the smell manipulation on trust in the robot, therefore, 'Task-environment' is listed under 'factors_that_did_not_impact_trust'.",10.1007/s12369-022-00918-6,https://link.springer.com/10.1007/s12369-022-00918-6,"The level of interpersonal trust among people is partially determined through the sense of smell. Hexanal, a molecule which smell resembles freshly cut grass, can increase trust in people. Here, we ask the question if smell can be leveraged to facilitate human–robot interaction and test whether hexanal also increases the level of trust during collaboration with a social robot. In a preregistered double-blind, placebo-controlled study, we tested if trial-by-trial and general trust during perceptual decision making in collaboration with a social robot is affected by hexanal across two samples (n 46 and n 44). It was hypothesized that unmasked hexanal and hexanal masked by eugenol, a molecule with a smell resembling clove, would increase the level of trust in human–robot interaction, compared to eugenol alone or a control condition consisting of only the neutral smelling solvent propylene glycol. Contrasting previous ﬁndings in human interaction, no signiﬁcant effect of unmasked or eugenolmasked hexanal on trust in robots was observed. These ﬁndings indicate that the conscious or nonconscious impact of smell on trust might not generalise to interactions with social robots. One explanation could be category- and context-dependency of smell leading to a mismatch between the natural smell of hexanal, a smell also occurring in human sweat, and the mechanical physical or mental representation of the robot."
"Cucciniello, Ilenia; Sangiovanni, Sara; Maggi, Gianpaolo; Rossi, Silvia",Validation of Robot Interactive Behaviors Through Users Emotional Perception and Their Effects on Trust,2021,1,317,288,29,"29 participants were excluded because they have not passed the validation questions, which checks if they have correctly heard the name of the robot in the video",Online Crowdsourcing,between-subjects,"Participants watched a video of a robot interacting with an actor, then completed questionnaires about their perceptions of the robot's behavior and their trust in the robot.",Participants watched a video of a robot administering a cognitive assessment task to an actor and then answered questionnaires.,Pepper,Humanoid Robots; Expressive Robots,Research; Social,Evaluation,Survey/Questionnaire Completion,passive observation,Participants observed a video of a robot interaction.,media,Participants watched a video of the robot interaction.,physical,The robot was a physical robot shown in a video.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions.,Questionnaires,Trust Perception Scale - HRI,,Trust was measured using a questionnaire.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's interaction style (friendly, neutral, authoritarian) was manipulated through verbal and non-verbal behaviors, including tone of voice, gestures, and eye color, to influence perceived trust.","The friendly robot behavior resulted in higher trust scores compared to the authoritarian behavior; no significant difference was found between friendly and neutral, or neutral and authoritarian.","The study found that the authoritarian robot, which was expected to elicit higher trust due to better performance in a previous study, actually resulted in lower trust scores than the friendly robot. This contradicts the initial hypothesis.",The friendly robot interaction style resulted in significantly higher trust scores compared to the authoritarian style.,"The robot administered a cognitive assessment task to a human actor, providing instructions and feedback. The human actor completed the task as instructed by the robot.",ANOVA; ANOVA; correlational analyses,"The study used a multivariate analysis of variance (MANOVA) to analyze differences in Self-Assessment Manikin (SaM) and Godspeed-SaM scores based on robot behavior (Neutral, Friendly, Authoritarian). Bonferroni-corrected post-hoc comparisons were used for significant results. Correlational analyses were performed to assess the consistency between SaM subscores and Godspeed-SaM subscales. An analysis of variance (ANOVA) was conducted to assess the impact of robot behavior, participant gender, and familiarity with robots on Trust Perception Scale-HRI scores. Post-hoc comparisons were also performed for significant ANOVA results.",TRUE,Robot-verbal-communication-style; Robot-nonverbal-communication; Robot-social-attitude,Robot-verbal-communication-style; Robot-nonverbal-communication; Robot-social-attitude,,"The study manipulated the robot's interaction style through verbal and non-verbal behaviors, which directly corresponds to 'Robot-verbal-communication-style' (e.g., tone of voice, speech speed, feedback) and 'Robot-nonverbal-communication' (e.g., gestures, eye colors, proxemics). The different styles (friendly, neutral, authoritarian) also represent a manipulation of the robot's social approach, thus 'Robot-social-attitude' is also included. The paper explicitly states that these manipulations were designed to create different perceived personalities and interaction styles. The results showed that these manipulations had a significant impact on trust, with the friendly style leading to higher trust scores than the authoritarian style. Therefore, all three manipulated factors are also listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1109/RO-MAN50785.2021.9515352,https://ieeexplore.ieee.org/document/9515352/,"When modeling the social behavior of a robot, the simulation of a speciﬁc personality or different interaction style may affect the perception of the interaction itself and the acceptability of the robot. Different interaction styles may be simulated through the use of verbal and non-verbal features that may not be easily recognized by the user as intended by the designer. For this reason, this study aimed to evaluate how three different robot interaction styles (i.e., Friendly, Neutral, and Authoritarian) were perceived by humans in the context of a robot carrying out cognitive tests. The Self-Assessment Manikin (SAM) was proposed to measure the perceived Valence, Arousal, and Dominance. We expected that a Neutral behavior is characterized by low Arousal, a Friendly by high Valence, and an Authoritarian by high Dominance. Moreover, the perception of a Socially Assistive Robot’s behavior is closely linked to trust, which is a key component to the success of any care-provider/user relationship. Hence, a Trust Perception Scale was used to explore the effect of the interaction style on trust. The results conﬁrmed our hypothesis and showed a signiﬁcant difference between each value with the others. Furthermore, we expected to obtain a higher value of trust with the Authoritarian since the performance of the users who interacted with the Authoritarian was better than the others. However, this hypothesis was not conﬁrmed by the results."
"Cui, Zixin; Zhuang, Xiangling; Lee, Seul Chan; Lee, Jieun; Li, Xintong; Itoh, Makoto",Development of a Chinese Human-Automation Trust Scale,2024,1,411,300,111,"42 invalid participants that have almost developed ongoing trust in HADS (1 participant), obviously misunderstood the introduction of HADS (7 participants) or answered questions carelessly (34 participants), questionnaires with one or more answers of nonexistent products or with obvious response patterns for one or more scales (69 participants)",Online Crowdsourcing,,"Participants completed questionnaires assessing trust in various automation systems after being introduced to them via text or video. The study was conducted in three phases, with each phase focusing on different automation systems and trust development stages. Phase 1 focused on initial trust in HADS, Phase 2 on initial and post-task trust in CADS and MOB GPS, and Phase 3 on initial and post-task trust in FA stereo-garage, ATC clothing, robovac, and FRPS.",Participants completed a questionnaire to assess their trust in different automation systems.,Unspecified,Autonomous Vehicles; Autonomous Vehicles; Mobile Robots; Other; Other; Other,Other,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the automation systems or watched videos.,media,Participants were introduced to the automation systems via videos.,hypothetical,"The robots were described in text or shown in videos, but were not physically present.",not autonomous,The robots' actions were not directly observed or interacted with by the participants.,Questionnaires; Custom Scales,,,Trust was assessed using a custom questionnaire.,no modeling,The study did not model trust computationally.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study did not manipulate any specific factors related to trust.,,"The study found that initial trust has two dimensions (performance + process and purpose), while post-task trust has three dimensions (performance, process, and purpose).","The study developed a Chinese Human-Automation Trust Scale (C-HATS) with different dimensions for initial and post-task trust, finding that initial trust has two dimensions while post-task trust has three.",Participants completed questionnaires to assess their trust in different automation systems after being introduced to them via text or video.,Factor analysis; Principal component analysis; bartlett's test of sphericity; kaiser-meyer-olkin test of sampling adequacy; independent-sample t-tests; item-total correlation; Multilevel Model; Pearson correlation,The study used exploratory factor analysis (EFA) with Principal Component Analysis (PCA) and Promax rotation to identify the underlying dimensions of the trust scale. Bartlett's Test of Sphericity and the Kaiser-Meyer-Olkin (KMO) test were used to assess the suitability of the data for EFA. Independent-sample t-tests were used to compare the mean scores of high-trust and low-trust groups on each item. Item-Total Correlation (ITC) was used to assess the correlation between each item and the total scale score. Confirmatory factor analysis (CFA) was used to assess the construct validity of the scale. Correlation analyses were used to assess test-retest reliability.,FALSE,,,,"The study did not manipulate any factors. The study aimed to develop a trust scale and assess its reliability and validity across different automation systems and trust development stages. The different automation systems (HADS, CADS, MOB GPS, FA stereo-garage, ATC clothing, robovac, and FRPS) were used as survey objects, but there was no manipulation of any specific factor related to these systems. The study focused on measuring trust through questionnaires after participants were introduced to the systems via text or video. The study did not manipulate any factors related to the robot's behavior, communication, or task performance. Therefore, no factors from the provided list were manipulated.",,,
"Cuijpers, Raymond H.; Bruna, Maarten T.; Ham, Jaap R. C.; Torta, Elena",Attitude towards Robots Depends on Interaction But Not on Anticipatory Behaviour,2011,1,14,14,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a task involving interaction with a robot in three different scenarios and three different robot behaviors, and then filled out a questionnaire.","Participants were asked to imagine scenarios and interact with the robot, including rejecting a phone call by touching the robot's head, waiting for the robot to arrive in a medical emergency, or being told to exercise.",Nao,Humanoid Robots,Research; Social,Social,Social Perception,minimal interaction,Participants interacted with the robot through verbal instructions and minimal physical touch.,real-world,The study was conducted in a real-world setting with a physical robot.,physical,The robot was physically present during the interaction.,wizard of oz (directly controlled),The robot was manually controlled by the experimenter from another room.,Questionnaires,Godspeed Questionnaire,,Trust was assessed using the Godspeed questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's approach behavior (following, intercepting, anticipating) and the scenario (phone call, medical emergency, health exercise) were manipulated to see how they affected the user's attitude towards the robot.","The robot's walking behavior did not affect the participants' ratings, but the scenario did, with the medical emergency scenario being rated more positively than the health exercise scenario in terms of appropriateness and perceived intelligence.","The study found no effect of the robot's anticipatory walking behavior on the participants' ratings, which was contrary to the researchers' expectations. The main effect was the scenario, with the medical emergency scenario being rated more positively than the health exercise scenario.","The attitude towards robots depends on the type of interaction and the urgency of the context, but not on the anticipatory behavior of the robot.","The robot approached the participant in different ways (following, intercepting, or anticipating) and delivered a message based on the scenario (phone call, medical emergency, or health exercise). The participant either touched the robot's head to reject a phone call or waited for the robot to arrive.",ANOVA; cronbach's alpha,"The study used a repeated measures ANOVA to test the effects of the robot's approaching behavior and the scenario on the different dimensions of the questionnaire (Animacy, Anthropomorphism, Likeability, Perceived Intelligence, Perceived Safety, and Appropriateness). Cronbach's alpha was used to test the consistency and reliability of the items within each scale of the questionnaire, and items with a detrimental effect on reliability were removed.",TRUE,Robot-task-strategy; Task-complexity,Task-complexity,Robot-task-strategy,"The study manipulated the robot's approach behavior (following, intercepting, anticipating), which is categorized as 'Robot-task-strategy' because it changes the way the robot completes the task of approaching the user without affecting the success of the task itself. The study also manipulated the scenario (phone call, medical emergency, health exercise), which is categorized as 'Task-complexity' because the scenarios differ in their perceived urgency and the actions required from the participant, thus changing the cognitive demands of the task. The results showed that the scenario (Task-complexity) had a significant impact on the participants' ratings of the robot, with the medical emergency scenario being rated more positively than the health exercise scenario in terms of appropriateness and perceived intelligence. This indicates that 'Task-complexity' impacted trust. However, the robot's approach behavior (Robot-task-strategy) did not significantly affect the participants' ratings, indicating that it did not impact trust.",,http://link.springer.com/10.1007/978-3-642-25504-5_17,"The care robot of the future should be able to navigate in domestic environments and perform meaningful tasks. Presumably, a robot that moves and interacts more intelligently gains more trust, is liked more and appears more humanlike. Here we test in three scenarios of diﬀering urgency whether anticipatory walking behaviour of a robot is appreciated as more intelligent and whether this results in a more positive attitude towards the robot. We ﬁnd no eﬀect of walking behaviour and a main eﬀect of urgency of the scenarios on perceived intelligence and on appropriateness. We interpret these results as that the type of interaction determines perceived intelligence and the attitude towards robots, but the degree of anticipation has no signiﬁcant eﬀect."
"De Visser, Ewart; Phillips, Elizabeth; Tenhundfeld, Nathan; Donadio, Bianca; Barentine, Christian; Kim, Boyoung; Madison, Anna; Ries, Anthony; Tossell, Chad",Trust in Automated Parking Systems,2022,2,45,31,14,"1 participant was excluded due to data quality, 13 participants were excluded due to less than two successful parking trials per condition",Real-World Environment,within-subjects,"Participants completed four parking trials in each of four conditions: Manual-Perpendicular, Auto-Perpendicular, Manual-Parallel, and Auto-Parallel, with eye-tracking and head movement data collected. Participants also completed the NASA-TLX and trust questionnaires after each set of four trials.",Participants were asked to park a Tesla Model X manually or using the Autopark feature in both perpendicular and parallel parking situations.,Tesla Model X,Autonomous Vehicles,Other: Automated parking,Supervision,Monitoring,minimal interaction,Participants interacted with the automated parking system by initiating the parking maneuver and monitoring its progress.,real-world,Participants interacted with the automated parking system in a real-world parking lot.,physical,Participants interacted with a physical Tesla Model X.,shared control (fixed rules),The robot autonomously executed the parking maneuver after the human initiated the process.,Behavioral Measures; Physiological Measures; Questionnaires,NASA Task Load Index (NASA-TLX),Eye-tracking Data; Performance Metrics,"Trust was assessed using questionnaires, behavioral measures (intervention behavior), and physiological measures (eye-tracking and head movements).",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The researchers manipulated the parking mode (manual vs. auto) and parking situation (perpendicular vs. parallel) to influence trust by varying the robot's performance and the task difficulty.,"Participants showed higher trust in the automated parallel parking compared to perpendicular parking, which was consistent with the system's performance.","The automated perpendicular parking was less efficient than manual parking, and participants intervened more in perpendicular parking, indicating lower trust. Participants also monitored the center display more in Auto conditions, reflecting a change in how the task was performed compared to Manual.","Automated parking was less successful and more error-prone than manual parking, but participants exhibited well-calibrated situation-specific trust by showing more trust in parallel parking than perpendicular parking.","The robot (Tesla Model X) autonomously parks the car after the human initiates the Autopark feature. The human drives the car and initiates the parking maneuver, and monitors the robot's actions.",ANOVA; t-test; Wilcoxon rank sum,"The study used repeated measures ANOVA to analyze parking efficiency (number of gear shifts), mental workload, and fixation duration, with factors like Parking Situation, Parking Mode, and Area of Interest. T-tests were used to compare self-confidence and trust levels between perpendicular and parallel parking conditions, as well as relative trust scores. Additionally, a Wilcoxon Signed Ranks Test was used to compare the frequency of use of automated parking for perpendicular and parallel parking in Study 2.",TRUE,Robot-autonomy; Task-complexity; Task-environment,Robot-autonomy; Task-complexity,Task-environment,"The study manipulated 'Robot-autonomy' by having participants either manually park the car or use the Autopark feature, which directly changes the level of decision authority given to the robot. This is explicitly stated in the 'Experimental Design' section: 'For the Parking Modes, participants either parked the Tesla themselves with no assistance (Manual) or were assisted by the Autopark system (Auto)'. The study also manipulated 'Task-complexity' by varying the parking situation (perpendicular vs. parallel), which influences the cognitive demands of the task. This is described in the 'Experimental Design' section: 'The two parking situations included a perpendicular or parallel parking situation between two vehicles'. The 'Task-environment' was also manipulated by using a real-world parking lot, which is described in the 'Parking Setup and Task Paradigm' section: 'Participants used a USAFA outdoor parking lot to perform all parking maneuvers'. The results showed that 'Robot-autonomy' and 'Task-complexity' impacted trust, as participants showed higher trust in the automated parallel parking (lower complexity) compared to perpendicular parking (higher complexity), and also showed different levels of trust between manual and auto conditions. The 'Task-environment' was consistent across all conditions and did not impact trust, as it was a real-world parking lot for all conditions.",10.31219/osf.io/5jmyf,https://osf.io/5jmyf,"In two studies, we evaluated the trust and usefulness of automated compared to manual parking using an experimental paradigm and by surveying owners of vehicles with automated parking features. In Study 1, we compared participants' ability to manually park a Tesla Model X and use the Autopark feature to complete perpendicular and parallel parking maneuvers. We investigated differences in parking success and duration, intervention behavior, self-reported levels of trust in and workload associated with the automation, as well as eye and head movements related to monitoring the automation. We found higher levels of trust in the automated parallel parking maneuvers compared to perpendicular parking. The Tesla’s automated perpendicular parking was found to be less efficient than manually executing this maneuver. Study 2 investigated the frequency with which owners of vehicles with automated parking features used those features and probed why they chose not to use them. Vehicle owners reported using their vehicle's autonomous parking features in ways consistent with the empirical findings from Study 1: higher usage rates of autonomous parallel parking. The results from both studies revealed that 1) automated parking is error-prone, 2) drivers nonetheless have calibrated trust in the automated parking system, and 3) the benefits of automated parallel parking surpass those of automated perpendicular parking with the current state of the technology."
"Dekarske, Jason; Bales, Gregory; Kong, Zhaodan; Joshi, Sanjay",Anytime Trust Rating Dynamics in a Human-Robot Interaction Task,2024,1,65,65,0,No participants were excluded,Online Crowdsourcing,within-subjects,Participants commanded a remote robot arm in a simulated space station to pick and place stowage. The robot's performance varied from trial to trial. Participants rated their trust on a nonobtrusive trust slider at any time throughout the experiment.,Participants remotely commanded a robot arm to sort stowage in a simulated space station environment.,Unspecified,Industrial Robot Arms,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the robot through a simulation interface.,simulation,The interaction took place in a simulated environment.,simulated,The robot was a simulated representation of a robot arm.,shared control (fixed rules),"The robot followed pre-programmed trajectories, but the human initiated the actions.",Real-time Trust Measures; Custom Scales,,Performance Metrics,Trust was measured using a real-time slider and performance metrics were collected.,"parametric models (e.g., regression)",A Cox Proportional Hazards Model was used to model the timing of trust ratings.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's performance was manipulated by using two different algorithms, one that always succeeded and one that failed 50% of the time. The task was framed as improving the way people work with smart technology.","The robot's performance significantly influenced the timing of trust ratings, with subjects rating their trust more often after the robot completed a grasp.","Subjects typically rated trust towards the end of the trial, suggesting they wanted complete evidence before evaluating their trust. The timing of the task outcome coincided with the trust report, but trust and task outcome played little role in the actual timing of subjects' trust reports.","The timing of trust ratings was primarily influenced by the robot's task performance, specifically the completion of a grasp, rather than the level of trust itself.",The robot arm picked and placed stowage items in a simulated space station. The human participant provided supervisory commands to the robot through a remote user interface.,cox proportional hazards model; survival analysis,"The study used a Cox Proportional Hazards Model within a Survival Analysis framework to model the time it took subjects to rate their trust in the robot. This analysis examined the relationship between the timing of trust ratings and factors such as robot performance (grasp success), the subject's current trust level, and the state of the robot's grasp (grasp completion). The model aimed to determine which factors significantly influenced when subjects chose to report their trust.",TRUE,Robot-accuracy,Robot-accuracy,,"The study explicitly manipulated the robot's performance by using two different algorithms, 'Gamma' which always succeeded, and 'Echo' which failed 50% of the time. This manipulation directly affects the robot's accuracy in completing the task, making 'Robot-accuracy' the appropriate category. The paper states, 'The robot's task performance changes from trial to trial according to its 'algorithm' identity.' and 'The robot's performance was modulated as an experimental condition. There are two 'algorithms' used by the robot for stowage placement: Gamma and Echo. These embodiments separated the task performance of the robot into two distinct categories. Gamma performs correct placements every time, while Echo places stowage in the wrong position 50% of the time.' The results section also indicates that the timing of trust ratings was influenced by the robot's performance, specifically the completion of a grasp, which is directly related to the robot's accuracy. The paper states, 'Interestingly, neither trust nor success has an impact compared to η, the coefficient of the grasp state covariate. However, the success coefficient is significant compared to the baseline hazard, the λ 0 constant, suggesting that the impending success of a grasp contributes to a higher probability of trust rating before the grasp has been completed.' This indicates that the robot's accuracy (success/failure) was a factor that impacted the timing of trust ratings, and thus impacted trust. The study did not find any other manipulated factors that did not impact trust.",,http://arxiv.org/abs/2408.00238,"Objective We model factors contributing to rating timing for a single-dimensional, any-time trust in robotics measure. Background Many studies view trust as a slow-changing value after subjects complete a trial or at regular intervals. Trust is a multifaceted concept that can be measured simultaneously with a human-robot interaction. Method 65 subjects commanded a remote robot arm in a simulated space station. The robot picked and placed stowage commanded by the subject, but the robot’s performance varied from trial to trial. Subjects rated their trust on a nonobtrusive trust slider at any time throughout the experiment. Results A Cox Proportional Hazards Model described the time it took subjects to rate their trust in the robot. A retrospective survey indicated that subjects based their trust on the robot’s performance or outcome of the task. Strong covariates representing the task’s state reflected this in the model. Conclusion Trust and robot task performance contributed little to the timing of the trust rating. The subjects’ exit survey responses aligned with the assumption that the robot’s task progress was the main reason for the timing of their trust rating."
"Dekarske, Jason; Kong, Zhaodan; Joshi, Sanjay",Dynamic Human Trust Modeling of Autonomous Agents With Varying Capability and Strategy,2024,1,80,80,0,No participants were excluded,Online Crowdsourcing,between-subjects,Participants were assigned to one of two groups and completed a visual search task with an autonomous agent. They then answered a questionnaire about their trust in the agent.,"Participants searched a 2D grid for outlier circles alongside an autonomous agent, and then reported their trust in the agent.",Unspecified,Mobile Robots,Research,Game,Cooperative Game,minimal interaction,Participants interacted with the robot through a screen-based task.,simulation,The interaction was conducted in a simulated 2D environment.,simulated,The robot was represented as a circle on a screen.,pre-programmed (non-adaptive),The robot followed pre-programmed search strategies without adapting to the user.,Questionnaires,Jian et al. Trust Scale,Performance Metrics,Trust was measured using a self-report questionnaire and performance metrics.,"parametric models (e.g., regression)",Trust was modeled using linear regression and ARIMAX time series models.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers manipulated the autonomous agent's capability and search strategy to influence trust.,"Trust was influenced by the agent's capability and the temporal ordering of its performance, with time series models showing improved prediction of trust.","The study found that the temporal ordering of agent performance significantly influenced trust estimation, and that an ARIMAX model was better at predicting trust than a linear regression model.","The temporal ordering of agent performance influences trust estimation, and time series models, particularly ARIMAX, enhance trust prediction.","The robot autonomously searched a grid for outliers using different strategies and capabilities, and reported the number of outliers it found. The human searched the same grid using a spotlight and reported their estimate of the total number of outliers.",ordinary least squares; arimax model; Linear regression; Akaike Information Criterion,"The study used Ordinary Least Squares regression to model self-reported trust, with capability and strategy as predictors. An ARIMAX time series model was also used to capture the temporal dynamics of trust, incorporating autoregressive, differencing, and moving average components. Model selection was guided by the Akaike Information Criterion (AIC). Multiple linear regression was used with categorical variables capability and strategy. The models were cross-validated between groups using root-mean-square-error of the residuals from a one-step-ahead forecast of trust.",TRUE,Robot-accuracy; Robot-task-strategy; Teaming,Robot-accuracy; Robot-task-strategy,,"The study manipulated the autonomous agent's capability (20%, 50%, or 100% of outliers found reported), which directly impacts the agent's accuracy in the task, thus 'Robot-accuracy'. The agent also used one of three search strategies: 'Lawnmower', 'Random', or 'Omniscient', which is a manipulation of 'Robot-task-strategy'. The study also involves a human and an autonomous agent working together to complete a task, which is a form of 'Teaming'. The paper states, 'The subjects' trust changes from trial to trial according to the agent's capability' and 'more legible strategies resulted in higher trust survey ratings', indicating that both 'Robot-accuracy' and 'Robot-task-strategy' impacted trust. There is no indication that the teaming aspect itself was manipulated to impact trust, but it is a core part of the study design.",,http://arxiv.org/abs/2404.19291,"Objective We model the dynamic trust of human subjects in a human-autonomy-teaming screen-based task. Background Trust is an emerging area of study in human-robot collaboration. Many studies have looked at the issue of robot performance as a sole predictor of human trust, but this could underestimate the complexity of the interaction. Method Subjects were paired with autonomous agents to search an on-screen grid to determine the number of outlier objects. In each trial, a different autonomous agent with a preassigned capability used one of three search strategies and then reported the number of outliers it found as a fraction of its capability. Then, the subject reported their total outlier estimate. Human subjects then evaluated statements about the agent’s behavior, reliability, and their trust in the agent. Results 80 subjects were recruited. Self-reported trust was modeled using Ordinary Least Squares, but the group that interacted with varying capability agents on a short time order produced a better performing ARIMAX model. Models were cross-validated between groups and found a moderate improvement in the next trial trust prediction. Conclusion A time series modeling approach reveals the effects of temporal ordering of agent performance on estimated trust. Recency bias may affect how subjects weigh the contribution of strategy or capability to trust. Understanding the connections between agent behavior, agent performance, and human trust is crucial to improving human-robot collaborative tasks."
"Delgosha, Mohammad Soltani; Hajiheydari, Nastaran",How human users engage with consumer robots? A dual model of psychological ownership and trust to explain post-adoption behaviours,2021,1,4000,403,29,29 responses were discarded due to at least one incorrect answer to the 'attention trap' questions,Online Crowdsourcing,,Participants completed a web-based survey after being recruited from robot owner Facebook groups. They were asked about their experiences with their robots and completed questionnaires measuring various constructs related to trust and psychological ownership. Participants were incentivized with a voucher and a chance to win a robot.,Participants completed a survey about their experiences with their consumer robots.,Unspecified,Expressive Robots; Mobile Robots,Social; Care,Evaluation,Survey/Questionnaire Completion,passive observation,Participants completed a survey without direct interaction with a robot.,media,"The study used a text-based survey, providing no immersive experience.",hypothetical,"The study did not involve a physical or simulated robot, only descriptions of robots.",not autonomous,The study did not involve any autonomous robot behavior.,Questionnaires,,,Trust was measured using questionnaires.,"parametric models (e.g., regression)",The study used partial least squares (PLS) to test the structural model.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,"The study did not manipulate any factors related to trust, but rather measured existing perceptions of trust and psychological ownership.",,"The study found that perceived control and self-investment significantly affect psychological ownership, while intimate knowing did not. Also, structural assurance and social presence significantly impact perceived trustworthiness, but situational normality did not.",Psychological ownership and perceived trustworthiness are key determinants of post-adoption behaviors such as intention to explore and cognitive absorption.,"Participants completed a survey about their experiences with their consumer robots, answering questions about their perceptions of trust, psychological ownership, and their engagement with the robots.",t-test; Partial least squares; cronbach's alpha; composite reliabilities; fornell-larcker test; Heterotrait-monotrait ratio; harman's single-factor test; marker-variable technique; Bootstrapping,"The study used a variety of statistical tests. A T-test was used to compare early and late respondents to assess nonresponse bias. Partial least squares (PLS) was used to test both the measurement and structural models, including the relationships between multiple independent and dependent variables and second-order constructs. Cronbach's alpha and composite reliabilities were used to assess the reliability of the constructs. The Fornell-Larcker test and heterotrait-monotrait (HTMT) ratio were used to establish discriminant validity. Harman's single-factor test and a marker-variable technique were used to test for common method variance. Finally, bootstrapping re-sampling technique was used to test the structural model and the moderating effects.",FALSE,,,,"The study did not manipulate any factors. It was a survey-based study where participants reported their experiences with their consumer robots. The study measured existing perceptions of trust and psychological ownership, but did not manipulate any variables to observe their effect on trust. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust due to manipulation.",10.1016/j.chb.2020.106660,https://linkinghub.elsevier.com/retrieve/pii/S0747563220304076,"Consumer robots are technically evolving and have a growing presence in our daily lives with enhanced inter­ active capabilities. While there is insightful literature on robot adoption, so far, research has done less to examine the post-adoption interaction of human-consumer robots. Drawing on trust in technology model and psycho­ logical ownership theory, this study proposes a conceptual dual model to explain robot users’ post-adoption behaviours, while considers the moderating roles of anthropomorphism and social presence. We empirically corroborated our model by asking from 403 current robot owners to illustrate theoretical paths to their postadoption behaviours including cognitive absorption and intention to explore. This study contributes to the extant literature of human-robot interaction by proposing a theoretically grounded and empirically tested framework that contextualizes psychological ownership theory, uncertainty reduction theory, and trust in technology model. We also highlight the implications for practitioners to leverage trust and psychological ownership mechanisms together for encouraging users to actively engage with robots."
"Demir, Mustafa; McNeese, Nathan J.; Gorman, Jaime C.; Cooke, Nancy J.; Myers, Christopher W.; Grimm, David A.",Exploration of Teammate Trust and Interaction Dynamics in Human-Autonomy Teaming,2021,1,44,44,0,No participants were excluded,Controlled Lab Environment,within-subjects,Participants were assigned to roles (navigator or photographer) and interacted with a simulated autonomous pilot (Wizard of Oz). They completed ten 40-minute missions with imposed automation and autonomy failures. Questionnaires and performance data were collected.,Participants worked in a team to take photographs of color-coded strategic target waypoints using a simulated remotely-piloted aircraft system (RPAS).,Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated autonomous pilot through text chat.,simulation,The study used a simulated remotely-piloted aircraft system (RPAS) environment.,simulated,The robot was a simulated autonomous pilot.,wizard of oz (directly controlled),The 'autonomous' pilot was actually a human experimenter using a script.,Questionnaires; Behavioral Measures; Performance-Based Measures; Real-time Trust Measures,Mayer and Davis' Trust/Trustworthiness Scales (1999); NASA Task Load Index (NASA-TLX),Performance Metrics; Speech Data,"Trust was measured using questionnaires, team communication flow, and performance metrics.","parametric models (e.g., regression)","Stepwise regression analyses were used to model the relationship between trust, interaction dynamics, and failure recovery.",Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The researchers introduced automation and autonomy failures to manipulate the robot's performance and behavior, and increased task difficulty.",Trust in the autonomous pilot decreased over time as failures increased. Moderate trust was positively related to autonomy failure recovery.,"The study found a U-shaped relationship between team communication predictability and trust, and an inverted U-shape relationship between team communication predictability and automation failure recovery. Trust in the autonomous pilot decreased over time as failures increased.","Team interaction dynamics are linked to the development of trust in human-autonomy teams, and both are related to autonomy and automation failure recovery.","The human participants, in the roles of navigator and photographer, communicated with a simulated autonomous pilot to take photographs of target waypoints. The pilot was controlled by a human experimenter using a script, and the human participants monitored the pilot's actions and provided feedback.",Factor analysis; ANOVA; Linear regression; joint recurrence quantification analysis (jrqa),"The study used Exploratory Factor Analysis (EFA) to reduce the dimensionality of the trust questionnaire into factors. Analysis of Variance (ANOVA) was used to examine how trust factors differed across sessions and roles. Stepwise regression analyses were conducted to test the relationships between trust, interaction dynamics, and failure recovery. Joint Recurrence Quantification Analysis (JRQA) was applied to team communication flow data to calculate interaction dynamics measures.",TRUE,Robot-accuracy; Robot-autonomy; Task-complexity,Robot-accuracy; Robot-autonomy,,"The study manipulated 'Robot-accuracy' by introducing automation failures that caused the robot to perform incorrectly (e.g., display failures while processing targets). This is explicitly stated in the paper: '1) automation-role-specific display failures that occurred while processing specific targets'. The study also manipulated 'Robot-autonomy' by having the 'autonomous' pilot behave abnormally (e.g., providing misinformation or incorrect actions), as stated: '2) autonomy-autonomous agent behaved abnormally while processing specific targets (i.e., it provided misinformation to other team members or demonstrated incorrect action)'. The study also manipulated 'Task-complexity' by increasing the difficulty of the task through the introduction of failures, as stated: 'The main manipulation consisted of three failures'. The paper also states that 'Each failure was imposed on a preselected target waypoint'. The results showed that 'trust in HATs would decrease as autonomy failures increased' and that 'moderate trust was positively related to autonomy failure recovery', indicating that both 'Robot-accuracy' and 'Robot-autonomy' impacted trust. There was no mention of any of the manipulated factors not impacting trust.",10.1109/THMS.2021.3115058,https://ieeexplore.ieee.org/document/9563095/,"This article considers human-autonomy teams (HATs) in which two human team members interact and collaborate with an autonomous teammate to achieve a common task while dealing with unexpected technological failures that were imposed either in automation or autonomy. A Wizard of Oz methodology is used to simulate the autonomous teammate. One of the critical aspects of HAT performance is the trust that develops over time as team members interact with each other in a dynamic task environment. For this reason, it is important to examine the dynamic nature of teammate trust through real-time measures of team interactions. This article examines team interaction and trust to understand better how they change under automation and autonomy failures. Thus, we address two research questions: 1) How does trust in HATs evolve over time?; and 2) How is the relationship between team interaction and trust impacted by the failures? We hypothesize that trust in HATs will decrease as autonomy failures increase. We also hypothesize that team interaction would be related to the development of trust and recovery from the failures. The results implicate three general trends: 1) team interaction dynamics are linked to the development of trust in HATs; 2) trust in the autonomous teammate is only associated with recovery from autonomy failures; 3) team interaction dynamics are related to both automation and autonomy failure recovery."
"Desai, Munjal; Medvedev, Mikhail; Vázquez, Marynel; McSheehy, Sean; Gadea-Omelchenko, Sofia; Bruggeman, Christian; Steinfeld, Aaron; Yanco, Holly",Effects of changing reliability on trust of robot systems,2012,1,24,24,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a trial run in fully autonomous mode, then a trial run in robot assisted mode, followed by five experimental runs with varying reliability of the robot's autonomy. Participants completed questionnaires after each run.","Participants were asked to drive a robot along a specified path, search for victims, avoid obstacles, and respond to secondary tasks.",iRobot ATRV-JR,Mobile Robots,Research,Navigation,Remote Navigation,minimal interaction,"Participants interacted with the robot through a user interface, controlling its movement and autonomy mode.",real-world,Participants interacted with a physical robot in a real-world environment.,physical,Participants interacted with a physical robot.,shared control (fixed rules),"The robot operated in either a fully autonomous mode or a robot-assisted mode, with fixed rules for each mode.",Questionnaires,Muir's Trust Questionnaire; Jian et al. Trust Scale; NASA Task Load Index (NASA-TLX),Performance Metrics,Trust was assessed using questionnaires and performance metrics.,"parametric models (e.g., regression)",A backwards stepwise regression was used to predict trust ratings.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The reliability of the robot's autonomy was directly manipulated by introducing periods of low reliability at different times during the task, which also influenced task difficulty.","Trust was highest in high reliability runs and reduced when reliability decreased, especially when drops occurred in the middle or end of the runs. Timing of reliability drops was important for trust.",Participants showed a positivity bias by initially trusting the robot. Participants switched autonomy modes more frequently when reliability dropped in the middle of the run. Participants tied trust more strongly to their own actions rather than robot performance.,"Trust was affected by drops in reliability, and the timing of these drops influenced trust, with drops in the middle or end of runs having a greater negative impact. Participants tied trust more strongly to their own actions rather than robot performance.","The robot moved along a predefined path, and the human controlled the robot's movement and autonomy mode, while also searching for victims and responding to secondary tasks.",REML; t-test; ANOVA; backwards stepwise regression; t-test,"The study used REML (Restricted Maximum Likelihood) analysis to examine the effects of Site and Reliability on trust scores from the Muir and Jian questionnaires, with Participants as a random effect. Student's t post hoc tests were used to compare the different reliability conditions. ANOVA was used to analyze participant ratings of robot and self-performance across reliability levels. A backwards stepwise regression was used to predict Muir trust ratings based on various factors including cognitive load, performance metrics, demographics, and post-run assessments. A one-tailed t-test was used to compare the time taken to switch away from autonomy versus the time taken to switch back to autonomy.",TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,,"The study explicitly manipulated the reliability of the robot's autonomy, which directly influenced the level of control the robot had over its actions. This is a clear manipulation of 'Robot-autonomy'. The task complexity was also manipulated by varying the frequency of secondary tasks, which increased the cognitive load on the participants. This is a manipulation of 'Task-complexity'. The paper states that 'Trust was affected by drops in reliability' and 'Reliability patterns also led to different mode switching behavior', indicating that 'Robot-autonomy' impacted trust. The paper does not explicitly state that task complexity impacted trust, but it was manipulated as a factor in the experiment.",10.1145/2157689.2157702,http://dl.acm.org/citation.cfm?doid=2157689.2157702,"Prior work in human-autonomy interaction has focused on plant systems that operate in highly structured environments. In contrast, many human-robot interaction (HRI) tasks are dynamic and unstructured, occurring in the open world. It is our belief that methods developed for the measurement and modeling of trust in traditional automation need alteration in order to be useful for HRI. Therefore, it is important to characterize the factors in HRI that inﬂuence trust. This study focused on the inﬂuence of changing autonomy reliability. Participants experienced a set of challenging robot handling scenarios that forced autonomy use and kept them focused on autonomy performance. The counterbalanced experiment included scenarios with different low reliability windows so that we could examine how drops in reliability altered trust and use of autonomy. Drops in reliability were shown to affect trust, the frequency and timing of autonomy mode switching, as well as participants’ self-assessments of performance. A regression analysis on a number of robot, personal, and scenario factors revealed that participants tie trust more strongly to their own actions rather than robot performance."
"Desai, Munjal; Kaniarasu, Poornima; Medvedev, Mikhail; Steinfeld, Aaron; Yanco, Holly",Impact of robot failures and feedback on real-time trust,2013,1,28,28,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants completed a demographic questionnaire, two trial runs, and five experimental runs with varying reliability and feedback conditions. They were prompted to indicate changes in trust during the runs and completed questionnaires after each run.","Participants controlled a robot to navigate a course, following labels and avoiding obstacles, while also acknowledging a secondary task and trust prompts.",iRobot ATRV-JR,Mobile Robots; UGVs,Research,Navigation,Path Following,minimal interaction,"Participants interacted with the robot through a user interface, controlling its movement and autonomy mode.",media,Participants viewed the robot's camera feed on a screen while controlling it.,physical,Participants interacted with a physical robot remotely through a user interface.,shared control (fixed rules),"Participants could switch between fully autonomous and assisted modes, with the assisted mode providing 75% user control.",Questionnaires; Real-time Trust Measures,Muir's Trust Questionnaire; NASA Task Load Index (NASA-TLX),Performance Metrics,"Trust was measured using questionnaires after each run and real-time prompts during the runs, with performance metrics also collected.","parametric models (e.g., regression)",A backward stepwise regression was used to predict trust based on user behavior data.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's reliability was manipulated by introducing periods of low reliability, and some participants received feedback about the robot's confidence in its sensors. The autonomy level was also manipulated by allowing participants to switch between autonomous and assisted modes.",Early drops in reliability had a more detrimental impact on real-time trust than later drops. Feedback about the robot's confidence did not significantly alter real-time trust levels but did influence control allocation strategies. Semantic feedback led to more abrupt changes in real-time trust compared to non-semantic feedback.,"The study found that post-run trust measures are biased by primacy-recency effects, masking the impact of early reliability drops. The study also found that early periods of low reliability have a more detrimental impact on trust than later periods of low reliability. The study also found that semantic feedback led to more abrupt changes in real-time trust compared to non-semantic feedback.","Early drops in robot reliability have a more detrimental impact on real-time trust than later drops, and robot confidence feedback can improve autonomy control allocation without altering real-time trust levels.","The robot autonomously navigated a course, reading labels and avoiding obstacles, or was controlled by the participant in assisted mode. The human participant monitored the robot's progress, switched between autonomy modes, and acknowledged secondary tasks and trust prompts.",ANOVA; Tukey HSD; t-test; backward stepwise regression,"The study used a two-way ANOVA to analyze the impact of reliability and group type on trust (measured by the Muir questionnaire) and on autonomy mode switches. Post-hoc comparisons using Tukey's HSD test were conducted to identify significant differences between reliability conditions. Paired two-tailed t-tests were used to compare normalized AUTC before and after periods of low reliability, and to compare AUTC values during different low reliability periods. Unpaired two-tailed t-tests were used to compare the rate of change of trust curves between semantic and non-semantic feedback conditions. A backward stepwise regression was performed to identify factors that could predict an operator's tendency to trust robots.",TRUE,Robot-accuracy; Robot-interface-design; Robot-autonomy,Robot-accuracy,Robot-interface-design,"The study manipulated 'Robot-accuracy' by introducing periods of low reliability where the robot would make mistakes in reading barcodes and navigating the course, as described in the 'Reliability patterns' section. This directly impacted the robot's performance on the task. The study also manipulated 'Robot-interface-design' by providing some participants with feedback about the robot's confidence in its sensors, using either semantic (emoticons) or non-semantic (colored lights) indicators, as described in the 'Modifications for the feedback condition' section. The 'Autonomy modes' section describes how the study manipulated 'Robot-autonomy' by allowing participants to switch between fully autonomous and assisted modes, where they had 75% control. The results showed that 'Robot-accuracy' (specifically, the timing of reliability drops) significantly impacted real-time trust, as detailed in the 'Real-time trust' section. While the feedback interface design did influence control allocation strategies, it did not significantly alter real-time trust levels, as stated in the 'Effect of feedback' section, thus 'Robot-interface-design' did not impact trust. The manipulation of 'Robot-autonomy' was not found to directly impact trust, but rather influenced control allocation strategies.",10.1109/HRI.2013.6483596,,"Prior work in human trust of autonomous robots suggests the timing of reliability drops impact trust and control allocation strategies. However, trust is traditionally measured post-run, thereby masking the real-time changes in trust, reducing sensitivity to factors like inertia, and subjecting the measure to biases like the primacy-recency effect. Likewise, little is known on how feedback of robot confidence interacts in real-time with trust and control allocation strategies. An experiment to examine these issues showed trust loss due to early reliability drops is masked in traditional post-run measures, trust demonstrates inertia, and feedback alters allocation strategies independent of trust. The implications of specific findings on development of trust models and robot design are also discussed."
"Di Dio, Cinzia; Manzi, Federico; Peretti, Giulia; Cangelosi, Angelo; Harris, Paul L.; Massaro, Davide; Marchetti, Antonella",Shall I Trust You? From Child–Robot Interaction to Trusting Relationships,2020,1,94,90,4,"1 child was removed from age group 3 years, 3 children were removed from age group 7 years",Educational Setting,mixed design,"Children played a guessing game with either a human or a robot partner in three phases: trust acquisition, trust loss, and trust restoration. They also completed questionnaires and cognitive tasks.","Children played a 'Guess where it is' game where they had to guess the location of a hidden doll, following the lead of either a human or robot partner.",Nao,Humanoid Robots; Expressive Robots,Research; Social,Game,Other Game subtask: The game involved guessing the location of a hidden object.,minimal interaction,Children interacted with the robot or human partner through a structured game with verbal instructions.,real-world,The interaction took place in a real-world setting with the physical presence of the robot or human.,physical,The robot was physically present during the interaction.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the child's behavior.,Behavioral Measures,Attribution of Mental States (AMS) Scale,,Trust was measured by observing how many trials it took for the child to follow the partner's guess.,no modeling,Trust was not modeled computationally; statistical analysis was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's and human's choices were manipulated to create phases of trust acquisition, loss, and restoration, influencing the child's expectations and behavior.","Children's trust decreased from acquisition to restoration. Younger children initially trusted the human more, while older children trusted the robot more after the trust loss phase.","Three-year-olds initially trusted the human more, while 7-year-olds tended to trust the robot more. There was a negative correlation between ToM and trust, indicating that children with higher ToM were less likely to trust.","Children's trust in a play partner, whether human or robot, is influenced by age, attachment history, and cognitive development, with younger children relying more on affect and older children on cognition.","The robot provided verbal and gestural cues to indicate the possible location of a hidden doll. The child then made their own guess, following or not following the robot's lead. The human partner performed the same actions as the robot.",ANOVA; Pearson correlation,"The study used a repeated measures General Linear Model (GLM) analysis to evaluate differences in children's tendency to trust the human and robot partner as a function of age and trust phase (acquisition, loss, and restoration). This analysis examined the number of trials until children followed their partner, stopped following, and again followed their partner. Additionally, correlation analyses (Pearson's r) were carried out to evaluate the relationship between the tendency to trust and the quality of attachment relationships, Theory of Mind (ToM), and executive function skills.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy,Task-complexity,"The study manipulated 'Robot-accuracy' by having the robot (or human) initially provide correct guesses in the 'Trusting Acquisition' phase, then incorrect guesses in the 'Mistrust Acquisition' phase, and finally correct guesses again in the 'Trusting Restoration' phase. This manipulation directly influenced the children's trust in the partner, as evidenced by the changes in their behavior (following or not following the partner's lead). The study also implicitly manipulated 'Task-complexity' by having the children play a guessing game with a specific structure and rules. However, the task complexity was constant across all conditions and did not directly impact trust levels. The task was designed to be simple and understandable for all age groups, and the complexity did not vary between the human and robot conditions. The study explicitly states that the game was designed to be neither collaborative nor competitive, and that the children were playing for the mere fun of it, which means that the task complexity was not a factor that was manipulated to influence trust. The study also states that the position of the doll was established a priori to correctly instruct (or program, if NAO) the play partner's choice during each phase of the game, which means that the robot's accuracy was directly manipulated.",10.3389/fpsyg.2020.00469,https://www.frontiersin.org/article/10.3389/fpsyg.2020.00469/full,"Studying trust in the context of human–robot interaction is of great importance given the increasing relevance and presence of robotic agents in the social sphere, including educational and clinical. We investigated the acquisition, loss, and restoration of trust when preschool and school-age children played with either a human or a humanoid robot in vivo. The relationship between trust and the representation of the quality of attachment relationships, Theory of Mind, and executive function skills was also investigated. Additionally, to outline children’s beliefs about the mental competencies of the robot, we further evaluated the attribution of mental states to the interactive agent. In general, no substantial differences were found in children’s trust in the play partner as a function of agency (human or robot). Nevertheless, 3-year-olds showed a trend toward trusting the human more than the robot, as opposed to 7-year-olds, who displayed the reverse pattern. These ﬁndings align with results showing that, for 3- and 7-yearolds, the cognitive ability to switch was signiﬁcantly associated with trust restoration in the human and the robot, respectively. Additionally, supporting previous ﬁndings, we found a dichotomy between attributions of mental states to the human and robot and children’s behavior: while attributing to the robot signiﬁcantly lower mental states than the human, in the Trusting Game, children behaved in a similar way when they related to the human and the robot. Altogether, the results of this study highlight that similar psychological mechanisms are at play when children are to establish a novel trustful relationship with a human and robot partner. Furthermore, the ﬁndings shed light on the interplay – during development – between children’s quality of attachment relationships and the development of a Theory of Mind, which act differently on trust dynamics as a function of the children’s age as well as the interactive partner’s nature (human vs. robot)."
"Dikmen, Murat; Burns, Catherine",Trust in autonomous vehicles: The case of Tesla Autopilot and Summon,2017,1,162,121,41,41 participants did not fully complete the survey,Online Crowdsourcing,mixed design,"Participants completed an online survey about their experiences with Tesla's Autopilot and Summon systems, including questions about their trust and confidence in these systems, as well as their initial trust and confidence. They also answered questions about frequency of use, knowledge about the systems, ease of learning, usefulness of the Autopilot display, perceived risk, and computer expertise.","Participants rated their trust in Autopilot and Summon, and answered questions about their experiences with these systems.",Unspecified,Autonomous Vehicles; Autonomous Vehicles,Other,Evaluation,Survey/Questionnaire Completion,passive observation,Participants responded to survey questions about their experiences with the autonomous systems.,media,The study used text-based survey questions without any visual or interactive elements.,hypothetical,"The study involved no physical or simulated robots, only descriptions of the autonomous systems.",fully autonomous (limited adaptation),The autonomous systems operate without direct human assistance but have limited adaptability.,Questionnaires; Custom Scales,,,Trust was measured using custom Likert scale items in a questionnaire.,"parametric models (e.g., regression)",The study used ANOVA and correlation analyses to examine relationships between trust and other variables.,Observational & Survey Studies,Quantitative Surveys,Indirect Manipulation,"The study indirectly influenced trust by asking participants about their experiences with the systems, including whether they experienced unexpected behaviors, which could affect their perception of the system's performance and their expectations. The role of the participant as a driver also influenced their trust.","Trust in Autopilot decreased for drivers who experienced unexpected behaviors. Trust in both Autopilot and Summon increased over time, regardless of whether participants experienced an automation failure. Trust in Autopilot decreased with age, but not for Summon.","The finding that initial trust was lower for those who experienced an Autopilot incident was unexpected. Also, the finding that trust in Autopilot decreased with age is in contrast with previous findings.","Tesla drivers reported high levels of trust in Autopilot and Summon, and trust increased over time regardless of whether they experienced an automation failure.","The robot (Autopilot and Summon) performs driving and parking tasks, while the human monitors the system and provides input when necessary. The human's primary task was to complete a survey about their experiences with these systems.",ANOVA; Pearson correlation; ANOVA,"The study used correlation analysis to examine the relationships between trust and other variables such as frequency of use, knowledge about the systems, ease of learning, and usefulness of the Autopilot display. One-way ANOVA was used to assess the effect of age on trust in Autopilot and Summon. A 2x2 mixed ANOVA was used to compare initial and current trust in both Autopilot and Summon, and to examine the effect of experiencing an incident on trust.",FALSE,Robot-accuracy; Task-environment,Robot-accuracy,Task-environment,"The study did not explicitly manipulate any factors. However, the study design implicitly influenced trust through the participants' experiences with the autonomous systems. The 'Robot-accuracy' factor is chosen because the study examines the impact of experiencing unexpected behaviors (failures) from the Autopilot and Summon systems on trust. These unexpected behaviors directly relate to the accuracy of the robot's actions. The 'Task-environment' factor is chosen because the study was conducted with real Tesla drivers in their real-world driving environment, which is a factor that could influence trust. However, the study did not manipulate the environment, so it is not considered a manipulated factor that impacted trust. The study found that experiencing an incident (a failure of the robot) impacted trust, while the real-world environment was not manipulated and therefore did not impact trust.",10.1109/SMC.2017.8122757,,"Autonomous driving is on the horizon. Vehicles with partially automated driving capabilities are already in the market. Before the widespread adoption however, human factors issues regarding automated driving need to be addressed. One of the key issues is how much drivers trust in automated driving systems and how they calibrate their trust and reliance based on their experience. In this paper, we report the results of a survey conducted with Tesla drivers about their experiences with two advanced driver assistance systems, Autopilot and Summon. We found that drivers have high levels of trust in Autopilot and Summon. Trust decreased with age for Autopilot but not for Summon. Drivers who experienced unexpected behaviors from their vehicles reported lower levels of trust in Autopilot. Over time, trust in these systems increased regardless of experience. Additionally, trust was correlated with several attitudinal and behavioral factors such as frequency of use, self-rated knowledge about these systems, and ease of learning. These findings highlight the importance of trust in real world use of autonomous cars. Also, the results suggest that previous findings on trust in automation are applicable to real world cases as well."
"Diogo, André; Ayanoglu, Hande; Teles, Júlia; Duarte, Emília",Trust on Service Robots: A Pilot Study on the Influence of Eyes in Humanoid Robots During a VR Emergency Egress,2020,1,20,20,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were introduced to a virtual environment and a robot, then experienced a simulated emergency egress where they had to follow the robot to safety. They completed a post-task questionnaire and the Godspeed questionnaire.",Participants were tasked with following a robot through a virtual building during a simulated emergency egress.,Unspecified,Humanoid Robots; Service and Assistive Robots,Research; Social; Care,Navigation,Guiding,minimal interaction,"Participants interacted with the robot in a virtual environment, following it through a simulated emergency.",simulation,The interaction took place in a virtual environment using a head-mounted display.,simulated,The robot was a virtual representation within the simulation.,pre-programmed (non-adaptive),The robot followed a pre-set path and did not adapt to the participant's actions.,Behavioral Measures; Questionnaires,Godspeed Questionnaire,,Trust was assessed using the percentage of choices to follow the robot and a post-experiment questionnaire.,no modeling,No computational model of trust was used in this study.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's appearance (presence or absence of eyes) and the environmental affordances (favorable vs. unfavorable corridor) were directly manipulated to assess their influence on trust.,No significant effect of either the existence of eyes or the environmental affordance on trust was found.,"The lack of significant effect of the robot's eyes on trust was unexpected, as previous studies have suggested that eyes are important for trust. Participants also reported feeling compelled to follow the robot, despite perceiving it as machinelike.","The study found that the presence of eyes on a service robot did not significantly influence trust during a virtual emergency egress, and that environmental affordances also did not affect trust.","The robot guided participants through a virtual building during a simulated emergency, and the participants followed the robot's lead at intersections.",Chi-squared; Mann-Whitney U; Wilcoxon rank sum,"The study used a chi-square test of independence to evaluate the effects of the robot's eyes on participants' decisions to follow it at intersections. Mann-Whitney U tests were used to assess differences between experimental conditions and gender on trust, hesitations, and loss of visual contact, as well as on the Godspeed questionnaire results. The Wilcoxon Signed Ranks test was used to examine the influence of environmental affordance on trust within each experimental condition.",TRUE,Robot-aesthetics; Task-environment,,Robot-aesthetics; Task-environment,"The study manipulated the robot's appearance by including or excluding eyes, which falls under 'Robot-aesthetics' as it is a change to the visual appeal of the robot. The study also manipulated the environment by using favorable (bright, wide corridor) and unfavorable (dark, narrow corridor) conditions, which is categorized as 'Task-environment' because it changes the working conditions. The results showed that neither the presence of eyes nor the environmental affordances had a significant impact on trust, therefore both factors are listed under 'factors_that_did_not_impact_trust'. No factors were found to impact trust.",,http://link.springer.com/10.1007/978-3-030-49062-1_39,"Robots are found to be good, capable and trustworthy companions in various areas, including high-risk situations or emergencies, but some limitations regarding their acceptance have been reported. Amongst other aspects of the Human-Robot Interaction, trust in the robot has been considered as a main indicator of acceptance. Thus, to investigate the dynamics of human-robot acceptance, this study used a virtual reality simulation of an emergency egress to assess the inﬂuence of the robot’s appearance on trust. In particular, we were interested in examining the inﬂuence of the eyes in the robot on the participants’ decision to follow it to the exit. Since the type of interaction scenario is also a factor with an impact on trust, two environmental affordance conditions (favourable vs. unfavourable) were tested because of their well-established impact on wayﬁnding decisions. The results show the participants trusted the robot and followed it to the exit but, although the results favour the robot with eyes, no statistically signiﬁcant differences were found in either environmental affordance. Moreover, despite perceiving the robot as machinelike and artiﬁcial, the majority of the participants felt compelled to follow it, also considering it friendly, kind, pleasant, nice, competent, knowledgeable, responsible, intelligent and sensible. Regardless of the existence of eyes, the service robot tested seems to be a promising solution for emergency egress situations in complex buildings."
"Kumar, Bimal; Dubey, Akash Dutt",Evaluation of trust in robots: A cognitive approach,2017,1,12,12,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were introduced to a Geminoid robot by an assistant, either with or without touch, followed by a trust game and questionnaires.",Participants played a trust game with the robot and completed questionnaires.,Geminoid DK,Humanoid Robots; Telepresence Robots,Research; Social,Game,Economic Game,direct-contact interaction,Participants had physical contact with the robot and an assistant.,real-world,The study was conducted in a real-world office setting with a physical robot.,physical,The robot was a physical android with a high degree of human-like appearance.,wizard of oz (directly controlled),The robot was teleoperated by a human operator.,Behavioral Measures; Questionnaires,,Video Data,"Trust was measured using a trust game and questionnaires, with video observation of interactions.",no modeling,No computational model of trust was used in this study.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The assistant either touched the participant's arm while introducing them to the robot or did not, influencing the interaction medium and the robot's perceived behavior.","Touch significantly increased trust in the trust game, but not in the questionnaire responses.","The study found a significant difference in trust behavior in the trust game between the touch and no-touch conditions, but not in the questionnaire responses, suggesting an unconscious effect of touch on trust.","Touch during the initial interaction with a robot significantly increases trust behavior in a trust game, even if not consciously perceived.","The robot, controlled by a human operator, greeted the participant. The human participant played a trust game with the robot, deciding how much money to invest.",,"The paper mentions that there was a significant increase of trust in the touch condition in the trust game, but does not specify the statistical test used to determine this. The paper also mentions that there was little difference between the touch and no touch condition in the questionnaire results, but again does not specify the statistical test used. Therefore, no specific statistical tests could be identified.",TRUE,Robot-nonverbal-communication,Robot-nonverbal-communication,,"The study manipulated whether the assistant touched the participant's arm while introducing them to the robot. This is classified as 'Robot-nonverbal-communication' because it involves physical contact and guidance, which are nonverbal cues that influence the interaction. The paper states, 'In the touch condition, the assistant would touch the participant on the arm guiding him or her to Geminoid DK. While still touching the participant's arm, the assistant instructed the participant to touch Geminoid DK's hand.' This manipulation was found to significantly impact trust in the trust game, as stated in the results: 'But the results of the trust game show that there is a significant increase of trust in the touch condition.' The questionnaire results did not show a significant difference between the touch and no-touch conditions, indicating that the manipulation did not impact trust as measured by the questionnaire. Therefore, 'Robot-nonverbal-communication' is listed as a factor that impacted trust, and no factors are listed as not impacting trust.",10.1109/ICCCI.2017.8117701,http://ieeexplore.ieee.org/document/8117701/,"The study of Human-Robot interaction faces one of the biggest challenges in measuring the trustworthiness of the robots. The enhancement and the augmentation of the human capabilities using the human robot integration are dependent on the reliability and dependability of the robots. These factors become more significant when the participation of the robot is the human robot integration is active and the cohesion between humans and robots is high. In order to measure the trust and other cognitive parameters of the robot, we have designed trust model in this research paper. This paper evaluates the trust of a customized robot while performing a task using three different algorithms. The algorithms used for the path planning task in this paper are simple artificial neural network; reinforcement based artificial neural network and Situation-Operator Model. The trust model proposed in this paper has been simulated using the results obtained while the robot performed its tasks using the three algorithms. The results show that the trust of the robot increases with each learning cycle thereby indicating that the training of the robot enhances the trust parameter of the robot."
"Drnec, Kim; Metcalfe, Jason S.",Paradigm Development for Identifying and Validating Indicators of Trust in Automation in the Operational Environment of Human Automation Integration,2016,1,72,72,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a driving simulation task with varying levels of automation reliability and type. They were fitted with psychophysiological sensors and completed a training session before the main experiment. The experiment consisted of five conditions: manual driving, full automation with high reliability, full automation with low reliability, speed automation with high reliability, and speed automation with low reliability. Surveys were administered before the experiment and between each condition.","Participants were instructed to drive a simulated vehicle around a two-lane course, maintaining lane position and a safe distance from the lead vehicle. They also had to respond to pedestrians appearing on the road by pressing a button.",Unspecified,Autonomous Vehicles,Research,Navigation,Path Following,minimal interaction,Participants interacted with a driving simulator and had the option to enable or disable the automation.,simulation,Participants used a driving simulator with a 6-degree of freedom ride motion simulator.,simulated,The robot was a simulated driving automation within the driving simulator.,shared control (fixed rules),"The automation had fixed rules for lane and speed control, with the human able to enable or disable it.",Behavioral Measures; Questionnaires,NASA Task Load Index (NASA-TLX); Trust in Automation Scale (TAS),Physiological Signals; Performance Metrics,"Trust was assessed using questionnaires, behavioral measures (time spent using automation), and physiological data.",no modeling,"The study did not use computational modeling of trust, but rather focused on statistical analysis of the collected data.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers manipulated automation reliability (high and low) and automation type (full control and speed control) to influence trust. Task difficulty was also manipulated by varying the frequency of lateral perturbations.,Trust was higher in high reliability conditions compared to low reliability conditions. The speed control automation was more trusted than the full control automation in low reliability conditions. Time spent using the automation reflected the changes in trust.,"The low reliability speed control condition was more trusted than the low reliability full control condition, suggesting that predictability may be more important than reliability in some cases. The study also found that workload was successfully balanced across the speed only conditions.","The study successfully developed an experimental paradigm that can be used to study trust in automation by manipulating automation reliability and workload, and demonstrated that these manipulations had the expected effects on trust levels.","The human participant drove a simulated vehicle, and could choose to engage or disengage the driving automation. The automation controlled either lane and speed, or speed only. The human also had to respond to pedestrians by pressing a button.",Mixed-effects model; ANOVA,"The study used a mixed model to analyze the relationship between subjective ratings of system trustworthiness and automation reliability, with reliability and type as fixed factors and subject data as random. A mixed-model ANOVA was used to confirm the interaction between automation type and reliability on NASA-TLX scores, assessing subjective workload.",TRUE,Robot-accuracy; Robot-autonomy; Task-complexity,Robot-accuracy; Robot-autonomy,Task-complexity,"The researchers manipulated 'Robot-accuracy' by varying the reliability of the driving automation (high and low reliability), which directly impacted the automation's performance in maintaining lane position and speed. This is described in the 'Reliability' section where it states ""The high reliability condition had narrow lane and range offset distributions whereas the distributions in the low reliability automation were broader."" They also manipulated 'Robot-autonomy' by having two automation types: full control (lane and speed) and speed control only, which changed the level of control the automation had over the vehicle. This is described in the 'Experimental Design' section where it states ""The two different automation capabilities were full control, i.e., both lane and range conforming ability, the second only controlled the speed of the vehicle."" The researchers also manipulated 'Task-complexity' by varying the frequency of lateral perturbations, which increased the cognitive load on the participant. This is described in the 'Workload' section where it states ""lateral perturbations were introduced more frequently in the SH as compared with the SL"". The paper states that trust was higher in high reliability conditions compared to low reliability conditions, and that the speed control automation was more trusted than the full control automation in low reliability conditions, indicating that 'Robot-accuracy' and 'Robot-autonomy' impacted trust. The paper also states that workload was successfully balanced across the speed only conditions, indicating that the manipulation of 'Task-complexity' did not impact trust.",,http://link.springer.com/10.1007/978-3-319-39952-2_16,"Calibrated trust in an automation is a key factor supporting full integration of the human user into human automation integrated systems. True integration is a requirement if system performance is to meet expectations. Trust in automation (TiA) has been studied using surveys, but thus far no valid, objective indicators of TiA exist. Further, these studies have been conducted in tightly controlled laboratory environments and therefore do not necessarily translate into real world applications that might improve joint system performance. Through a literature review, constraints on an operational paradigm aimed at developing indicators of TiA were established. Our goal in this paper was to develop an operational paradigm designed to develop valid TiA indicators using methods from human factors and cognitive neuroscience. The operational environment chosen was driving automation because most adults are familiar with the task and its consequent structure and therefore required little training. Initial behavioral and survey data conﬁrm that the design constraints were met. We therefore believe that our paradigm provides a valid means of performing operational experiments aimed at further understanding TiA and its psychophysiological underpinnings."
"Du, Na; Haspiel, Jacob; Zhang, Qiaoning; Tilbury, Dawn; Pradhan, Anuj K.; Yang, X. Jessie; Robert, Lionel P.","Look who’s talking now: Implications of AV’s explanations on driver’s trust, AV preference, anxiety and mental workload",2019,1,32,32,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants experienced four AV explanation conditions in a driving simulator: no explanation, explanation before action, explanation after action, and explanation before action with driver approval. Each condition involved a 6-8 minute drive with three unexpected events. Participants completed questionnaires after each drive.",Participants were asked to experience driving in an automated vehicle under different explanation conditions and to evaluate their experience.,Unspecified,Autonomous Vehicles,Research,Evaluation,Survey/Questionnaire Completion,minimal interaction,"Participants interacted with the automated vehicle in a driving simulator, with limited direct interaction.",simulation,The study used a high-fidelity driving simulator to create an immersive experience.,simulated,The automated vehicle was simulated within the driving simulator environment.,fully autonomous (limited adaptation),"The automated vehicle operated autonomously, but with limited adaptation to unexpected events.",Questionnaires; Custom Scales,Muir's Trust Questionnaire; NASA Task Load Index (NASA-TLX),,Trust was measured using a questionnaire adapted from Muir's Trust Questionnaire and a custom scale for preference and anxiety.,no modeling,"The study did not use computational modeling of trust, relying on statistical analysis of questionnaire data.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The study manipulated the timing of explanations (before or after action) and the degree of autonomy (with or without driver approval) to influence trust.,"Explanations before action led to higher trust and preference compared to no explanation or explanations after action. Lower autonomy (driver approval) did not significantly increase trust ratings, but did increase trust rankings.","The study found that providing explanations before the AV acted was more effective at increasing trust and preference than providing explanations after the AV acted. The study also found that the preference survey was highly correlated with trust ratings and did not have high discriminant validity. The study also found that the lower degree of autonomy condition did not lead to an increase in trust, preference or mental workload, nor did it reduce anxiety, which was unexpected.","Providing explanations before an automated vehicle acts increases trust and preference, while the timing of the explanation is more important than the explanation itself.","The robot (simulated AV) performed driving actions, including responding to unexpected events, while the human participant observed the driving and completed questionnaires after each drive.",ANOVA; Friedman test,"The study used a one-way repeated measures ANOVA to examine the relationship between the four AV explanation conditions (independent variable) and the subjective attitudes (trust, preference, anxiety, and mental workload) of the participants (dependent variables). Post hoc comparisons with Bonferroni correction were used to determine specific differences between conditions. Additionally, the Friedman test, a non-parametric test, was used to analyze the ordinal trust rankings of the four AV explanation conditions. Post hoc comparisons were also conducted for the Friedman test.",TRUE,Robot-verbal-communication-content; Robot-autonomy,Robot-verbal-communication-content,Robot-autonomy,"The study manipulated the timing of explanations provided by the AV (before or after action, or no explanation), which falls under 'Robot-verbal-communication-content' because it changes what information is communicated to the driver. The study also manipulated the degree of autonomy by including a condition where the driver had to approve the AV's action before it was executed, which is categorized as 'Robot-autonomy'. The timing of the explanation ('Robot-verbal-communication-content') was found to impact trust, with explanations before action leading to higher trust. The degree of autonomy ('Robot-autonomy') did not significantly impact trust ratings, although it did impact trust rankings, but this was not considered a significant impact on trust in the study's analysis.",10.1016/j.trc.2019.05.025,https://linkinghub.elsevier.com/retrieve/pii/S0968090X18313640,"Explanations given by automation are often used to promote automation adoption. However, it remains unclear whether explanations promote acceptance of automated vehicles (AVs). In this study, we conducted a within-subject experiment in a driving simulator with 32 participants, using four different conditions. The four conditions included: (1) no explanation, (2) explanation given before or (3) after the AV acted and (4) the option for the driver to approve or disapprove the AV’s action after hearing the explanation. We examined four AV outcomes: trust, preference for AV, anxiety and mental workload. Results suggest that explanations provided before an AV acted were associated with higher trust in and preference for the AV, but there was no difference in anxiety and workload. These results have important implications for the adoption of AVs."
"Du, Na; Yang, X. Jessie; Zhou, Feng",Psychophysiological responses to takeover requests in conditionally automated driving,2020,1,102,102,0,No participants were excluded,Controlled Lab Environment,within-subjects,Participants completed a driving simulation task with automated driving and takeover requests. They performed an N-back task during automated driving and responded to takeover requests. Psychophysiological data was collected throughout the experiment.,"Participants engaged in a simulated driving task with automated driving and takeover scenarios, while also performing a visual N-back memory task.",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a driving simulator and responded to takeover requests.,simulation,The study used a high-fidelity driving simulator to create an immersive experience.,simulated,The robot was a simulated autonomous vehicle within the driving simulator.,shared control (fixed rules),The vehicle operated autonomously but required human takeover at specific points.,Physiological Measures,,Physiological Signals; Eye-tracking Data,"Trust was assessed using physiological measures such as heart rate, GSR, and eye-tracking data.",no modeling,Trust was not modeled computationally in this study.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated cognitive load via the N-back task, traffic density, and takeover request lead time to influence driver states.","The study did not directly measure trust, but the manipulations influenced physiological responses related to workload, attention, and stress.","The study found that shorter takeover lead times led to increased stress responses, and heavy traffic density resulted in more heart rate acceleration patterns. There were no direct measures of trust, but the physiological responses suggest that these factors impact the driver's state.","Drivers showed lower heart rate variability, narrower gaze dispersion, and shorter eyes-on-road time during high cognitive load. Shorter takeover lead times led to inhibited blinks and higher GSR, while heavy traffic increased heart rate acceleration.",The simulated vehicle drove autonomously until a takeover request was issued. The human participant performed an N-back task during autonomous driving and then took over control of the vehicle when prompted.,linear mixed models; pairwise t-tests; Chi-squared; Pearson correlation,"The study used linear mixed models to examine the effects of cognitive load, TOR lead time, traffic density, and their interactions on continuous dependent variables, with subjects treated as random effects. Pairwise t-tests were used to compare psychophysiological measures after TORs second by second when main effects were significant. A chi-squared test was used to examine the dependence of heart rate change patterns with independent variables. Pearson correlation coefficients were used to explore the relationships between emotions, takeover performance, and other physiological data.",TRUE,Task-complexity; Task-environment; Robot-autonomy,,,"The study manipulated 'Task-complexity' by varying the difficulty of the N-back task (1-back vs. 2-back), which directly influenced the cognitive load on the participants. This is explicitly stated in the 'Experimental design' section: 'The cognitive load was manipulated via the difficulty of the NDRTs (low: 1-back memory task; high: 2-back memory task)'. The study also manipulated 'Task-environment' by changing the traffic density (light vs. heavy), as described in the 'Experimental design' section: 'There were respectively 15 and 0 oncoming vehicles per kilometer in heavy and light traffic conditions'. Finally, 'Robot-autonomy' was manipulated through the takeover request lead time (4s vs. 7s), which is also described in the 'Experimental design' section: 'The TOR lead time was 4 or 7 seconds'. While the study did not directly measure trust, these manipulations influenced physiological responses related to workload, attention, and stress, which are all factors that can impact trust. The study did not find any direct impact on trust, but rather on physiological responses. Therefore, there are no factors that impacted or did not impact trust.",,http://arxiv.org/abs/2010.03047,"In SAE Level 3 automated driving, taking over control from automation raises significant safety concerns because drivers out of the vehicle control loop have difficulty negotiating takeover transitions. Existing studies on takeover transitions have focused on drivers' behavioral responses to takeover requests (TORs). As a complement, this exploratory study aimed to examine drivers' psychophysiological responses to TORs as a result of varying non-driving-related tasks (NDRTs), traffic density and TOR lead time. A total number of 102 drivers were recruited and each of them experienced 8 takeover events in a high fidelity fixed-base driving simulator. Drivers' gaze behaviors, heart rate (HR) activities, galvanic skin responses (GSRs), and facial expressions were recorded and analyzed during two stages. First, during the automated driving stage, we found that drivers had lower heart rate variability, narrower horizontal gaze dispersion, and shorter eyes-on-road time when they had a high level of cognitive load relative to a low level of cognitive load. Second, during the takeover transition stage, 4s lead time led to inhibited blink numbers and larger maximum and mean GSR phasic activation compared to 7s lead time, whilst heavy traffic density resulted in increased HR acceleration patterns than light traffic density. Our results showed that psychophysiological measures can indicate specific internal states of drivers, including their workload, emotions, attention, and situation awareness in a continuous, non-invasive and real-time manner. The findings provide additional support for the value of using psychophysiological measures in automated driving and for future applications in driver monitoring systems and adaptive alert systems."
"Du, Huiying; Zhu, Ge; Zheng, Jiali",Why travelers trust and accept self-driving cars: An empirical study,2021,1,200,173,27,27 questionnaires were not valid,Educational Setting,,"Participants completed a questionnaire about their familiarity with and concerns about autonomous vehicles, and their personal information. The questionnaire used a 7-point Likert scale.",Participants completed a questionnaire about their attitudes towards self-driving cars.,Unspecified,Autonomous Vehicles,Other,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about self-driving cars in the questionnaire.,media,The interaction was based on text descriptions of self-driving cars.,hypothetical,The robot was only described hypothetically in the questionnaire.,not autonomous,The robot's actions were only described hypothetically.,Questionnaires,,,Trust was measured using a questionnaire.,"parametric models (e.g., regression)",The study used partial least squares path modeling (PLS-PM) to analyze the structural equation model.,Observational & Survey Studies,Quantitative Surveys,Indirect Manipulation,"The questionnaire introduced self-driving cars as fully autonomous vehicles for personal use, which may have influenced participants' expectations and framed the task.","The study found that mass media indirectly influences trust through self-efficacy and subjective norms, but does not directly influence trust.","The study found that mass media does not directly influence trust, which contradicts some previous research. The study also found that self-efficacy and subjective norms strongly influence trust.","Mass media influences user's attitude and adoption intention through self-efficacy and subjective norms, but not directly.",The robot is described as a fully autonomous vehicle. The human participant completes a questionnaire about their attitudes towards self-driving cars.,Partial least squares; cronbach's alpha; composite reliability (cr); average variance extraction (ave); Heterotrait-monotrait ratio,"The study used Partial Least Squares Path Modeling (PLS-PM) to analyze the structural equation model, examining the relationships between mass media, self-efficacy, subjective norms, trust, and adoption intention. Cronbach's alpha was used to assess the reliability of the measurement scales. Composite reliability (CR) and Average Variance Extraction (AVE) were used to establish convergent validity. Heterogeneity of the correlation (HTMT) was used to assess discriminant validity. The effect size f² was also evaluated to examine the predictor effect in the structural model.",TRUE,Task-environment,Task-environment,,"The study introduced self-driving cars as 'fully autonomous cars serving for personal use' at the beginning of the questionnaire. This framing of the task and the environment in which the participants considered the technology is an implicit manipulation. The study did not manipulate any other factors directly. The study found that mass media indirectly influences trust through self-efficacy and subjective norms, but does not directly influence trust. The framing of the task as a fully autonomous vehicle for personal use is a manipulation of the task environment, which influenced the participants' perceptions and attitudes towards the technology, and thus impacted trust. The study did not manipulate any other factors, so no other factors impacted or did not impact trust.",10.1016/j.tbs.2020.06.012,https://linkinghub.elsevier.com/retrieve/pii/S2214367X20301952,"Automated vehicle technology is becoming increasingly mature with the development of Artiﬁcial Intelligence and information communications technology. It is important to understand the factors aﬀecting the use of automated vehicles. This study investigates user acceptance and the willingness to use fully driverless cars (selfdriving cars). Based on Social Cognitive Theory (SCT), we developed a new acceptance model to explore the impact of mass media on adopting self-driving cars. A survey was designed and distributed, and 173 responded. The results show that 84.4% of the respondents are willing to accept driverless cars. At this early stage, the reports from mass media signiﬁcantly inﬂuence people’s perception of self-driving cars. The media aﬀect selfeﬃcacy and subjective norms, and thereby people’s trust and behavior change. Moreover, subjective norms, selfeﬃcacy, and trust signiﬁcantly inﬂuence their intention to use self-driving cars. This article provides practical guidance to promote self-driving cars: positive media reports will signiﬁcantly enhance people’s trust and intention to use driverless cars."
"Duan, Ding; Li, Yuling; Li, Hongbing",Factors affecting trust in the transparency index for stable and intuitive physical human–robot cooperation,2024,1,1,1,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were asked to guide a 4-DOF robot to follow a target velocity and force, with different admittance gains applied randomly. The subject was trained before the experiment.","Participants were asked to perform two tasks: a reference motion tracking task where they guided the robot to follow a target velocity, and a reference force tracking task where they guided the robot to follow a target force.",Unspecified,Industrial Robot Arms; Collaborative Robots (Cobots),Research,Manipulation,Other Manipulation subtask: The robot was guided by the human to follow a target velocity and force.,direct-contact interaction,Participants physically interacted with the robot by guiding it.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The study used a physical robot for the interaction.,shared control (fixed rules),"The robot's behavior was determined by a fixed admittance control law, with the human providing the guiding force.",Performance-Based Measures,,Performance Metrics,Trust was assessed using performance metrics such as velocity and force tracking accuracy.,no modeling,Trust was not modeled computationally; only descriptive statistics were used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The admittance gain was directly manipulated, which influenced the robot's responsiveness and the difficulty of the tracking task.","The study found that a specific admittance gain (k=12 mm/Ns) resulted in better velocity tracking, suggesting that system transparency is related to the admittance gain.","The study found that higher admittance gains led to increased velocity oscillations and reduced force control, indicating a trade-off between responsiveness and stability. The optimal admittance gain for velocity tracking was found to be k=12 mm/Ns.","The study found that the admittance gain significantly affects the transparency and performance of the HRC system, with an optimal gain for velocity tracking at k=12 mm/Ns.",The robot was a 4-DOF manipulator that moved in response to the human's guiding force. The human participant guided the robot's end-effector to follow a target velocity and force.,,No specific statistical tests were mentioned in the paper. The analysis focused on descriptive statistics and visual comparisons of performance metrics such as velocity and force tracking accuracy under different admittance gain conditions. The study primarily used graphs and qualitative observations to analyze the impact of admittance gain on system performance and transparency.,TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the admittance gain (k) of the robot, which directly affected the robot's responsiveness and performance in tracking the target velocity and force. This manipulation directly influenced the robot's accuracy in performing the task, as different admittance gains resulted in varying levels of velocity and force tracking errors. Specifically, the paper states, 'When the value of the admittance gain was k = 12 mm=Ns... the subject could better achieve the speed tracking of the robot, and at that time, the speed tracking error was minimized.' This indicates that the manipulation of admittance gain directly impacted the robot's performance, which is a key aspect of 'Robot-accuracy'. The study also found that 'the amplitude of the end-effector velocity oscillations during the test was higher under higher admittance gain,' further demonstrating the impact of the manipulation on the robot's performance. The study also states that 'A large performance improvement is evident between admittance gain k = 12 mm=Ns and k = 20 mm=Ns as shown in Figure 16(b) and (c).' This shows that the manipulation of the admittance gain directly impacted the robot's performance, which is a key aspect of 'Robot-accuracy'. The study did not manipulate any other factors from the list provided.",10.1177/01423312231200340,https://journals.sagepub.com/doi/10.1177/01423312231200340,"Safety, stability, and intuitive control are the three most basic requirements for the design of a physical human–robot cooperation (HRC) system. High transparency is one of the main design objectives for dynamically coupled HRC systems. However, determinants of system transparency and its influence on system performance are unachievable with the current methods available. This paper presents an overview of admittance control as a method of physical interaction control between robots and human operators, in particular on the analysis of the influencing factors to the computational transparency index. The influence of frequency, sampling point number, and cutoff frequency of the weighting function on the transparency of the admittance controller were analyzed and experimentally verified. The controller was implemented on a four degree-of-freedom interactive manipulator to verify system transparency and stability. The experimental results validate the proposed framework."
"Duarte, Eduardo Kochenborger; Shiomi, Masahiro; Vinel, Alexey; Cooney, Martin","Trust in Robot Self-Defense: People Would Prefer a Competent, Tele-Operated Robot That Tries to Help <sup>*</sup>",2023,1,180,160,20,20 participants were excluded due to invalid answers with missing data or the same value input for each question,Online Crowdsourcing,mixed design,"Participants read study aims, watched two videos (autonomous and operator, with one of three defense capabilities), and completed the MDMT questionnaire for each video.",Participants watched videos of a robot in a self-defense scenario and rated their trust in the robot.,Unspecified,Humanoid Robots,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants watched videos of robot interactions.,media,Participants watched 3D animation videos of the robot.,simulated,The robot was presented as a 3D animation.,wizard of oz (directly controlled),The robot was either controlled by a human operator or acted autonomously.,Multidimensional Measures; Questionnaires,Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using the Multi-Dimensional Measure of Trust (MDMT) questionnaire.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the robot's control (tele-operated or autonomous) and its defense capability (successful, unsuccessful, or no intervention) to see how it affected trust.","The study found that a tele-operated robot was perceived as more reliable, while an autonomous robot was perceived as more ethical and competent when successful. A robot that tried to help was perceived as more ethical and benevolent.","The study found that autonomous robots were perceived as more ethical and competent than tele-operated robots when successful, which was unexpected. The study also found that a robot that tried to help but failed was perceived as more ethical and benevolent than a robot that did not try.","People found a tele-operated robot to be more reliable, and that attempting to help but failing is more acceptable than just observing. However, autonomous robots were perceived as more ethical and competent when successful.","Participants watched videos of a humanoid robot either being controlled by a human or acting autonomously in a self-defense scenario. The robot either successfully defended a victim, failed to defend, or did not intervene. Participants then completed a questionnaire to rate their trust in the robot.",ANOVA,"A mixed two-way ANOVA was conducted to analyze the effects of control (autonomous or operator) and defense (none, insufficient, or sufficient) factors on the MDMT subscales (reliable, competent, ethical, transparent, and benevolent). The analysis examined main effects and interaction effects of these factors on the trust subscales. Multiple comparisons with the Bonferroni method were used to further investigate significant differences.",TRUE,Robot-autonomy; Robot-accuracy,Robot-autonomy; Robot-accuracy,,"The study explicitly manipulated the robot's control (tele-operated or autonomous), which directly corresponds to 'Robot-autonomy'. The study also manipulated the robot's defense capability (successful, unsuccessful, or no intervention), which directly corresponds to 'Robot-accuracy' as it influences the robot's performance in the self-defense task. The results showed that both factors impacted trust levels. The paper states, 'In the analysis of the reliable subscale, the analysis results showed significant differences in the control factor (F (1, 157) = 6.427, p = 0.012, η 2 = 0.039), and the defense factor (F (2, 157) = 33.322, p < 0.001, η 2 = 0.298)'. This indicates that both the robot's autonomy and its accuracy significantly impacted the reliability subscale of trust. Further, the paper states, 'In the analysis of the competent subscale, the analysis results showed significant differences in the defense factor (F (2, 157) = 31.256, p < 0.001, η 2 = 0.285) and in the interaction effects (F (2, 157) = 3.468, p = 0.034, η 2 = 0.42)'. This shows that the robot's accuracy impacted the competence subscale of trust. The paper also states, 'In the analysis of the ethical subscale, the analysis results showed significant differences in the control factor (F(1, 157) = 9.250, p=0.003, η 2 = 0.056), in the defense factor (F(2, 157)= 16.560, p<0.001, η 2 = 0.174), and in the interaction effects (F(2, 157)= 7.565, p<0.001, η 2 = 0.88)'. This shows that both the robot's autonomy and accuracy impacted the ethical subscale of trust. The paper also states, 'In the analysis of the transparent subscale, the analysis results showed a significant difference in the control factor (F(1, 157) = 5.493, p=0.020, η 2 = 0.034) and the defense factor (F(2, 157)= 17.435, p<0.001, η 2 = 0.182)'. This shows that both the robot's autonomy and accuracy impacted the transparency subscale of trust. Finally, the paper states, 'In the analysis of the benevolent subscale, the analysis results showed a significant difference in the control factor (F(1, 157) = 11.145, p=0.001, η 2 = 0.066) and the defense factor (F(2, 157)= 24.729, p<0.001, η 2 = 0.240)'. This shows that both the robot's autonomy and accuracy impacted the benevolence subscale of trust. Since both factors were found to have a significant impact on trust, they are both included in 'factors_that_impacted_trust'. There were no factors that were manipulated that did not impact trust, so 'factors_that_did_not_impact_trust' is empty.",10.1109/RO-MAN57019.2023.10309336,https://ieeexplore.ieee.org/document/10309336/,"Motivated by the expectation that robot presence at crime scenes will become increasingly prevalent, the question arises of how they can protect humans in their care or vicinity. The current paper delves into the concept of “robot selfdefense” and explores whether a robot should be tele-operated or autonomous, and how humans perceive imperfections in robot performance. To gain insight into how people feel, an online survey was conducted with 180 participants, who watched six videos of a robot defending a victim. The study provides insights into trust in human-robot interactions and sheds light on the complex dynamics involved in robot selfdefense. The results indicate that people found a tele-operated robot to be more accepted, and that attempting to help but failing is more acceptable than just observing."
"Dyck, Leonie; Beierling, Helen; Helmert, Robin; Vollmer, Anna-Lisa",Technical Transparency for Robot Navigation Through AR Visualizations,2023,1,30,30,0,No participants were excluded,Controlled Lab Environment,mixed design,Participants were randomly assigned to two groups. Group 1 saw each navigation twice: first without and then with AR visualizations. Group 2 saw the navigations with AR visualizations once. Participants completed questionnaires after each navigation scenario.,"Participants observed a robot navigating through three different scenarios (obstacle, collision, and cancellation) and answered questions about their understanding, trust, and acceptance of the robot.",Scitos G5,Mobile Robots,Research,Navigation,Path Following,passive observation,Participants observed the robot's navigation through AR visualizations.,real-world,Participants experienced the robot's navigation through augmented reality visualizations.,physical,A physical robot was present in the study.,pre-programmed (non-adaptive),The robot followed a pre-programmed path and reacted to obstacles without adapting to user input.,Questionnaires; Custom Scales,Godspeed Questionnaire,,Trust was measured using questionnaires and custom scales.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Mixed Reality Studies,Direct Manipulation,"The presence or absence of AR visualizations was manipulated to influence understanding and trust. The first group saw the robot without visualizations first, and then with visualizations, while the second group only saw the robot with visualizations.","Trust increased within the group that saw the robot twice, but there were no significant differences between the groups.","The study found a familiarization effect, where repeated exposure to the robot increased understanding and trust, which could not be attributed to the AR visualizations alone. There was a significant increase in anthropomorphism in Group 1_2 compared to Group 1_1, which is unexpected.","The study found that understanding and trust increased within the group that saw the robot twice, but this could be attributed to a familiarization effect rather than the AR visualizations.","The robot navigated through a room, encountering obstacles, collisions, and cancellations. The human participant observed the robot's navigation through AR visualizations and answered questionnaires about their understanding, trust, and acceptance of the robot.",t-test; Aligned Rank Transformation,"The study used one-sided t-tests to compare means within Group 1 (pre and post AR visualization exposure) and between groups (Group 1 and Group 2) for understanding, acceptance (anthropomorphism, likeability, and intelligence), and trust. An Aligned Rank Transformation was used to analyze the effects of group and scenario on understanding, including interaction effects.",TRUE,Robot-interface-design,,Robot-interface-design,"The study manipulated the presence or absence of AR visualizations, which falls under the category of 'Robot-interface-design' as it involves changes to interactive elements (specifically, the AR display). The study found that while understanding and trust increased within the group that saw the robot twice, this could be attributed to a familiarization effect rather than the AR visualizations themselves. Therefore, the 'Robot-interface-design' did not have a direct impact on trust, as the increase was likely due to repeated exposure rather than the visualization itself. The study explicitly states, 'Our results showed that understanding (of behavior and robot architecture) and trust increased within Group 1, but we did not find any significant differences between Group 1_1 and Group 2. A familiarization effect could explain these findings: Participants in Group 1 interacted with the system twice and saw the navigation scenarios twice. This repetition might have increased their understanding of and trust in the robot and its behavior. The visualizations might thus not be responsible for the observed increase in understanding and trust.' This indicates that the manipulation of the interface did not directly impact trust, but rather the repeated exposure did.",10.1145/3568294.3580181,https://dl.acm.org/doi/10.1145/3568294.3580181,"Since robots can facilitate our everyday life by assisting us in basic tasks, they are continuously integrated into our life. However, for a robot to establish itself, a user must accept and trust its doing. As the saying goes, you don’t trust things you don’t understand. Therefore, the base hypothesis of this paper is that providing technical transparency for users can increase understanding of the robot architecture and its behaviors as well as trust and acceptance towards it. In this work, we aim to improve a robot’s understanding, trust, and acceptance by displaying transparent visualizations of its intention and perception in augmented reality. We conducted a user study where robot navigation with certain interruptions was demonstrated to two groups. The first group did not have AR visualizations displayed during the first demonstration; in the second demonstration, the visualizations were shown. The second group had the visualizations displayed throughout only one demonstration. Results showed that understanding increased with AR visualizations when prior knowledge had been gained in previous demonstrations."
"Dzindolet, Mary T.; Peterson, Scott A.; Pomranky, Regina A.; Pierce, Linda G.; Beck, Hall P.",The role of trust in automation reliance,2003,3,15,15,0,No participants were excluded,Controlled Lab Environment,,"Participants viewed 200 slides of Fort Sill terrain and indicated the presence or absence of a camouflaged soldier. They then estimated their and the automated aid's performance, trust, and errors.",Participants were asked to identify the presence or absence of a camouflaged soldier in a series of images.,Unspecified,Other,Research,Evaluation,Image Analysis,minimal interaction,"Participants viewed images and made decisions, with no physical interaction.",media,Participants viewed static images of terrain.,simulated,The robot was a simulated contrast detector.,not autonomous,The robot's decisions were pre-determined and not based on real-time interaction.,Questionnaires,,,Trust was measured using a single item questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,"The study assessed initial trust without manipulating any specific factors, but the instructions influenced human expectations.","Participants showed a positivity bias towards the automated aid, initially deeming it trustworthy.",Participants initially trusted the automated aid and expected it to perform better than themselves.,"People have a positivity bias towards unfamiliar automated decision aids, deeming them trustworthy before interaction.","The human participant viewed images and indicated if a soldier was present, while the simulated robot provided a decision.",t-test; t-test; correlational analyses,"The study used t-tests to compare the mean trust in the automated aid with the scale midpoint and to compare trust in self vs. trust in the aid. A dependent t-test was used to compare the participants' level of trust in their own decisions with the level of trust in the automated aid's decisions. Correlational analyses were used to examine the relationships between trust, expected performance, and expected errors.",FALSE,,,,"Study 1 did not manipulate any factors. The study assessed initial trust without manipulating any specific factors, but the instructions influenced human expectations. The study aimed to explore human operators' level of trust in an automated decision aid prior to interaction with the aid. Therefore, no factors were manipulated.",10.1016/S1071-5819(03)00038-7,https://linkinghub.elsevier.com/retrieve/pii/S1071581903000387,"A recent and dramatic increase in the use of automation has not yielded comparable improvements in performance. Researchers have found human operators often underutilize (disuse) and overly rely on (misuse) automated aids (Parasuraman and Riley, 1997). Three studies were performed with Cameron University students to explore the relationship among automation reliability, trust, and reliance. With the assistance of an automated decision aid, participants viewed slides of Fort Sill terrain and indicated the presence or absence of a camouﬂaged soldier. Results from the three studies indicate that trust is an important factor in understanding automation reliance decisions. Participants initially considered the automated decision aid trustworthy and reliable. After observing the automated aid make errors, participants distrusted even reliable aids, unless an explanation was provided regarding why the aid might err. Knowing why the aid might err increased trust in the decision aid and increased automation reliance, even when the trust was unwarranted. Our studies suggest a need for future research focused on understanding automation use, examining individual differences in automation reliance, and developing valid and reliable self-report measures of trust in automation."
"Dzindolet, Mary T.; Peterson, Scott A.; Pomranky, Regina A.; Pierce, Linda G.; Beck, Hall P.",The role of trust in automation reliance,2003,3,180,180,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants viewed 200 slides and indicated the presence or absence of a soldier. Some participants saw the aid's decision, some received continuous or cumulative feedback, and some received no feedback. They then chose to rely on their or the aid's decisions.","Participants were asked to identify the presence or absence of a camouflaged soldier in a series of images, and then choose to rely on their own or the automated aid's decisions.",Unspecified,Other,Research,Evaluation,Image Analysis,minimal interaction,"Participants viewed images and made decisions, with no physical interaction.",media,Participants viewed static images of terrain.,simulated,The robot was a simulated contrast detector.,not autonomous,The robot's decisions were pre-determined and not based on real-time interaction.,Questionnaires,,,Trust was measured using a single item questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the aid's performance level, the provision of the aid's decisions, and the type of feedback to influence trust.",Participants who viewed the aid's decisions and received continuous feedback were more likely to trust a superior aid. Obvious errors led to distrust.,"Participants showed a bias towards self-reliance, which was attenuated only when they were prevented from viewing the aid's decisions and received continuous feedback.",Preventing participants from seeing the aid's decisions and providing continuous feedback improved appropriate automation reliance.,"The human participant viewed images and indicated if a soldier was present, while the simulated robot provided a decision. Participants then chose to rely on their own or the robot's decisions.",Chi-squared; ANOVA; t-test,"The study used chi-square tests of independence to determine the relationship between experimental conditions and automation reliance decisions. ANOVA was used to examine the effects of aid performance and feedback type on trust and error perception. T-tests were used to compare mean responses to the scale midpoint for trust and error perception, and to compare survey responses between participants who chose self-reliance and those who chose automation reliance.",TRUE,Robot-accuracy; Robot-interface-design,Robot-accuracy,,"Study 2 manipulated the aid's performance level (Robot-accuracy) by having the aid make either half or twice as many errors as the participant. This directly impacted trust, as participants trusted superior aids more. The study also manipulated the provision of feedback (Robot-interface-design) by providing no feedback, cumulative feedback, or continuous feedback. The presence or absence of the aid's decision was also manipulated, which is considered part of the interface design. The feedback type and the presence of the aid's decision did not directly impact trust levels, but they did impact reliance decisions. The study aimed to examine the effects of obvious errors and distorted views of the aid's ability on automation usage decisions.",10.1016/S1071-5819(03)00038-7,https://linkinghub.elsevier.com/retrieve/pii/S1071581903000387,"A recent and dramatic increase in the use of automation has not yielded comparable improvements in performance. Researchers have found human operators often underutilize (disuse) and overly rely on (misuse) automated aids (Parasuraman and Riley, 1997). Three studies were performed with Cameron University students to explore the relationship among automation reliability, trust, and reliance. With the assistance of an automated decision aid, participants viewed slides of Fort Sill terrain and indicated the presence or absence of a camouﬂaged soldier. Results from the three studies indicate that trust is an important factor in understanding automation reliance decisions. Participants initially considered the automated decision aid trustworthy and reliable. After observing the automated aid make errors, participants distrusted even reliable aids, unless an explanation was provided regarding why the aid might err. Knowing why the aid might err increased trust in the decision aid and increased automation reliance, even when the trust was unwarranted. Our studies suggest a need for future research focused on understanding automation use, examining individual differences in automation reliance, and developing valid and reliable self-report measures of trust in automation."
"Dzindolet, Mary T.; Peterson, Scott A.; Pomranky, Regina A.; Pierce, Linda G.; Beck, Hall P.",The role of trust in automation reliance,2003,3,24,24,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants viewed 200 slides and received cumulative feedback. Then, they performed 100 trials where they saw the aid's decision before making their own. They also performed a secondary task. Some participants received a rationale for the aid's errors.","Participants were asked to identify the presence or absence of a camouflaged soldier in a series of images, while also performing a secondary auditory task.",Unspecified,Other,Research,Evaluation,Image Analysis,minimal interaction,"Participants viewed images and made decisions, with no physical interaction.",media,Participants viewed static images of terrain.,simulated,The robot was a simulated contrast detector.,not autonomous,The robot's decisions were pre-determined and not based on real-time interaction.,Questionnaires,,,Trust was measured using a single item questionnaire.,"parametric models (e.g., regression)",Regression analysis was used to determine if trust mediated the relationship between rationale and reliance.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the aid's performance level, provided a rationale for the aid's errors, and added a secondary task to increase workload.","Providing a rationale for the aid's errors increased trust and reliance, even when the aid was inferior. Trust mediated the relationship between rationale and reliance.","Participants were equally likely to rely on the aid when it made both false alarms and misses, even though the rationale focused on false alarms.","Providing a rationale for why an automated aid might err increased trust and automation reliance, even when the aid was inferior.","The human participant viewed images and indicated if a soldier was present, while the simulated robot provided a decision. Participants also listened to a recording and pressed a button when they heard a specific word.",ANOVA; Linear regression,"The study used analysis of variance (ANOVA) to examine the effects of aid performance, rationale provision, and aid decision on error rates. Simple and multiple regression analyses were used to determine if trust mediated the relationship between rationale and reliance.",TRUE,Robot-accuracy; Robot-verbal-communication-content; Task-complexity,Robot-verbal-communication-content; Robot-accuracy,,"Study 3 manipulated the aid's performance level (Robot-accuracy) by having the aid make either half or twice as many errors as the participant. This impacted trust, as superior aids were deemed more trustworthy. The study also manipulated the provision of a rationale for the aid's errors (Robot-verbal-communication-content), which impacted trust and reliance. The study also added a secondary auditory task (Task-complexity) to increase workload, but this did not directly impact trust. The study aimed to explore the effect of providing a rationale for the aid's errors on trust and automation reliance.",10.1016/S1071-5819(03)00038-7,https://linkinghub.elsevier.com/retrieve/pii/S1071581903000387,"A recent and dramatic increase in the use of automation has not yielded comparable improvements in performance. Researchers have found human operators often underutilize (disuse) and overly rely on (misuse) automated aids (Parasuraman and Riley, 1997). Three studies were performed with Cameron University students to explore the relationship among automation reliability, trust, and reliance. With the assistance of an automated decision aid, participants viewed slides of Fort Sill terrain and indicated the presence or absence of a camouﬂaged soldier. Results from the three studies indicate that trust is an important factor in understanding automation reliance decisions. Participants initially considered the automated decision aid trustworthy and reliable. After observing the automated aid make errors, participants distrusted even reliable aids, unless an explanation was provided regarding why the aid might err. Knowing why the aid might err increased trust in the decision aid and increased automation reliance, even when the trust was unwarranted. Our studies suggest a need for future research focused on understanding automation use, examining individual differences in automation reliance, and developing valid and reliable self-report measures of trust in automation."
"Edelmann, Aaron; Stumper, Stefan; Petzoldt, Tibor",Specific Feedback Matters - The Role of Specific Feedback in the Development of Trust in Automated Driving Systems,2019,1,22,22,0,No participants were excluded,Real-World Environment,within-subjects,"Participants drove an Audi A8 and parked in 20 different parking spaces over two sessions. In the first session, they received non-specific feedback when the system aborted. In the second session, they received specific feedback. Trust and system attractiveness were measured at multiple time points.",Participants used an intelligent parking assistant (IPA) to park a car in various parking spaces.,Unspecified,Autonomous Vehicles,Other,Navigation,Other Navigation subtask: The robot autonomously parks the car.,minimal interaction,"Participants interacted with the automated system while driving, but did not have direct physical contact with the robot.",real-world,The study was conducted in a real-world driving environment.,physical,Participants interacted with a physical car equipped with an automated parking system.,shared control (fixed rules),"The robot autonomously controlled the parking maneuver, but the driver could take over at any time.",Questionnaires,Trust in Automation Scale (TAS),,"Trust was measured using a 12-item questionnaire before and after each session, and a single item during the parking maneuvers.",no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the specificity of feedback provided to the driver when the automated parking system aborted, with non-specific feedback in the first session and specific feedback in the second session.","Specific feedback led to an increase in trust over time, while non-specific feedback did not.","Trust did not increase with non-specific feedback, and the power law of learning was observed with specific feedback. The perceived magnitude of errors was higher with non-specific feedback.","Specific feedback is necessary for the development of trust in automated driving systems, while non-specific feedback does not lead to an increase in trust.","The robot autonomously parks the car, while the human monitors the process and takes over when the system aborts. The human also provides feedback on the system's performance and their trust in it.",ANOVA; t-test; Kolmogorov-Smirnov,"The study used a repeated measures ANOVA (rmANOVA) to assess changes in trust and system attractiveness over time within each feedback condition. T-tests were used to compare trust scores after 20 parking maneuvers with trust scores after a two-week follow-up period, and to compare delta scores for trust and perceived magnitude of faults between the non-specific and specific feedback conditions. A Kolmogorov-Smirnov test was used to compare the fit of a least squares model to a linear function.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the specificity of feedback provided to the driver when the automated parking system aborted. In the first session, participants received non-specific feedback ('System abort, driver take-over necessary'), while in the second session, they received specific feedback corresponding to situational factors and system constraints (e.g., 'System abort due to limited maneuver space'). This manipulation directly altered the content of the verbal communication from the robot, making 'Robot-verbal-communication-content' the appropriate category. The results showed that specific feedback led to an increase in trust over time, while non-specific feedback did not, indicating that the manipulated factor, 'Robot-verbal-communication-content', impacted trust. There were no other factors manipulated in the study.",10.1109/IVS.2019.8814126,https://ieeexplore.ieee.org/document/8814126/,"In order to beneﬁt from the potential of automated driving, the systems have to be used appropriately by drivers. Usage is in turn dependent on trust, which is, among other things, inﬂuenced by system knowledge and experience. One way of conveying system knowledge is by presenting feedback of limitations and constraints. The present study investigates the inﬂuence of speciﬁc as opposed to non-speciﬁc feedback on the evolution of trust with an intelligent parking assistant (IPA). The research focused on the question how the trust development can be described in a level 2 automation system and how it differs between feedback speciﬁcity levels. Therefore, 22 participants took part in an on-road longitudinal study, in which they performed a total of forty parking maneuvers while using the IPA. During the ﬁrst twenty parking maneuvers no speciﬁc feedback was presented in case of a fault due to system limitations, whereas during the second twenty parking maneuvers speciﬁc feedback corresponding to situational factors and system constraints was given. The results show that non-speciﬁc feedback does not lead to an increase of trust at all. The errors that occur due to system and sensor limitations remain inexplicable and thus unpredictable. On the other hand, speciﬁc feedback that corresponds to the situation leads to the formation and stabilization of trust in accordance with the power law of learning. Therefore, it is advisable that users ideally receive speciﬁc feedback in order to gain knowledge of a system’s limitations. This promotes the development of trust and ensures appropriate system use."
"Edmonds, Mark; Gao, Feng; Liu, Hangxin; Xie, Xu; Qi, Siyuan; Rothrock, Brandon; Zhu, Yixin; Wu, Ying Nian; Lu, Hongjing; Zhu, Song-Chun",A tale of two explanations: Enhancing human trust by explaining robot behavior,2019,1,163,150,13,13 participants were removed for failing to understand the haptic display panel,Controlled Lab Environment,between-subjects,"Participants were randomly assigned to one of five groups. During a familiarization phase, all groups viewed RGB videos of a robot opening a medicine bottle, and some groups also viewed an explanation panel. In a prediction phase, all groups viewed only RGB videos and predicted the robot's next action.",Participants observed a robot attempting to open a medicine bottle and then rated their trust in the robot's ability to perform the task. They also predicted the robot's next action in a new execution.,Baxter,Humanoid Robots; Collaborative Robots,Research,Manipulation,Object Assembly,passive observation,Participants observed videos of the robot performing a task.,media,Participants viewed videos of the robot performing the task.,physical,Participants observed videos of a physical robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions.,Questionnaires; Performance-Based Measures,Trust in Automated Systems Scale,Video Data; Performance Metrics,Trust was measured using a questionnaire and by assessing the accuracy of participants' predictions of the robot's actions.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The type of explanation provided to participants was manipulated, including no explanation, symbolic, haptic, combined, or text-based explanations, to influence trust.",Providing symbolic explanations or combined symbolic and haptic explanations increased trust compared to no explanation or text-based explanations. Haptic explanations alone did not significantly impact trust.,The study found that explanations that are best suited to foster trust do not necessarily correspond to the model components contributing to the best task performance. The GEP group showed the highest trust ratings and prediction accuracy.,Comprehensive and real-time visualizations of the robot's internal decisions were more effective in promoting human trust than explanations based on summary text descriptions.,The robot attempted to open a medicine bottle using a learned manipulation strategy. Human participants observed the robot's actions and provided trust ratings and action predictions.,ANOVA; t-test; Chi-squared,"The study used ANOVA to determine if there was a significant main effect of the different explanation groups on trust ratings and prediction accuracy. Independent samples t-tests were then used to compare specific group differences in trust ratings and prediction accuracy. A chi-squared test was used to determine if the performance of different models (GEP, symbolic, and haptic) in opening the medicine bottles were statistically different.",TRUE,Robot-verbal-communication-content; Robot-interface-design,Robot-verbal-communication-content,Robot-interface-design,"The study manipulated the type of explanation provided to participants, which falls under 'Robot-verbal-communication-content'. The explanations included no explanation, symbolic, haptic, combined (GEP), or text-based explanations. This manipulation directly altered the information communicated to the participants about the robot's actions and decision-making process. The study also manipulated the presence and type of visual display of the explanation, which falls under 'Robot-interface-design'. The different explanation panels (symbolic, haptic, combined, text) represent different interface designs. The study found that the content of the explanation (symbolic and combined) significantly impacted trust, while the haptic explanation did not significantly impact trust compared to the baseline. The text explanation also did not significantly impact trust compared to the symbolic or combined explanations. Therefore, 'Robot-verbal-communication-content' impacted trust, while 'Robot-interface-design' did not have a significant impact on trust, as the haptic explanation was also a visual display but did not impact trust.",10.1126/scirobotics.aay4663,https://robotics.sciencemag.org/lookup/doi/10.1126/scirobotics.aay4663,"The ability to provide comprehensive explanations of chosen actions is a hallmark of intelligence. Lack of this ability impedes the general acceptance of AI and robot systems in critical tasks. This paper examines what forms of explanations best foster human trust in machines and proposes a framework in which explanations are generated from both functional and mechanistic perspectives. The robot system learns from human demonstrations to open medicine bottles using (i) an embodied haptic prediction model to extract knowledge from sensory feedback, (ii) a stochastic grammar model induced to capture the compositional structure of a multistep task, and (iii) an improved Earley parsing algorithm to jointly leverage both the haptic and grammar models. The robot system not only shows the ability to learn from human demonstrators but also succeeds in opening new, unseen bottles. Using different forms of explanations generated by the robot system, we conducted a psychological experiment to examine what forms of explanations best foster human trust in the robot. We found that comprehensive and real-time visualizations of the robot’s internal decisions were more effective in promoting human trust than explanations based on summary text descriptions. In addition, forms of explanation that are best suited to foster trust do not necessarily correspond to the model components contributing to the best task performance. This divergence shows a need for the robotics community to integrate model components to enhance both task execution and human trust in machines."
"El Jouhri, Aboubakr; El Sharkawy, Ashraf; Paksoy, Hakan; Youssif, Omar; He, Xiaolin; Kim, Soyeon; Happee, Riender",The influence of a color themed HMI on trust and take-over performance in automated vehicles,2023,1,45,45,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were divided into three groups, each using a different HMI (baseline, red-themed, and blue-themed). They completed a driving simulator scenario with four take-over requests (TORs) while performing a non-driving related task (NDRT). Questionnaires were administered before and after the simulation, and questions were asked after each TOR.","Participants were asked to drive in a simulator with automated driving, engage in a non-driving related task, and take over control of the vehicle when prompted by a take-over request (TOR).",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the automated vehicle through a driving simulator and responded to take-over requests.,simulation,"The interaction took place in a driving simulator, providing a virtual environment for the driving task.",simulated,The automated vehicle was simulated within the driving simulator environment.,shared control (fixed rules),The automated vehicle operated autonomously but requested human intervention at specific points based on pre-defined rules.,Questionnaires; Performance-Based Measures,,Performance Metrics,Trust was assessed using pre- and post-drive questionnaires and performance metrics such as reaction time and take-over success rate.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the HMI by using different colors (red and blue) for ambient lighting and display, and compared it to a baseline auditory HMI. The color was intended to influence the perceived urgency and thus the reaction time.","The color-themed HMIs increased trust in automation compared to the baseline HMI. Red ambient lighting was perceived as more urgent, but the color did not significantly affect overall trust or take-over performance.","The red-themed HMI led to the shortest reaction times, but the difference between red and blue was not statistically significant. The automation intervened in a majority of take-over requests, indicating that the time window for human intervention was short.",A color-themed HMI with ambient lighting and an informative display significantly increased trust in automated vehicles and reduced take-over reaction time compared to a baseline auditory HMI.,"The robot (automated vehicle) drove autonomously until a take-over request was triggered. The human participant was required to monitor the driving environment, engage in a non-driving related task, and take over control of the vehicle by braking when prompted by the TOR.",t-test; t-test,"The study used two-sample t-tests to compare the baseline group with the two color-themed HMI groups combined (p BC value) and to compare the red group with the blue group (p RB value). These tests were used to evaluate the differences in trust, HMI experience, take-over perception, and take-over performance between the different HMI conditions. Specifically, the p BC value assessed the difference between the simple auditory HMI and the more advanced color-themed HMIs, while the p RB value assessed the effect of HMI color (red vs. blue).",TRUE,Robot-interface-design,Robot-interface-design,,"The study manipulated the Human-Machine Interface (HMI) by using different colors (red and blue) for ambient lighting and display, and compared it to a baseline auditory HMI. This falls under 'Robot-interface-design' because it involves changes to interactive elements of the system, specifically the visual display and ambient lighting. The paper states, 'The HMI of the baseline group consisted of only an auditory interface, in which no ambient lighting or display was shown during a TOR. The HMI of the color themed groups consisted of the same auditory and a visual interface as shown in Table 1 and Figures 2A, B.' The results show that the color-themed HMIs significantly increased trust compared to the baseline HMI, as stated in the abstract: 'Compared to the baseline HMI, the color themed HMI is more trustworthy'. Therefore, 'Robot-interface-design' is listed as a factor that impacted trust. The study also found that while red was perceived as more urgent, the color of the HMI (red vs. blue) did not significantly affect overall trust or take-over performance, as stated in the results: 'However in G4-G9 no significant differences are found between the blue themed and red themed HMI.' Therefore, no factors are listed as not impacting trust.",10.3389/fpsyg.2023.1128285,https://www.frontiersin.org/articles/10.3389/fpsyg.2023.1128285/full,"Introduction               SAE Level 3 is known as conditional driving automation. As long as certain conditions are met, there is no need to supervise the technology and the driver can engage in non-driving related tasks (NDRTs). However, a human driver must be present and alert to take over when the automation is facing its system limits. When such an emergency takes place, the automation uses the human machine interface (HMI) to send a take-over request (TOR) to the driver.                                         Methods               We investigated the influence of a color themed HMI on the trust and take-over performance in automated vehicles. Using a driving simulator, we tested 45 participants divided in three groups with a baseline auditory HMI and two advanced color themed HMIs consisting of a display and ambient lighting with the colors red and blue. Trust in automation was assessed using questionnaires while take-over performance was assessed through response time and success rate.                                         Results               Compared to the baseline HMI, the color themed HMI is more trustworthy, and participants understood their driving tasks better. Results show that the color themed HMI is perceived as more pleasant compared to the baseline HMI and leads to shorter reaction times. Red ambient lighting is seen as more urging than blue, but HMI color did not significantly affect the general HMI perception and TOR performance.                                         Discussion               Further research can explore the use of color and other modalities to express varying urgency levels and validate findings in complex on road driving conditions."
"Elias, Alex; Galvez Trigo, Maria Jose",Unveiling Trust Dynamics with a Mobile Service Robot: Exploring Various Interaction Styles for an Agricultural Task,2024,1,30,30,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants completed pre-interaction questionnaires, then performed a manipulation task with a robot using three different interaction styles, and then completed post-interaction questionnaires, followed by a semi-structured interview.",Participants used a mobile manipulator robot to move a piece of fruit from one location to another using three different interaction styles.,TIAGo,Mobile Manipulators,Research,Manipulation,Object Passing,direct-contact interaction,Participants physically interacted with the robot during the task.,real-world,The interaction took place in a real-world lab setting with a physical robot.,physical,The study used a physical robot for the interaction.,shared control (fixed rules),"The robot's actions were controlled by the participant using different interaction styles, including pre-programmed movements, learning by demonstration, and joystick control.",Questionnaires,Negative Attitude towards Robots Scale (NARS); Godspeed Questionnaire,,Trust was measured using pre- and post-interaction questionnaires.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the interaction style with the robot, including fully programmed, learning by demonstration, and controller, and also included the use of TTS for some participants, which influenced the interaction medium and the robot's behavior.","Interacting with the robot increased trust levels, with Learning by Demonstration being the most trusted interaction style. The use of TTS also increased trust improvement scores.","Older participants showed a higher trust improvement score after interacting with the robot, and participants with no prior experience with robotics showed a larger improvement in trust levels than those with prior experience.","Interacting with the robot, particularly through Learning by Demonstration, significantly increased trust levels, especially for those without prior robotics experience, and the use of TTS further improved trust.","The robot moved a piece of fruit from one table to another. The human either observed the robot, guided the robot's arm movements through kinesthetic teaching, or controlled the robot's arm using a joystick.",t-test; Chi-squared; Chi-squared; ANOVA; cross-tabulation chi-square test,A paired sample t-test was used to compare trust levels before and after interaction with the robot. A Chi-Square test was used to determine which of the three interaction styles participants trusted most. A chi-square test of association was conducted to examine the relationship between interaction types and trust. A One-Way ANOVA was used to determine if age had an effect on the overall improvement score of trust. A Cross-Tabulation Chi-Square test was used to determine if there were any correlations between the ages of the participants and which interaction they trusted more.,TRUE,Robot-autonomy; Robot-verbal-communication-content,Robot-autonomy; Robot-verbal-communication-content,,"The study manipulated the level of control participants had over the robot, which directly relates to 'Robot-autonomy'. The three interaction styles (fully programmed, learning by demonstration, and controller) represent different levels of autonomy given to the robot. The study also manipulated the presence of text-to-speech (TTS) which is a form of 'Robot-verbal-communication-content', as it changed what the robot communicated to the participants. The results showed that both the interaction style (autonomy) and the presence of TTS (verbal communication) impacted trust levels. The paper states, 'The various types of interactions also contributed to the assigned level of trust, with Learning by Demonstration determined to be the most trusted interaction' and 'the improvement score on trust levels was significantly higher for those participants the robot communicated with using TTS'. There were no factors that were manipulated that did not impact trust.",10.1109/RO-MAN60168.2024.10731301,https://ieeexplore.ieee.org/document/10731301/,"As robotics, particularly in agriculture, become more prevalent, understanding the role that different factors play on the trust levels that users have in these robots becomes crucial to facilitate their adoption and integration into the industry. In this paper we present the results of a withinsubjects study that included between-subject factors exploring how prior experience with robotics and different interaction styles with a mobile manipulator robot may affect trust levels in said robot before and after the completion of an agriculturerelated manipulation task. The results show that interacting with the robot helps improve trust levels, particularly for those without prior experience with robotics, who present a higher trust improvement score, and that an interaction style involving physical human-robot interaction (pHRI), more specifically Learning by Demonstration, was favoured versus less direct interaction styles. We found that incorporating Text-to-Speech (TTS) can be a good design choice when trying to improve trust, and that the improvement score for trust before and after interaction with the robot was significantly higher for older age groups, with these participants being more conservative with their reported trust level before the interaction. Overall, these results offer insights into different interaction styles and their effect on trust levels for an agriculture-related manipulation task, and open the door to future work exploring further interaction styles and task variations."
"Elson, J S; Derrick, Douglas C; Ligon, Gina S",Trusting a Humanoid Robot: Exploring Personality and Trusting Effects in a Human-robot Partnership,2021,1,58,58,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a personality assessment, then performed two survival simulations with a robot partner, with trust and perception assessments after each simulation.","Participants completed two survival simulations, ranking items individually and then collaboratively with a robot partner.",Unspecified,Humanoid Robots,Research,Evaluation,Ranking,minimal interaction,"Participants interacted with a robot partner through a computer interface, with limited verbal interaction.",simulation,The interaction was conducted through a web application simulating a survival scenario.,physical,"Participants interacted with a physical humanoid robot, but the interaction was mediated through a computer interface.",wizard of oz (directly controlled),The robot's responses were manually activated by a researcher using a Wizard of Oz methodology.,Questionnaires,Empirically Derived (ED) Scale,Eye-tracking Data,Trust was measured using a questionnaire and eye-tracking data was collected to measure system utilization.,"parametric models (e.g., regression)","Regression analysis was used to model the relationship between personality, utilization, and trust.",Empirical HRI Studies,Wizard-of-Oz Studies,Indirect Manipulation,"The task was framed as a high-stakes survival simulation with a risk of losing participation credit, influencing participants' expectations and potentially their trust.",Higher openness scores and greater utilization of the robot's solutions were associated with higher trust.,"The study found that openness was a significant predictor of trust, which contrasts with prior research that focused on extraversion. The study also found a correlation between utilization and trust, measured through eye-tracking.",Individuals scoring high in openness may be more trusting of humanoid robots than individuals low in openness.,"The robot provided solutions to survival scenarios, and the human ranked items based on their importance for survival, using the robot's solutions as a reference.",Pearson correlation; Linear regression,"The study used correlations to examine the relationships between the Big Five personality traits, utilization of the robot's solutions, and trust. A regression analysis was then performed to determine the amount of variance in trust accounted for by openness scores and utilization, and to identify the predictive power of each variable on trust.",TRUE,Task-constraints,Task-constraints,,"The study explicitly states that participants were informed that they would need to rank 75% or more of their items correctly (as compared to the expert's ranking) or they would not receive participation credit for the study. This introduces a performance limit and a risk of losing participation credit, which is a manipulation of task constraints. The paper also states that the risk and uncertainty are essential components for trust and that the inclusion of real individual risk (loss of participation credit) was an important aspect of the study. The results showed that higher utilization and openness scores were associated with higher trust. The task constraint of potential loss of credit was a factor that impacted trust by influencing the participants' engagement with the task and their reliance on the robot's solutions. There were no other factors manipulated in the study.",,,"Research on trust between humans and machines has primarily investigated factors relating to environmental or system characteristics, largely neglecting individual differences that play an important role in human behavior and cognition. This study examines the role of the Big Five personality traits on trust in a partnership between a human user and a humanoid robot. A Wizard of Oz methodology was used in an experiment to simulate an artificially intelligent robot that could be leveraged as a partner to complete a life or death survival simulation. Eye-tracking was employed to measure system utilization and validated psychometric instruments were used to measure trust and personality traits. Results suggest that individuals scoring high on the openness personality trait may have greater trust in a humanoid robot partner than those with low scores in the openness personality dimension."
"Emaminejad, Newsha; Kath, Lisa; Akhavian, Reza",Assessing Trust in Construction AI-Powered Collaborative Robots Using Structural Equation Modeling,2024,1,600,600,0,No participants were excluded,Online Crowdsourcing,,"Participants completed an online survey after watching a video of construction cobots. The survey included demographic questions, Likert-scale questions about trust factors, and rank-order questions about preferences. Participants also rated attributes of seven different cobot applications.","Participants completed a survey about their perceptions of trust in construction cobots, including rating various attributes and ranking preferences.",Unspecified,Collaborative Robots (Cobots),Industrial,Evaluation,Survey/Questionnaire Completion,passive observation,Participants watched a video of cobots and then completed a survey.,media,Participants watched a video of cobots in action.,simulated,Participants viewed videos of robots.,not autonomous,"The robots were shown in a video, but there was no interaction with the robots.",Questionnaires; Custom Scales,,,Trust was assessed using Likert-scale questions and rank-order questions in a survey.,"parametric models (e.g., regression)",Structural equation modeling (SEM) was used to analyze the relationships between trust factors.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study did not manipulate any factors; it assessed trust through a survey.,,"The study found that safety and reliability are significant factors for the adoption of AI-powered cobots in construction. The study also found that transparency of cobots' inner workings can benefit accuracy, robustness, security, privacy, and communication and result in higher levels of automation, all of which demonstrated as contributors to trust. The study also found that direct experience and hands-on involvement with cobots positively influenced trust.","The study's key finding is that perceived safety and reliability of AI-powered cobots are crucial for establishing trust among construction workers, and that training and robustness are key contributors to safety perceptions.","Participants watched a video of construction cobots and then completed a survey about their perceptions of trust in these robots. The survey included questions about various factors influencing trust, and participants also ranked their preferences for different training methods and cobot attributes.",Structural equation modeling; Multilevel Model; Shapiro-Wilk,"The study employed Structural Equation Modeling (SEM) to examine the complex relationships among various factors influencing trust in construction cobots. SEM was used to test the hypothesized relationships between latent and observed variables, assessing the strength and direction of these relationships. Confirmatory Factor Analysis (CFA) was performed to evaluate the validity of the measurement constructs in the model, ensuring that the observed variables effectively measured the intended latent variables. Shapiro-Wilk tests were conducted to assess the normality of converted categorical variables used in the SEM analysis.",FALSE,,,,"The study did not manipulate any factors. Participants watched a video of cobots and then completed a survey. The survey included questions about various factors influencing trust, and participants also ranked their preferences for different training methods and cobot attributes. The study aimed to assess the relationships between various factors and trust, but it did not involve any intentional manipulation of these factors by the researchers. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust due to manipulation.",10.1061/JCCEE5.CPENG-5660,https://ascelibrary.org/doi/10.1061/JCCEE5.CPENG-5660,"This study aimed to investigate the key technical and psychological factors that impact the architecture, engineering, and construction (AEC) professionals’ trust in collaborative robots (cobots) powered by artificial intelligence (AI). This study seeks to address the critical knowledge gaps surrounding the establishment and reinforcement of trust among AEC professionals in their collaboration with AI-powered cobots. In the context of the construction industry, where the complexities of tasks often necessitate human–robot teamwork, understanding the technical and psychological factors influencing trust is paramount. Such trust dynamics play a pivotal role in determining the effectiveness of human–robot collaboration on construction sites. This research employed a nationwide survey of 600 AEC industry practitioners to shed light on these influential factors, providing valuable insights to calibrate trust levels and facilitate the seamless integration of AI-powered cobots into the AEC industry. Additionally, it aimed to gather insights into opportunities for promoting the adoption, cultivation, and training of a skilled workforce to effectively leverage this technology. A structural equation modeling (SEM) analysis revealed that safety and reliability are significant factors for the adoption of AI-powered cobots in construction. Fear of being replaced resulting from the use of cobots can have a substantial effect on the mental health of the affected workers. A lower error rate in jobs involving cobots, safety measurements, and security of data collected by cobots from jobsites significantly impact reliability, and the transparency of cobots’ inner workings can benefit accuracy, robustness, security, privacy, and communication and result in higher levels of automation, all of which demonstrated as contributors to trust. The study’s findings provide critical insights into the perceptions and experiences of AEC professionals toward adoption of cobots in construction and help project teams determine the adoption approach that aligns with the company’s goals workers’ welfare. DOI: 10.1061/JCCEE5.CPENG-5660. © 2024 American Society of Civil Engineers."
"Esterwood, Connor; Robert, Lionel P",Do You Still Trust Me? Human-Robot Trust Repair Strategies,2021,1,164,164,37,37 participants were excluded for failing attention check questions,Online Crowdsourcing,between-subjects,"Participants completed a training scenario, then a pretest questionnaire, then were assigned to an experimental condition where they interacted with a robot that made mistakes and provided a repair strategy, and finally completed a post-test questionnaire.","Participants worked with a robot to load boxes onto a conveyor belt, with the robot making mistakes and providing a repair strategy.",Unspecified,Humanoid Robots; Industrial Robot Arms,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated robot through a computer interface.,simulation,The interaction took place in a simulated factory environment.,simulated,The robots were presented as virtual representations.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires,,,Trust was measured using a post-test questionnaire.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the robot's anthropomorphism and the type of repair strategy used after a mistake, which was intended to influence trust.","Explanations were more effective at repairing integrity than apologies, denials, or promises. Promises led to higher benevolence than explanations or denials. Anthropomorphism interacted with repair strategy, with explanations being more effective for integrity when given by an anthropomorphic robot.","Explanations were most effective for integrity when given by an anthropomorphic robot, while promises led to the highest integrity when given by a mechanoid robot. Promises given by an anthropomorphic robot led to the lowest integrity. Benevolence was highest for explanations given by an anthropomorphic robot, and for promises given by a mechanoid robot.","Explanations were more effective at repairing integrity than apologies, denials, or promises, and the effectiveness of repair strategies varied based on the robot's anthropomorphism.","The robot picked up boxes and presented them to the human, who approved or rejected them. The robot made mistakes by presenting the wrong box, and then provided a repair strategy. The human's role was to review the boxes and ensure they were correct.",ANOVA; post-hoc investigation,"The study used ANOVA to examine the main effects of repair strategy on the robot's integrity and benevolence, and to examine the interaction effects between anthropomorphism and repair strategy on integrity and benevolence. Post-hoc investigations were conducted to further explore significant main and interaction effects, including comparisons of means between different repair strategies.",TRUE,Robot-verbal-communication-content; Robot-aesthetics,Robot-verbal-communication-content; Robot-aesthetics,,"The study manipulated the content of the robot's verbal communication by varying the repair strategy (apology, denial, explanation, or promise) after a mistake. This is classified as 'Robot-verbal-communication-content' because the core manipulation is the specific message conveyed by the robot. The study also manipulated the robot's appearance, using either a high-anthropomorphism (Pepper-like) or low-anthropomorphism (generic robotic arm) robot. This is classified as 'Robot-aesthetics' because it is a change in the visual appearance of the robot. The results showed that both the repair strategy (verbal communication content) and the robot's anthropomorphism (aesthetics) impacted the participants' perception of the robot's integrity and benevolence, which are components of trust. There were no factors that were manipulated that did not impact trust.",,,"Trust is vital to promoting human and robot collaboration, but like human teammates, robots make mistakes that undermine trust. As a result, a human’s perception of his or her robot teammate’s trustworthiness can dramatically decrease [1], [2], [3], [4]. Trustworthiness consists of three distinct dimensions: ability (i.e. competency), benevolence (i.e. concern for the trustor) and integrity (i.e. honesty) [5], [6]. Taken together, decreases in trustworthiness decreases trust in the robot [7]. To address this, we conducted a 2 (high vs. low anthropomorphism) x 4 (trust repair strategies) between-subjects experiment. Preliminary results of the ﬁrst 164 participants (between 19 and 24 per cell) highlight which repair strategies are effective relative to ability, integrity and benevolence and the robot’s anthropomorphism. Overall, this paper contributes to the HRI trust repair literature."
"Esterwood, Connor; Ali, Arsha; George, Zariq; Dubrow, Samantha; Smereka, Jonathon; Riegner, Kayla; Tilbury, Dawn; Jr, Lionel P Robert",Promises and Trust Repair in UGVs,2023,1,20,20,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were trained on the virtual environment and then completed a pre-test survey. During the main study, participants encountered 6 takeover requests and 2 trust violations followed by either a promise or no repair. Participants then completed a post-test questionnaire.",Participants monitored two UGVs and performed a secondary task of moving a box to collide with falling shapes.,Unspecified,Unmanned Ground Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the UGVs through a virtual interface.,simulation,The study used an immersive virtual environment.,simulated,The UGVs were presented as virtual entities in the simulation.,shared control (fixed rules),The UGVs operated autonomously but required human takeover at specific points.,Questionnaires,Mayer and Davis' Trust/Trustworthiness Scales (1999),,Trustworthiness was measured using a post-test questionnaire.,"parametric models (e.g., regression)",Generalized linear models were used to analyze the impact of promises on trustworthiness.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The UGVs either made a promise after a trust violation or remained silent, influencing expectations and behavior.",Promises had a marginally significant positive effect on overall trustworthiness and specifically on benevolence.,"The study found that promises were effective in repairing benevolence but not ability or integrity, which is a notable trend in trust repair.",Promises had a marginally significant positive impact on overall trustworthiness and specifically on benevolence after a trust violation.,"The UGVs autonomously navigated a virtual environment, and the human monitored their status and took over control when requested. The human also performed a secondary task of moving a box to collide with falling shapes.",generalized linear models (glm),"Generalized linear models (GLM) were used to examine the impact of the experimental condition (promise vs. no repair) on trustworthiness, and its sub-components (ability, integrity, and benevolence). Covariates such as trust propensity, pre-study trustworthiness in robots, prior experience with AVs, and familiarity with AVs were included in the models. The purpose was to determine the overall effect of promises on trustworthiness and to isolate the individual effects of promises on ability, integrity, and benevolence.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated whether the UGV provided a promise ('I promise I'll ask for help when I need it next time') after a trust violation or remained silent. This is a manipulation of the content of the robot's verbal communication, specifically a promise, which directly relates to the 'Robot-verbal-communication-content' category. The results showed that this manipulation had a marginally significant impact on overall trustworthiness and specifically on benevolence, indicating that the 'Robot-verbal-communication-content' factor influenced trust. There were no other factors manipulated in the study, and no other factors were found to have an impact on trust.",,,"Unmanned ground vehicles (UGVs) are autonomous robots capable of performing tasks through selfnavigation and decision-making. They have the potential to replace humans in dangerous driving scenarios. However, UGVs must be viewed as trustworthy to be accepted, and like any automation, they can make mistakes that decrease human trust in them. Trust repair strategies can mitigate the consequences of trust violations, but they are not always effective. To better understand their effectiveness on UGVs, we designed a between-subjects study examining promises on a UGV’s trustworthiness. Preliminary results showed that promises had a marginal impact on overall trustworthiness but were influential in repairing benevolence but not ability or integrity. These findings have implications for the design of UGV’s and trust repair theory."
"Esterwood, Connor; Robert, Lionel P.",The theory of mind and human–robot trust repair,2023,1,696,400,296,296 subjects were excluded due to incorrect responses to attention-check questions,Online Crowdsourcing,between-subjects,"Participants were trained in a virtual environment, completed a pre-test survey, and then performed a box-sorting task with a virtual robot that made errors and used trust repair strategies. Participants completed trust questionnaires at six points during the study.","Participants worked with a virtual robot to sort and load boxes, acting as a 'checker' while the robot acted as a 'picker'.",Unspecified,Other,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a virtual robot in a simulated environment.,simulation,The study used an immersive virtual environment to simulate the interaction.,simulated,The robot was a virtual representation within the simulation.,pre-programmed (non-adaptive),"The robot followed a pre-programmed sequence of actions, including making errors and using trust repair strategies.",Questionnaires,,,Trust was measured using a 3-item questionnaire at six points during the study.,"parametric models (e.g., regression)",The study used mixed linear effects models to analyze the impact of trust repair strategies and mind perception on trust change.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's performance was manipulated by having it make errors, and its behavior was manipulated by using different trust repair strategies (apologies, denials, promises). The human's expectations were influenced by the robot's actions and statements.",Apologies and denials were more effective at repairing trust when participants perceived the robot as having a higher degree of conscious experience. Promises were not significantly impacted by the perception of conscious experience.,"The study found that the effectiveness of apologies and denials was moderated by the perception of the robot's conscious experience, while promises were not. The study also found that trust repair strategies remained effective even after multiple trust violations when conscious experience was high.",Perceptions of a robot's conscious experience moderate the effectiveness of apologies and denials in human-robot interaction.,The robot picked boxes from a stack and presented them to the human. The human checked if the box was correct and approved or rejected it. The robot made errors and used trust repair strategies after each error.,pairwise t tests; mixed linear effects models; likelihood ratio test; Simple slopes analysis; pairwise comparison of slopes,"The study used pairwise t-tests with a Bonferroni adjustment to conduct a manipulation check, comparing trust change between the no-error and no-repair conditions. Mixed linear effects models were used to analyze the impact of trust repair strategies and mind perception on trust change, with a likelihood ratio test to compare nested models and select the best fit. Simple slopes analysis was used to probe the interaction between conscious experience and repair strategy, and pairwise comparisons of slopes were conducted to compare the effectiveness of different repair strategies.",TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-verbal-communication-content,,"The study manipulated the robot's verbal communication content by having it use different trust repair strategies (apologies, denials, promises) after making errors. This is explicitly stated in the paper: 'The experimental conditions differed by repair strategy where the robot deployed either apologies, promises, or denials after each time it provided an incorrect box to a participant'. The robot's accuracy was also manipulated by having it make errors (picking the wrong box) at specific points in the task, which is described as 'the robot picking the wrong box at three evenly distributed trust violation events (box 3, box 6, and box 9)'. The study found that the content of the robot's verbal communication (apologies and denials) impacted trust, as their effectiveness was moderated by the perception of the robot's conscious experience. This is stated in the results: 'a significant two-way interaction effect between apologies and perceived conscious experience ( p = 0.03 ) was also observed' and 'apologies and denials appear to be more effective when subjects ascribed the robot greater levels of conscious experience'. The study did not find any significant impact of the robot's accuracy on trust, as the errors were a constant factor across all conditions, and the focus was on the repair strategies. The study did not find any factors that did not impact trust, as the study did not explicitly test for factors that did not impact trust.",10.1038/s41598-023-37032-0,https://www.nature.com/articles/s41598-023-37032-0,"Abstract             Nothing is perfect and robots can make as many mistakes as any human, which can lead to a decrease in trust in them. However, it is possible, for robots to repair a human’s trust in them after they have made mistakes through various trust repair strategies such as apologies, denials, and promises. Presently, the efficacy of these trust repairs in the human–robot interaction literature has been mixed. One reason for this might be that humans have different perceptions of a robot’s mind. For example, some repairs may be more effective when humans believe that robots are capable of experiencing emotion. Likewise, other repairs might be more effective when humans believe robots possess intentionality. A key element that determines these beliefs is mind perception. Therefore understanding how mind perception impacts trust repair may be vital to understanding trust repair in human–robot interaction. To investigate this, we conducted a study involving 400 participants recruited via Amazon Mechanical Turk to determine whether mind perception influenced the effectiveness of three distinct repair strategies. The study employed an online platform where the robot and participant worked in a warehouse to pick and load 10 boxes. The robot made three mistakes over the course of the task and employed either a promise, denial, or apology after each mistake. Participants then rated their trust in the robot before and after it made the mistake. Results of this study indicated that overall, individual differences in mind perception are vital considerations when seeking to implement effective apologies and denials between humans and robots."
"Esterwood, Connor; Jr, Lionel P. Robert",Three Strikes and you are out!: The impacts of multiple human–robot trust violations and repairs on robot trustworthiness,2023,1,240,240,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants completed a tutorial, a pre-test questionnaire, then were assigned to one of six experimental conditions, and completed a post-test questionnaire.",Participants worked with a robot to sort boxes in a virtual warehouse environment.,Unspecified,Mobile Robots,Industrial,Manipulation,Sorting/Arranging,minimal interaction,Participants interacted with a robot in a virtual environment.,simulation,The interaction took place in a virtual warehouse environment.,simulated,The robot was a virtual representation in the simulation.,pre-programmed (non-adaptive),The robot followed a pre-set sequence of actions.,Questionnaires,,,Trust was measured using a questionnaire at the end of the study.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's performance was manipulated by having it make errors, and its behavior was manipulated by providing different trust repair strategies after errors.","Trust was significantly lower after the robot made errors, and no repair strategy fully restored trust to pre-violation levels. Denials were the least effective repair strategy.","Apologies, explanations, and promises were equally ineffective at repairing trustworthiness, ability, and integrity after repeated trust violations. Denials were consistently the least effective repair strategy. Benevolence was more easily repaired than ability and integrity.",No trust repair strategy was able to fully restore trustworthiness to pre-violation levels after repeated trust violations.,The robot picked boxes from a queue and presented them to the participant. The participant approved or rejected the boxes based on a serial number. The robot made errors by picking the wrong box at three time points.,Kruskal-Wallis; dunn's tests,"The study used Kruskal-Wallis rank sum tests, followed by post hoc Dunn's tests of multiple comparisons with a Benjamini-Hochberg correction to control for multiple hypothesis testing. These tests were used to compare differences in trustworthiness, ability, benevolence, and integrity across different experimental conditions (repair strategies, no repair, and perfect performance). The Kruskal-Wallis test was used to determine if there were significant differences between groups, and the Dunn's test was used for post-hoc comparisons to identify which specific groups differed significantly from each other. The tests were chosen because the data were non-normally distributed.",TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-verbal-communication-content; Robot-accuracy,,"The study manipulated the robot's verbal communication by providing different trust repair strategies (apologies, denials, explanations, promises) after the robot made errors. This falls under 'Robot-verbal-communication-content' because the content of the robot's communication was directly manipulated. The robot's accuracy was also manipulated by having it make errors at specific points in the task, which directly impacted task performance metrics. The study found that both the robot's accuracy (errors) and the content of its verbal communication (repair strategies) impacted trust levels. No factors were found to not impact trust.",10.1016/j.chb.2023.107658,https://linkinghub.elsevier.com/retrieve/pii/S0747563223000092,"Robots like human co-workers can make mistakes violating a human’s trust in them. When mistakes happen, humans can see robots as less trustworthy which ultimately decreases their trust in them. Trust repair strategies can be employed to mitigate the negative impacts of these trust violations. Yet, it is not clear whether such strategies can fully repair trust nor how effective they are after repeated trust violations. To address these shortcomings, this study examined the impact of four distinct trust repair strategies: apologies, denials, explanations, and promises on overall trustworthiness and its sub-dimensions: ability, benevolence, and integrity after repeated trust violations. To accomplish this, a between-subjects experiment was conducted where participants worked with a robot co-worker to accomplish a task. The robot violated the participant’s trust and then provided a particular repair strategy. Results indicated that after repeated trust violations, none of the repair strategies ever fully repaired trustworthiness and two of its sub-dimensions: ability and integrity. In addition, after repeated interactions, apologies, explanations, and promises appeared to function similarly to one another, while denials were consistently the least effective at repairing trustworthiness and its sub-dimensions. In sum, this paper contributes to the literature on human–robot trust repair through both of these original findings."
"Ezenyilimba, Akuadasuo; Wong, Margaret; Hehr, Alexander; Demir, Mustafa; Wolff, Alexandra; Chiou, Erin; Cooke, Nancy",Impact of Transparency and Explanations on Trust and Situation Awareness in Human–Robot Teams,2023,1,48,61,0,No participants were excluded,Online Crowdsourcing,mixed design,"Participants were onboarded, completed a training mission, then completed two actual missions with a virtual robot in Minecraft, followed by questionnaires.","Participants worked with a virtual robot to locate victims and environmental changes in a simulated USAR environment, and assisted the robot during interactive tasks.",Unspecified,Mobile Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the robot through a simulated environment and text-based communication.,simulation,The interaction was conducted in a simulated Minecraft environment.,simulated,The robot was represented as a virtual entity within the Minecraft simulation.,wizard of oz (directly controlled),The robot's actions were controlled by an experimenter using a Wizard-of-Oz method.,Questionnaires; Custom Scales,Trust Perception Scale - HRI; NASA Task Load Index (NASA-TLX),,Trust was measured using questionnaires and a custom scale.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated robot transparency and explanation levels, which influenced the robot's behavior and the task difficulty, and the interaction medium was a virtual environment.",Trust decreased with increased workload and decreased in the baseline condition with limited robot information.,"The baseline condition had the lowest SA and trust scores, and the highest workload. The full condition (full transparency and explanation) did not significantly improve SA compared to the explanation-only condition, suggesting that context-based explanations are more important than transparency alone.",Context-driven robot explanations and transparency information improve human trust and situation awareness in human-robot teams.,"The robot searched a virtual building room by room, while the human monitored the robot's camera feed, reported victims and environmental changes, and assisted the robot during interactive tasks.",ANOVA,"The study used split-plot ANOVAs to analyze the effects of different conditions (full, explanation-only, transparency-only, and baseline) and missions (Mission 1 and Mission 2) on situation awareness (SA), trust in the robot, team trust, and perceived workload. The ANOVAs examined main effects of condition and mission, as well as their interaction effects. The purpose was to determine how robot transparency and explanations impact these dependent variables.",TRUE,Robot-verbal-communication-content; Robot-interface-design; Task-complexity,Robot-verbal-communication-content; Task-complexity,Robot-interface-design,"The study manipulated the level of robot transparency and explanation, which falls under 'Robot-verbal-communication-content' because it directly changes what information the robot communicates to the human teammate. Specifically, the study varied whether the robot provided explanations for its actions and the level of transparency through status indicators and maps. The study also manipulated the 'Task-complexity' by increasing the number of victims, environmental changes, and interactive tasks between Mission 1 and Mission 2, which directly impacted the cognitive demands on the participants. The 'Robot-interface-design' was also manipulated by providing different levels of information through the interface (e.g., static vs. dynamic maps, status indicators), but the results showed that the interface design alone did not significantly impact trust, as the full condition (with full transparency) did not significantly improve SA compared to the explanation-only condition. The study found that 'Robot-verbal-communication-content' (specifically, context-based explanations) and 'Task-complexity' (increased workload) impacted trust. The baseline condition, with limited robot information, resulted in lower trust, while increased workload in Mission 2 also decreased trust. The interface design, while manipulated, did not have a significant impact on trust on its own, as the explanation-only condition performed similarly to the full condition.",10.1177/15553434221136358,http://journals.sagepub.com/doi/10.1177/15553434221136358,"Urban Search and Rescue (USAR) missions continue to beneﬁt from the incorporation of human–robot teams (HRTs). USAR environments can be ambiguous, hazardous, and unstable. The integration of robot teammates into USAR missions has enabled human teammates to access areas of uncertainty, including hazardous locations. For HRTs to be effective, it is pertinent to understand the factors that inﬂuence team effectiveness, such as having shared goals, mutual understanding, and efﬁcient communication. The purpose of our research is to determine how to (1) better establish human trust, (2) identify useful levels of robot transparency and robot explanations, (3) ensure situation awareness, and (4) encourage a bipartisan role amongst teammates. By implementing robot transparency and robot explanations, we found that the driving factors for effective HRTs rely on robot explanations that are context-driven and are readily available to the human teammate."
"Fahim, Md Abdullah Al; Khan, Mohammad Maifi Hasan; Jensen, Theodore; Albayram, Yusuf",Human vs. Automation: Which One Will You Trust More If You Are About to Lose Money?,2023,1,298,215,83,"43 participants were excluded for incorrect attention check responses, 6 participants were excluded from the automated partner groups for selecting the wrong partner type, 20 participants were excluded for not believing they were playing with a human partner, 14 participants were excluded from the human partner group for granting camera permission",Online Crowdsourcing,mixed design,"Participants completed a target identification task with a partner (either human or automated) across five rounds, allocating images to the partner before each round, and received feedback after each round. Participants then completed a survey.","Participants classified images of vehicles as dangerous or not dangerous, collaborating with a partner (either human or automated) to identify vehicles.",Unspecified,Other,Research,Game,Cooperative Game,minimal interaction,Participants interacted with a simulated partner through a computer interface.,simulation,The interaction was conducted through a computer interface simulating a target identification task.,simulated,"The partner was a simulated entity, either an automated system or a human, represented through a computer interface.",pre-programmed (non-adaptive),The partner's actions were pre-programmed with fixed reliability levels and did not adapt to the participant's behavior.,Behavioral Measures; Questionnaires,Propensity to Trust Scales; Mayer and Davis' Trust/Trustworthiness Scales (1999),,"Trust was measured using questionnaires and behavioral measures, specifically the number of images allocated to the partner.",no modeling,"The study did not use computational models of trust, relying on statistical analysis of collected data.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the partner type (human or automated), the reliability of the partner, and the risk level associated with the task to influence trust.","Reliability significantly impacted trust, with higher reliability leading to higher trust. Risk did not have a main effect on trust, but it did interact with partner type and reliability to influence behavioral trust. Participants in the low risk-human partner groups did not calibrate their trust appropriately, allocating similarly to both low and high reliability partners, while participants in the high risk-human partner groups did calibrate their trust appropriately.","The study found that participants in the low risk-human partner groups did not calibrate their trust appropriately, allocating similarly to both low and high reliability partners, while participants in the high risk-human partner groups did calibrate their trust appropriately. This suggests that social norms may influence trust differently under varying risk conditions. The study also did not find evidence of automation bias.","The study's key finding is that the effect of human-like characteristics on trust should not be considered independent of risk, as participants were more likely to calibrate their dependence on a human partner in high-risk situations compared to low-risk situations.",The robot partner (either automated or simulated human) classified images of vehicles as dangerous or not dangerous. The human participant also classified images and allocated a number of images to the partner before each round.,t-test; ANOVA; ANCOVA; ANCOVA; mixed design (split-plot) ancova; mixed design (split-plot) anova,"The study used a variety of statistical tests to analyze the data. T-tests were used to compare means between two groups, specifically for manipulation checks of reliability and risk levels. ANOVA was used to examine the effect of risk and reliability on the manipulation check items. MANCOVA was used to analyze the effect of partner type, risk, and reliability on perceived trustworthiness characteristics, controlling for propensity to trust and risk propensity. Follow-up ANCOVAs were conducted to examine the effects on individual trustworthiness characteristics. Mixed design (split-plot) ANCOVA was used to analyze the effect of partner type, risk, reliability, and rounds on behavioral trust (allocation to partner), controlling for propensity to trust and risk propensity. Mixed design (split-plot) ANOVA was used to analyze the effect of partner type, risk, reliability, and rounds on accuracy and time scores.",TRUE,Robot-accuracy; Teaming; Task-constraints,Robot-accuracy,Teaming; Task-constraints,"The study manipulated 'Robot-accuracy' by varying the image classification accuracy of the partner (60% or 90%). This is explicitly stated in the 'Reliability levels' section of the paper: 'We followed this general guideline and set the reliability levels at 60% (low reliability) and 90% (high reliability)'. The study also manipulated 'Teaming' by having participants collaborate with either a human or an automated partner, as described in the 'Partner type levels' section: 'Human partner participants were told that they would be collaborating with another human (i.e., another MTurk worker). In reality, both partner groups interacted with the same automated system.' The study also manipulated 'Task-constraints' by varying the amount of money participants stood to lose based on their performance, as described in the 'Risk levels' section: 'The low risk groups participants had $1.00 at stake and guaranteed earning of $4.00. On the other hand, the high risk groups participants had $3.00 at stake and guaranteed compensation of $2.00.' The results showed that 'Robot-accuracy' significantly impacted trust, as stated in the 'A main effect of reliability' section: 'Results from the perceived trustworthiness characteristics items confirmed that the high reliability participants reported a higher level of trust in the partner compared to the low reliability participants. A similar result was found for behavioral trust as well.' The study found that 'Teaming' did not have a main effect on trust, as stated in the 'A lack of automation bias' section: 'We observed that the numbers of allocations to the partners were similar in both partner type groups.' The study also found that 'Task-constraints' did not have a main effect on trust, as stated in the 'A lack of main effect of risk on trust' section: 'Results did not support our hypothesis H2, and we did not observe any effect of risk on trust.'",10.1080/10447318.2022.2076772,https://www.tandfonline.com/doi/full/10.1080/10447318.2022.2076772,"In the context of teamwork, prior efforts noted that trust degradation due to failures is slower in the case of human partners compared to automated agents. In our work, we wanted to investigate whether this holds true when the cost of mistakes (i.e., risk) is high. Toward that, we designed a 2 (partner: automation/human) Â 2 (risk: low/high) Â 2 (reliability: low/high) betweengroup study where participants completed five rounds of a target identification task with a partner (either automation or human). The findings suggest that users’ perceived trustworthiness of the partner is affected by reliability groups appropriately, irrespective of risk groups. However, with human partners, although participants fail to calibrate dependence appropriately in low-risk groups, dependence is appropriate in high-risk groups. The finding suggests that the effect of human-like characteristics on trust should not be considered independent of risk. The implications of our findings are discussed in the paper."
"Ferraro, James; Clark, Logan; Christy, Naomi; Mouloua, Mustapha",Effects of Automation Reliability and Trust on System Monitoring Performance in Simulated Flight Tasks,2018,1,70,65,5,5 participants' data was not analyzed due to outliers or bad data,Controlled Lab Environment,between-subjects,"Participants completed a demographic survey, then an 8-minute training session on the MATB-II, followed by three 30-minute MATB-II trials, and completed the NASA TLX after each trial.","Participants monitored four system gauges and responded to out-of-range events not corrected by an automated system, while also responding to radio communications, performing a fuel management task, and using a joystick for a navigation task.",Unspecified,Other,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated system through a computer interface.,simulation,The interaction was conducted in a simulated environment using a computer program.,simulated,The robot was represented as a simulated system within the MATB-II program.,pre-programmed (non-adaptive),"The automated system had a pre-programmed level of reliability, detecting and correcting errors without adapting to user input.",Questionnaires,Jian et al. Trust Scale; NASA Task Load Index (NASA-TLX),Performance Metrics,Trust was measured using a questionnaire and performance metrics.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the automated system was manipulated, with some participants experiencing a 50% reliability and others a 90% reliability, influencing the frequency of errors and the need for operator intervention.",The study found no significant relationship between the manipulated reliability and the reported trust in automation.,"The study found that more reliable automation resulted in poorer monitoring performance, which is counterintuitive to the goal of automation. There was no significant relationship between trust and performance, which is not consistent with previous research.","Automation reliability significantly affected monitoring performance, with less reliable automation leading to better error detection by participants.","The automated system detected and corrected errors in the system gauges, and the human participant monitored the gauges and corrected errors that the system missed. The human also performed other tasks such as responding to radio communications, managing fuel, and navigating using a joystick.",ANOVA; Pearson correlation,"The study used ANOVA to examine the impact of automation reliability and trial number on system monitoring performance, specifically the number of errors corrected by participants. A correlation analysis was also used to assess the relationship between participants' reported trust in automation and their system monitoring performance.",TRUE,Robot-accuracy,,Robot-accuracy,"The study manipulated the reliability of the automated system, which directly impacts the accuracy of the robot's actions in detecting and correcting errors. This is described in the paper as 'two levels of automation reliability were presented in the current study (R50% and R90%)'. This manipulation directly influences the success rate of the automated system, making 'Robot-accuracy' the most appropriate category. The study found that the manipulated reliability did not significantly impact the reported trust in automation, as stated in the results section: 'Participants' reported level of trust in automation did not appear to have any relationship with system monitoring performance (r(63)= .04, p= .76). Further analysis did not reveal any differences between R50 (M= 5.00, SD= 1.21) and R90 (M= 4.99, SD= 1.10) conditions in overall trust in automation.' Therefore, 'Robot-accuracy' is placed in the 'factors_that_did_not_impact_trust' list.",10.1177/1541931218621283,http://journals.sagepub.com/doi/10.1177/1541931218621283,"The current study was designed to empirically examine the effects of trust and automation reliability on multi-tasking performance in a simulated cockpit setting using the Multi-Attribute Task Battery II (MATB-II). The MATB-II simulates tasks often performed by pilots in-flight, tasking the operator with attending to automated systems and correcting errors when they inevitably occur. Over the course of three 30-minute trials, two levels of automation reliability were presented in the current study (R50% and R90%). It was hypothesized that automation reliability and trust would affect both workload and performance in this multi-tasking environment. Results indicated that reliability significantly affected monitoring performance on the MATB-II. More reliable automation resulted in poorer monitoring performance, while trust appeared to have little impact. These results provide further evidence for how operators trust and interact with automation, a topic that is relevant to the implementation of automated systems in a variety of human-machine systems such as aviation."
"Ferreira, Beatriz Quintino; Karipidou, Kelly; Rosa, Filipe; Petisca, Sofia; Alves-Oliveira, Patrícia; Paiva, Ana",A Study on Trust in a Robotic Suitcase,2016,1,18,18,7,7 participants were excluded because they were only able to perform one of the conditions,Real-World Environment,mixed design,"Participants filled a pre-trust questionnaire, then interacted with a robotic suitcase (aBag) in two conditions: following closely and following at a distance, while carrying their belongings to a vending machine. After each interaction, they filled a post-trust questionnaire.",Participants used a robotic suitcase to carry their belongings to a vending machine and back.,Unspecified,Mobile Robots; Service and Assistive Robots,Research; Care,Navigation,Path Following,minimal interaction,Participants interacted with the robot by having it follow them while carrying their belongings.,real-world,The interaction took place in a real-world environment with a physical robot.,physical,The robot was a physical suitcase attached to a remotely controlled car.,wizard of oz (directly controlled),The robot was remotely controlled by a researcher using the Wizard of Oz technique.,Behavioral Measures; Questionnaires,Schaefer's Trust Questionnaire/Scale,Video Data,Trust was measured using a questionnaire and by coding the number of times participants looked back at the robot.,no modeling,No computational model of trust was used; only descriptive statistics were used.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's following distance was manipulated to be either close or far from the user, and the interaction was in a real-world setting.","Trust was significantly higher after the interaction, but there was no significant difference in trust between the two distance conditions. However, there were indications that trust was higher when the robot was further away.","Participants seemed to trust the robot more when it was further away, which was opposite to the initial hypothesis. Also, female participants tended to look back at the robot more often than male participants in the further distance condition.","The perceived human-robot trust was significantly higher after the participants interacted with the robotic suitcase, regardless of the distance condition.","The robot (aBag) followed the participant while they walked to a vending machine and back. The human participant placed their belongings inside the robot and walked to the vending machine, while the robot followed them.",Shapiro-Wilk; Wilcoxon signed-rank test; ANOVA; Mann-Whitney U,"The study used several statistical tests. The Shapiro-Wilk test was used to check for normality of the trust scores. The Related-Samples Wilcoxon Signed Rank Test was used to compare pre- and post-interaction trust scores. An ANOVA test was used to compare trust scores across the two experimental conditions and the pre-interaction scores. Finally, the Independent-Samples Mann-Whitney U test was used to compare the number of times male and female participants looked back at the robot in condition 2.",TRUE,Robot-nonverbal-communication,,Robot-nonverbal-communication,"The study manipulated the distance between the robot and the participant, which is a form of proxemics and falls under 'Robot-nonverbal-communication'. The paper states, 'Two different conditions were created: (1) aBag follows the participant at a close range; (2) aBag follows the participant on a further distance.' This manipulation of distance is a change in the robot's physical movement and its spatial relationship to the user. Although the study found that trust was significantly higher after the interaction, there was no significant difference in trust between the two distance conditions. Therefore, the manipulation of 'Robot-nonverbal-communication' did not significantly impact trust levels between the two conditions, but the study did find that trust was higher after the interaction regardless of the condition.",,http://link.springer.com/10.1007/978-3-319-47437-3_18,"This work presents a study on human-robot interaction between a prototype of a robotic suitcase – aBag – and people using it. Importantly, for an autonomous robotic suitcase to be successful as a product, people need to trust it. Therefore, a study was performed, where participants used aBag (remotely operated using the Wizard of Oz technique) for carrying their belongings. Two diﬀerent conditions were created: (1) aBag follows the participant at a close range; (2) aBag follows the participant on a further distance. We expected that participants would trust more aBag when it was following them at a close range, but interestingly participants seemed to trust more when aBag was further away. Also, regardless of the conditions, the level of trust in aBag was signiﬁcantly higher after the interaction compared to before, bringing positive results to the development of this kind of robotic apparatus."
"Ferronato, Priscilla; Bashir, Masooda",An Examination of Dispositional Trust in Human and Autonomous System Interactions,2020,1,344,344,0,No participants were excluded,Online Crowdsourcing,,"Participants completed an online survey that included questions about their personality traits, cultural orientation, and dispositional trust in autonomous systems.","Participants completed questionnaires assessing personality traits, cultural orientation, and dispositional trust in autonomous systems.",Unspecified,Other,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the autonomous systems in the context of the questionnaires.,media,The interaction was based on text descriptions of autonomous systems.,hypothetical,"The robot was only described in text, with no visual representation.",not autonomous,The robot's actions were hypothetical and not physically present.,Questionnaires,Big Five Inventory Scale; Disposition to Trust Questionnaire; Interpersonal Trust Scale/Questionnaire,,Trust was measured using a questionnaire.,"parametric models (e.g., regression)",Linear regression was used to analyze the relationship between trust and other variables.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study assessed trust without intentionally altering any specific factors.,,"The study found a negative relationship between extroversion and trust in AS, which contradicts some previous findings. Also, agreeableness did not show a significant influence on trust when analyzed with other variables.","Horizontal cultural values have a strong positive relationship with dispositional trust in autonomous systems, while extroversion has a negative relationship.","Participants completed questionnaires about their personality, cultural orientation, and trust in autonomous systems; no robot was present.",Linear regression; Linear regression,"The study used linear regression to examine the relationship between trust in autonomous systems and individual variables such as personality traits, cultural orientation, age, and gender. Multiple linear regression was then used to explore the combined influence of these factors on trust, investigating how the relationships change when variables are analyzed simultaneously.",FALSE,,,,"The study did not manipulate any factors. It was an observational study that used questionnaires to assess the relationship between personality traits, cultural orientation, demographics, and dispositional trust in autonomous systems. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust due to manipulation.",,http://link.springer.com/10.1007/978-3-030-49065-2_30,"The rapid advancement of technology has changed the human and AS interactions, blurring the boundaries of what must be a human or automation action. The successfully implementation of humanin-the-loop is essential for the new relationship between humans and AS, in which control is shared and a team-mate collaboration arises. We believe that only through the best understanding of human factors and individual di_erences it will be possible to work towards the formation and calibration of trust in human and AS interactions. Therefore, this study conducted an online questionnaire to investigate the in?ence of personality traits, culture orientation, and individual di_erences on dispositional trust, as an e_ort to map out humans? baseline trust in autonomous systems. We found that while some factors presented signi?ant relation with trust in autonomous systems when analyzed as an isolated variable, such as agreeableness trait, they do not have signi?ant results when investigated concomitantly to other factors. Thus, we were able to identify that some individual di_erences ? cultural values, extrovertion trait, and age ? presented stronger in?ence on the dispositional trust in automation. Thus, our study provides valuable information about human factors that mediate trust, which supports the optimization and improvement of the overall interaction between humans and autonomous systems."
"Filip, George; Meng, Xiaolin; Burnett, Gary; Harvey, Catherine",Designing and calibrating trust through situational awareness of the vehicle (SAV) feedback,2016,2,291,291,0,No participants were excluded,Survey/Interview,,Participants completed an online survey with Likert scale and open-ended questions after being given a description and picture of a driverless vehicle.,Participants rated their agreement with statements about factors influencing trust in driverless vehicles and answered open-ended questions about the most important factors.,Unspecified,Autonomous Vehicles,Other,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the driverless vehicle and completed a survey.,media,Participants were shown a picture of a driverless vehicle.,hypothetical,"The robot was described and shown in a picture, but not physically present.",not autonomous,The robot's actions were described but not demonstrated or simulated.,Questionnaires,,,Trust was assessed using a Likert scale questionnaire and open-ended questions.,no modeling,No computational model of trust was developed.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study assessed trust without intentionally altering any specific factors.,,"Participants rated the car's sensing capabilities, reliability, and ability to take back control as the most important factors influencing their trust.","The most important factors influencing trust in driverless vehicles are the car's sensing capabilities, reliability, and ability to take back control.","Participants read a description and viewed a picture of a driverless car, then completed a survey about their trust in the vehicle.",,"The study used descriptive statistics, specifically means, to rank the importance of different factors influencing trust in driverless vehicles based on Likert scale responses. A word counter was used to analyze open-ended responses, identifying the most frequently mentioned factors.",FALSE,,,,"The study did not manipulate any factors. Participants were asked to rate their agreement with statements about factors influencing trust in driverless vehicles and answer open-ended questions. There was no intentional manipulation of any variable to observe its effect on trust. The study was observational and survey-based, focusing on identifying factors that participants believe influence trust, rather than manipulating those factors.",10.1049/cp.2016.1171,,"Based on the literature and an exploratory survey with 291 participants that we have deployed, factors that can influence users' trust into automated vehicles have been identified. Twelve experts from the fields of Human Factors, Robotics, Navigation, Positioning and Connectivity fields have been interviewed either face to face or by phone for their insights regarding the importance and use of the Situational Awareness of the Vehicle (sensing, positioning, processing and acting). The experts expressed their opinions regarding the most appropriate types of feedback that can lead to a better calibration of trust in the automated vehicles as well as offered insights into the need to avoid issues such as mode confusion or overloading users with information. Following up on the results obtained we propose a visual interface to be used as a tool to calibrate the trust of the users in automated vehicles. Furthermore, we offer insights for potential hand-over and take-over of control issues as well as general guidelines for the design of the HMI in highly automated vehicles. Moreover, we propose an initial working definition for the Situational Awareness of the Vehicle."
"Filip, George; Meng, Xiaolin; Burnett, Gary; Harvey, Catherine",Designing and calibrating trust through situational awareness of the vehicle (SAV) feedback,2016,2,12,12,0,No participants were excluded,Survey/Interview,,Experts were interviewed either face-to-face or by phone using a semi-structured interview format.,Experts were asked questions about the importance of Situational Awareness of the Vehicle (SAV) on users' trust in automated vehicles.,Unspecified,Autonomous Vehicles,Other,Evaluation,Expert Consultations,passive observation,Experts were interviewed about their opinions on automated vehicles.,media,The interaction was based on verbal communication and discussion.,hypothetical,"The robot was not physically present, and the discussion was based on hypothetical scenarios.",not autonomous,The robot's actions were discussed but not demonstrated or simulated.,,,,"Trust was not directly measured, but rather discussed in the interviews.",no modeling,No computational model of trust was developed.,Observational & Survey Studies,Expert Consultations,No Manipulation,The study assessed expert opinions without intentionally altering any specific factors.,,"Experts agreed that SAV is important for trust and that users should be informed of its status, with a traffic light system being the most appropriate feedback method.",Experts agreed that the Situation Awareness of the Vehicle (SAV) is a crucial factor influencing trust in automated vehicles and that users should be informed of its status through a simplified feedback system.,Experts were interviewed about their opinions on the importance of SAV for trust in automated vehicles.,,"The study used thematic analysis, a qualitative method, to analyze the semi-structured interviews with experts. Thematic analysis was used to identify recurring themes and patterns in the experts' responses regarding the importance of SAV on users' trust in automated vehicles. No statistical tests were used.",FALSE,,,,"The study did not manipulate any factors. Experts were interviewed about their opinions on the importance of SAV for trust in automated vehicles. The study was observational and interview-based, focusing on gathering expert opinions rather than manipulating any variables to observe their effect on trust. The experts were asked about their views on SAV and its importance for trust, but no factors were intentionally altered by the researchers.",10.1049/cp.2016.1171,,"Based on the literature and an exploratory survey with 291 participants that we have deployed, factors that can influence users' trust into automated vehicles have been identified. Twelve experts from the fields of Human Factors, Robotics, Navigation, Positioning and Connectivity fields have been interviewed either face to face or by phone for their insights regarding the importance and use of the Situational Awareness of the Vehicle (sensing, positioning, processing and acting). The experts expressed their opinions regarding the most appropriate types of feedback that can lead to a better calibration of trust in the automated vehicles as well as offered insights into the need to avoid issues such as mode confusion or overloading users with information. Following up on the results obtained we propose a visual interface to be used as a tool to calibrate the trust of the users in automated vehicles. Furthermore, we offer insights for potential hand-over and take-over of control issues as well as general guidelines for the design of the HMI in highly automated vehicles. Moreover, we propose an initial working definition for the Situational Awareness of the Vehicle."
"Fischer, Katrin; Velentza, Anna-Maria; Lucas, Gale; Williams, Dmitri",Seeing Eye to Eye with Robots: An Experimental Study Predicting Trust in Social Robots for Domestic Use,2024,1,232,198,34,34 participants were excluded because they did not pass all attention checks and answer all survey questions,Online Crowdsourcing,between-subjects,"Participants were randomly assigned to view a video of the Astro robot from either an eye-level or high-angle perspective, then completed questionnaires regarding their trust, willingness to purchase and use the robot, first impressions of warmth and competence, and usability, as well as open-ended questions.",Participants watched a video of the Astro robot and then completed questionnaires.,Amazon Astro,Mobile Robots; Service and Assistive Robots,Social; Care,Evaluation,Survey/Questionnaire Completion,passive observation,Participants passively observed the robot through a video.,media,Participants watched a video of the robot.,physical,The robot was a physical robot shown in a video.,pre-programmed (non-adaptive),The robot's actions in the video were pre-programmed.,Questionnaires; Custom Scales,System Usability Scale (SUS),Video Data,Trust was measured using questionnaires and custom scales.,"parametric models (e.g., regression)",Multiple linear regression was used to model trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The perspective from which the robot was shown in the video was manipulated (eye-level vs. high-angle) to influence trust.,The eye-level perspective elicited higher ratings of trust compared to the high-angle perspective.,"Participants in the high-angle perspective used negatively valenced words to describe the robot's operational capabilities, personality, and appearance, while participants in the eye-level condition did not.",An eye-level video perspective elicits higher ratings of trust in robots compared to a high-angle perspective.,"The robot demonstrated its abilities through pre-programmed actions in a video, while the human participant watched the video and completed questionnaires.",t-test; Linear regression; Logistic regression,"The study used Welch's t-test to compare the effect of video perspective (eye-level vs. high-angle) on trust ratings. Multiple linear regression was used to examine the relationship between trust and factors like warmth, competence, familiarity, and usability. Finally, logistic regression was used to predict purchase intention based on factors like age and negative attitudes towards robots.",TRUE,Robot-nonverbal-communication,Robot-nonverbal-communication,,"The study manipulated the perspective from which the robot was shown in the video (eye-level vs. high-angle). This manipulation directly affects the visual presentation of the robot's physical presence and movements, which falls under 'Robot-nonverbal-communication'. The paper states, 'The viewpoint of the robot will affect trust, i.e. an eye-level perspective will elicit higher ratings of trust in the robot.' and 'Results showed that the eye-level robot perspective elicited significantly higher ratings of trust...than the high-angle perspective'. This indicates that the manipulated factor, 'Robot-nonverbal-communication', impacted trust. There were no other factors manipulated in the study, so there are no factors that did not impact trust.",10.1109/RO-MAN60168.2024.10731371,https://ieeexplore.ieee.org/document/10731371/,"The use of social robots in service tasks is spreading, showcasing advantages for both consumers and service providers. However, their widespread adoption is hindered by a notable lack of trust. Our study aims to uncover insights into the factors influencing the adoption of social robots in home settings, exploring the factors that lead users to trust and eventually adopt robots. We designed two experimental conditions, presenting the Amazon Astro robot from different perspectives (high-angle and eye-level) and demonstrating its different abilities to 198 people recruited from MTurk. We employed both quantitative (trust, first impressions of warmth and competence as well as usability, familiarity, and attitudes) questionnaires and qualitative (word analysis) assessments, and results showed that participants had higher trust scores when seeing the robot from an eye-level perspective. In addition, usability, familiarity and competence were shown to explain a significant amount of variance in trust. While existing negative attitudes towards robots and the participants’ age were shown to be the strongest predictors for participants’ willingness to purchase a robot, trust was able to significantly affect use intention. We contribute to the broader understanding of the challenges and opportunities in integrating social robots into daily life, shedding light on the dynamics between technological innovation and consumer adoption."
"Flook, Rebecca; Shrinah, Anas; Wijnen, Luc; Eder, Kerstin; Melhuish, Chris; Lemaignan, Séverin",On the impact of different types of errors on trust in human-robot interaction: Are laboratory-based HRI experiments trustworthy?,2019,2,109,100,9,9 were excluded due to unintentional robotic technical failures or incorrect completion of the questionnaires and in one case the participant avoiding the intentional mistake,Controlled Lab Environment,between-subjects,"Participants completed a pre-study questionnaire, then interacted with a robot to complete an assembly task, followed by a post-study questionnaire and debriefing.","Participants worked with a robot to assemble a toy using plastic nuts and bolts, following the robot's verbal and physical guidance.",TIAGo,Mobile Manipulators,Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically interacted with the robot during the assembly task.,real-world,The interaction took place in a real-world lab setting with a physical robot.,physical,A physical robot was used in the study.,wizard of oz (directly controlled),The robot's actions were controlled by a human operator using a Wizard of Oz method.,Questionnaires; Custom Scales,Ten Item Personality Inventory (TIPI); Negative Attitude towards Robots Scale (NARS); Godspeed Questionnaire,,Trust was measured using post-hoc questionnaires and study-specific Likert scale questions.,no modeling,No computational model of trust was used; statistical tests were used to analyze the data.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's behavior was manipulated to include technical failures or decision-level errors, with or without acknowledgement of the error, to see how it affected trust.","A weak effect of errors on reported trust was found, but no significant differences between error types or acknowledgement of errors were observed.","The robot in the technical failure condition was found to be statistically more likeable than in the no-error condition, which is an unexpected result. There was a weak yet statistically significant correlation between subscale 2 of the NARS and the level of trust.","The study found a weak effect of errors on reported trust, but no significant differences between error types or acknowledgement of errors were observed.","The robot provided verbal instructions and pointed to objects, while the human assembled a toy based on the robot's guidance.",independent t-tests; Mann-Whitney U,"Independent T-tests were used to analyze the subscales from the TIPI and NARS questionnaires in relation to the Trust and Willingness to Work Scale. Mann-Whitney U tests were used to compare the Trust and Willingness to Work Scale results between the no-error and error conditions, as well as between acknowledgement and no-acknowledgement conditions. Further Mann-Whitney tests were performed to investigate the interaction between error type and acknowledgement.",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-accuracy,Robot-verbal-communication-content,"The study manipulated 'Robot-accuracy' by introducing technical failures (robot knocking items) and decision-level errors (incorrect guidance) during the assembly task. This directly impacted the robot's ability to perform the task correctly, thus affecting task performance. The study also manipulated 'Robot-verbal-communication-content' by having the robot either acknowledge or not acknowledge the errors. The results showed that the presence of errors ('Robot-accuracy') had a weak impact on trust, but the acknowledgement of errors ('Robot-verbal-communication-content') did not significantly impact trust levels. The paper states, 'While we found a (weak) general effect of errors on reported and observed level of trust, no significant differences between the type of errors were found in either of our studies.' and 'No significant difference in the reported trust level and the willingness to work with the robot again in the four investigated environments were found.' when comparing acknowledgement vs no acknowledgement.",10.1075/is.18067.flo,http://www.jbe-platform.com/content/journals/10.1075/is.18067.flo,"Abstract             Trust is a key dimension of human-robot interaction (HRI), and has often been studied in the HRI community. A                     common challenge arises from the difficulty of assessing trust levels in ecologically invalid environments: we present in this                     paper two independent laboratory studies, totalling 160 participants, where we investigate the impact of different types of errors                     on resulting trust, using both behavioural and subjective measures of trust. While we found a (weak) general effect of errors on                     reported and observed level of trust, no significant differences between the type of errors were found in either of our studies.                     We discuss this negative result in light of our experimental protocols, and argue for the community to move towards alternative                     methodologies to assess trust."
"Flook, Rebecca; Shrinah, Anas; Wijnen, Luc; Eder, Kerstin; Melhuish, Chris; Lemaignan, Séverin",On the impact of different types of errors on trust in human-robot interaction: Are laboratory-based HRI experiments trustworthy?,2019,2,60,60,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants performed three tasks involving measuring 'stop distances' from a robot, with different error conditions during the introduction.","Participants approached the robot and stopped when they felt uncomfortable, and the robot approached the participant and stopped when the participant said 'stop'.",Pepper,Humanoid Robots; Expressive Robots,Research,Navigation,Path Following,minimal interaction,Participants interacted with the robot by approaching it and indicating when to stop.,real-world,The interaction took place in a real-world lab setting with a physical robot.,physical,A physical robot was used in the study.,pre-programmed (non-adaptive),The robot followed a pre-programmed path and did not adapt to the user's behavior.,Behavioral Measures; Questionnaires,Ten Item Personality Inventory (TIPI); Godspeed Questionnaire,,Trust was measured using proxemics and post-study questionnaires.,no modeling,No computational model of trust was used; statistical tests were used to analyze the data.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's behavior was manipulated to include technical or socio-cognitive errors during the introduction to see how it affected proxemics.,"Participants approached the robot closer in the error conditions compared to the baseline, which is counterintuitive. The robot was also perceived as less anthropomorphic after making an error.","Participants approached the robot closer when a technical error was observed, which is a counterintuitive result. The robot was scored as less anthropomorphic after making a technical error and both anthropomorphism and animacy got a lower score after making a social error.","The study found that participants approached the robot closer after observing an error, which is counterintuitive, and that the type of error did not significantly impact the approach distances.","The robot moved towards the participant, and the participant indicated when the robot should stop, and vice versa.",ANOVA; Mann-Whitney U,"A two-way MANOVA was performed to analyze the influence of error condition and approach order on the measured stop distances. Mann-Whitney U tests were used to compare the Godspeed questionnaire results between the error and baseline conditions, and between the two error conditions. Additionally, correlations between stop distances and personality traits were investigated, but no significant correlations were found.",TRUE,Robot-accuracy; Robot-social-attitude,Robot-accuracy,Robot-social-attitude,"The study manipulated 'Robot-accuracy' by introducing technical errors (robot knocking over items) and socio-cognitive errors (incorrect gender recognition) during the introduction phase. These errors directly impacted the robot's ability to perform correctly, thus affecting task performance. The study also manipulated 'Robot-social-attitude' by having the robot make a social error (gender confusion). The results showed that the presence of errors ('Robot-accuracy') had an impact on proxemics (participants approached closer), but the type of error ('Robot-social-attitude' vs technical) did not significantly impact trust levels. The paper states, 'The analysis showed no significant difference on the approach distances between the technical error and social error' and 'This means that there is no difference in the type of error as far as perception of the robot is concerned in the five factors of the Godspeed questionnaire.'",10.1075/is.18067.flo,http://www.jbe-platform.com/content/journals/10.1075/is.18067.flo,"Abstract             Trust is a key dimension of human-robot interaction (HRI), and has often been studied in the HRI community. A                     common challenge arises from the difficulty of assessing trust levels in ecologically invalid environments: we present in this                     paper two independent laboratory studies, totalling 160 participants, where we investigate the impact of different types of errors                     on resulting trust, using both behavioural and subjective measures of trust. While we found a (weak) general effect of errors on                     reported and observed level of trust, no significant differences between the type of errors were found in either of our studies.                     We discuss this negative result in light of our experimental protocols, and argue for the community to move towards alternative                     methodologies to assess trust."
"Fooladi Mahani, Maziar; Jiang, Longsheng; Wang, Yue",A Bayesian Trust Inference Model for Human-Multi-Robot Teams,2020,1,20,20,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a simulated search and rescue task with three UAVs, with three sub-sessions: one for robot performance model parameter determination, and two for trust model training and validation. Participants provided trust feedback every 15 seconds and could switch between autonomous and manual detection modes for each UAV.","Participants supervised three UAVs in a simulated search and rescue mission, monitoring their performance and intervening when necessary by switching to manual detection mode.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the robots through a simulation interface, with limited direct interaction.",simulation,The interaction was through a computer simulation of a search and rescue mission.,simulated,The robots were represented as simulated UAVs in the computer interface.,shared control (fixed rules),The robots operated autonomously but participants could switch them to manual control.,Real-time Trust Measures; Questionnaires,,"Performance Metrics; robot data (sensor data, etc.)",Trust was measured through real-time feedback and performance metrics.,hidden markov model,A dynamic Bayesian network (DBN) was used to model trust as a hidden state.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's performance was manipulated through simulated detection accuracy, and the human could switch between autonomous and manual control, influencing the robot's autonomy. Participants were also given feedback on the robot's performance.","The model showed that trust was influenced by robot performance, with higher performance leading to higher trust, and human interventions indicating a lapse of trust. The model was able to predict human interventions with 72.2% accuracy.","The study found that detection accuracy was the most important factor influencing the human's perception of robot performance. The model's prediction accuracy was higher for robots with better overall performance, suggesting a potential over-trust in poorly performing robots.",The Bayesian trust inference model can infer the degrees of human trust in multiple mobile robots and also predict human interactions with relatively high accuracy (72.2%).,The robot autonomously navigated and performed visual detection in a search and rescue scenario. The human monitored the robot's performance and could intervene by switching to manual detection mode.,t-test; least square regression,"The study used least square regression to determine the parameters of the robot performance model for each participant, combining velocity, task progress, and detection accuracy. A t-test was used to compare the robot performance model weights (specifically w_b) between two groups of participants, and another t-test was used to compare the prediction accuracy of the trust model between two groups of participants based on their initial biases.",TRUE,Robot-accuracy; Robot-autonomy,Robot-accuracy,,"The study manipulated 'Robot-accuracy' by varying the simulated detection accuracy of the UAVs, as stated in the abstract: 'A linear model for robot performance in navigation and perception is first devised.' and in the results section: 'Clearly, a robot's detection accuracy was viewed as the most important contributor to performance; the more accurate the detection is, the better the robot performance is perceived.' The study also manipulated 'Robot-autonomy' by allowing participants to switch between autonomous and manual detection modes for each UAV, as described in the abstract: 'the human trust feedback to each individual robot and the human intervention are the outputs (observations)'. The results section also states: 'The participants were required to switch between the autonomous and manual detection modes for each UAV.' The study found that 'Robot-accuracy' impacted trust, as evidenced by the statement: 'The model showed that trust was influenced by robot performance, with higher performance leading to higher trust, and human interventions indicating a lapse of trust.' There is no evidence that the study found any of the manipulated factors to not impact trust.",10.1007/s12369-020-00705-1,http://link.springer.com/10.1007/s12369-020-00705-1,"In this paper, we develop a Bayesian inference model for the degree of human trust in multiple mobile robots. A linear model for robot performance in navigation and perception is ﬁrst devised. We then propose a computational trust model for the human multi-robot team based on a dynamic Bayesian network (DBN). In the trust DBN, the robot performance is the network input, the human trust feedback to each individual robot and the human intervention are the outputs (observations). The categorical Boltzmann machine is used to capture the multinomial distributions that model the conditional dependencies of the DBN. We introduce the expectation maximization (EM) algorithm for the model learning and personalization. A factorial form of the EM algorithm is adopted for the multi-robot system where each robot has its corresponding latent trust state in the human mind. Bayesian inference is conducted to ﬁnd the trust states, i.e., the trust belief. Based on the inferred trust states, we further derive the formulation to predict human interventions for model validation. A simulated human-UAV collaborative search mission is conducted with humans-in-the-loop. The experiment results show the Bayesian trust inference model can infer the degrees of human trust in multiple mobile robots and also predict human interactions with relatively high accuracy (72.2%). These ﬁndings conﬁrm the effectiveness of DBNs in modeling human trust towards multi-robot systems."
"Fratczak, Piotr; Goh, Yee Mey; Kinnell, Peter; Soltoggio, Andrea; Justham, Laura",Understanding Human Behaviour in Industrial Human-Robot Interaction by Means of Virtual Reality,2019,1,32,32,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants wore a VR headset and sensors, performed a simple task of touching spheres with the correct hand, while a virtual robot moved with varying speeds and predictability. The experiment consisted of six 2-minute parts with different robot behaviors, including sudden unexpected movements. A control group performed the same task with an idle robot.",Participants were asked to touch colored spheres with the corresponding hand while avoiding collisions with a virtual robot.,Unspecified,Industrial Robot Arms,Research; Industrial,Supervision,Monitoring,minimal interaction,Participants interacted with a virtual robot in a simulated environment.,simulation,Participants were immersed in a virtual reality environment.,simulated,The robot was a virtual representation in the VR environment.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of movements.,Behavioral Measures; Physiological Measures; Questionnaires,,Physiological Signals; Video Data; Performance Metrics,"Trust was assessed using questionnaires, behavioral measures (posture, head movement), and physiological data (heart rate, breathing).",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's speed and predictability were directly manipulated to influence the human's sense of safety and trust.,"The robot's erratic behavior decreased trust, leading to postural changes and increased physiological stress in some participants. Trust was regained over time, but subsequent unexpected movements jeopardized this.",Participants were divided into responders and non-responders based on their reaction to the robot's sudden movements. Responders showed a stronger physical and physiological response to the robot's erratic behavior. Participants reported that the robot was most distracting when its speed and movement were changing randomly.,"Humans tend to monitor robots and get distracted by them, especially when the robot's movements become erratic, leading to a decrease in trust and a shift in focus from the task to the robot.","The robot moved with varying speeds and predictability, including sudden unexpected movements. The human participant's task was to touch colored spheres with the correct hand while avoiding collisions with the robot.",,"No specific statistical tests are mentioned in the paper. The analysis primarily involves comparing extracted features (motion and physiological data) across different experimental conditions and participant groups (responders, non-responders, and control). The comparisons are descriptive, focusing on trends and differences in means and changes over time, rather than inferential statistics.",TRUE,Robot-accuracy; Robot-task-strategy,Robot-accuracy,Robot-task-strategy,"The study manipulated the robot's movement speed and predictability, which directly influenced the robot's accuracy in terms of how predictable its movements were. This is classified as 'Robot-accuracy' because the changes in speed and predictability directly affected the robot's ability to perform its movements in a way that was consistent and expected by the participants, thus impacting the task performance from the human's perspective. The robot's movement type (predictable vs. chaotic) is classified as 'Robot-task-strategy' because it changed the way the robot performed its task (moving objects) without directly influencing the success rate of the robot's task, but it did influence the human's perception of the robot's behavior. The paper states, 'The results show that the robot's behaviour does not influence the performance of human in a significant way, however, it has a large impact on their posture, focus and trust.' This indicates that the robot's accuracy (predictability) impacted trust, while the robot's task strategy (movement type) did not directly impact the human's task performance, but did influence their posture and focus. The robot's erratic behavior (changes in speed and movement) decreased trust, as evidenced by postural changes and increased physiological stress in some participants. The paper also mentions that 'Participants reported that the robot was most distracting when its speed and movement were changing randomly,' further supporting the impact of 'Robot-accuracy' on trust. The robot's task strategy (predictable vs chaotic movement) did not directly impact trust, but rather influenced the human's focus and posture.",10.1145/3363384.3363403,https://dl.acm.org/doi/10.1145/3363384.3363403,"As industry automation is evolving, the barriers between humans and machines are slowly disappearing. With humans and intelligent robots working closer together it is imperative to ensure not only physical safety but also the mental and emotional well-being of the workers. This paper uses the HTC Vive Virtual Reality headset to simulate different Human-Robot Interaction situations in which humans and robots constantly operate in a common workspace. It analyses the influence of an industrial robot’s actions on human behaviour. The results show that the robot’s behaviour does not influence the performance of human in a significant way, however, it has a large impact on their posture, focus and trust. It is shown that the human tends to naturally regain trust over time, however, the rate at which this takes place is variable and dependent on the robot’s behaviour."
"Fratczak, Piotr; Goh, Yee Mey; Kinnell, Peter; Justham, Laura; Soltoggio, Andrea",Robot apology as a post-accident trust-recovery control strategy in industrial human-robot interaction,2021,1,63,63,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were seated and wore a VR headset. They performed a task while a virtual robot also performed tasks. The robot made unexpected movements, and in one condition, apologized. Participant's motion and questionnaire data were collected.",Participants touched red or green spheres with the corresponding hand while avoiding contact with a virtual industrial robot.,Yaskawa Motoman SDA20D,Industrial Robot Arms,Industrial; Research,Manipulation,"The human and robot shared a workspace, but their tasks were independent; the human's task was to touch spheres, while the robot performed pre-programmed movements.",minimal interaction,Participants interacted with a virtual robot in a simulated environment.,simulation,The study used a virtual reality environment to simulate the interaction.,simulated,The robot was a virtual model in the VR environment.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of movements without adapting to the user.,Behavioral Measures; Questionnaires,,Video Data; Performance Metrics,Trust was assessed using questionnaires and behavioral measures such as proximity to the robot and response time.,no modeling,"The study did not use computational models of trust, but rather used statistical analysis.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot either apologized after making a mistake or did not, and the task was framed as a cooperative situation.","The robot's apology led to increased trust, faster recovery of posture, and faster response times, while the lack of apology led to slower response times and fewer mistakes.","Participants were clustered into responders and non-responders based on their reaction to the robot's sudden movements. Responders in the control strategy group recovered their posture faster after the robot apologized, while non-responders were not significantly influenced by the robot's actions. The questionnaire data showed that the robot was perceived as less scary and more predictable when it apologized.","A robot apology can positively influence people in a virtual industrial HRI situation, speeding up human recovery after a trust-violating event.","The robot moved objects in the workspace, sometimes chaotically, and made sudden unexpected movements. The human touched spheres with the correct hand while avoiding the robot.",ANOVA; Pearson correlation,"The study used ANOVA to compare the means of different groups (responders, non-responders, and control group) across various motion features (proximity to the robot, angular velocity of the headset, headset movement ratio, controller velocity, controller acceleration, mistake rate, and response time) at different parts of the experiment. Pearson's correlation was used to analyze the relationship between the change of proximity, mistake rate, and response time across all participants and parts of the experiment.",TRUE,Robot-verbal-communication-content; Robot-task-strategy,Robot-verbal-communication-content,Robot-task-strategy,"The study manipulated the robot's verbal communication by having it either apologize and explain its actions after making a sudden movement or remain silent (NCS vs CS conditions). This is classified as 'Robot-verbal-communication-content' because the content of the robot's communication (apology, explanation) was the manipulated factor. The robot also changed its task strategy by slowing down after an apology if the participant responded negatively, which is classified as 'Robot-task-strategy'. However, since no participants responded negatively, this manipulation did not have an impact on the results. The study found that the robot's apology (verbal communication content) significantly impacted trust, as evidenced by changes in participants' proximity to the robot, response time, and mistake rate. The robot's task strategy did not impact trust because the robot's behavior was the same for all participants in the CS group, as no one responded negatively to the apology.",10.1016/j.ergon.2020.103078,https://linkinghub.elsevier.com/retrieve/pii/S0169814120306685,"Due to safety requirements for Human-Robot Interaction (HRI), industrial robots have to meet high standards of safety requirements (ISO 10218). However, even if robots are incapable of causing serious physical harm, they still may influence people’s mental and emotional wellbeing, as well as their trust, behaviour and performance in close collaboration. This work uses an HTC Vive Virtual Reality headset to study the potential of using robot control strategies to positively influence human post-accident behaviour. In the designed scenario, a virtual industrial robot first makes sudden unexpected movements, after which it either does or does not attempt to apologise for them. The results show that after the robot tries to communicate with the participants, the robot is reported to be less scary, more predictable and easier to work with. Furthermore, postural analysis shows that the participants who were the most affected by the robot’s sudden movement recover 74% of their postural displacement within 60 s after the event if the robot apologised, and only 34% if it did not apologise. It is concluded, that apologies, which are commonly used as a trust-recovery strategy in social robotics, can positively influence people engaged with industrial robotics as well."
"Freedy, Amos; DeVisser, Ewart; Weltman, Gershon; Coeyman, Nicole",Measurement of trust in human-robot collaboration,2007,1,12,12,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were trained on the UGV controller station, then completed 15 trials with varying UGV competency levels (low, medium, high). After each trial, they completed the NASA TLX and a subjective trust and self-confidence measure.","Participants controlled a UGV in a simulated environment, targeting and firing at enemies, while monitoring the UGV's autonomous capabilities and intervening when necessary.",Unspecified,Unmanned Ground Vehicles,Research,Navigation,Remote Navigation,minimal interaction,Participants interacted with the robot through a simulation interface.,simulation,The interaction took place in a simulated environment.,simulated,The robot was represented in a simulation.,shared control (fixed rules),"The robot had autonomous targeting and firing capabilities, but the human could intervene.",Questionnaires; Behavioral Measures,NASA Task Load Index (NASA-TLX),Performance Metrics,Trust was measured using questionnaires and behavioral data.,"parametric models (e.g., regression)",The study used a rational decision model to calculate an expected loss score based on observed behavior.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's targeting and firing competency was manipulated at three levels (low, medium, high) to influence trust.","Subjective trust increased over trials for the low competency UGV, while it remained relatively stable for medium and high competency UGVs. The medium competency UGV showed a slight decrease in trust over trials.","Operators compensated for low UGV competence, but not for medium competence. Initial experiences with UGV competence influenced subsequent manual control. Subjective trust increased over trials for low competence UGV, suggesting trust is also an expectation of performance level.","Subjective trust increases over time when the robot's performance is consistently low, suggesting that trust is not just about correct performance, but also about the expectation of a certain level of performance.","The robot autonomously targeted and fired at enemies, while the human monitored the robot's actions and intervened when necessary by manually controlling the robot's targeting and firing.",,"The paper describes a rational decision model to calculate an expected loss score based on observed behavior, but does not explicitly mention any specific statistical tests like ANOVA, t-tests, or regression analysis. The analysis focuses on calculating an expected loss score and comparing it to operator override behavior, and categorizing operator behavior into 'over-trust,' 'under-trust,' and 'proper-trust' clusters. The analysis is primarily descriptive and does not involve inferential statistical testing.",TRUE,Robot-accuracy; Robot-autonomy,Robot-accuracy,Robot-autonomy,"The study manipulated the UGV's targeting and firing competency at three levels (low, medium, high), which directly impacts the robot's accuracy in performing the task. This is a manipulation of 'Robot-accuracy' because it directly affects the success rate of the robot in the task. The study also involved a level of autonomy where the robot autonomously targeted and fired, but the human could intervene. This is a manipulation of 'Robot-autonomy' because it changes the level of decision authority between the human and the robot. The results showed that subjective trust increased over trials for the low competency UGV, while it remained relatively stable for medium and high competency UGVs, indicating that 'Robot-accuracy' impacted trust. The study did not find that the level of autonomy impacted trust, as the level of autonomy was consistent across all conditions, and the manipulation was in the robot's competency, not the level of autonomy itself.",10.1109/CTS.2007.4621745,,"We describe a collaborative performance model that captures the critical performance attributes of the distinctive human-robotic decision and control environment. The literature and our initial experimental studies show that the element of trust in human-robot collaboration is an extremely important factor in the performance model, and accordingly we have focused much of our attention on deriving suitable and practical measures of this variable. In this paper we describe the formulation of a decision-analytical based measure of trust as well as the results of two initial experiments designed to examine trust in a tactical human-robot collaborative task performed in our new mixed initiative team performance assessment system (MITPAS) simulation environment."
"Gallimore, Darci; Lyons, Joseph B.; Vo, Thy; Mahoney, Sean; Wynne, Kevin T.",Trusting Robocop: Gender-Based Effects on Trust of an Autonomous Robot,2019,1,204,200,4,4 participants were dropped for failure to respond adequately to attention check items or for poor effort,Online Crowdsourcing,between-subjects,"Participants viewed a video of a security robot interacting with three individuals, then completed a survey.",Participants watched a video of a security robot and rated their trust and perceptions of the robot.,Unspecified,Mobile Robots; UGVs,Social; Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants passively observed a video of a robot interaction.,media,Participants watched a video of a robot interaction.,physical,The robot was a physical robot shown in the video.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions.,Questionnaires; Custom Scales,Mayer and Davis' Trust/Trustworthiness Scales (1999); Reliance Intention Scale,,Trust was measured using questionnaires and custom scales.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,The study did not manipulate any specific factors related to the robot or task; it assessed trust based on a video observation.,Females reported higher trust and perceived trustworthiness of the robot compared to males.,"Females reported higher trust and perceived trustworthiness of the robot compared to males, which is inconsistent with some findings in the economic behavioral literature.",Females reported higher trust and perceived trustworthiness of the autonomous security robot compared to males.,"The robot was shown in a video guarding an entry control point, preventing unauthorized personnel from entering a secure area, and using a non-lethal weapon on one individual. The human participants watched the video and completed a survey.",Shapiro-Wilk; Mann-Whitney U; t-test,"The normality of the data was tested using Shapiro-Wilk tests. Since the data was non-normal, Mann-Whitney U tests were used to compare the distributions for males and females on measures of reliance intentions, ability, benevolence, and integrity. A t-test was used to compare military use and public use ratings.",FALSE,,,,"The study did not manipulate any factors related to the robot or task. The study assessed trust based on a video observation. The study design implicitly influenced trust outcomes by presenting a scenario where the robot used a non-lethal weapon, but this was not an intentional manipulation. The study focused on gender differences in trust, but gender was not manipulated; it was a pre-existing individual difference. Therefore, no factors were intentionally manipulated by the researchers.",10.3389/fpsyg.2019.00482,https://www.frontiersin.org/article/10.3389/fpsyg.2019.00482/full,"Little is known regarding public opinion of autonomous robots. Trust of these robots is a pertinent topic as this construct relates to one’s willingness to be vulnerable to such systems. The current research examined gender-based effects of trust in the context of an autonomous security robot. Participants (N = 200; 63% male) viewed a video depicting an autonomous guard robot interacting with humans using Amazon’s Mechanical Turk. The robot was equipped with a non-lethal device to deter nonauthorized visitors and the video depicted the robot using this non-lethal device on one of the three humans in the video. However, the scenario was designed to create uncertainty regarding who was at fault – the robot or the human. Following the video, participants rated their trust in the robot, perceived trustworthiness of the robot, and their desire to utilize similar autonomous robots in several different contexts that varied from military use to commercial use to home use. The results of the study demonstrated that females reported higher trust and perceived trustworthiness of the robot relative to males. Implications for the role of individual differences in trust of robots are discussed."
"Gao, Fei; Clare, Andrew S; Macbeth, Jamie C; Cummings, M L",Modeling the Impact of Operator Trust on Performance in Multiple Robot Control,2013,1,144,144,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants monitored cameras of robots and marked victim positions on a map. Robots searched autonomously, but operators could take manual control. Each team of two completed three 25-minute trials.","Participants monitored robots searching for victims and marked their locations on a map, with the option to manually control the robots.",Unspecified,Mobile Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the robots through a simulation interface.,simulation,The interaction took place in a 3D simulation environment.,simulated,The robots were represented as virtual entities in the simulation.,shared control (fixed rules),"The robots operated autonomously with a path-planning algorithm, but participants could intervene with manual control.",Behavioral Measures; Questionnaires,NASA Task Load Index (NASA-TLX),Performance Metrics,Trust was assessed through behavioral measures (teleoperation frequency) and workload ratings (NASA-TLX).,no modeling,"Trust was not modeled computationally, but a system dynamics model was developed to simulate the impact of trust on performance.",Empirical HRI Studies,Virtual/Simulated Studies,Indirect Manipulation,"The automation's performance was indirectly manipulated by its suboptimal path-planning, which influenced operator trust and intervention frequency. The task difficulty was also indirectly influenced by the automation's performance, as operators had to intervene more when the automation was not performing well. The human expectations were influenced by the fact that the automation was not performing as expected.","Trust in the automation decreased over the trials, leading to increased teleoperation.","Operators spent more time on teleoperation in later trials, indicating a loss of trust. Operators with low teleoperation had worse performance, while high teleoperation did not improve performance compared to moderate teleoperation, suggesting diminishing returns.","Operator trust in automation decreased over time due to perceived suboptimal performance, leading to increased manual control, which improved performance up to a point, after which cognitive overload limited further gains.",The robots autonomously searched for victims using a path-planning algorithm. The human participants monitored the robots' cameras and marked the locations of victims on a map. They could also take manual control of the robots when they deemed it necessary.,ANOVA; ANOVA; ANOVA,"The study used ANOVA tests to analyze the impact of trial sequence on teleoperation time, the relationship between teleoperation clusters and performance (victims found), and the differences in NASA-TLX workload ratings among the teleoperation clusters. Specifically, the first ANOVA examined the effect of trial sequence on the time spent on teleoperation. The second ANOVA compared the performance (total victims found) across different clusters of teleoperation frequency. The third ANOVA analyzed the differences in NASA-TLX workload ratings, specifically the temporal demand dimension, among the different teleoperation clusters.",TRUE,Robot-accuracy; Task-complexity; Robot-autonomy,Robot-accuracy; Robot-autonomy,Task-complexity,"The study manipulated 'Robot-accuracy' by using a suboptimal path-planning algorithm for the robots, which led to them revisiting already explored areas and missing some areas, as stated in the paper: 'In interviews after the experiment, many participants said that the path-planning algorithm did not do a very good job and could not be trusted. They stated that robots often went back to places already explored, sometimes multiple times, while leaving some other places unexplored.' This manipulation directly impacted the perceived performance of the robots and thus influenced operator trust. The 'Task-complexity' was indirectly influenced by the automation's performance, as operators had to intervene more when the automation was not performing well, which increased the mental workload. However, the task itself (monitoring and marking victims) remained the same, and the study did not explicitly manipulate the task's inherent complexity. The study also manipulated 'Robot-autonomy' by allowing participants to switch between autonomous and manual control of the robots. The paper states: 'By default, robots searched autonomously for victims based on a path-planning algorithm. Operators could choose to take manual control and teleoperate an individual robot during this process when they felt it was necessary.' The level of autonomy was not directly manipulated, but the option to switch between autonomous and manual control was a key aspect of the study design and impacted trust. The study found that the perceived 'Robot-accuracy' and the ability to switch between 'Robot-autonomy' impacted trust, as operators lost trust in the automation due to its suboptimal performance and chose to intervene more frequently. The 'Task-complexity' was not found to directly impact trust, as the task itself remained constant, and the changes in workload were a consequence of the automation's performance and the operator's response to it, rather than a direct manipulation of the task's complexity.",,,"We developed a system dynamics model to simulate the impact of operator trust on performance in multiple robot control. Analysis of a simulated urban search and rescue experiment showed that operators decided to manually control the robots when they lost trust in the autonomous planner that was directing the robots. Operators who rarely used manual control performed the worst. However, the operators who most frequently used manual control reported higher workload and did not perform any better than operators with moderate manual control usage. Based on these findings, we implemented a model where trust and performance form a feedback loop, in which operators perceive the performance of the system, calibrate their trust, and adjust their control of the robots. A second feedback loop incorporates the impact of trust on cognitive workload and system performance. The model was able to replicate the quantitative performance of three groups of operators within 2.3%. This model could help us gain a greater understanding of how operators build and lose trust in automation and the impact of those changes in trust on performance and workload, which is crucial to the development of future systems involving humanautomation collaboration."
"Gao, Yuan; Sibirtseva, Elena; Castellano, Ginevra; Kragic, Danica",Fast Adaptation with Meta-Reinforcement Learning for Trust Modelling in Human-Robot Interaction,2019,1,24,24,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were given a description of the experiment, signed a consent form, and filled out a pre-study questionnaire. They then completed four sessions of three runs each in a mixed reality escape room, with the first run being a test run. After each session, participants filled out a questionnaire evaluating perceived bi-directional trust. Finally, they completed a questionnaire about their overall experience.",Participants collaborated with a robot to escape a mixed reality escape room by answering the robot's questions.,Pepper,Humanoid Robots; Expressive Robots,Research; Social,Game,Puzzle/Logic Game,minimal interaction,Participants interacted with the robot through verbal instructions and responses in a mixed reality environment.,real-world,The interaction took place in a mixed reality environment using a HoloLens headset.,physical,Participants interacted with a physical Pepper robot in the study.,shared control (adaptive),The robot adapted its behavior based on the participant's responses using a meta-learning algorithm.,Questionnaires,Propensity to Trust Scales,Speech Data,Trust was measured using questionnaires and speech data was collected for trust modeling.,"deep learning (e.g., neural networks, reinforcement learning)",A meta-learning based policy gradient method was used to model trust.,Empirical HRI Studies,Mixed Reality Studies,Direct Manipulation,"The adaptation algorithm was manipulated, with one group using a meta-learning based approach and the other using a statistical approach, influencing the robot's performance, behavior, and autonomy level.","The meta-learning based adaptation increased the perceived trustworthiness of the robot and how much the robot trusted the participant, compared to the statistical adaptation model.","The dynamics of perceived trust towards the robot differed significantly between the two conditions, with the meta-learning group showing a gradual increase in trust over sessions, while the statistical group showed a drop in trust in the second session. The dynamics of perceived robot's trust towards the participant was not significantly different between groups, possibly due to the meta-learning algorithm converging quickly.",A meta-learning based policy gradient method increased the perceived bi-directional trust in a human-robot interaction scenario compared to a statistical adaptation model.,"The robot asked verbal questions to the participant, and the participant answered 'yes' or 'no'. The robot adapted its questions based on the participant's responses, and the goal was for the participant to escape the mixed reality room.",ANOVA,"A mixed design repeated measures one-way ANOVA was used to analyze the bi-directional perceived trust. The conditions (meta-learning vs. statistical adaptation) were treated as between-subjects factors, and the session numbers were treated as within-subjects factors. The dependent variables were perceived trust towards the robot and perceived robot's trust towards the participant. The ANOVA was used to determine the effect of the condition, session number, and their interaction on the trust measures.",TRUE,Robot-adaptability,Robot-adaptability,,"The study explicitly manipulated the robot's adaptation algorithm, comparing a meta-learning based approach (C2) with a statistical approach (C1). This manipulation directly influenced how quickly and effectively the robot adapted to the participant's responses. The paper states, 'We investigate the effect of our adaptation model on the perceived bi-directional trust.' and 'Our main hypothesis is that, due to faster adaptation, our model increases the participant's perception of the robot's trustworthiness and how much, in their opinion, the robot trust them.' This clearly indicates that the adaptation algorithm was the manipulated factor. The results showed that the meta-learning based adaptation (C2) led to higher perceived trust and a different trust dynamic compared to the statistical approach (C1). Therefore, 'Robot-adaptability' was the factor that impacted trust. There were no other factors manipulated in the study.",10.1109/IROS40897.2019.8967924,https://ieeexplore.ieee.org/document/8967924/,"In socially assistive robotics, an important research area is the development of adaptation techniques and their effect on human-robot interaction. We present a metalearning based policy gradient method for addressing the problem of adaptation in human-robot interaction and also investigate its role as a mechanism for trust modelling. By building an escape room scenario in mixed reality with a robot, we test our hypothesis that bi-directional trust can be inﬂuenced by different adaptation algorithms. We found that our proposed model increased the perceived trustworthiness of the robot and inﬂuenced the dynamics of gaining human’s trust. Additionally, participants evaluated that the robot perceived them as more trustworthy during the interactions with the meta-learning based adaptation compared to the previously studied statistical adaptation model."
"García-Corretjer, Marialejandra; Ros, Raquel; Mallol, Roger; Miralles, David",Empathy as an engaging strategy in social robotics: a pilot study,2023,1,18,18,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a demographic survey and an empathy baseline questionnaire, then completed two trials of a maze game with a robot, one with an argumentative strategy and one with a turn-based strategy, followed by post-questionnaires and cued-recall debriefs.","Participants collaborated with a robot to solve a maze game, using either an argumentative or turn-based strategy.",Unspecified,Mobile Robots; Expressive Robots,Research; Social,Game,Cooperative Game,minimal interaction,"Participants interacted with the robot through a game, with verbal instructions and some physical presence of the robot.",real-world,The interaction took place in a real-world setting with a physical robot and a projected game environment.,physical,A physical robot was present and interacted with the participant.,shared control (fixed rules),"The robot operated independently but used fixed rules to respond to the human user, with some adaptation based on user input.",Behavioral Measures; Questionnaires,,,"Trust was assessed using questionnaires and behavioral measures, but no data was collected for trust modeling.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The collaborative strategy was manipulated, with one condition allowing for argumentation and the other being turn-based, influencing the robot's behavior and the participant's role.","The argumentative strategy promoted teamwork attitudes and a sense of trust, while the turn-based strategy did not.","The study found that an active collaborative strategy promoted teamwork attitudes and a sense of trust, while a passive strategy did not. There was a trend for joint affective expressions to be more frequent in the passive condition, which was unexpected. The cued-recall method was more effective at capturing the development of trust than the questionnaires.","An active collaborative strategy, which allows for reflection and argumentation, promotes a greater sense of closeness and trust between humans and robots compared to a passive strategy.","The robot and human collaborated to solve a maze game. The robot suggested moves and expressed emotions, while the human provided input through a keyboard and mouse, and also expressed their emotional state. The core task was to agree on the next move to reach the exit of the maze.",Wilcoxon rank sum; Spearman correlation; Wilcoxon rank sum,The study used Wilcoxon matched pairs tests to compare responses from questionnaires and cued-recall annotations between the two experimental conditions (ARG-based and TURN-based). Spearman correlation analyses were used to assess the influence of participants' empathy levels and age on their responses. Wilcoxon rank-sum tests were used to explore the influence of the order of the strategies and gender on the responses.,TRUE,Robot-autonomy; Robot-emotional-display; Robot-verbal-communication-content; Teaming,Robot-autonomy; Teaming,Robot-emotional-display; Robot-verbal-communication-content,"The study manipulated the level of robot autonomy by using two different collaborative strategies: an argumentative (ARG-based) strategy and a turn-based (TURN-based) strategy. In the ARG-based condition, the robot actively participated in decision-making, considering the user's emotional state and providing arguments for its choices, which represents a form of shared control. In the TURN-based condition, the robot's autonomy was limited to taking turns, with no discussion or consideration of the user's input, representing a lower level of shared control. This manipulation of autonomy directly impacted trust, as the ARG-based strategy promoted teamwork attitudes and a sense of trust, while the TURN-based strategy did not. The robot's emotional displays (facial expressions, sounds, LEDs) and verbal communication content (polite and neutral utterances) were also manipulated, but these did not have a significant impact on trust levels. The study also manipulated the teaming aspect by having the robot either actively collaborate with the user (ARG-based) or passively take turns (TURN-based). The active collaboration promoted teamwork attitudes and a sense of trust, while the passive strategy did not. The paper states, 'Quantitative and qualitative analysis of the pilot study confirmed that a general sense of closeness with the robot was perceived when applying the active strategy' and 'quantitative analysis partially supported the hypothesis that an active collaborative strategy will promote teamwork attitudes, where the human is open to the robot’s suggestions and to act as a teammate'. This indicates that the manipulation of autonomy and teaming had a direct impact on trust, while the emotional displays and verbal communication content did not significantly impact trust.",10.1007/s11257-022-09322-1,https://link.springer.com/10.1007/s11257-022-09322-1,"Abstract                            Empathy plays a fundamental role in building relationships. To foster close relationships and lasting engagements between humans and robots, empathy models can provide direct clues into how it can be done. In this study, we focus on capturing in a quantitative way indicators of early empathy realization between a human and a robot using a process that encompasses affective attachment, trust, expectations and reflecting on the other’s perspective within a set of collaborative strategies. We hypothesize that an active collaboration strategy is conducive to a more meaningful and purposeful engagement of realizing empathy between a human and a robot compared to a passive one. With a deliberate design, the interaction with the robot was presented as a maze game where a human and a robot must collaborate in order to reach the goal using two strategies: one maintaining control individually taking turns (passive strategy) and the other one where both must agree on their next move based on reflection and argumentation. Quantitative and qualitative analysis of the pilot study confirmed that a general sense of closeness with the robot was perceived when applying the active strategy. Regarding the specific indicators of empathy realization: (1)               affective attachment               , affective emulation was equally present throughout the experiment in both conditions, and thus, no conclusion could be reached; (2)               trust               , quantitative analysis partially supported the hypothesis that an active collaborative strategy will promote teamwork attitudes, where the human is open to the robot’s suggestions and to act as a teammate; and (3)               regulating expectation               , quantitative analysis confirmed that a collaborative strategy promoted a discovery process that regulates the subject’s expectation toward the robot. Overall, we can conclude that an active collaborative strategy impacts favorably the process of realizing empathy compared to a passive one. The results are compelling to move the design of this experiment forward into more comprehensive studies, ultimately leading to a path where we can clearly study engagements that reduce abandonment and disillusionment with the process of realizing empathy as the core design for active collaborative strategies."
"Gaudiello, Ilaria; Zibetti, Elisabetta; Lefort, Sébastien; Chetouani, Mohamed; Ivaldi, Serena",Trust as indicator of robot functional and social acceptance. An experimental study on user conformation to iCub answers,2016,1,56,56,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants completed questionnaires, imagined a HRI scenario, and then performed functional and social decision-making tasks with the iCub robot.","Participants made decisions about functional (weight, sound, color) and social (context-appropriate item selection) issues, and their conformation to the robot's answers was measured.",iCub,Humanoid Robots,Research; Social,Evaluation,Ranking,minimal interaction,Participants interacted with the robot through verbal instructions and responses.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical humanoid robot.,wizard of oz (directly controlled),The robot's responses were controlled by a human operator.,Behavioral Measures; Questionnaires,Negative Attitude towards Robots Scale (NARS); N/A,Video Data; Speech Data,Trust was assessed through conformation to robot answers and questionnaires.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"Participants were assigned to different imagined HRI scenarios (collaborative, competitive, neutral) and the robot always contradicted the participant's answers.","Participants conformed more to the robot's answers in functional tasks than in social tasks, and the imagined scenario did not significantly influence trust.","Participants showed a general tendency not to conform to the robot's answers, except in functional tasks. The few participants who conformed in social tasks did not conform in functional tasks. There was a negative correlation between desire for control and negative attitudes towards social influence of robots.","Participants trusted the robot's functional savvy more than its social savvy, as evidenced by higher conformation rates in functional decision-making tasks.","The robot provided answers to questions about functional and social issues, and the human participant decided whether to conform to the robot's answer or keep their initial decision.",Wilcoxon rank sum; Kruskal-Wallis; Spearman correlation,"The study used a Wilcoxon test for paired samples to compare conformation scores between functional and social tasks. A Kruskal-Wallis test was used to assess the effect of the imagined HRI scenario (collaborative, competitive, neutral) on conformation scores in both functional and social tasks. Spearman non-parametric correlation tests were used to examine the relationship between conformation scores and scores on the Negative Attitude towards Robots Scale (NARS-S2) and the Desire for Control (DFC) scale.",TRUE,Teaming; Robot-autonomy,,Teaming,"The study manipulated the imagined HRI scenario (collaborative, competitive, or neutral), which falls under the 'Teaming' category as it changes the nature of the interaction between the human and the robot. The robot's responses were controlled by a human operator, which is a form of 'Wizard of Oz' control, thus the 'Robot-autonomy' was also manipulated by the study design. The study found that the imagined HRI scenario ('Teaming') did not significantly influence trust, as measured by conformation to the robot's answers. The study did not find any factors that impacted trust, as the only factor that was found to have a significant impact was the type of task (functional vs. social), which is not a manipulated factor. The robot's autonomy was not directly manipulated as it was always wizard of oz, but it is a factor that was present in the study design.",10.1016/j.chb.2016.03.057,https://linkinghub.elsevier.com/retrieve/pii/S074756321630228X,"To investigate the dynamics of human-robot acceptance, we carried out an experimental study with 56 adult participants and the iCub robot. Trust in the robot has been considered as a main indicator of acceptance and measured by the participants' conformation to the iCub's answers to questions on functional and social tasks characterized by perceptual and socio cognitive uncertainty. In particular, we were interested in understanding whether (i) trust in functional savvy is a prerequisite for trust in social savvy, and (ii) to what extent factors such as participants' desire for control, attitude towards social inﬂuence of robots, and imagined collaborative vs. competitive scenario, may inﬂuence their trust in the iCub. We found that participants conformed more to the iCub's answers in the functional than in the social tasks. Moreover, the few participants conforming to the iCub's answers in the social task also conformed less in the functional issues. Trust in the robot's functional savvy does not thus seem to be a pre-requisite for trust in its social savvy. Finally, the examined factors did not inﬂuence the trust in iCub. Results are discussed with relation to methodology of human-robot interaction (HRI) research."
"Geiskkovitch, Denise Y.; Thiessen, Raquel; Young, James E.; Glenwright, Melanie R.",What? That's Not a Chair!: How Robot Informational Errors Affect Children's Trust Towards Robots,2019,1,21,17,4,"2 children were too shy to interact with the robots, 1 child was afraid of the robot, 1 child did not follow task directions",Educational Setting,within-subjects,"Children observed two robots, one making errors and one not, during a history phase. Then, in the same label phase, the robots labeled uncommon objects with made-up names, and the child was asked to pick the object matching the name. In the contrast label phase, the robots labeled objects, and the child was asked to pick an object with a different name. Finally, in the clean-up task, the robots gave conflicting instructions on where to place a basket of toys.","Children were asked to observe two robots labeling objects, and then choose which robot to trust based on their previous accuracy.",Nao,Humanoid Robots,Research; Social,Social,Social Perception,minimal interaction,Children interacted with the robots by observing them and responding to their instructions.,real-world,The study was conducted in a real-world setting with physical robots.,physical,The study used physical robots.,wizard of oz (directly controlled),The robots were controlled by a research assistant through a Wizard of Oz interface.,Behavioral Measures,,,Trust was measured by observing which robot the child chose to follow.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers manipulated the robot's performance by having one robot consistently provide correct labels and the other consistently provide incorrect labels, which was intended to affect trust.","Children trusted the previously correct robot more in the same label phase, but not in the contrast label phase. There was a potential gender effect in the contrast label phase.","The study found a potential gender effect in the contrast label phase, with girls more often siding with the previously correct robot and boys picking the previously incorrect robot's answer. The study failed to replicate previous findings in the contrast label phase.","Children develop trust models of robots based on previous errors, similar to how they do with people, but this effect was not consistent across all tasks.","The robots labeled objects, and the children were asked to choose which robot to trust based on the labels provided. The children also had to follow the robots' instructions in a clean-up task.",t-test; ANOVA; Chi-squared,"The study used a one-sample t-test to compare children's choices in the same-label phase against chance (50%). An ANOVA was used to explore a potential gender effect in the contrast-label phase. Chi-squared tests were used to analyze the clean-up tasks, assessing whether children complied with the previously correct robot's instructions.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy,Task-complexity,"The study manipulated 'Robot-accuracy' by having one robot consistently provide correct labels for objects during the history phase, while the other robot provided incorrect labels. This was explicitly done to see how the children's trust would be affected by the robot's accuracy. The paper states, 'During this phase children observed the two robots label common objects...One of the robots (the previously correct robot) provided correct labels for the objects, while the other robot (previously incorrect robot) provided incorrect labels'. This manipulation was intended to influence the children's trust in the robots. The study also manipulated 'Task-complexity' by having two different testing phases: the same label phase, where the task was simple, and the contrast label phase, where the task was more complex. The paper states, 'The goal of this phase was to test children's trust towards the two robots when information is simple and clear' (same label phase) and 'The contrast phase was meant to test children's trust towards robots in a more complex situation than the same label phase, with some ambiguity introduced' (contrast label phase). The results showed that 'Robot-accuracy' impacted trust in the same label phase, as children trusted the previously correct robot more. However, 'Task-complexity' impacted trust in the contrast label phase, as the children did not trust the previously correct robot more in this phase. Therefore, 'Robot-accuracy' impacted trust, while 'Task-complexity' did not impact trust in the way the researchers expected.",10.1109/HRI.2019.8673024,https://ieeexplore.ieee.org/document/8673024/,"Robots that interact with children are becoming more common in places such as child care and hospital environments. While such robots may mistakenly provide nonsensical information, or have mechanical malfunctions, we know little of how these robot errors are perceived by children, and how they impact trust. This is particularly important when robots provide children with information or instructions, such as in education or health care. Drawing inspiration from established psychology literature investigating how children trust entities who teach or provide them with information (informants), we designed and conducted an experiment to examine how robot errors affect how young children (3-5 years old) trust robots. Our results suggest that children utilize their understanding of people to develop their perceptions of robots, and use this to determine how to interact with robots. Specifically, we found that children developed their trust model of a robot based on the robot’s previous errors, similar to how they would for a person. We however failed to replicate other prior findings with robots. Our results provide insight into how children as young as 3 years old might perceive robot errors and develop trust."
"Gempton, Nicole; Skalistis, Stefanos; Furness, Jane; Shaikh, Siraj; Petrovic, Dobrila",Autonomous Control in Military Logistics Vehicles: Trust and Safety Analysis,2013,1,3,3,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants drove a simulated vehicle in a convoy, experiencing both manual and autonomous control periods, with physiological and performance data collected.","Participants drove a simulated military logistics vehicle, following a lead vehicle in a convoy, with periods of both manual and autonomous control.",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a driving simulator, experiencing both manual and autonomous control.",simulation,The study used a virtual driving simulator to simulate a driving environment.,simulated,The robot was a simulated vehicle within the driving simulator.,shared control (fixed rules),"The vehicle had periods of autonomous control with fixed rules, and the human driver could take over.",Physiological Measures; Performance-Based Measures,,Physiological Signals; Performance Metrics,Trust was assessed using heart rate and reaction time measures.,no modeling,No computational model of trust was developed in this study.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The level of autonomy was manipulated by switching between manual and autonomous control, and the task difficulty was varied by including poor visibility and loud noises.","The study found that drivers' heart rate increased slightly during periods of autonomous control, suggesting a potential impact on trust, but the impact was not consistent across all drivers.","The study found that drivers' reaction time improved during autonomous control, which is an unexpected result given that the driver role changes to a monitoring task.","The study found that drivers' reaction time improved during autonomous control, suggesting that autonomy may not necessarily lead to skill decay.","The robot (simulated vehicle) autonomously followed a path and maintained a target speed, while the human driver monitored the vehicle and took over control when needed.",,"The paper describes the collection of various data points such as heart rate, reaction time, and vehicle parameters. However, it does not explicitly mention the use of any specific statistical tests. The analysis is primarily descriptive, focusing on trends and averages in the collected data, such as changes in heart rate during autonomous control and improvements in reaction time. There is no mention of inferential statistics or hypothesis testing.",TRUE,Robot-autonomy; Task-complexity; Task-environment,Robot-autonomy,,"The study manipulated 'Robot-autonomy' by switching between manual and autonomous control periods, as stated in the 'Experimental Design' section: 'Some experiments were designed to allow drivers to have manual control throughout the journey, whereas others were designed to (uniformly) incur periods of autonomous control'. The study also manipulated 'Task-complexity' by including a secondary Stroop test, as described in the 'Experimental Design' section: 'The secondary task we used was a stroop test...used to assess the drivers' cognitive load during the driving/monitoring task'. The 'Task-environment' was manipulated by including 'short periods of poor visibility and loud (bang) noises' as described in the 'Experimental Design' section. The study found that 'drivers had a slight increase in their average HR through periods of autonomous control', indicating that 'Robot-autonomy' impacted trust. There is no evidence in the paper that the other manipulated factors did not impact trust, so they are not included in the 'factors_that_did_not_impact_trust' list.",,http://link.springer.com/10.1007/978-3-642-39354-9_28,"Ground vehicles are increasingly designed to incorporate autonomous control for better performance, control and efficiency. Such control is particularly critical for military logistics vehicles where drivers are carrying sensitive loads through potentially threatening routes. It is imperative therefore to evaluate what role does autonomy play to help safety, and whether drivers trust autonomous control. In this paper we investigate the use of semiautonomous vehicles used for military logistics and carry out human factors analysis to reflect on trust and safety issues that emerge from the driving of such vehicles."
"George, Ceenu; Eiband, Malin; Hufnagel, Michael; Hussmann, Heinrich",Trusting Strangers in Immersive Virtual Reality,2018,1,21,21,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a trust game in VR with both a human-like and robot avatar, and completed pre- and post-game questionnaires.",Participants played a trust game in virtual reality with different avatar types.,Unspecified,Humanoid Robots,Research,Game,Economic Game,minimal interaction,Participants interacted with avatars in a virtual environment.,simulation,Participants were immersed in a virtual reality environment using an Oculus Rift.,simulated,The robot was a virtual avatar in the VR environment.,wizard of oz (directly controlled),The robot avatar's actions were controlled by an assistant.,Behavioral Measures; Questionnaires,Interpersonal Trust Scale/Questionnaire; N/A,,Trust was measured using a trust game and questionnaires.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The avatar's appearance was manipulated to be either human-like or robot-like to see how it affected trust.,"No significant difference in trust was found between the two avatar types, but participants felt more comfortable with the human-like avatar.","Although there was no significant difference in the amount of money sent to each avatar, participants reported feeling more comfortable with the human-like avatar, indicating a potential disconnect between behavior and subjective feelings.","There was no significant difference in trust towards human-like and robot avatars in a virtual trust game, but participants felt more comfortable with the human-like avatar.","Participants played a trust game where they decided how much money to send to a virtual avatar, which was either human-like or robot-like. The avatar was controlled by an assistant who returned the money sent by the participant.",t-test,A dependent t-test was used to compare the social presence scores between the human-like and robot avatar conditions. The test aimed to determine if there was a statistically significant difference in the perceived level of togetherness between the two avatar types.,TRUE,Robot-aesthetics,,Robot-aesthetics,"The study manipulated the appearance of the avatar to be either human-like or robot-like. This falls under the category of 'Robot-aesthetics' as it is a change to the visual appeal of the robot. The paper states, 'Two virtual avatars, human-like and robot, were developed to depict the trustee.' The results showed that there was no significant difference in the amount of money sent to each avatar, indicating that the manipulation of aesthetics did not impact trust levels. The paper states, 'We could not confirm a significant difference between the amount sent to the human-like (Mean = 6.8, S D = 3.5) vs robot (Mean = 6.3, S D = 3.4) avatar. This indicates that there may not be a difference in trust towards these categories of avatars.' Therefore, 'Robot-aesthetics' is listed under 'factors_that_did_not_impact_trust'.",10.1145/3180308.3180355,https://dl.acm.org/doi/10.1145/3180308.3180355,"Social interactions in immersive virtual reality (IVR) beneﬁt from more realistic designed avatars whilst head mounted displays (HMD) are simultaneously offering virtual reality experiences with improving levels of immersion and presence. The combination of these developments creates a need to understand how users remit trust towards avatars in IVR. We evaluated trust towards two categories of avatars (robot vs. human-like) in VR by conducting a lab study (N=21) where participants had to play a trust game (TG) with each avatar. Our ﬁndings highlight that although the trust game revealed equal trust levels towards both categories of avatars, participants felt a signiﬁcant sense of ""togetherness"" with the human-like avatar compared to the robot."
"Ghazali, Aimi S.; Ham, Jaap; Barakova, Emilia I.; Markopoulos, Panos",Effects of Robot Facial Characteristics and Gender in Persuasive Human-Robot Interaction,2018,1,72,72,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants first interacted with a demonstrator robot with a neutral face, then played a game with an advisor robot that had either trustworthy or untrustworthy facial features and either the same or opposite gender as the participant. Participants completed questionnaires after each interaction.",Participants played a beverage creation game where they could ask a robot advisor for help in selecting ingredients.,SociBot TM,Humanoid Robots; Expressive Robots,Social; Research,Game,Economic Game,minimal interaction,Participants interacted with the robot through a game interface and verbal instructions.,real-world,Participants interacted with a physical robot in a lab setting.,physical,The robot was a physical entity with a human-like head and torso.,wizard of oz (directly controlled),The robot's advice was controlled by the experimenter using the Wizard of Oz technique.,Behavioral Measures; Questionnaires; Custom Scales,Jian et al. Trust Scale; Interpersonal Trust Scale/Questionnaire,Video Data,"Trust was measured using questionnaires, behavioral data (help requests), and video analysis of facial expressions.","parametric models (e.g., regression)","Regression analysis was used to investigate the relationship between facial characteristics, liking, trust, and psychological reactance.",Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's facial characteristics (trustworthy vs. untrustworthy) and gender (same vs. opposite as the participant) were directly manipulated to influence trust and psychological reactance. The robot also provided advice to the participant.,"Trusting beliefs and behaviors were higher for robots with trustworthy facial features. Gender similarity did not affect trust, but opposite-gender robots caused higher psychological reactance. Liking of the robot mediated the effect of facial characteristics on trust and reactance.","The study found that liking fully mediated the influence of facial characteristics on trusting beliefs and psychological reactance. Also, psychological reactance was a strong predictor of trusting beliefs but not of trusting behavior. Female participants showed lower trusting beliefs and higher psychological reactance than male participants.","Robots with trustworthy facial characteristics engender higher trusting beliefs and lower psychological reactance, and this effect is mediated by liking of the robot.","The robot acted as an advisor, providing persuasive advice to participants on which ingredients to choose for a beverage creation game. Participants could choose to follow the robot's advice or make their own selections.",ANOVA; ANCOVA; ANOVA; Pearson correlation; Linear regression,"The study used a variety of statistical tests to analyze the data. ANOVA was used for manipulation checks and to examine the effect of facial characteristics on trusting behaviors. ANCOVA was used to test the hypotheses related to trusting beliefs and psychological reactance, controlling for baseline scores. MANOVA was used to examine the effect of gender similarity on trusting behaviors. Pearson correlation was used to assess the relationship between trusting beliefs, trusting behaviors, and psychological reactance. Regression analysis was used to investigate the mediating role of liking on the relationship between facial characteristics and trusting beliefs, trusting behaviors, and psychological reactance.",TRUE,Robot-emotional-display; Robot-social-attitude,Robot-emotional-display,Robot-social-attitude,"The study manipulated the robot's facial characteristics to display either trustworthy or untrustworthy features, which falls under 'Robot-emotional-display' as it directly relates to the robot's emotional expression. The study also manipulated the robot's gender to be either the same or opposite of the participant, which is categorized as 'Robot-social-attitude' because it influences the social approach and perceived similarity of the robot. The results showed that the robot's facial characteristics ('Robot-emotional-display') significantly impacted trust, with trustworthy faces leading to higher trust. However, the gender of the robot ('Robot-social-attitude') did not significantly impact trust levels, although it did influence psychological reactance.",10.3389/frobt.2018.00073,https://www.frontiersin.org/article/10.3389/frobt.2018.00073/full,"The growing interest in social robotics makes it relevant to examine the potential of robots as persuasive agents and, more speciﬁcally, to examine how robot characteristics inﬂuence the way people experience such interactions and comply with the persuasive attempts by robots. The purpose of this research is to identify how the (ostensible) gender and the facial characteristics of a robot inﬂuence the extent to which people trust it and the psychological reactance they experience from its persuasive attempts. This paper reports a laboratory study where SociBotTM, a robot capable of displaying different faces and dynamic social cues, delivered persuasive messages to participants while playing a game. In-game choice behavior was logged, and trust and reactance toward the advisor were measured using questionnaires. Results show that a robotic advisor with upturned eyebrows and lips (features that people tend to trust more in humans) is more persuasive, evokes more trust, and less psychological reactance compared to one displaying eyebrows pointing down and lips curled downwards at the edges (facial characteristics typically not trusted in humans). Gender of the robot did not affect trust, but participants experienced higher psychological reactance when interacting with a robot of the opposite gender. Remarkably, mediation analysis showed that liking of the robot fully mediates the inﬂuence of facial characteristics on trusting beliefs and psychological reactance. Also, psychological reactance was a strong and reliable predictor of trusting beliefs but not of trusting behavior. These results suggest robots that are intended to inﬂuence human behavior should be designed to have facial characteristics we trust in humans and could be personalized to have the same gender as the user. Furthermore, personalization and adaptation techniques designed to make people like the robot more may help ensure they will also trust the robot."
"Ginwalla, Neville Z.; Kaewkuekool, Sittichai; Bowling, Shannon R.; Gramopadhye, Anand K.; Melloy, Brian J.",Measurement of Trust in Humans in Hybrid Inspection for Different Levels of Error Randomness,2002,1,8,8,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were trained on PCB inspection, then completed inspection tasks with varying error randomness, and rated their trust at different stages.",Participants inspected simulated PCB images and identified defects.,Unspecified,Other,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a computer simulation of a hybrid inspection system.,simulation,The interaction was through a computer simulation of a PCB inspection task.,simulated,The robot was represented as a simulated system on a computer screen.,shared control (fixed rules),"The system had pre-programmed error patterns, and the human could override the system's decisions.",Questionnaires,Muir's Trust Questionnaire,Performance Metrics,Trust was measured using a questionnaire and performance data was collected.,"parametric models (e.g., regression)",The study used ANOVA and regression models to analyze trust data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The level of error randomness was manipulated to see how it affected trust.,"Trust decreased when the system made errors, and the pattern of errors significantly impacted trust levels.","Trust decreased significantly from the initial stage to the second stage when errors were introduced, and did not recover in the third stage.","Human trust in a hybrid inspection system is sensitive to error randomness, with trust decreasing when errors occur.","The simulated system presented PCB images, and the human inspected them, making accept/reject decisions and overriding the system when necessary.",ANOVA; t-test; Linear regression,"The study used a mixed model ANOVA to analyze the overall trust scores, examining the effects of error randomness levels, stages of the experiment, and their interaction. Further ANOVAs were conducted to test the significance of trust components at each stage of every level of error randomness. T-tests were used to compare trust scores between different stages within each error randomness level. A stepwise regression model was used to determine the significance of the components of trust at each stage in each level of error randomness.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the level of error randomness in the hybrid inspection system. This directly affects the accuracy of the system's performance, as the system makes errors at different rates and patterns. The paper states, 'The four levels consider (Le., ranging from completely systematic to completely random) could be faced by system designer under normal working condition.' and 'Eighteen errors (9 false alarms and 9 misses) were introduced in a set of 48 boards as in the following levels: (3) The errors were randomly distributed, (2) three errors were introduced periodically after every four good decisions, (1) three errors were introduced after three correct decisions, and (4) three errors were introduced in every seven PCB boards'. This manipulation of error patterns directly impacts the system's accuracy, making 'Robot-accuracy' the most appropriate category. The results section shows that trust decreased when errors were introduced, and the pattern of errors significantly impacted trust levels, indicating that 'Robot-accuracy' impacted trust. The paper states, 'Analysis of the results revealed that human trust in the hybrid inspection system is sensitive to error randomness.' and 'A significant difference was seen between level of error randomness 1-3 (p = 0.0003), 2-3 (p = 0.0023) and 3-4 (p = 0.0002)'. There were no other factors manipulated that were found to not impact trust.",10.1177/154193120204602311,http://journals.sagepub.com/doi/10.1177/154193120204602311,"The focus of this research is on the effect of human trust in a hybrid inspection system with different levels of error randomness. The experimental designs were developed to conduct inspection tasks with four levels of error randomness, and subjects were requested to rate their trust at different levels in the system. Each randomness level comprised of stages, and human trust variation for each stage was observed. These levels were administered through the use of a hybrid inspection simulator. Analysis of the results revealed that human trust in the hybrid inspection system is sensitive to error randomness."
"Giorgi, Ioanna; Tirotto, Francesca Ausilia; Hagen, Oksana; Aider, Farida; Gianni, Mario; Palomino, Marco; Masala, Giovanni L.",Friendly But Faulty: A Pilot Study on the Perceived Trust of Older Adults in a Social Robot,2022,1,18,17,1,1 participant was excluded for showing a strong response bias,Controlled Lab Environment,between-subjects,"Participants were briefed about interacting with a robot, then interacted with the robot in one of four conditions, and finally completed a questionnaire and a recorded interview.",Participants were asked to interact with a robot that would administer the intake of health supplements.,Nao,Humanoid Robots; Expressive Robots,Care; Social; Research,Social,Conversation,direct-contact interaction,"Participants had direct interaction with the robot, including a handshake.",real-world,The interaction took place in a lab designed to resemble a living room.,physical,Participants interacted with a physical robot.,fully autonomous (limited adaptation),"The robot operated autonomously, with limited adaptation to the interaction.",Questionnaires; Behavioral Measures,,Video Data; Speech Data,Trust was assessed using questionnaires and behavioral measures from video analysis.,no modeling,Trust was not modeled computationally; descriptive statistics were used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's attitude (warm/cold) and conduct (error/no-error) were manipulated to influence trust. The warm attitude included a handshake and empathic speech, while the error condition involved the robot suggesting the wrong supplement.",Trust decreased when the robot committed an error. A warm attitude increased trust only when the robot did not make an error. The robot's attitude did not act as an efficient recovery strategy.,"The study found that a warm robot attitude did not compensate for a robot error. Interestingly, participants gave higher positive ratings to a cold robot that made an error compared to a cold robot that did not make an error.",The robot's performance (error vs. no-error) had a greater impact on trust than the robot's attitude (warm vs. cold).,"The robot initiated a handshake, then suggested which supplement the participant should take. The participant was instructed to interact with the robot and respond to its prompts.",Spearman correlation,Spearman's correlation was used to assess the association between trust and intention to use robots at home. The study also used descriptive statistics to analyze the perceived trust based on the experimental conditions (attitude and conduct) and the willingness to use robots at home based on different levels of trust.,TRUE,Robot-social-attitude; Robot-accuracy,Robot-accuracy,Robot-social-attitude,"The study manipulated the robot's social attitude by having it act either warm (empathic speech, handshake) or cold (impersonal). This is described in the paper as 'We ran two behavioural conditions of the robot: warm and cold, where warm indicates empathic behaviour of the robot including benevolent speech (imitating empathy) and presence of human-robot touch, and cold indicates an aloof robot behaviour and absence of human-robot touch' (lines 251-256). The robot's accuracy was manipulated by having it either correctly administer the supplement or make an error by suggesting the wrong supplement first, then correcting itself. This is described as 'The conduct of the robot was compromised by introducing an intentional robot error during the interaction... In the error condition, the robot would mislead the participant by first indicating the wrong supplement and correcting itself immediately to recover. In the no-error condition, the robot delivered the correct supplement' (lines 260-269). The results showed that the robot's accuracy (error vs. no-error) had a significant impact on trust, as stated in the paper 'The participants perceived the robot as less trustworthy when it committed an error' (lines 630-631). However, the robot's social attitude (warm vs. cold) did not have a main effect on trust, as stated in the paper 'the main descriptive effect showed no meaningful impact of the type of attitude on trust' (lines 632-633). Although the warm attitude did increase trust when the robot did not make an error, it did not act as a recovery strategy when the robot made an error, as stated in the paper 'when the robot commits an error, interacting with a warm robot does not change trust perception compared to a cold robot' (lines 644-646).",10.1109/ACCESS.2022.3202942,https://ieeexplore.ieee.org/document/9869833/,"The efforts to promote ageing-in-place of healthy older adults via cybernetic support are fundamental to avoid possible consequences associated with relocation to facilities, including the loss of social ties and autonomy, and feelings of loneliness. This requires an understanding of key factors that affect the involvement of robots in eldercare and the elderly willingness to embrace the robots’ domestic use. Trust is argued to be the main foundation of an effective adult-care provider, which might be more significant if such providers are robots. Establishing, and maintaining trust usually involves two main dimensions: 1) the robot’s reliability (i.e., performance) and 2) the robot’s intrinsic attributes, including its degree of anthropomorphism and benevolence. We conducted a pilot study using a mixed methods approach to explore the extent to which these dimensions and their interaction influenced elderly trust in a humanoid social robot. Using two independent variables, type of attitude (warm, cold) and type of conduct (error, no-error), we aimed to investigate if the older adult participants would trust a purposefully faulty robot when the robot exerted a warm behaviour enhanced with non-functional touch more than a robot that did not, and in what way the robot error affected trust. Lastly, we also investigated the relationship between trust and a proxy variable of actual use of robots (i.e., intention to use robots at home). Given the volatile and context-dependent nature of trust, our close-to real-world scenario of elder-robot interaction involved the administration of health supplements, in which the severity of robot error might have a greater implication on the perceived trust."
"Giorgi, Ioanna; Minutolo, Aniello; Tirotto, Francesca; Hagen, Oksana; Esposito, Massimo; Gianni, Mario; Palomino, Marco; Masala, Giovanni L.","I am Robot, Your Health Adviser for Older Adults: Do You Trust My Advice?",2023,1,30,30,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were introduced to a fictional dispenser who recommended supplements based on a wellbeing questionnaire. Participants then interacted with a NAO robot, which either provided information-type advice or recommendation-type advice on the supplements. Post-interaction, participants completed a questionnaire.",Participants were asked to interact with a robot to receive advice on health supplements.,Nao,Humanoid Robots,Care; Research; Social,Social,Social Guidance/Coaching,minimal interaction,Participants interacted verbally with the robot in a controlled lab setting.,real-world,Participants interacted with a physical robot in a lab designed to resemble a living room.,physical,Participants interacted with a physical humanoid robot.,shared control (fixed rules),The robot followed pre-programmed rules to respond to user questions and provide advice.,Questionnaires,,Video Data; Speech Data,Trust was measured using pre- and post-interaction questionnaires.,"parametric models (e.g., regression)",Linear mixed-effects regression models were used to analyze the trust data.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The type of advice given by the robot was manipulated, either information-type or recommendation-type, to influence trust.","Trust in the robot remained stable regardless of the type of advice given for supplements, but trust in the robot for prescription medicines decreased when the robot gave information-type advice, and remained stable when the robot gave recommendation-type advice.","The study found that older adults' trust in robots for prescription medicines decreased after receiving information-type advice on supplements, but remained stable after receiving recommendation-type advice, suggesting a protective role of robot-based recommendations on trust. There was a conflict in the results, where the interaction effect was not significant in the main analysis, but post-hoc tests showed a significant difference between groups.","Older adults continued to trust the robot regardless of the type of advice received, highlighting a type of protective role of robot-based recommendations on their trust.","The robot provided information or recommendations about health supplements, and the human participant asked questions and followed the robot's instructions.",Mixed-effects model; non-parametric permutation mixed-anovas; Spearman correlation,"The study used Linear Mixed-Effects Regression Modelling (LMER) to analyze the trust data, treating time and group as fixed effects and participant ID as a random effect. Nested model comparisons were performed using chi-square difference tests. Non-parametric permutation mixed-ANOVAs were used to corroborate the LMER results. Post-hoc tests using Dunnett-style comparisons were conducted on estimated marginal means. Spearman's Rank correlation coefficient was used to explore the associations between trust and intention to use robots.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the type of advice given by the robot. In one condition, the robot provided information-type advice, which was neutral and based on the human dispenser's instructions. In the other condition, the robot provided recommendation-type advice, which included an additional supplement not initially suggested by the human dispenser. This manipulation directly altered the content of the robot's verbal communication, making 'Robot-verbal-communication-content' the appropriate category. The results showed that the type of advice (information vs. recommendation) impacted trust in the robot for prescription medicines, with trust decreasing after information-type advice but remaining stable after recommendation-type advice. This indicates that 'Robot-verbal-communication-content' was a factor that impacted trust. The study did not find any factors that did not impact trust.",10.1007/s12369-023-01019-8,https://link.springer.com/10.1007/s12369-023-01019-8,"Artiﬁcial intelligence and robotic solutions are seeing rapid development for use across multiple occupations and sectors, including health and social care. As robots grow more prominent in our work and home environments, whether people would favour them in receiving useful advice becomes a pressing question. In the context of human–robot interaction (HRI), little is known about people’s advice-taking behaviour and trust in the advice of robots. To this aim, we conducted an experimental study with older adults to measure their trust and compliance with robot-based advice in health-related situations. In our experiment, older adults were instructed by a ﬁctional human dispenser to ask a humanoid robot for advice on certain vitamins and over-the-counter supplements supplied by the dispenser. In the ﬁrst experimented condition, the robot would give only information-type advice, i.e., neutral informative advice on the supplements given by the human. In the second condition, the robot would give recommendation-type advice, i.e., advice in favour of more supplements than those suggested initially by the human. We measured the trust of the participants in the type of robot-based advice, anticipating that they would be more trusting of information-type advice. Moreover, we measured the compliance with the advice, for participants who received robot-based recommendations, and a closer proxy of the actual use of robot health advisers in home environments or facilities in the foreseeable future. Our ﬁndings indicated that older adults continued to trust the robot regardless of the type of advice received, highlighting a type of protective role of robot-based recommendations on their trust. We also found that higher trust in the robot resulted in higher compliance with its advice. The results underpinned the likeliness of older adults welcoming a robot at their homes or health facilities."
"Gold, Christian; Körber, Moritz; Hohenberger, Christoph; Lechner, David; Bengler, Klaus",Trust in Automation – Before and After the Experience of Take-over Scenarios in a Highly Automated Vehicle,2015,1,72,69,3,"Two participants from the young drivers group were excluded since they closed their eyes for more than 20 seconds within one minute during data recording, Another participant of the elderly group did not react to one of the TORs",Controlled Lab Environment,mixed design,"Participants completed a questionnaire before and after a driving simulation. They were briefed about the automated system and its limits. Participants then completed a short introductory drive, followed by the main drive with three take-over situations. Eye tracking data was recorded during the main drive.",Participants experienced highly automated driving in a simulator and had to take over control in critical situations.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the automated driving system in a simulator.,simulation,Participants experienced the automated driving in a driving simulator.,simulated,The automated vehicle was simulated in a driving simulator.,shared control (fixed rules),"The automated system controlled the vehicle, but participants had to take over in specific situations.",Behavioral Measures; Questionnaires,Jian et al. Trust Scale,Eye-tracking Data,Trust was measured using questionnaires and eye-tracking data.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the experience of automated driving by including take-over situations, which influenced the perceived performance of the system and the difficulty of the task. Participants were also given information about the system's capabilities, which influenced their expectations.","The driving experience increased self-reported trust in automation, while decreasing ratings of safety gain and discharge of the driver. Crash involvement was correlated with lower trust ratings.","Horizontal gaze deviation did not correlate with self-reported trust, although it behaved as expected. Older participants showed higher trust and more positive ratings of the system than younger participants.","The experience of automated driving in a simulator increased self-reported trust in automation, while decreasing the perceived safety gain and discharge of the driver.","The robot (automated vehicle) controlled the driving, including lane changes, while the human monitored the system and took over control when prompted by the system in take-over situations.",ANOVA; t-test; Spearman correlation,"The study used a mixed-design ANOVA to analyze questionnaire data, examining the effects of repetition (pre- and post-simulation) and age group on subjective evaluations of the automated driving experience. T-tests were used for post-hoc pairwise comparisons and to compare means between groups. Spearman correlations were calculated to assess the relationship between crash involvement and questionnaire ratings, as well as between gaze deviation and self-reported trust.",TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,,"The study manipulated 'Robot-autonomy' by having the automated system control the vehicle but requiring participants to take over in specific take-over situations (TORs). This is explicitly stated in the 'Procedure' section: '...there will be situations the automation is not capable of solving. In such situations, the system emits a warning signal and they have to take over vehicle control again.' The 'Take-over situations' section further details how these situations were triggered. The study also implicitly manipulated 'Task-complexity' by introducing take-over scenarios, which increased the cognitive load on the participants as they had to transition from monitoring to actively controlling the vehicle. This is described in the 'Take-over situations' section: 'In order to handle these situations, participants had to regain control and brake or steer to change lanes and avoid the collision.' The results indicate that the experience of the take-over situations (manipulation of 'Robot-autonomy') impacted trust, as described in the 'Results' section: 'Results indicate that the driving experience increased self-reported trust in automation and lead to a decrease in other measured constructs like safety gain.' The study did not explicitly state that the manipulation of 'Task-complexity' had an impact on trust, but it was a factor that was manipulated by the study design.",10.1016/j.promfg.2015.07.847,https://linkinghub.elsevier.com/retrieve/pii/S2351978915008483,"Highly automated vehicles (Level 3, [1]) are likely to enter the market within the next decade. By removing the driver from the driver-vehiclesystem, positive impacts, for instance on road-safety or fuel consumption, are expected. These predicted effects can only arise if automated vehicles are accepted by society. Trust as well as the attitude towards technology has been found to be a precursor in the acceptance formation process.Therefore, we conducted a driving simulator experiment within the interdisciplinary research group at the Munich Center of Technology in Society (MCTS) in order to investigate how the experience of automated driving will change trust in automation and the attitude of the driver towards automation. The sample consisted of 72 participants between 19 and 79 years (M=44.97,SD=22.16). Participants completed a questionnaire before and after the driving simulator experience to assess trust in automation, safety gain,intentionto use and other constructs in order to analyze the change caused by the driving simulation experience. Besides participants’ ratings from the questionnaires, their gaze behavior was recorded in order to measure a change of trust by a change in scanning behavior.The participants drove highly automated on a three lane highway ata speed of 120 km/h. As critical situations are expected to have a significant impact on trust in automation, the participants experienced three take-over scenarios (system limits). Results indicate that the driving experience increased self-reported trust in automation and lead to a decrease in other measured constructs like safety gain. Older participants rated the vehicle automation more positively than younger drivers.Horizontal gaze behavior could not be confirmed as a metric for measuring trust in automation, although this measure behaved as expected and analogous to the self-reported level of trust."
"Gombolay, Matthew; Yang, Xi Jessie; Hayes, Bradley; Seo, Nicole; Liu, Zixi; Wadhwania, Samir; Yu, Tania; Shah, Neel; Golen, Toni; Shah, Julie",Robotic assistance in the coordination of patient care,2018,1,24,24,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a simulation of a labor and delivery floor four times, each lasting 10 minutes, receiving recommendations from either a robot or a computer, and then completed a questionnaire after each trial.","Participants acted as resource nurses in a simulation, assigning nurses to patients and moving patients between rooms, while receiving recommendations from a robot or computer.",Nao,Humanoid Robots,Research; Social,Supervision,Monitoring,minimal interaction,Participants interacted with the robot through a simulation and received verbal recommendations.,simulation,Participants interacted with a high-fidelity simulation of a labor and delivery floor.,physical,Participants interacted with a physical Nao robot that provided verbal recommendations.,shared control (fixed rules),"The robot provided recommendations based on a learned policy, but the human could accept or reject them.",Behavioral Measures; Questionnaires,Jian et al. Trust Scale,Performance Metrics,Trust was measured using a questionnaire and behavioral measures of compliance with the robot's recommendations.,no modeling,Trust was not modeled computationally; the study focused on statistical analysis of trust scores and behavioral data.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The study manipulated the embodiment of the decision support system (robot vs. computer) and the quality of the advice provided (high vs. low) to see how these factors affected trust and reliance.,"The study found that trust ratings were similar for both the robot and computer conditions, but the robot was rated more favorably on attitudinal measures. The robot also showed more consistent performance when the quality of advice changed.","The study found that participants' rates of Type I and Type II errors associated with computer-based support increased significantly when they had received high-quality advice in the previous trial, but this was not the case for the robotic support. The subjective assessment of the robot was also more robust to advice quality changes than the computer-based decision support.","Experts performing decision-making tasks may be less susceptible to the negative effects of support embodiment, and a robotic assistant may be able to participate in decision making with nurses without eliciting inappropriate dependence.","The robot provided verbal recommendations for nurse-patient assignments and room assignments, while the human participant decided whether to accept or reject the robot's advice.",z-test for two proportions; two one-sided tests (tost) equivalence test; contrast test; repeated-measures analysis of variance (ranova); Friedman test; χ 2 test,"The study used a variety of statistical tests to analyze the data. Z-tests for two proportions were used to compare Type I and Type II error rates, positive predictive value (PPV), and negative predictive value (NPV) between the robot and computer conditions. TOST equivalence tests were used to determine if error rates, PPV, and NPV were statistically equivalent between the two conditions. Contrast tests were used to compare Type I and Type II error rates across different trials and conditions. Repeated-measures ANOVA (RANOVA) was used to assess the effect of advice quality on trust ratings and to analyze reaction times. Friedman tests were used to compare attitudinal assessments of the robot and computer conditions. Finally, a chi-squared test was used to compare the variability of subjective evaluations under the robot and computer conditions.",TRUE,Robot-autonomy; Robot-accuracy,Robot-accuracy,Robot-autonomy,"The study manipulated the embodiment of the decision support system (robot vs. computer), which is categorized as 'Robot-autonomy' because it changes the level of decision authority by providing recommendations through a physical robot or a computer interface. The study also manipulated the quality of the advice provided (high vs. low), which is categorized as 'Robot-accuracy' because it directly impacts the performance of the decision support system. The study found that the quality of advice ('Robot-accuracy') impacted trust ratings, with higher trust ratings for high-quality advice. However, the embodiment of the decision support system ('Robot-autonomy') did not significantly impact trust ratings, as trust ratings were similar for both the robot and computer conditions. The paper states, 'a RANOVA yielded no statistically significant difference in user trust between the robotic (M = 4.22) and computer-based (M = 4.16) embodiment conditions'. Therefore, 'Robot-autonomy' is listed as a factor that did not impact trust.",10.1177/0278364918778344,http://journals.sagepub.com/doi/10.1177/0278364918778344,"We conducted a study to investigate trust in and dependence upon robotic decision support among nurses and doctors on a labor and delivery ﬂoor. There is evidence that suggestions provided by embodied agents engender inappropriate degrees of trust and reliance among humans. This concern represents a critical barrier that must be addressed before ﬁelding intelligent hospital service robots that take initiative to coordinate patient care. We conducted our experiment with nurses and physicians, and evaluated the subjects’ levels of trust in and dependence upon high- and low-quality recommendations issued by robotic versus computer-based decision support. The decision support, generated through action-driven learning from expert demonstration, produced high-quality recommendations that were accepted by nurses and physicians at a compliance rate of 90%. Rates of Type I and Type II errors were comparable between robotic and computer-based decision support. Furthermore, embodiment appeared to beneﬁt performance, as indicated by a higher degree of appropriate dependence after the quality of recommendations changed over the course of the experiment. These results support the notion that a robotic assistant may be able to safely and effectively assist with patient care. Finally, we conducted a pilot demonstration in which a robot-assisted resource nurses on a labor and delivery ﬂoor at a tertiary care center."
"Gompei, Takayuki; Umemuro, Hiroyuki",Factors and Development of Cognitive and Affective Trust on Social Robots,2018,1,56,56,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants completed an initial questionnaire, then interacted with a NAO robot in a conversation session, followed by an experience session with robot functions, and completed trust questionnaires at three time points.",Participants engaged in a conversation with a robot and experienced several of its functions.,Nao,Humanoid Robots; Expressive Robots,Social; Research,Social,Conversation,minimal interaction,Participants had verbal interaction with the robot and experienced its functions.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical NAO robot.,pre-programmed (non-adaptive),The robot followed pre-programmed scripts for conversation and functions.,Questionnaires; Custom Scales; Multidimensional Measures,Schaefer's Trust Questionnaire/Scale,,Trust was assessed using questionnaires and custom scales at three time points.,no modeling,The study did not use computational models of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's conversation topics and the presence of mistakes during function demonstrations were manipulated to influence trust.,"Casual conversation topics increased affective trust, while robot mistakes decreased cognitive trust. Cognitive trust increased over the whole interaction period, while affective trust developed in the earlier phase.","Affective trust developed in the earlier phase of interaction, while cognitive trust developed over the whole interaction period. Prior experiences with robots did not significantly influence trust. Familiarity, interest, negative attitude, and utility sub-dimensions of robot attitudes were related to affective trust.","Ten factors of trust were identified, with Security, Teammate, and Performance relating to cognitive trust, and Teammate, Performance, Autonomy, and Friendliness relating to affective trust. Affective trust developed earlier, while cognitive trust developed over the whole interaction period.","The robot engaged in conversations with participants, asking personal questions or providing information. Participants experienced the robot's functions, such as weather reports and message sending, with some functions including programmed mistakes.",Multilevel Model; Pearson correlation; ANOVA; t-test,"The study used factor analysis to identify underlying dimensions of trust from questionnaire responses. Pearson's correlation was used to examine the relationships between the extracted trust factors and cognitive/affective trust scores at different time points. ANOVAs were conducted to assess the effects of measurement phase, conversation topic, mistake condition, and gender on cognitive and affective trust scores. Finally, t-tests were used to compare trust scores between groups with and without prior robot experience, and between groups with high and low satisfaction scores.",TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-verbal-communication-content; Robot-accuracy,,"The study manipulated the content of the robot's verbal communication by having it engage in casual conversation, provide information, or only respond to questions. This is categorized as 'Robot-verbal-communication-content' because the manipulation directly altered the topics and nature of the robot's speech. The study also manipulated the robot's accuracy by having it make mistakes during some function demonstrations, which is categorized as 'Robot-accuracy' because it directly affected the robot's performance on the task. The results showed that the conversation topic influenced affective trust, and the robot's mistakes influenced cognitive trust, therefore both 'Robot-verbal-communication-content' and 'Robot-accuracy' are listed as factors that impacted trust. There were no other factors manipulated in the study.",,http://link.springer.com/10.1007/978-3-030-05204-1_5,"The purpose of this study is to investigate the factors that contribute to cognitive and aﬀective trust of social robots. Also investigated were the changes within two diﬀerent types of trust over time and variables that inﬂuence trust. Elements of trust extracted from literature were used to evaluate people’s trust of social robot in an experiment. As a result of a factor analysis, ten factors that construct trust were extracted. These factors were further analyzed in relations with both cognitive and aﬀective trust. Factors such as Security, Teammate, and Performance were found to relate with cognitive trust, while factors such as Teammate, Performance, Autonomy, and Friendliness appeared to relate with aﬀective trust. Furthermore, changes in cognitive and aﬀective trust over the time phases of the interaction were investigated. Aﬀective trust appeared to develop in the earlier phase, while cognitive trust appeared to develop over the whole period of the interaction. Conversation topics had inﬂuence on aﬀective trust, while robot’s mistakes had inﬂuence on the cognitive trust. On the other hand, prior experiences with social robots did now show any signiﬁcant relations with neither cognitive nor aﬀective trust. Finally, Familiarity attitude appeared to relate with both cognitive and aﬀective trust, while other sub-dimensions of robot attitudes such as Interest, Negative attitude, and Utility appeared to relate with aﬀective trust."
"Gonzalez-Pacheco, Victor; Malfaz, Maria; Castillo, Jose Carlos; Castro-Gonzalez, Alvaro; Alonso-Martín, Fernando; Salichs, Miguel A.",How Much Should a Robot Trust the User Feedback? Analyzing the Impact of Verbal Answers in Active Learning,2016,1,24,24,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants trained a robot to recognize poses, then answered questionnaires about the importance of different body parts for each pose. The robot used these answers to filter features for learning.","Participants taught a robot three different poses in two experiments, and then answered questions about the importance of different body parts for each pose.",Maggie,Humanoid Robots; Expressive Robots,Research; Social,Social,Tutoring,minimal interaction,Participants verbally instructed the robot and answered questions.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,shared control (fixed rules),The robot used fixed rules to filter data based on user input.,Questionnaires,,Speech Data,Trust was assessed indirectly through user responses to questionnaires.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers manipulated the type of questions asked to the users (Free Speech, Yes/No, Rank) and how the robot used the user's answers to filter data, which influenced the robot's learning performance.","The study found that simplistic user answers could negatively impact the robot's learning, suggesting that over-reliance on user input can decrease trust in the robot's performance. The Extended Filter was introduced to mitigate this effect by reducing the robot's confidence in user answers.","The study found that users sometimes provided overly simplistic answers, which negatively impacted the robot's learning. The Extended Filter was created to address this issue by including more information than the user provided.","Inaccurate user verbal responses in Active Learning may lead to loss of relevant data, and the Extended Filter can mitigate this by reducing the robot's confidence in user answers.","The robot asked users to teach it poses, and then asked questions about which body parts were important for each pose. The human provided verbal instructions and answered questions.",,"The paper describes the calculation of thresholds for filtering data based on user responses, including the use of standard deviation and confidence intervals. However, it does not explicitly mention any inferential statistical tests like t-tests, ANOVAs, or regression analysis. The analysis focuses on comparing the performance of different filtering methods (FSQF, YNQF, RQF, and EF) using the F1-score as a metric, but no statistical tests are used to compare these scores.",TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-accuracy,Robot-verbal-communication-content,"The study manipulated the type of questions asked to the users (Free Speech, Yes/No, Rank) which is categorized as 'Robot-verbal-communication-content'. This manipulation influenced how the robot filtered data, which directly impacted the robot's learning performance, categorized as 'Robot-accuracy'. The paper states that simplistic user answers negatively impacted the robot's learning, suggesting that over-reliance on user input can decrease trust in the robot's performance. This indicates that 'Robot-accuracy' impacted trust. The type of question asked did not directly impact trust, but rather the robot's performance based on the user's answers, so 'Robot-verbal-communication-content' did not directly impact trust.",,http://link.springer.com/10.1007/978-3-319-47437-3_19,"This paper assesses how the accuracy in user’s answers inﬂuence the learning of a social robot when it is trained to recognize poses using Active Learning. We study the performance of a robot trained to recognize the same poses actively and passively and we show that, sometimes, the user might give simplistic answers producing a negative impact on the robot’s learning. To reduce this eﬀect, we provide a method based on lowering the trust in the user’s responses. We conduct experiments with 24 users, indicating that our method maintains the beneﬁts of AL even when the user answers are not accurate. With this method the robot incorporates domain knowledge from the users, mitigating the impact of low quality answers."
"Goubard, Cedric; Demiris, Yiannis",Learning Self-Confidence from Semantic Action Embeddings for Improved Trust in Human-Robot Interaction,2024,1,31,31,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants cooked twice with a robot, once with a self-confidence module active and once without, with the order counterbalanced between participants. Questionnaires were administered before and after each session.","Participants followed a recipe to make a salad, with the robot delivering ingredients.",Unspecified,Mobile Manipulators; Service and Assistive Robots,Care; Research,Manipulation,Cooking/Food Preparation,direct-contact interaction,Participants physically interacted with the robot during a cooking task.,real-world,The study was conducted in a real-world setting with a physical robot.,physical,A physical robot was used in the study.,shared control (adaptive),The robot adapted its behavior based on its self-confidence and could ask for help.,Behavioral Measures; Questionnaires,Multi-Dimensional Measure of Trust (MDMT); Trust Perception Scale - HRI,Performance Metrics,Trust was measured using questionnaires and behavioral measures such as intervention counts.,"parametric models (e.g., regression)",Bayesian data analysis was used to model the reported trust and intervention count.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's self-confidence module was activated or deactivated, influencing its performance and autonomy by allowing it to ask for help when low confidence was predicted.",Self-confidence awareness had a positive impact on reported performance-based trust and reduced the number of interventions.,The order in which participants encountered the policies did not affect their reported trust or number of interventions.,A self-confidence-aware robot policy increased capability-based human trust.,"The robot delivered ingredients to the human, who then used them to make a salad. The robot could ask for help when it predicted a failure.",bayesian data analysis; ordered-logit model; Poisson regression,"Bayesian data analysis was used to analyze the reported trust from questionnaires (MDMT and TPS-HRI) using an ordered-logit model, which is suitable for Likert-scale data. A Poisson model was used to analyze the intervention count. These models were used to compare the effects of the robot's self-confidence module on trust and intervention counts, and to assess the impact of the order in which participants encountered the policies. The analysis focused on the contrast between groups and the overlap between the 95% Highest Density Interval (HDI) and the Region of Practical Equivalence (ROPE) to determine statistical significance.",TRUE,Robot-autonomy,Robot-autonomy,,"The study manipulated the robot's autonomy by activating or deactivating a self-confidence module. When active, the robot could ask for help when it predicted a failure, thus influencing its decision-making authority and level of shared control. This is described in the paper as 'The robot could ask for help when it predicted a failure' and 'the robot's self-confidence module was activated or deactivated, influencing its performance and autonomy'. The results showed that this manipulation of the robot's autonomy had a positive impact on reported performance-based trust and reduced the number of interventions, as stated in the paper: 'self-confidence awareness had a positive impact on reported performance-based trust, and reduced the number of interventions'. The order in which participants encountered the policies did not affect their reported trust or number of interventions, so this was not a factor that impacted trust.",10.1109/ICRA57147.2024.10611445,https://ieeexplore.ieee.org/document/10611445/,"In Human-Robot Interaction (HRI) scenarios, human factors like trust can greatly impact task performance and interaction quality. Recent research has confirmed that perceived robot proficiency is a major antecedent of trust. By making robots aware of their capabilities, we can allow them to choose when to perform low-confidence actions, thus actively controlling the risk of trust reduction. In this paper, we propose Self-Confidence through Observed Novel Experiences (SCONE), a policy to learn self-confidence from experience using semantic action embeddings. Using an assistive cooking setting, we show that the semantic aspect allows SCONE to learn self-confidence faster than existing approaches, while also achieving promising performance in simple instructions following. Finally, we share results from a pilot study with 31 participants, showing that such a self-confidence-aware policy increases capability-based human trust."
"Graf, Luisa; Torkar, Miha; Stuckelmaier, Emily; Sichler, Romaric; Malafosse, Pierre; Fischer, Kerstin; Palinko, Oskar",Perceived Trustworthiness of an Interactive Robotic System,2022,1,38,37,1,1 participant did not complete the post-experimental survey,Controlled Lab Environment,within-subjects,"Participants played a quiz game and could use a robot joker or a majority vote joker for help. The jokers were controlled manually and always gave the same answer. After the game, participants completed a questionnaire.",Participants played a quiz game and chose between a robot joker or a majority vote joker for help.,Unspecified,Other,Educational; Research,Game,Competitive Game,minimal interaction,Participants interacted with the robot through a game interface.,simulation,The interaction was through a simulated game environment.,simulated,The robot was presented as a simulated entity within the game.,wizard of oz (directly controlled),The robot's actions were controlled manually by the researchers.,Behavioral Measures; Questionnaires,,Performance Metrics,Trust was measured through joker usage and a post-game questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Wizard-of-Oz Studies,Indirect Manipulation,"The robot and human jokers were presented as having different sources of knowledge, influencing expectations of their performance, and the robot's performance was manipulated by controlling the answers.","Participants showed a preference for the robot joker, indicating higher trust in the robot's knowledge, even though the robot's performance was not better than the human joker.","Participants showed overtrust in the robot, preferring it even when its performance was slightly worse than the human joker. The perceived intelligence of the robot was correlated with its performance.","Participants exhibited a higher level of trust in the robotic system's knowledge compared to the collective knowledge of humans, despite the robot's performance not being superior.","The robot provided answers to quiz questions via a joker system, and the human participants chose between the robot joker and a human majority vote joker to help them answer the questions.",Chi-squared; Mann-Whitney U,"A Chi-squared test of independence was used to compare the frequency of robot joker usage versus majority vote joker usage, testing the hypothesis that both jokers were equally trustworthy. A Mann-Whitney-U test was used to compare the perceived intelligence of the robot between two groups of participants: those for whom the robot answered the majority of questions correctly and those for whom the robot answered the majority of questions incorrectly.",TRUE,Robot-accuracy; Teaming,Robot-accuracy,,"The study manipulated 'Robot-accuracy' by controlling the answers provided by the robot joker and the majority vote joker. Although both jokers always gave the same answer, the perceived accuracy of the robot was manipulated by the researchers by controlling the answers to be correct or incorrect. This is described in the paper: 'To ensure comparable results, the robot and majority vote jokers were controlled manually without the participants' knowledge... instead the answers were manually created in advance and both jokers always answered exactly the same to ensure their parity.' The study also manipulated 'Teaming' by having participants compete in a quiz game where they could choose between a robot joker or a majority vote joker for help. This is described in the paper: 'The game was constructed as a quiz, in which participants had to agree or disagree with 20 true/false statements. In case the players did not know the answer, they had the option of using one of two available jokers: a robot joker or a majority vote.' The results showed that the perceived intelligence of the robot, which is directly related to its accuracy, impacted trust. This is described in the paper: 'A Mann-Whitney-U test was performed on the group of participants for which the robot answered the majority of the questions correctly (more than 50%) and the group for which the majority of the questions were answered incorrectly (less than 50%) by the robot. The results... a statistically significant difference can be observed for perceived intelligence depending on the robot's performance.' The paper does not explicitly state that any of the manipulated factors did not impact trust.",10.1109/HRI53351.2022.9889667,https://ieeexplore.ieee.org/document/9889667/,"This paper compares how much people trust in a robotic system and in a group of people. Our experiment focuses on trust in robot knowledge versus trust in collective human knowledge tested with the help of a quiz game. During the experiments, people are competing with each other and gain or lose points based on their decisions. A joker system was designed that provides evidence on whether people rely rather on the robot or on human collective knowledge. In this setup, perceived intelligence is therefore highly correlated with trustworthiness. In the scope of this joker-picking system, overall results show that the trust in the robotic system is higher than the trust in collective human knowledge."
"Gray, Carly E.; Chesser, Amber; Atchley, Andrew; Smitherman, R. Cooper; Tenhundfeld, Nathan L.","Humanlikeness and Aesthetic Customization's Effect on Trust, Performance, and Affect",2022,1,53,53,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants completed a tutorial, then a bomb-defusing simulation where they either customized a robot avatar or were assigned one, and then completed a survey.",Participants controlled a robot avatar in a bomb-defusing simulation.,Unspecified,Mobile Robots,Research,Game,Cooperative Game,minimal interaction,Participants interacted with a robot avatar through a computer simulation.,simulation,Participants interacted with a robot in a virtual environment.,simulated,The robot was a virtual representation in a simulation.,wizard of oz (directly controlled),The robot's movement was directly controlled by the participant.,Questionnaires,Trust in Automated Systems Scale,Performance Metrics,Trust was measured using a questionnaire and performance metrics.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"Participants could customize the robot's appearance, and were given control over the robot in the simulation, which was intended to influence trust.",The study found no significant effect of customization or humanlikeness on trust.,"No participants chose to customize the machinelike robot, which was an unexpected outcome. The study also found no significant effects of customization or humanlikeness on any of the measured variables.","The study found no significant effect of aesthetic customization or humanlikeness of a robot avatar on trust, performance, or affect in a bomb-defusing simulation.","The robot moved through a virtual environment, and the human controlled the robot's movement and defused bombs by pressing a key when the robot was close enough to a bomb.",ANOVA; Bayesian ANOVA,"The study used both Frequentist and Bayesian one-way ANOVAs to compare the conditions of participants assigned a machinelike robot, those assigned a humanlike robot, and those that selected a humanlike robot on measures such as the number of bombs defused, positive and negative affect, trust, attachment, identification, immersion, and control. The analyses aimed to determine if there were significant differences between these conditions on the measured variables.",TRUE,Robot-aesthetics; Task-complexity,,Robot-aesthetics,"The study manipulated the robot's aesthetics by allowing participants in one condition to customize the robot's appearance (form and color), while others were assigned a pre-set robot. This is explicitly stated in the abstract: 'participants were randomly assigned to one of two (humanlike or machinelike) robot avatars or were given the ability to customize one. The customizable robot avatar allows the participant to select either a humanlike or machinelike robot and customize the color of the wheels and casing.' The study also manipulated task complexity by introducing uncertainty with respect to the bomb-defusing distance and required button presses, as described in the abstract: 'The game design incorporates a high-risk environment and uncertainty with respect to the bombdefusing distance and required button presses to encourage cautious guidance of the robot.' The results section states that there was no significant difference in trust based on the robot condition, indicating that the aesthetic manipulation did not impact trust. The discussion section also states 'All analyses generated nonsignificant results...Therefore, none of our hypotheses were supported.' This indicates that the aesthetic manipulation did not impact trust.",10.1109/SIEDS55548.2022.9799376,https://ieeexplore.ieee.org/document/9799376/,"Human-machine interactions have become a staple of people's daily lives through the use of mobile devices, robotics, and a myriad of smart technologies. Previous research has established that anthropomorphism can significantly affect subjective perceptions of, and interactions with, machines. Furthermore, the ability to customize digital tools has been shown to affect user preferences, video game enjoyment, and the efficacy of digital mental health interventions. This study examined whether the customization of a machine teammate could influence the performance of the human-machine team and generate an affective response on the part of the human teammate. To evaluate this premise, we developed a bomb-defusing task simulation using the Unity game engine wherein participants were randomly assigned to one of two (humanlike or machinelike) robot avatars or were given the ability to customize one. The customizable robot avatar allows the participant to select either a humanlike or machinelike robot and customize the color of the wheels and casing. The customization is aesthetic in nature and has no effect on the functionality of the robot. The game design incorporates a high-risk environment and uncertainty with respect to the bombdefusing distance and required button presses to encourage cautious guidance of the robot. We predicted that the ability to customize the robot will increase performance and subjective measures of trust, affect, attachment, identification, immersion, and control. We also predicted that the humanlikeness of the robot would increase performance and our subjective measures. Finally, we expected to see a significant effect of customization and humanlikeness such that the customization and humanlikeness have an additive effect on performance and our subjective measures. The results of all analyses were nonsignificant. These results may help inform the design of such systems and address fears that customization could lead to over-empathizing with a machine teammate in a way that would reduce use in high-risk environments."
"Gregory, Jason M.; Sanchez, Felix; Lancaster, Eli; Agha-Mohammadi, Ali-Akbar; Gupta, Satyandra K.",Using Decision Support in Human-in-the-Loop Experimental Design Toward Building Trustworthy Autonomous Systems,2023,1,12,12,1,1 person was removed from the study due to insufficient recent experience,Online Crowdsourcing,between-subjects,"Participants were given background information about experimental design and a simulated robot. They were then asked to design an experiment for an autonomous waypoint mission in a dense forest by selecting hardware, software, and parameter values. One group received decision support in the form of a questionnaire before designing their experiment.","Participants designed an experiment for an autonomous waypoint mission by selecting hardware, software, and parameter values.",Unspecified,Mobile Robots; Unmanned Ground Vehicles,Research,Evaluation,Survey/Questionnaire Completion,minimal interaction,Participants interacted with a simulated robot through an online interface.,simulation,Participants interacted with a simulated robot in a virtual environment.,simulated,The robot was presented as a simulated entity in the online study.,pre-programmed (non-adaptive),The robot's behavior was pre-programmed and did not adapt to the user's input.,Questionnaires,,,Trust was assessed using an opinion-based exit survey.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the presence of a decision support system (DSS) in the form of a questionnaire, which was intended to influence the quality of experimental design decisions.","The study did not directly measure trust, but the DSS showed promise in reducing suboptimal decisions, and participants found it useful and not burdensome.","Participants in the control group viewed the DSS slightly more favorably than the participants in the assisted group, possibly because they could imagine the utility of assisted thought after making their decisions unassisted. Participants preferred to type their responses to the DSS questions rather than answering silently.","Experienced field roboticists make suboptimal decisions and mistakes when designing experiments, and a questionnaire-based DSS shows promise in reducing these errors.","The robot was simulated and performed a waypoint mission. The human participant designed the experiment by selecting hardware, software, and parameter values for the robot.",,"No statistical tests were explicitly mentioned in the paper. The study used a qualitative evaluation of the experimental designs by an expert experimenter to categorize them as good, suboptimal, or bad. The study also used descriptive statistics (mean and standard deviation) to summarize the Likert scale responses from the exit survey.",TRUE,Task-complexity,,,"The study manipulated the presence of a decision support system (DSS) in the form of a questionnaire. While the DSS itself doesn't directly change the robot's behavior, it does influence the complexity of the task for the participants. The DSS is designed to guide the participants' decision-making process, which can be seen as a manipulation of the cognitive demands of the experimental design task. The control group performed the task without the DSS, while the assisted group had to consider the questions posed by the DSS, thus increasing the cognitive load and complexity of the task. The study did not directly measure trust, but the DSS showed promise in reducing suboptimal decisions, and participants found it useful and not burdensome. The study did not find any factors that directly impacted or did not impact trust.",10.1109/RO-MAN57019.2023.10309571,https://ieeexplore.ieee.org/document/10309571/,"Experimental design of autonomous systems involves defining experimental inputs to maximize the experimenter’s information gained, minimize costs, and balance risk. This effectively leads to improved understanding and trustworthiness, which are necessary for deployment in realworld settings. Since experimental design is inherently a humanin-the-loop, sequential decision making problem, and decisions are being made about complex systems, an investigation into decision-making quality and decision-supporting methods is warranted. In this work, we investigate a decision support system (DSS) to augment the human’s experimental design decision making abilities, and conduct an exploratory user study to investigate the potential for decision support. Our findings show that experimenters, including experienced field roboticists, make suboptimal decisions and mistakes during the experimental design process, which suggests robotics research could benefit from DSSs. Our proposed DSS shows promise in some select aspects of experimental design, including helping to reduce suboptimal decisions, and participants in the user study reported favorable opinions of using such a system, including a sense of usefulness and lack of burden. The broader implication of this work is the identification of decision support in experimental design as one way to help bridge the gap between academia and industry by way of accelerated, informative experimentation and increased system explainability."
"Grounds, Christopher B.; Ensing, Annette R.",Automation Distrust Impacts on Command and Control Decision Time,2000,1,16,16,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were divided into two groups: one received 100% reliable recommendations, and the other received 50% reliable recommendations. Each group completed 24 trials, with the first 12 discarded for learning effects. The last 12 trials were analyzed for decision time and accuracy. Participants also completed a post-test survey and drew their trust rating over time.",Participants were asked to authorize or deny system recommendations on a THAAD command and control screen.,Unspecified,Other,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a system through a screen, making decisions based on recommendations.",simulation,The interaction was through a simulated command and control screen.,simulated,The robot was represented through a simulated system interface.,shared control (fixed rules),"The system provided recommendations, but the human had the final say in authorizing them.",Behavioral Measures; Questionnaires,,Performance Metrics,"Trust was assessed through decision time, accuracy, and post-test survey responses.",no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The reliability of the system recommendations was manipulated to be either 100% or 50% to influence trust.,The 50% reliability group showed decreased trust and slower decision times compared to the 100% reliability group.,"One participant in the 50% group drew a sine wave for their trust rating, indicating fluctuating trust levels.","Distrust in automation significantly degrades decision time, with a 50% unreliable system leading to slower decisions compared to a 100% reliable system.","The system provided recommendations for actions, and the human participant had to authorize or deny these recommendations by selecting 'Agree' or 'Disagree' on the screen.",ANOVA,An ANOVA was used to analyze the difference in performance time between the two groups (100% reliable recommendations vs. 50% reliable recommendations). The analysis focused on the last 12 trials after discarding the first 12 for learning effects. The purpose was to determine if the reliability of the system recommendations had a significant impact on decision time.,TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the reliability of the system recommendations, with one group receiving 100% reliable recommendations and the other receiving 50% reliable recommendations. This directly impacts the accuracy of the robot's actions, as the system's recommendations are the robot's actions in this context. The paper states, 'To perform this experiment, two separate (but equal) groups of participants were tested—one group made decisions based upon system recommendations that were always correct; the other group made decisions based upon system recommendations that were correct half of the time.' This manipulation of reliability directly influenced the participants' trust in the system, as evidenced by the slower decision times and increased errors in the 50% reliability group. The paper states, 'The distrust of the 50% group kept their performance at a level that, on average, caused them to make slower decisions.' Therefore, 'Robot-accuracy' is the most appropriate category, as it directly relates to the system's performance and its impact on trust.",10.1177/154193120004402293,http://journals.sagepub.com/doi/10.1177/154193120004402293,"Research into how an air defense command and control soldier will interact with automation and decision aid recommendations is evolving to show that, when trust in the automation is in question, a degradation in decision time of at least 30% will exist when compared to an operator that is not distrustful. Current work involves methods to decrease the temporal effects of distrust on operator authorization, including the accuracy of decisions when reliability is in question, using configural displays and other holistic display methods."
"Gruber, Dara; Aune, Ashley; Koutstaal, Wilma",Can Semi-Anthropomorphism Influence Trust and Compliance?: Exploring Image Use in App Interfaces,2018,1,105,105,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants completed 40 navigation tasks using a smartphone app with varying levels of anthropomorphic imagery and information reliability. They received a printed map, a task card, and then interacted with the app to accept or reject a suggested route. They also rated their confidence and trust.","Participants were asked to make route choices based on a suggested route from an automated decision aid on a smartphone app, and they could choose to accept or reject the suggestion.",Unspecified,Other,Research,Evaluation,Rating,minimal interaction,Participants interacted with a smartphone app to make decisions.,simulation,The interaction was through a simulated smartphone app interface.,simulated,The robot was represented by images on a smartphone screen.,pre-programmed (non-adaptive),The automated decision aid provided pre-programmed suggestions without adapting to user input.,Behavioral Measures; Questionnaires,,Performance Metrics,Trust was measured using behavioral data and subjective ratings.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the level of anthropomorphism of the aid's image and the reliability of the information provided, which was intended to influence trust.","The study found that semi-anthropomorphic imagery did not significantly influence trust, but information reliability and advice type did affect trust and compliance.","The study found that participants in the 90% reliability condition performed worse when receiving bad advice than those in the 70% reliability condition, indicating overreliance on highly reliable systems. The non anthropomorphic condition showed higher objective trust and poorer performance under bad advice trials compared to the high anthropomorphic condition under 70% reliability.","Semi-anthropomorphic imagery does not significantly alter trust in an automated aid, but higher information reliability leads to overreliance on the system, even when it provides bad advice.","The automated decision aid on the smartphone app provided a suggested route, and the human participant decided whether to accept or reject the suggestion. The participant also rated their confidence and trust in the aid.",ANOVA; Spearman correlation,"The study used mixed factor ANOVAs to analyze the effects of reliability, anthropomorphism, and advice type on performance and trust measures. Specifically, a 2x3x2 mixed factor ANOVA was used to assess performance data, and a 2x3 mixed factor ANOVA was used for subjective trust data. A rank order correlation was used to assess the relationship between behavioral time to decision and self-reported confidence.",TRUE,Robot-interface-design; Robot-accuracy,Robot-accuracy,Robot-interface-design,"The study manipulated the visual appearance of the automated decision aid on the smartphone app, using three levels of anthropomorphism (non, low, high), which falls under 'Robot-interface-design' as it involves changes to the visual elements of the interface. The study also manipulated the reliability of the information provided by the aid (70% or 90% accuracy), which directly impacts the performance of the aid and thus is categorized as 'Robot-accuracy'. The results showed that the level of anthropomorphism did not significantly influence trust, while the reliability of the information did impact trust and compliance. Specifically, participants in the 90% reliability condition showed overreliance on the system, even when it provided bad advice, indicating that 'Robot-accuracy' impacted trust. The paper states, 'levels of semi-anthropomorphic imagery do not influence trust' and 'Participants in the 90% reliability condition reported significantly higher subjective trust than participants in the 70% reliability condition.' This indicates that 'Robot-interface-design' did not impact trust, while 'Robot-accuracy' did.",10.1145/3183654.3183700,https://dl.acm.org/doi/10.1145/3183654.3183700,"Relying on automated suggestions from technology to make decisions has become a part of our everyday life. Most existing research examines trust in automation for high-risk domains as well as vast recommender systems for low-risk consumer use. This research aims to evaluate human trust in automated decision aids for low-risk everyday use, where one automated suggestion is presented and a few alternative options are available. In the current study, we investigate whether anthropomorphic imagery and information reliability can inﬂuence perceived trust of the automated decision aid. We report on results from 105 participants, which suggest that levels of semi-anthropomorphic imagery do not inﬂuence trust in the way that a full anthropomorphic image of a human might. Our work contributes to a deeper understanding of how to utilize design, speciﬁcally visual imagery, to appropriately calibrate trust in automated decision aids for everyday use."
"Guidolin, Mattia; Berti, Nicola; Gnesotto, Paride; Battini, Daria; Reggiani, Monica",Trust the Robot! Enabling Flexible Collaboration With Humans via Multi-Sensor Data Integration,2024,1,1,1,0,No participants were excluded,Controlled Lab Environment,within-subjects,"A volunteer simulated a walking-worker assembly line, performing six assembly cycles of a wooden bedside table. The volunteer's body pose was tracked using both markerless and inertial motion capture systems. An AprilTag was used to assess the accuracy of the body pose estimation.","The participant performed a repetitive assembly task of a wooden bedside table, moving between four different workstations, one of which included a collaborative robot.",Unspecified,Collaborative Robots,Industrial,Manipulation,Object Assembly,minimal interaction,"The participant worked alongside a collaborative robot, but the interaction was limited to a shared workspace.",real-world,The study was conducted in a real-world lab setting with physical objects and a robot.,physical,A physical collaborative robot was present in the study.,pre-programmed (non-adaptive),The robot performed pre-programmed actions without adapting to the user.,,,,Trust was not directly measured in this study.,no modeling,Trust was not modeled in this study.,Empirical HRI Studies,Physical Robot Studies,No Manipulation,No factors were manipulated in this study; the focus was on data collection and algorithm validation.,,"The study demonstrated the effectiveness of the proposed alignment algorithm in mitigating sensor drift and occlusions in motion capture data. The inertial data was significantly affected by drift, while the markerless data had gaps due to occlusions.","The proposed algorithm effectively aligns inertial and markerless motion capture data in real-time, providing accurate body pose estimation even with sensor drift and occlusions.","The robot assisted the human in lifting heavy parts during the assembly task, while the human moved between workstations to complete the assembly of a wooden bedside table.",,"No statistical tests were explicitly mentioned in the paper. The analysis focused on descriptive measures such as the drift of inertial sensors, the accuracy of markerless motion capture, and the performance of the proposed alignment algorithm. The paper presents a spaghetti chart to visualize the worker's centroid position and analyzes the translation and rotation distances between reference frames, but no formal statistical tests were conducted.",FALSE,,,,"The study did not manipulate any factors. The focus was on data collection and algorithm validation for real-time body pose estimation using multi-sensor data fusion. The paper explicitly states that no factors were manipulated, and the goal was to develop and test an algorithm for aligning data from different motion capture systems. The study aimed to demonstrate the effectiveness of the proposed alignment algorithm in mitigating sensor drift and occlusions in motion capture data, not to investigate the impact of any manipulated factors on trust. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust.",10.1109/ETFA61755.2024.10710349,https://ieeexplore.ieee.org/document/10710349/,"Collaborative robots in manufacturing offer significant potential for developing synergies between the skills of the workforce and the capabilities of the robots, increasing efficiency and reducing the psycho-physical effort required from workers. One of the main barriers to the adoption of these systems is the workers’ lack of trust in robots, which can negatively affect both performance and well-being. To address this issue, a precise monitoring phase must be conducted with sensors and algorithms that enable data fusion from different sources. Nevertheless, in an operative context, the presence of cumbersome setups to monitor both workers and cobots can slow down performance and create bias and unsatisfactory working conditions. For this reason, this work proposes a framework that describes the transition from an integrated Human Digital Twin toward a lighter monitoring setup, exploiting the potential of Machine Learning algorithms during the operative phase to reduce the number of required sensors. In fact, while extensive data is valuable during the design phase of collaborative workstations, the operational phase should minimize sensor use, leveraging pre-gathered data to train Machine Learning networks for estimating missing quantities. To achieve such a level of data quality in the pre-deployment and design phases, we introduce an algorithm for real-time alignment of body poses estimated by different Motion Capture technologies. This method provides accurate, occlusion-robust body pose estimation while also solving the drifting phenomena that affect inertial measurement units. Consequently, the proposed approach establishes a robust foundation for enhancing Human-Robot Collaboration by ensuring precise and reliable real-time body pose estimation, a crucial step for advancing safety and efficiency in the manufacturing field."
"Guo, Yaohui; Yang, X. Jessie",Modeling and Predicting Trust Dynamics in Human–Robot Teaming: A Bayesian Inference Approach,2020,1,39,39,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a simulated surveillance task with four drones, controlling the drones with a joystick and detecting threats in images captured by the drones. Participants switched between controlling and detection tasks. They had two practice sessions before the main experiment, which consisted of 100 trials. Participants reported their perceived reliability of the drones, trust in automation, and confidence after each trial.",Participants controlled four drones using a joystick and detected potential threats in images captured by the drones.,Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the drones through a simulation, controlling them with a joystick and observing their performance.",simulation,The interaction was conducted in a simulated environment where participants controlled and observed the drones.,simulated,"The drones were represented in a simulation, not as physical robots.",shared control (fixed rules),"The drones had a fixed level of autonomy, detecting threats based on pre-set rules, while participants controlled their movement.",Questionnaires,,Performance Metrics,Trust was measured using self-reported questionnaires and the robot's performance data was used for modeling.,"parametric models (e.g., regression)",A Bayesian inference model was used to predict trust based on the robot's performance history.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the drones was set at 70, 80, and 90%, which directly influenced the robot's performance and thus the user's trust.","The study found that the robot's failures had a greater impact on trust than its successes, and that trust stabilized over repeated interactions.","The study identified three distinct types of trust dynamics: Bayesian decision maker, oscillator, and disbeliever. Some participants consistently reported low trust.","The proposed personalized trust prediction model, based on Bayesian inference, significantly outperformed existing models in predicting trust dynamics.",The robot (drones) detected potential threats and reported 'danger' when a threat was detected. The human participant controlled the drones using a joystick and monitored the images captured by the drones to detect threats.,ANOVA; pairwise comparisons with bonferroni adjustments,A repeated-measures Analysis of Variance (ANOVA) was conducted to compare the performance of the proposed trust prediction model with two existing models (ARMAV and Optimo). This was followed by pairwise comparisons with Bonferroni adjustments to determine which models differed significantly from each other. The root mean square error (RMSE) was used as the dependent variable to evaluate the difference between the predicted and actual trust values.,TRUE,Robot-accuracy,Robot-accuracy,,"The study explicitly manipulated the reliability of the drones, setting it at 70, 80, and 90%. This directly influenced the robot's performance in detecting threats, which is a measure of its accuracy. The paper states, 'The system reliability of the drones was set as 70, 80, and 90% according to the signal detection theory (SDT)'. This manipulation of the robot's success/failure rate directly impacts the 'Robot-accuracy'. The study found that the robot's failures had a greater impact on trust than its successes, indicating that the manipulated factor, 'Robot-accuracy', directly influenced trust levels. There were no other factors manipulated in the study.",10.1007/s12369-020-00703-3,http://link.springer.com/10.1007/s12369-020-00703-3,"Trust in automation, or more recently trust in autonomy, has received extensive research attention in the past three decades. The majority of prior literature adopted a “snapshot” view of trust and typically evaluated trust through questionnaires administered at the end of an experiment. This “snapshot” view, however, does not acknowledge that trust is a dynamic variable that can strengthen or decay over time. To ﬁll the research gap, the present study aims to model trust dynamics when a human interacts with a robotic agent over time. The underlying premise of the study is that by interacting with a robotic agent and observing its performance over time, a rational human agent will update his/her trust in the robotic agent accordingly. Based on this premise, we develop a personalized trust prediction model and learn its parameters using Bayesian inference. Our proposed model adheres to three properties of trust dynamics characterizing human agents’ trust development process de facto and thus guarantees high model explicability and generalizability. We tested the proposed method using an existing dataset involving 39 human participants interacting with four drones in a simulated surveillance mission. The proposed method obtained a root mean square error of 0.072, signiﬁcantly outperforming existing prediction methods. Moreover, we identiﬁed three distinct types of trust dynamics, the Bayesian decision maker, the oscillator, and the disbeliever, respectively. This prediction model can be used for the design of individualized and adaptive technologies."
"Guo, Tiffany; Obidat, Omar; Rodriguez, Laury; Parron, Jesse; Wang, Weitian",Reasoning the Trust of Humans in Robots through Physiological Biometrics in Human-Robot Collaborative Contexts,2022,1,1,1,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants handed an object to a robot, which responded based on the participant's facial expression and predicted trust level. The robot's actions were recorded.",Participants handed an object to a robot.,Franka Emika Panda,Industrial Robot Arms; Collaborative Robots (Cobots),Research,Manipulation,Object Passing,direct-contact interaction,Participants physically handed an object to the robot.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,A physical robot was used in the study.,shared control (adaptive),The robot adapted its behavior based on the participant's facial expression and predicted trust level.,Physiological Measures; Real-time Trust Measures,,Video Data; Physiological Signals,Trust was assessed using real-time facial expression analysis and mapped to trust levels.,"deep learning (e.g., neural networks, reinforcement learning)",An Extreme Learning Machine (ELM) was used to predict human emotion and trust levels.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's movement speed and behavior were directly manipulated based on the predicted trust level of the human.,"The robot's behavior changed based on the predicted trust level, with the robot slowing down or backing away when low or no trust was detected.","The ELM model achieved high accuracy in predicting trust levels, but the calculation time for facial landmark detection caused a lag in the robot's response.","An Extreme Learning Machine (ELM) model was developed and tested, enabling a robot to predict human emotion and trust levels with over 90% accuracy and respond accordingly in real-time during a collaborative object hand-over task.",The human participant handed an object to the robot. The robot moved to receive the object at different speeds or backed away based on the human's facial expression and predicted trust level.,,"No statistical tests were explicitly mentioned in the paper. The study focused on developing and testing an Extreme Learning Machine (ELM) model for predicting human emotion and trust levels. The model's performance was evaluated based on its accuracy in predicting trust levels, which was reported as approximately 91% in offline testing. The study also described the real-time application of the model in a human-robot collaboration task, where the robot's behavior was adjusted based on the predicted trust level. However, no specific statistical tests were used to compare different conditions or analyze the results.",TRUE,Robot-autonomy; Robot-nonverbal-communication; Robot-task-strategy,Robot-autonomy; Robot-nonverbal-communication; Robot-task-strategy,,"The study manipulated the robot's behavior based on the predicted trust level of the human. This is reflected in the 'Robot-autonomy' category because the robot adapted its actions (moving at different speeds or backing away) based on the human's facial expression and predicted trust level, indicating a change in its decision-making authority. The robot's movement speed and direction were also directly manipulated, which falls under 'Robot-nonverbal-communication' as these are physical movements. The robot's strategy for completing the task (picking up the object) was also changed based on the predicted trust level, which is why 'Robot-task-strategy' is included. The paper explicitly states that the robot moved at different speeds or backed away based on the human's facial expression and predicted trust level, indicating that these manipulations impacted trust. The robot's actions were directly tied to the predicted trust level, and the paper describes how the robot's behavior changed based on the predicted trust level, with the robot slowing down or backing away when low or no trust was detected. This shows that the manipulations were designed to impact trust and did so.",10.1109/URTC56832.2022.10002210,https://ieeexplore.ieee.org/document/10002210/,"With the rapid recent growth of automation and artificial intelligence, human-robot collaboration (HRC) is playing a significant role across a variety of fields. Trust between humans and robots is an important element to enable the efficiency and success of HRC. The lack of trust of humans in robots can have critical consequences, especially in real-world applications in which humans must adapt to unfamiliar situations. In this work, we develop a novel and effective approach for robots to actively reason and respond to dynamic human emotions and trust levels during shared tasks. We implement a real-world validation experiment in the context of human-robot object hand-over, which shows the robot’s ability to correctly identify and predict the human’s trust levels in real-time and assist the human accordingly in human-robot collaborative tasks. Future work on how to improve the performance of the proposed approach is also discussed."
"Guo, Yaohui; Yang, X. Jessie; Shi, Cong",Enabling Team of Teams: A Trust Inference and Propagation (TIP) Model in Multi-Human Multi-Robot Teams,2023,1,30,30,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were paired and performed a threat detection task with two drones for 15 sessions. After each session, they reported their trust in each drone and their teammate. Drone assignments were randomized each session.","Participants performed a simulated threat detection task, clicking 'Danger' or 'Clear' buttons based on the presence of threats in four views, with drone assistance.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,Participants interacted with the drones through a computer interface.,simulation,The task was performed in a simulated environment on a computer.,simulated,The drones were represented as visual elements on a computer screen.,shared control (fixed rules),"The drones assisted participants by highlighting views with threats, following pre-set rules.",Questionnaires; Custom Scales,,Performance Metrics,Trust was measured using self-reported ratings after each session and performance metrics were collected.,"parametric models (e.g., regression)","The study used a Beta distribution to model trust, with parameters updated based on direct and indirect experiences.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The performance of the two drones was manipulated, with one drone having a higher detection accuracy than the other. The task difficulty was also manipulated by setting a 3-second timer for each location.","The drone with higher detection accuracy received higher trust ratings, and trust converged over time.","The within-team trust deviation decreased significantly over time, indicating trust convergence within teams. The within-team trust deviation was significantly smaller than the between-team trust deviation at the end of the experiment, suggesting trust propagation.","The TIP model, which accounts for both direct and indirect experiences, successfully captured trust dynamics in multi-human multi-robot teams and outperformed a direct-experience-only model.",The drones assisted participants by highlighting potential threat locations. Participants were tasked with identifying threats in multiple views and clicking 'Danger' or 'Clear' buttons.,ANOVA,The study used a two-way repeated measures ANOVA to analyze the effects of drone type and time on within-team trust averages and deviations. It also used ANOVA to compare within-team and between-team trust deviations at the beginning and end of the experiment.,TRUE,Robot-accuracy; Task-constraints,Robot-accuracy,Task-constraints,"The study manipulated the accuracy of the two drones, with one drone having a higher detection accuracy (90%) than the other (60%). This is a direct manipulation of the robot's performance, which falls under 'Robot-accuracy'. The study also introduced a 3-second timer for each location, which is a manipulation of 'Task-constraints' by adding time pressure. The results show that the drone with higher detection accuracy received higher trust ratings, indicating that 'Robot-accuracy' impacted trust. The timer was a constant constraint across all conditions and did not impact trust, as it was not a variable that changed between conditions.",10.15607/RSS.2023.XIX.003,http://arxiv.org/abs/2305.12614,"Trust has been identified as a central factor for effective human-robot teaming. Existing literature on trust modeling predominantly focuses on dyadic human-autonomy teams where one human agent interacts with one robot. There is little, if not no, research on trust modeling in teams consisting of multiple human agents and multiple robotic agents. To fill this research gap, we present the trust inference and propagation (TIP) model for trust modeling in multi-human multi-robot teams. In a multi-human multi-robot team, we postulate that there exist two types of experiences that a human agent has with a robot: direct and indirect experiences. The TIP model presents a novel mathematical framework that explicitly accounts for both types of experiences. To evaluate the model, we conducted a human-subject experiment with 15 pairs of participants (${N=30}$). Each pair performed a search and detection task with two drones. Results show that our TIP model successfully captured the underlying trust dynamics and significantly outperformed a baseline model. To the best of our knowledge, the TIP model is the first mathematical framework for computational trust modeling in multi-human multi-robot teams."
"Guo, Yaohui; Yang, X. Jessie; Shi, Cong",TIP: A Trust Inference and Propagation Model in Multi-Human Multi-Robot Teams,2023,1,30,30,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a threat detection task with two drones for 15 sessions. After each session, they reported their trust in the drone they worked with, their trust in their teammate, and their trust in the drone their teammate worked with.","Participants performed a simulated threat detection task, identifying threats in four views at each location with the assistance of a drone.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,Participants interacted with the drones through a computer interface.,simulation,The task was performed in a simulated environment on a computer.,simulated,The drones were represented as virtual entities on a computer screen.,shared control (fixed rules),"The drones assisted participants by highlighting potential threats, but the participants made the final decisions.",Questionnaires,,Performance Metrics,Trust was measured using self-reported trust ratings after each session and performance metrics.,"parametric models (e.g., regression)",The study used a Beta distribution to model trust and updated trust values based on direct and indirect experiences.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The threat detection accuracy of the drones was set to different levels, and participants received feedback on their performance and the drone's performance after each session.","The TIP model, which accounted for both direct and indirect experiences, better captured trust dynamics than a model that only accounted for direct experience.","The TIP model could not fit the trust curve for some participants due to trust oscillation, but the fitted curve had a similar trend with the ground truth.","The TIP model, which accounts for both direct and indirect experiences, successfully captured people's trust dynamics in a multi-human multi-robot team and outperformed a direct-experience-only model.",The robot drones assisted participants by highlighting potential threats in a simulated environment. The human participants were tasked with identifying threats in the environment and clicking the 'Danger' or 'Clear' button.,paired-sample t-test,A paired-sample t-test was used to compare the fitting error of the proposed TIP model with that of a direct-update-only model. The test was conducted separately for each drone to determine if the TIP model's fitting error was significantly smaller than the direct-update-only model's error.,TRUE,Robot-accuracy,Robot-accuracy,,"The study explicitly states that 'The threat detection accuracy of the practice drone, drone , and drone were set to 80%, 90%, and 60%, respectively.' This indicates a direct manipulation of the robot's accuracy, which is a factor that influences task performance metrics. The results section also shows that the TIP model, which accounts for both direct and indirect experiences, better captured trust dynamics than a model that only accounted for direct experience. This indicates that the manipulation of robot accuracy impacted trust levels. There were no other factors manipulated, and no factors that were manipulated that did not impact trust.",10.1145/3568294.3580164,http://arxiv.org/abs/2301.10928,"Trust has been identified as a central factor for effective human-robot teaming. Existing literature on trust modeling predominantly focuses on dyadic human-autonomy teams where one human agent interacts with one robot. There is little, if not no, research on trust modeling in teams consisting of multiple human agents and multiple robotic agents. To fill this research gap, we present the trust inference and propagation (TIP) model for trust modeling in multi-human multi-robot teams. We assert that in a multi-human multi-robot team, there exist two types of experiences that any human agent has with any robot: direct and indirect experiences. The TIP model presents a novel mathematical framework that explicitly accounts for both types of experiences. To evaluate the model, we conducted a human-subject experiment with 15 pairs of participants (N=30). Each pair performed a search and detection task with two drones. Results show that our TIP model successfully captured the underlying trust dynamics and significantly outperformed a baseline model. To the best of our knowledge, the TIP model is the first mathematical framework for computational trust modeling in multi-human multi-robot teams."
"Gupta, Kunal",Measuring Human Trust in a Virtual Assistant using Physiological Sensing in Virtual Reality,2020,1,24,24,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a VR task with varying cognitive load and voice assistant accuracy, while physiological, behavioral, and subjective data were collected.","Participants performed a shape selection task while also completing an N-back task, with a voice assistant providing directional guidance.",Unspecified,Other,Social; Research,Game,Cooperative Game,minimal interaction,Participants interacted with a virtual agent through audio instructions in a VR environment.,simulation,The interaction took place in a virtual reality environment using an HMD.,simulated,The robot was a virtual voice assistant within the VR environment.,pre-programmed (non-adaptive),The virtual assistant provided pre-programmed directional advice with either 50% or 100% accuracy.,Behavioral Measures; Physiological Measures; Questionnaires,System Trust Scale (STS),Physiological Signals; Video Data,"Trust was assessed using questionnaires, physiological signals (EEG, GSR, HRV), and behavioral data (head movement).",no modeling,"The study did not use computational modeling of trust, but rather focused on statistical analysis of the collected data.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers manipulated the cognitive load of the task and the accuracy of the voice assistant to influence trust.,"Trust was significantly affected by both cognitive load and voice assistant accuracy, with higher trust reported for accurate assistants and lower cognitive load conditions.",The study found that EEG alpha band power decreased with higher cognitive load and that GSR mean and peak frequency differed significantly across levels of trust. Some participants preferred no assistance if the helper was inconsistent.,"The study found that human trust towards virtual agents can be measured using physiological, behavioral, and subjective measures, and that trust is influenced by both cognitive load and agent accuracy.","The virtual agent provided directional audio cues to guide the participant to a target object, while the participant searched for the object and completed an N-back task.",ANOVA; Aligned Rank Transformation; Bonferroni correction; linear mixed-effects models; Shapiro-Wilk; ANOVA,"The study used several statistical tests to analyze the collected data. ANOVA was used to analyze the effects of cognitive load and accuracy on EEG alpha band power, head movement, rounds per second, System Trust Scale scores, and NASA TLX scores. Aligned Rank Transform was used for non-parametric factorial analysis of EEG alpha band power and Subjective Mental Effort Questionnaire data, with Bonferroni post-hoc tests for comparisons. Linear Mixed-Effects Models were used to analyze GSR data, with participant as a random factor. The Shapiro-Wilk test was used to test the normality of the data for EEG alpha band power, LF/HF ratio, and SMEQ responses. Repeated measures ANOVA was used to analyze the effects of cognitive load and accuracy on head movement, rounds per second, System Trust Scale scores, and NASA TLX scores.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy; Task-complexity,,"The study explicitly manipulated the accuracy of the voice assistant (50% or 100% accuracy) which directly impacts the task performance, thus it is classified as 'Robot-accuracy'. The study also manipulated the cognitive load of the task by varying the time limit, number of objects, and difficulty of the N-back task, which is classified as 'Task-complexity'. The results showed that both the accuracy of the voice assistant and the cognitive load of the task significantly impacted the trust levels reported by the participants, as indicated by the significant main effects of both factors on the System Trust Scale (STS) scores. Therefore, both 'Robot-accuracy' and 'Task-complexity' are listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",,,"With the advancement of Artiﬁcial Intelligence technology to make smart devices, understanding how humans develop trust in virtual agents is emerging as a critical research ﬁeld. Through our research, we report on a novel methodology to investigate user’s trust in auditory assistance in a Virtual Reality (VR) based search task, under both high and low cognitive load and under varying levels of agent accuracy. We collected physiological sensor data such as electroencephalography (EEG), galvanic skin response (GSR), and heart-rate variability (HRV), subjective data through questionnaire such as System Trust Scale (STS), Subjective Mental Effort Questionnaire (SMEQ) and NASA-TLX. We also collected a behavioral measure of trust (congruency of users’ head motion in response to valid/ invalid verbal advice from the agent). Our results indicate that our custom VR environment enables researchers to measure and understand human trust in virtual agents using the matrices, and both cognitive load and agent accuracy play an important role in trust formation. We discuss the implications of the research and directions for future work."
"Gutzwiller, Robert S.; Reeder, John",Human interactive machine learning for trust in teams of autonomous robots,2017,1,60,60,0,Some participants were excluded due to missing data,Controlled Lab Environment,mixed design,"Participants completed a trust pre-experiment survey, then performed in three phases: training, comparison, and labeling.","Participants trained autonomous robot behaviors, chose between IML and black box plans, and labeled plans as IML or black box.",Unspecified,Mobile Robots; Swarm Robots,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the robots through a screen, selecting plans for further evolution.",simulation,Participants viewed the robot teams in a simulated environment on a screen.,simulated,The robots were represented as virtual agents in a simulated environment.,shared control (adaptive),"The robots' behaviors were influenced by human input during the training phase, adapting to user choices.",Questionnaires; Custom Scales,,,Trust was measured using a pre-experiment survey and a sliding trust scale after plan selection.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the level of human involvement in the machine learning process (IML vs. black box) and the frequency of user input, which influenced the robot's behavior and user expectations.","Participants chose IML plans more often, but trusted them slightly less than black box plans.","Participants chose IML plans more often, but trusted them slightly less than black box plans, which is an unexpected result. Participants completing the 25 step condition first choose IML plans more often.","Interactive machine learning (IML) resulted in behaviors that participants chose more often and were able to recognize, but they trusted IML plans slightly less than black box plans.","The robots searched virtual areas, leaving signal decay trails. Human participants selected good behaviors to evolve further, and then chose between IML and black box plans, and labeled plans as IML or black box.",ANOVA; ANOVA; ANOVA,"The study used ANOVA to analyze the impact of user involvement (10 vs 25 steps) on plan choice, and to compare trust ratings between IML and black box plans. Specifically, one ANOVA tested the interaction between plan choice and the amount of user involvement. Another ANOVA tested the effect of the amount of human involvement on trust ratings. Finally, an ANOVA was used to compare trust ratings between black box and IML plans.",TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,Task-complexity,"The study manipulated 'Robot-autonomy' by varying the level of human involvement in the machine learning process (IML vs. black box). In the IML condition, participants directly influenced the robot's behavior through training, while in the black box condition, the robot's behavior was determined by a non-interactive ML technique. This manipulation directly impacted trust, as participants trusted black box plans more than IML plans, despite choosing IML plans more often. The study also manipulated 'Task-complexity' by varying the frequency of user input during the training phase (every 10 or 25 steps of evolution). This manipulation did not directly impact trust ratings, as there was no interaction between the amount of user involvement and trust ratings. However, the amount of user involvement did impact plan choice, with participants completing the 25 step condition first choosing IML plans more often. The manipulation of task complexity is supported by the text: 'Participants were counter-balanced across the frequency of user input in IML (a decision prior to every 10 or every 25 steps of evolution). With fewer steps of evolution, the human has more ""say"" in the outcome.' The impact of robot autonomy is supported by the text: 'Participants trusted black box plans (M= 63) more than IML (M= 60) plans (F(1,34)= 6.5, p= .02, n p 2 = .16).'",10.1109/COGSIMA.2017.7929607,http://ieeexplore.ieee.org/document/7929607/,"Unmanned systems are increasing in number, while their manning requirements remain the same. To decrease manpower demands, machine learning techniques and autonomy are gaining traction and visibility. One barrier is human perception and understanding of autonomy. Machine learning techniques can result in “black box” algorithms that may yield high fitness, but poor comprehension by operators. However, Interactive Machine Learning (IML), a method to incorporate human input over the course of algorithm development by using neuro-evolutionary machine-learning techniques, may offer a solution. IML is evaluated here for its impact on developing autonomous team behaviors in an area search task. Initial findings show that IML-generated search plans were chosen over plans generated using a non-interactive ML technique, even though the participants trusted them slightly less. Further, participants discriminated each of the two types of plans from each other with a high degree of accuracy, suggesting the IML approach imparts behavioral characteristics into algorithms, making them more recognizable. Together the results lay the foundation for exploring how to team humans successfully with ML behavior."
"Guznov, S.; Lyons, J.; Pfahler, M.; Heironimus, A.; Woolley, M.; Friedman, J.; Neimeier, A.",Robot Transparency and Team Orientation Effects on Human–Robot Teaming,2020,1,88,88,0,Approximately 4% of data were excluded due to the ISAAC having a reliability of less than 50% or equal to 100%,Controlled Lab Environment,mixed design,"Participants were trained on the task, then completed the task with different levels of transparency and team orientation. They then completed post-task questionnaires.","Participants supervised an autonomous ground robot navigating a course, stopping it if it went off-path.",ISAAC,Unmanned Ground Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the robot through a screen interface, with limited direct interaction.",media,Participants viewed a degraded video feed of the robot's navigation.,physical,The robot was a physical ground robot navigating a real course.,fully autonomous (limited adaptation),"The robot navigated autonomously, but with limited adaptation to the environment.",Questionnaires,State Trust,Performance Metrics,Trust was measured using a questionnaire and performance metrics.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The study manipulated robot communication transparency (Level 2 and Level 3) and team orientation (I and We statements) to influence trust.,Team-oriented communication (We statements) resulted in lower trust compared to non-team-oriented communication (I statements). Transparency did not significantly affect trust.,"Team-oriented communication (We statements) unexpectedly decreased trust and situation awareness. Level 3 transparency increased workload, and there was no effect of transparency on trust.","Higher transparency (Level 3) improved performance when the robot's messages were accurate, but also increased workload. Team-oriented communication (We statements) decreased trust and situation awareness.","The robot autonomously navigated a course, sending text messages about its actions. The human monitored the robot's progress and stopped it if it went off-path.",ANOVA; Pearson correlation,"The study used mixed-model ANOVAs to analyze the effects of transparency (within-subjects) and team orientation (between-subjects) on various dependent variables, including hits, false alarms, perceptual accuracy, workload, state trust, and situation awareness. Follow-up ANOVAs were conducted on sub-scales of workload and situation awareness. Pearson's correlation was used to examine relationships between self-report metrics and performance scores.",TRUE,Robot-verbal-communication-content; Robot-verbal-communication-style,Robot-verbal-communication-style,Robot-verbal-communication-content,"The study manipulated two factors related to robot communication. 'Robot-verbal-communication-content' was manipulated by varying the level of transparency in the robot's text messages (Level 2 and Level 3), where Level 3 included more information about the robot's future states. This is a manipulation of the content of the communication. 'Robot-verbal-communication-style' was manipulated by changing the pronouns used in the robot's messages (I vs. We), which is a manipulation of the style of communication. The results showed that the 'Robot-verbal-communication-style' (I vs. We) impacted trust, with 'We' statements leading to lower trust. The 'Robot-verbal-communication-content' (transparency level) did not significantly impact trust.",10.1080/10447318.2019.1676519,https://www.tandfonline.com/doi/full/10.1080/10447318.2019.1676519,"Human–robot team members often have to interact in a situation when the team members are not physically collocated requiring effective communication to establish and maintain effective human–robot performance. Previous research suggests that characteristics of robot communication can have positive impact on human–robot interaction outcomes such as trust, situation awareness, workload, and performance. In this study, we investigated the joint effects of robot communication transparency (low to high, increasing the amount of information provided through text messages) and team orientation (i.e., robot’s text messages communicating team membership with We statements) properties of messages sent by a ground robot to a human teammate who was responsible for supervisory control. The results showed both benefits and limitations of increased transparency indicating the importance of careful implementation of transparency methods. Team orientation manipulation showed to be ineffective (and potentially detrimental) also indicating that caution needs to be exercised when implementing elements intended to improve group cohesion and team inclusiveness. Limitations and future directions are discussed."
"Ha, Taehyun; Kim, Sangyeon; Seo, Donghak; Lee, Sangwon",Effects of explanation types and perceived risk on trust in autonomous vehicles,2020,1,48,48,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were assigned to one of 12 autonomous driving conditions, experienced the driving situation for about 10 minutes using a virtual reality device, and then reported their perceptions of risk and trust.",Participants experienced a simulated autonomous driving scenario and rated their perceived risk and trust in the vehicle.,Unspecified,Autonomous Vehicles,Research,Evaluation,Rating,minimal interaction,Participants experienced a simulated driving scenario with no direct physical interaction with the vehicle.,simulation,Participants experienced the driving scenario through a virtual reality simulation.,simulated,The autonomous vehicle was represented as a simulation in a virtual environment.,fully autonomous (limited adaptation),"The autonomous vehicle operated without human intervention, following a pre-recorded path.",Questionnaires,,,Trust was measured using a 15-item questionnaire.,"parametric models (e.g., regression)",The study used OLS regression analysis to examine the effects of explanation type and perceived risk on trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the perceived risk of the driving situation (weather and speed) and the type of explanation provided by the autonomous vehicle (no, simple, or attributional) to influence trust.","The type of explanation significantly affected trust, and this effect was moderated by perceived risk. Attributional explanations were most effective at low risk, but least effective at high risk.",The study found that the perceived risk of autonomous driving situations was contrary to the risk perception in human driving situations. Participants perceived the highest risk when an autonomous vehicle drove with a slow speed on a clear day and the lowest risk when an autonomous vehicle drove with a slow speed on a snowy night. This is contrary to the pre-study results and may be due to the representativeness heuristic.,"The effect of explanation type on trust in autonomous vehicles is moderated by the perceived risk of the driving situation. Attributional explanations are most effective at low risk, but least effective at high risk.","The autonomous vehicle drove along a pre-recorded path, and the human participant observed the driving scenario through a virtual reality device and rated their perceived risk and trust.",ANOVA; ols regression analysis,"The study used ANOVA to check for differences in perceived risk across the four driving conditions. OLS regression analysis, using PROCESS, was then employed to examine the effects of explanation type on trust, with perceived risk as a moderator. This analysis aimed to determine if the impact of explanation type on trust varied depending on the level of perceived risk.",TRUE,Robot-verbal-communication-content; Task-environment,Robot-verbal-communication-content,Task-environment,"The study manipulated the type of explanation provided by the autonomous vehicle, which falls under 'Robot-verbal-communication-content'. The explanations varied from no explanation, to simple explanations, to attributional explanations. The study also manipulated the driving environment by changing the weather (clear day vs. snowy night) and driving speed (fast vs. slow), which is categorized as 'Task-environment'. The results showed that the type of explanation significantly affected trust, indicating that 'Robot-verbal-communication-content' impacted trust. While the driving environment was manipulated, the study found that the perceived risk of the driving conditions did not significantly impact trust directly, but rather moderated the effect of the explanation type on trust. Therefore, 'Task-environment' is not listed as a factor that directly impacted trust.",10.1016/j.trf.2020.06.021,https://linkinghub.elsevier.com/retrieve/pii/S1369847820304587,"Despite technological advances, trust still remains as a major issue facing autonomous vehicles. Existing studies have reported that explanations of the status of automation systems can be an effective strategy to increase trust, but these effects can differ depending on the forms of explanations and autonomous driving situations. To address this issue, this study examines the effects of explanation types and perceived risk on trust in autonomous vehicles. Three types of explanations (i.e., no, simple, and attributional explanations) are designed based on attribution theory. Additionally, four autonomous driving situations with different levels of risk are designed based on a simulator program. Results show that explanation type signiﬁcantly affects trust in autonomous vehicles, and the perceived risk of driving situations signiﬁcantly moderates the effect of the explanation type. At a high level of perceived risk, attributional explanations and no explanations lead to the lowest and highest values in trust, respectively. However, at a low level of perceived risk, these effects reverse."
"Hald, Kasper; Rehmn, Matthias; Moeslund, Thomas B.",Human-Robot Trust Assessment Using Motion Tracking & Galvanic Skin Response,2020,1,40,40,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants performed a collaborative drawing task with a robot, holding a piece of paper while the robot drew on it. The robot's speed was changed unexpectedly midway through the experiment. Participants completed questionnaires and had their galvanic skin response (GSR) measured.",Participants held a piece of paper on a table while a robot drew a square on it.,Sawyer,Industrial Robot Arms; Collaborative Robots (Cobots),Research; Industrial,Manipulation,Drawing,direct-contact interaction,Participants physically held paper while the robot drew on it.,real-world,The study involved a real robot in a physical environment.,physical,A physical robot was used in the experiment.,pre-programmed (non-adaptive),The robot followed a pre-programmed path without adapting to the user.,Behavioral Measures; Physiological Measures; Questionnaires,,Physiological Signals; robot data,"Trust was assessed using questionnaires, GSR, and motion tracking.",no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's movement speed was changed unexpectedly to induce a decrease in trust, and participants were given different starting speeds.","Reported trust decreased significantly when the robot's speed was increased, but not when it was decreased.","The study found that only increases in robot speed significantly affected reported trust, while decreases did not. The motion tracking method did not show significant effects on participant movement.",Reported trust towards the robot was significantly affected when the robot's movement speed was unexpectedly increased.,The robot drew a square on a piece of paper held by the participant. The participant's role was to hold the paper in place while the robot drew.,Shapiro-Wilk; Wilcoxon rank sum; kendall's rank correlation; Spearman correlation,"The Shapiro-Wilk test was used to assess the normality of the data distributions for reported trust scores, tracking data, and GSR measurements. Since the data was found to be non-normally distributed, non-parametric tests were used. The Wilcoxon rank sum test was used to compare reported trust scores before and after changes in robot speed, and to compare trust scores between different speed conditions and ear protection conditions. It was also used to compare movement data and GSR measurements between conditions. Kendall's and Spearman's rank correlations were used to assess the correlation between movement data and reported trust scores.",TRUE,Robot-accuracy; Task-environment,Robot-accuracy,Task-environment,"The study manipulated the robot's movement speed, which directly affects the robot's performance on the drawing task, thus it is classified as 'Robot-accuracy'. The study also manipulated whether participants wore ear protection, which changes the auditory environment and is classified as 'Task-environment'. The results showed that changes in robot speed (specifically increases) significantly impacted reported trust, while the presence or absence of ear protection did not have a significant effect on trust. Therefore, 'Robot-accuracy' is listed as a factor that impacted trust, and 'Task-environment' is listed as a factor that did not impact trust. The paper states: 'The test conditions, whether the participant is wearing ear and the robot's beginning speed, are counter-balanced with ten participants for each combination of conditions, leading to a total of forty participants.' and 'The test shows significant difference for participants for whom speed was increase, both with (W = 88, p <.01) and without ear protection (W = 100, p <.01), while not significant for participants who experienced a decrease in speed.' and 'There were no significant effects from the ear protection in any condition.'",10.1109/IROS45743.2020.9341267,https://ieeexplore.ieee.org/document/9341267/,"In this study we set out to design a computer vision-based system to assess human-robot trust in real time during close-proximity human-robot collaboration. This paper presents the setup and hardware for an augmented realityenabled human-robot collaboration cell as well as a method of measuring operator proximity using an infrared camera. We tested this setup as a tool for assessing trust through physical apprehension signals in a collaborative drawing task, where participants hold a piece of paper on a table while the robot draws between their hands. Midway through the test we attempt to induce a decrease in trust with an unexpected change in robot speed and evaluate subject motions along with self-reported trust and emotional arousal through galvanic skin response. After performing the experiment with forty participants, we found that reported trust was signiﬁcantly affected when robot movement speed was increased. The galvanic skin response measurement were not signiﬁcantly different between the test conditions. The motion tracking method used in this study did not suggest that subjects’ motions were signiﬁcantly affected by the decrease in trust."
"Hald, Kasper; Rehm, Matthias; Moeslund, Thomas B.",Human-Robot Trust Assessment Using Top-Down Visual Tracking After Robot Task Execution Mistakes,2021,1,20,20,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants performed a collaborative pick-and-place task with a robot, with a trust-dampening action introduced midway through the tasks. Participants completed trust questionnaires after each task and had the option to stop the robot.","Participants and a robot moved cones from one side of a table to the other, with the robot moving blue cones and the participant moving red cones.",Sawyer,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Passing,direct-contact interaction,Participants physically collaborated with the robot in a pick-and-place task.,real-world,The study involved a real-world interaction with a physical robot.,physical,The study used a physical robot for the interaction.,pre-programmed (non-adaptive),The robot followed pre-programmed movements without adapting to the participant's actions.,Behavioral Measures; Questionnaires,Schaefer's Trust Questionnaire/Scale,Video Data,"Trust was assessed using questionnaires and behavioral measures, including proximity tracking.",no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's behavior was manipulated by introducing irregular movements or task execution mistakes, and the task was manipulated by having participants work simultaneously or in turns with the robot. These manipulations were intended to affect the participant's perception of the robot's predictability and dependability, and thus their trust.","Trust significantly decreased when the robot made a mistake in task execution, especially during simultaneous collaboration. Irregular movements did not significantly affect trust. The trust-dampening actions did not increase the number of participant interruptions.","Participants showed a trend of moving closer to and further away from the robot throughout the experiment, making the proximity data inconclusive. There was a higher decrease in trust during simultaneous collaboration than turn-taking when the robot performed a wrong task.","Trust in the robot was significantly decreased when the robot performed actions that went counter to the shared objective, but not when the robot changed its movement pattern while otherwise performing the task correctly.","The robot moved blue cones from one side of the table to the other using a rod, while the human moved red cones to their corresponding locations. The robot's movements were pre-programmed, and the human could interrupt the robot if needed.",ANOVA; Kruskal-Wallis; t-test; Wilcoxon rank sum,"The study used ANOVA for parametric data and Kruskal-Wallis tests for non-parametric data to perform multi-variate analyses based on tasks and conditions. Pairwise t-tests were used for parametric data and Wilcoxon rank sum tests were used for non-parametric data to test the differences between before and after trust-dampening actions, as well as the differences between the effects of conditions. Specifically, ANOVA was used to analyze the effect of test conditions on proximity, while Kruskal-Wallis was used to analyze the effect of test conditions on trust scores and changes in delta trust. Pairwise comparisons were used to further investigate significant effects found in the multi-variate analyses.",TRUE,Robot-accuracy; Robot-task-strategy; Task-constraints,Robot-accuracy; Task-constraints,Robot-task-strategy,"The study manipulated the robot's actions to introduce errors, which directly impacted the task performance (Robot-accuracy). Specifically, the robot either moved the wrong cone or moved in an irregular path. The robot's movement path was manipulated to test predictability, where the robot moved in an arch closer to the participant (Robot-task-strategy). The study also manipulated the collaboration format, having participants work simultaneously or in turns with the robot, which affected the time pressure and attentional demands on the participant (Task-constraints). The results showed that the robot making a mistake in task execution (Robot-accuracy) significantly decreased trust, especially during simultaneous collaboration (Task-constraints). The irregular movement pattern (Robot-task-strategy) did not significantly affect trust. The manipulation of collaboration format (Task-constraints) also impacted trust, with simultaneous collaboration leading to a greater decrease in trust when the robot made a mistake.",10.1109/RO-MAN50785.2021.9515501,https://ieeexplore.ieee.org/document/9515501/,"With increased interest in close-proximity humanrobot collaboration in production settings it is important that we understand how robot behaviors and mistakes affect humanrobot trust, as a lack of trust can cause loss in productivity and over-trust can lead to hazardous misuse. We designed a system for real-time human-robot trust assessment using a top-down depth camera tracking setup with the goal of using signs of physical apprehension to infer decreases in trust toward the robot. In an experiment with 20 participants we evaluated the tracking system in a repetitive collaborative pick-and-place task where the participant and the robot had to move a set of cones across a table. Midway through the tasks we disrupted the participants expectations by having the robot perform a trust-dampening action. Throughout the tasks we measured the participant’s preferred proximity and their trust toward the robot. Comparing irregular robot movements versus task execution mistakes as well simultaneous versus turntaking collaboration, we found reported trust was signiﬁcantly decreased when the robot performed an execution mistake going counter to the shared objective. This decrease was higher for participant working simultaneously as the robot. The effect of the trust-dampening actions on preferred proximity was inconclusive due to unexplained movement trends between tasks throughout the experiment. Despite being given the option to stop the robot in case of abnormal behavior, the trustdampening actions did not increase the number of participant disruptions for the actions we tested."
"Hald, Kasper; Weitz, Katharina; André, Elisabeth; Rehm, Matthias",“An Error Occurred!” - Trust Repair With Virtual Robot Using Levels of Mistake Explanation,2021,2,20,20,0,No participants were excluded,Online Crowdsourcing,mixed design,"Participants watched videos of a virtual robot performing a sorting task, with and without errors. They then rated the robot's performance, likeability, and the quality of explanations provided for errors. The study used a between-subjects design for explanation modality and a within-subjects design for error type.",Participants watched videos of a virtual robot sorting bottles and rated the robot's performance and likeability. They also evaluated explanations for robot errors.,Sawyer,Industrial Robot Arms; Collaborative Robots (Cobots),Research,Evaluation,Rating,passive observation,Participants passively observed videos of the robot.,media,Participants watched videos of the robot performing a task.,simulated,The robot was presented as a simulated model in videos.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires; Custom Scales,Explanation Satisfaction Scale (ESS),,Trust was assessed using a custom scale and items from the Explanation Satisfaction Scale.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's performance was manipulated by introducing errors, and the type of error and explanation were varied to influence trust.",Robot errors significantly decreased trust and likeability. Explanation type had a small effect on trust ratings.,Participants preferred solution-oriented explanations over technical ones. The type of error influenced trust ratings.,"Robot errors significantly decreased trust and likeability, and participants preferred solution-oriented explanations.","The robot sorted bottles based on shape, and the human observed and rated the robot's performance and explanations.",t-test; t-test,"Paired t-tests were used to compare the robot's performance and likeability ratings between the no-error condition and the two error conditions (calibration and color vision errors). Independent samples t-tests were used to analyze the differences in explanation quality, likeability, and performance between the two explanation modalities (textual vs. auditory).",TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-accuracy,Robot-verbal-communication-content,"The study manipulated 'Robot-accuracy' by having the robot make errors (calibration and color vision errors) in some conditions and not in others. This directly impacted the robot's performance on the sorting task, which was measured by the participants' ratings. The study also manipulated 'Robot-verbal-communication-content' by providing different explanations (textual or auditory) for the errors. While the type of error and the presence of an explanation did influence the participants' ratings of the explanation quality, the modality of the explanation (textual vs. auditory) did not significantly impact trust, likeability, or performance ratings. The robot errors significantly decreased trust and likeability, but the explanation modality did not have a significant impact on trust ratings. Therefore, 'Robot-accuracy' impacted trust, while 'Robot-verbal-communication-content' did not.",10.1145/3472307.3484170,https://dl.acm.org/doi/10.1145/3472307.3484170,"Human-robot collaboration in industrial settings is an expanding research field in robotics. When working together, robot mistakes are an important factor to decrease trust and therefore interferes with cooperation. It is unclear whether explanations help to restore human-robot trust after a mistake. In our study, we investigate whether system explanations as a trust-repairing action after a robot makes a mistake in a collaborative task is helpful. Our pilot study revealed that users are more interested in solutions to errors than they are in just why the error happened. Therefore, in our main study, we evaluated three levels of mistake explanations (no explanation, explanation, and explanation with solution) after a robot in VR made a mistake in executing a shared objective. After testing with 30 participants we found that the robot making a mistake significantly affects trust toward the robot, compared to it completing the task successfully. While participants found the explanations helpful to trust or distrust the robot, the levels of the explanation did not lead to an increase in trust towards the robot after a mistake. In addition, we found no significant impact of explanations on self-efficacy and the emotional state of the participants. Our results show that explanations alone are not sufficient to increase human-computer trust after robot mistakes."
"Hald, Kasper; Weitz, Katharina; André, Elisabeth; Rehm, Matthias",“An Error Occurred!” - Trust Repair With Virtual Robot Using Levels of Mistake Explanation,2021,2,30,30,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants performed a collaborative sorting task with a virtual robot in VR. The robot either succeeded or made a mistake, followed by different levels of explanation. Participants completed trust questionnaires after each task.",Participants collaborated with a virtual robot to sort bottles by shape in a VR environment.,Sawyer,Industrial Robot Arms; Collaborative Robots (Cobots),Research,Manipulation,Sorting/Arranging,minimal interaction,Participants interacted with a virtual robot in a VR environment.,simulation,Participants interacted with the robot in a virtual reality environment.,simulated,The robot was presented as a simulated model in a VR environment.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires; Custom Scales,Schaefer's Trust Questionnaire/Scale; Explanation Satisfaction Scale (ESS),,Trust was assessed using the Schaefer HRT questionnaire and items from the Explanation Satisfaction Scale.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's performance was manipulated by introducing an error, and the level of explanation provided was varied to influence trust.","Robot errors significantly decreased trust. While explanations were rated as helpful, they did not significantly increase trust in the robot itself.","Participants found explanations helpful to decide whether to trust or distrust the robot, but this did not translate to increased trust in the robot itself. There was a significant difference in trust ratings between the explanation and no explanation conditions in the post-test questionnaire, but not in the VR task itself.","Explanations alone are not sufficient to recover trust after robot mistakes, even when participants retrospectively rate the explanation as helpful.","The robot sorted blue bottles, and the human sorted red bottles, with the goal of sorting them by shape. The robot made a mistake in the second trial.",pair-wise t-test; ANOVA; one-way manova,"Pair-wise t-tests were used to compare the Human-Robot Trust (HRT) scores between the first and second tasks (successful vs. error) for each explanation condition. A one-way ANOVA was used to compare the HRT scores after the mistake and the delta of HRT scores between tasks across the different explanation levels. A one-way MANOVA was used to evaluate the differences in explanation satisfaction, trust in the explanation, self-efficacy, and emotional state between the three explanation conditions.",TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-accuracy,Robot-verbal-communication-content,"The study manipulated 'Robot-accuracy' by having the robot either succeed or make a mistake in the sorting task. This directly impacted the robot's performance on the task. The study also manipulated 'Robot-verbal-communication-content' by providing different levels of explanation (no explanation, explanation, and explanation with solution) after the robot made a mistake. While the explanations were rated as helpful in the post-test questionnaire, they did not significantly increase trust in the robot itself during the VR task. The robot errors significantly decreased trust, but the level of explanation did not have a significant impact on trust ratings during the task. Therefore, 'Robot-accuracy' impacted trust, while 'Robot-verbal-communication-content' did not.",10.1145/3472307.3484170,https://dl.acm.org/doi/10.1145/3472307.3484170,"Human-robot collaboration in industrial settings is an expanding research field in robotics. When working together, robot mistakes are an important factor to decrease trust and therefore interferes with cooperation. It is unclear whether explanations help to restore human-robot trust after a mistake. In our study, we investigate whether system explanations as a trust-repairing action after a robot makes a mistake in a collaborative task is helpful. Our pilot study revealed that users are more interested in solutions to errors than they are in just why the error happened. Therefore, in our main study, we evaluated three levels of mistake explanations (no explanation, explanation, and explanation with solution) after a robot in VR made a mistake in executing a shared objective. After testing with 30 participants we found that the robot making a mistake significantly affects trust toward the robot, compared to it completing the task successfully. While participants found the explanations helpful to trust or distrust the robot, the levels of the explanation did not lead to an increase in trust towards the robot after a mistake. In addition, we found no significant impact of explanations on self-efficacy and the emotional state of the participants. Our results show that explanations alone are not sufficient to increase human-computer trust after robot mistakes."
"Hamacher, Adriana; Bianchi-Berthouze, Nadia; Pipe, Anthony G.; Eder, Kerstin",Believing in BERT: Using expressive communication to enhance trust and counteract operational error in physical Human-robot interaction,2016,1,23,21,2,2 participants were excluded due to the robot malfunctioning to the point where the subjects could not complete the tasks,Controlled Lab Environment,within-subjects,"Participants interacted with a robot in a mock cooking scenario, evaluating three different versions of the same robot with varying communicative abilities and error rates. They completed pre- and post-experiment questionnaires, and were interviewed after the interaction.",Participants were asked to receive ingredients from a robot to make an omelet.,BERT2,Humanoid Robots; Expressive Robots,Research; Social,Manipulation,Object Passing,direct-contact interaction,Participants physically interacted with the robot during the task.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The robot was a physical entity present in the interaction.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions without adapting to the user.,Behavioral Measures; Questionnaires,NASA Task Load Index (NASA-TLX),Video Data; Speech Data,Trust was assessed using self-reported questionnaire data and behavioral observations.,no modeling,Trust was not modeled computationally; descriptive statistics were used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's communicative ability, error rate, and feedback were manipulated to influence trust. The robot either did not communicate, made an error and attempted to rectify it, or communicated, made an error, apologized, and attempted to rectify it.","The communicative robot with error mitigation was rated higher in trust than the robot that made an error without communication. The most efficient robot was rated higher than the robot that made an error without communication, but not significantly different from the communicative robot.","Participants were reluctant to disappoint the communicative robot, even lying to avoid hurting its feelings. Some participants also attempted to help the robot when it made a mistake. There was a conflict between the self-reported data and the behavioral data, as some participants reported that the communicative robot was faster, even though it was slower.","An expressive robot was preferred over a more efficient one, despite a trade-off in time taken to do the task, suggesting that human-like attributes can mitigate dissatisfaction arising from unexpected behavior.","The robot handed participants polystyrene eggs and a container of salt. The human participant received the items and was asked to whisk the eggs. The robot's behavior varied across conditions, including making errors and communicating with the participant.",Friedman test; Wilcoxon rank sum,"The study used Friedman tests to investigate the effect of the independent variables (robot conditions) on the dependent variables (satisfaction, temporal demand, trust, and frustration) measured using Likert scale data from the post-experiment questionnaire. When the Friedman tests showed significance, Wilcoxon matched pairs tests were used for post-hoc comparisons between the different robot conditions. A Bonferroni correction was applied to the Wilcoxon tests to account for multiple comparisons.",TRUE,Robot-verbal-communication-content; Robot-emotional-display; Robot-accuracy,Robot-verbal-communication-content; Robot-emotional-display,Robot-accuracy,"The study manipulated several factors related to the robot's behavior. 'Robot-verbal-communication-content' was manipulated by having one condition (BERT C) provide verbal feedback, ask questions, and apologize for errors, while the other two conditions (BERT A and B) remained silent. This is explicitly stated in the paper: 'BERT C was the only candidate able to talk. On each occasion the system verified whether participants were ready to receive the egg... BERT C also dropped an egg, but appeared conscious of its mistake and apologized.' 'Robot-emotional-display' was manipulated by having BERT C display a sad facial expression after dropping the egg, while the other conditions did not show any emotional response. This is described as: 'BERT C's ""I'm sorry"" and exaggerated look of sadness... was marked.' 'Robot-accuracy' was manipulated by having BERT B and C drop an egg, while BERT A did not. This is described as: 'The non-communicative candidate, labeled A, performed the most efficiently, never dropping an egg. B was also mute and, in addition, dropped one of the eggs.' The results showed that the communicative and emotional aspects of the robot (BERT C) significantly impacted trust, as participants preferred it despite its errors and longer task time. The robot's accuracy (BERT A being more accurate) did not significantly impact trust compared to BERT C, as participants preferred the communicative robot. The robot that made an error without communication (BERT B) was rated lower in trust than both BERT A and C. Therefore, 'Robot-verbal-communication-content' and 'Robot-emotional-display' impacted trust, while 'Robot-accuracy' did not have a significant impact on trust compared to the other two factors.",10.1109/ROMAN.2016.7745163,http://ieeexplore.ieee.org/document/7745163/,"Strategies are necessary to mitigate the impact of unexpected behavior in collaborative robotics, and research to develop solutions is lacking. Our aim here was to explore the benefits of an affective interaction, as opposed to a more efficient, less error prone but non-communicative one. The experiment took the form of an omelet-making task, with a wide range of participants interacting directly with BERT2, a humanoid robot assistant. Having significant implications for design, results suggest that efficiency is not the most important aspect of performance for users; a personable, expressive robot was found to be preferable over a more efficient one, despite a considerable trade off in time taken to perform the task. Our findings also suggest that a robot exhibiting human-like characteristics may make users reluctant to ‘hurt its feelings’; they may even lie in order to avoid this."
"Hannibal, Glenda; Weiss, Astrid; Charisi, Vicky","""The robot may not notice my discomfort"" – Examining the Experience of Vulnerability for Trust in Human-Robot Interaction",2021,1,98,98,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of three shopping scenarios (economy, privacy, transparency). They completed pre-interaction questionnaires, interacted with a robot in the assigned scenario, and then completed a post-interaction trust questionnaire and open-ended questions about their experience of vulnerability.",Participants engaged in a simulated online clothes shopping task where they were assisted by a robot.,Pepper,Humanoid Robots; Expressive Robots,Social; Research,Social,Social Guidance/Coaching,minimal interaction,Participants interacted with the robot through an online survey with images of the robot.,media,The interaction was presented through static images of the robot.,physical,The robot was represented by photos of a physical robot.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user's input.,Questionnaires; Multidimensional Measures,Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using the MDMT scale and open-ended questions.,no modeling,No computational model of trust was developed.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the robot's behavior by introducing subtle trust violations in three different scenarios (economy, privacy, transparency) to see how it affected trust.",The economy scenario resulted in significantly lower trust ratings compared to the privacy and transparency scenarios. Participants who reported experiencing vulnerability also rated the robot as less reliable and ethical.,"The privacy scenario was rated best on all trust scales, followed by transparency, and economy. Younger participants were more likely to answer that they did not experience vulnerability and rated the robot as more trustworthy.",The study demonstrated that it is possible to measure human experience of vulnerability in everyday life situations and that subtle trust violations can impact trust in HRI.,"The robot acted as a sales assistant, offering assistance and recommendations for clothes shopping. The human participant selected items and responded to the robot's prompts, including trust violations.",ANOVA; gabrierl's pairwise posthoc test; Pearson correlation,"The study used ANOVA to investigate the effect of the three different trust violation scenarios (economy, privacy, transparency) on participants' trust ratings (MDMT scales). A Gabrierl's Pairwise posthoc test was used to further analyze significant differences found by the ANOVA. Additionally, Pearson correlation was used to examine the relationship between age and trust ratings, as well as age and the experience of vulnerability.",TRUE,Robot-morality; Robot-verbal-communication-content,Robot-morality; Robot-verbal-communication-content,,"The study manipulated the robot's behavior by introducing subtle trust violations in three different scenarios (economy, privacy, transparency). In the economy scenario, the robot made a calculation mistake, which can be classified as a violation of morality as it involves a breach of trust related to financial transactions. In the privacy scenario, the robot suggested changing the gender on the customer account and asked if this information could be used for training, which is a violation of privacy and thus a moral violation. In the transparency scenario, the robot asked for irrelevant information, which can be seen as a violation of transparency and thus a moral violation. The content of the robot's communication also varied across scenarios, with different prompts and questions being used, thus manipulating 'Robot-verbal-communication-content'. The study found that the economy scenario resulted in significantly lower trust ratings, indicating that both 'Robot-morality' and 'Robot-verbal-communication-content' impacted trust. There were no factors that were manipulated that did not impact trust.",10.1109/RO-MAN50785.2021.9515513,https://ieeexplore.ieee.org/document/9515513/,"Ensuring trust in human-robot interaction (HRI) is considered essential for widespread use of robots in society and everyday life. While the majority of studies use gamebased and high-risk scenarios with low familiarity to gain a deeper understanding of human trust in robots, scenarios with more subtle trust violations that could happen in everyday life situations are less often considered. In this paper, we present a theory-driven approach to studying the situated trust in HRI by focusing on the experience of vulnerability. Focusing on vulnerability not only challenges previous work on trust in HRI from a theoretical perspective, but is also useful for guiding empirical investigations. As a ﬁrst proof-of-concept study, we conducted an interactive online survey that demonstrates that it is possible to measure human experience of vulnerability in the ordinary, mundane, and familiar situation of clothes shopping. We conclude that the inclusion of subtle trust violation scenarios occurring in the everyday life situation of clothes shopping enables a better understanding of situated trust in HRI, which is of special importance when considering more near-future applications of robots."
"Haring, Kerstin Sophie; Matsumoto, Yoshio; Watanabe, Katsumi",How Do People Perceive and Trust a Lifelike Robot,2013,1,56,55,1,1 subject was eliminated due to participant bias (demand characteristics),Controlled Lab Environment,mixed design,"Participants completed questionnaires about their personality and robot perception, then interacted with the robot in three tasks, and finally played a trust game with the robot. Questionnaires were administered before and after the interaction tasks.","Participants interacted with the robot by moving a box in the first two trials and touching the robot's hand in the third trial, followed by an economic trust game.",Geminoid-F,Humanoid Robots; Android Robots,Research; Social,Game,Economic Game,direct-contact interaction,Participants physically interacted with the robot and completed tasks.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,The study used a physical android robot.,wizard of oz (directly controlled),The robot was tele-operated by a researcher.,Behavioral Measures; Questionnaires,Godspeed Questionnaire; Eysenck Personality questionnaire,Video Data,Trust was measured using questionnaires and behavioral measures in a trust game.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's head movements and the payback amount in the trust game were manipulated to influence trust.,"Extroverted participants sent more money in the trust game, and a higher payback from the robot influenced the perception of the robot as more life-like.",Participants perceived the robot as less anthropomorphic and intelligent but safer after the interaction. The amount sent in the trust game was higher if participants rated the robot as more intelligent before the interaction. Participants came closer to the robot over the three trials.,"People's perception of the robot changed after interacting with it, becoming less anthropomorphic and intelligent but safer, and extroverted individuals showed higher trust in the robot.","The robot engaged participants in a short conversation, instructed them to move a box in the first two trials, and asked them to touch its hand in the third trial. Participants then played a trust game with the robot.",pairwise t-test; t-test; Pearson correlation; ANOVA; 2-way anova,"The study used several statistical tests. Pairwise t-tests were used to compare distances between the participant and the robot across the three interaction trials. Paired t-tests were used to compare the robot perception ratings before and after the interaction tasks. Correlation analyses were used to examine the relationship between robot perception and the amount sent in the trust game, as well as the relationship between personality traits and the amount sent in the trust game. ANOVA was used to examine the influence of virtual agent exposure on robot perception and the influence of the payback condition on animacy ratings. A 2-way ANOVA was used to examine the influence of pet ownership and payback condition on robot perception.",TRUE,Robot-nonverbal-communication; Teaming; Robot-verbal-communication-content,Robot-nonverbal-communication; Teaming,,"The study manipulated the robot's head movements (Robot-nonverbal-communication) by having it turn its head towards the boxes when talking about them. This was done in one of the two conditions. The study also manipulated the payback amount in the trust game (Teaming), where the robot would return either more or less money than the participant sent. This manipulation was intended to influence the perception of the robot and trust. The robot also engaged in a short conversation with the participants before each task, which included instructions (Robot-verbal-communication-content). The head movement manipulation and the payback amount in the trust game were found to influence trust, as the payback amount influenced the perception of the robot as more life-like and extroverted participants sent more money in the trust game. The content of the conversation was not found to have a direct impact on trust, but it was part of the experimental procedure.",,,"This paper reports the results from an experiment examining people’s perception and trust when interacting with an android robot. Also, they engaged in an economic trust game with the robot. We used proxemics, the physical distance to the robot, and questionnaires to measure the participants` character and their perception of the robot. We found influences of the subject’s character onto the amount sent in the trust game and distance changes over the 3 interaction tasks. The perception of the robot changed after the interaction trials towards less anthropomorph and less intelligent, but safer. This study would enable future researches to compare different robot types, personality traits and cross-cultural effects."
"Haring, Kerstin Sophie; Matsumoto, Yoshio; Watanabe, Katsumi",Perception and Trust Towards a Lifelike Android Robot in Japan,2014,1,56,55,1,1 subject was eliminated due to participant bias (demand characteristics),Controlled Lab Environment,mixed design,"Participants first completed questionnaires about their personality and perception of the robot. Then, they interacted with the robot in three trials: moving a box twice and touching the robot's hand once. Finally, they played an economic trust game with the robot.","Participants interacted with the robot by moving a box, touching its hand, and playing an economic trust game.",Actroid-F,Android Robots,Research; Social,Game,Economic Game,direct-contact interaction,"Participants had direct physical interaction with the robot, including touch.",real-world,The interaction took place in a real-world setting with a physical robot.,physical,The robot was physically present during the interaction.,wizard of oz (directly controlled),The robot was tele-operated by a researcher.,Behavioral Measures; Questionnaires,Godspeed Questionnaire,Video Data,Trust was assessed using an economic trust game and questionnaires.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers manipulated the payback amount in the trust game, which influenced the participants' perception of the robot's behavior and their expectations.","A higher payback in the trust game led to a more positive perception of the robot, particularly for participants who had never owned a pet.","Participants' perception of the robot changed after the interaction, with the robot being rated as less anthropomorphic and intelligent but safer. Extraverted participants sent more money in the trust game. Participants with more exposure to virtual agents rated the robot as more life-like before the interaction, but this changed after the interaction.","The study found that direct interaction with a highly realistic android robot significantly changed participants' perceptions of the robot, and that extraverted individuals were more likely to trust the robot in an economic game.","The robot engaged participants in a short conversation, instructed them to move a box, and asked them to touch its hand. The human participant moved the box, touched the robot's hand, and made decisions in the trust game.",pairwise t-test; t-test; Pearson correlation; ANOVA; 2-way anova,"The study used several statistical tests to analyze the data. Pairwise t-tests were used to compare the distance between the participant and the robot across the three interaction trials. Paired t-tests were used to compare the Godspeed questionnaire results before and after the interaction trials. Correlation analyses were used to examine the relationship between robot perception, personality traits, and the amount sent in the trust game. ANOVA was used to examine the influence of virtual agent exposure on robot perception and the influence of payback condition on animacy rating. A 2-way ANOVA was used to examine the influence of pet ownership and payback condition on robot perception.",TRUE,Robot-nonverbal-communication; Task-complexity; Teaming,Teaming,,"The study manipulated several factors. 'Robot-nonverbal-communication' was manipulated by having the robot turn its head towards the boxes when talking about them, which is a nonverbal cue. 'Task-complexity' was manipulated by having participants perform different tasks (moving a box, touching the hand) with varying levels of complexity. 'Teaming' was manipulated through the economic trust game, where participants had to decide how much money to send to the robot, which was then manipulated by the researchers to provide a higher or lower payback. The 'Teaming' factor was found to impact trust, as the payback amount influenced the participants' perception of the robot, particularly for those who had never owned a pet. The study did not explicitly state that the other factors had an impact on trust, but they were manipulated as part of the experimental design.",,http://link.springer.com/10.1007/978-94-017-9115-1_37,"This paper reports the results from an experiment examining people’s perception and trust when interacting with an android robot. Also, they engaged in an economic trust game with the robot. We used the physical distance to the robot, and questionnaires to measure the participants’ character and their perception of the robot. We found inﬂuences of the subject’s character onto the amount sent in the trust game and distance changes over the three interaction tasks. The perception of the robot changed after the interaction trials towards less anthropomorph and less intelligent, but safer."
"Haring, Kerstin Sophie; Silvera-Tawil, David; Matsumoto, Yoshio; Velonaki, Mari; Watanabe, Katsumi",Perception of an Android Robot in Japan and Australia: A Cross-Cultural Comparison,2014,1,111,111,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants completed questionnaires, interacted with the robot in three tasks, and then played an economic trust game. Questionnaires were administered before and after the interaction tasks.","Participants moved a box, touched the robot's hand, and played an economic trust game with the robot.",Actroid-F,Humanoid Robots; Android Robots,Research; Social,Game,Economic Game,direct-contact interaction,Participants physically interacted with the robot during the tasks.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The robot was physically present during the interaction.,pre-programmed (non-adaptive),The robot followed a fixed protocol during the interaction tasks.,Behavioral Measures; Questionnaires,Godspeed Questionnaire,Video Data,Trust was assessed using an economic trust game and the Godspeed questionnaire.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,No Manipulation,"The study did not directly manipulate trust, but the interaction with a physical robot influenced trust.",Australian participants showed higher trust in the economic game compared to Japanese participants.,"Australian participants rated the robot higher in anthropomorphism, likeability, and perceived intelligence, and also showed higher trust in the economic game, contradicting the stereotype of Japanese people being more accepting of robots. Japanese participants came closer to the robot in each consecutive task, while this effect was not observed in the Australian participants.","Australian participants exhibited higher trust towards the android robot in an economic trust game compared to Japanese participants, despite rating the robot as less safe.","The robot asked participants to move a box, touch its hand, and then played an economic trust game with them. Participants moved a box, touched the robot's hand, and decided how much money to send to the robot in the trust game.",Chi-squared; t-test; Pearson correlation,"The study used a Chi-square test to compare pet ownership between Australian and Japanese participants. T-tests were used to compare means between groups (e.g., psychoticism, extraversion scores, anthropomorphism ratings, likeability ratings, perceived intelligence, perceived safety, and amount of money sent in the economic trust game) and within groups (e.g., changes in anthropomorphism, perceived intelligence, and perceived safety before and after interaction, and distance to the robot across tasks). Correlation analysis was used to examine the relationship between extraversion and the payback amount in the trust game.",FALSE,Task-environment,,,"The study did not explicitly manipulate any factors to influence trust. However, the study was conducted in two different countries (Japan and Australia), which constitutes a change in the task environment. This is because the cultural context and the participants' prior experiences with robots (or lack thereof) in each country could influence their perception and trust towards the robot. The study did not manipulate any robot-related factors, such as its behavior, communication, or appearance. The robot followed a fixed protocol in both countries. Therefore, the only factor that was implicitly manipulated was the task environment.",,http://link.springer.com/10.1007/978-3-319-11973-1_17,"This paper reports the results from two experiments, conducted in Japan and Australia, to examine people’s perception and trust towards an android robot. Experimental results show that, in contrast to popular belief, Australian participants perceived the robot more positive than Japanese participants. This is the ﬁrst study directly comparing human perception of a physically present android robot in two diﬀerent countries."
"Hashemian, Mojgan; Paradeda, Raul; Guerra, Carla; Paiva, Ana",Do You Trust Me? Investigating the Formation of Trust in Social Robots,2019,2,42,42,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants signed a consent form, completed a pre-interaction trust questionnaire, interacted with the Emys robot in one of four conditions (ST SAD, NST SAD, ST JOY, NST JOY), made a fictional donation, and completed a post-interaction trust questionnaire.",Participants interacted with a robot that complained about a mechanical fault and asked for a fictional donation.,Emys,Expressive Robots,Research; Social,Social,Persuasion,minimal interaction,Participants interacted with the robot in a controlled setting.,real-world,Participants interacted with a physical robot in a lab setting.,physical,The robot was physically present during the interaction.,pre-programmed (non-adaptive),The robot followed a pre-programmed script.,Questionnaires,Human-Robot Trust Scale/Questionnaire (HRTS/HRITS),,Trust was measured using a pre- and post-interaction questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's emotional expression (sad or joyful) and the presence of small talk were manipulated to influence trust.,Starting with small talk and showing sad facial expressions increased trust.,"The study found that the combination of small talk and sad facial expressions led to higher trust levels, particularly among female participants.",Starting a conversation with small talk and showing sad facial expressions enhances the trust level of people interacting with the robot.,The robot tells a story about a mechanical fault and asks for a fictional donation. The human participant listens to the robot and decides on a donation amount.,Mann-Whitney U; Sign test; Kruskal-Wallis,"The study used U Mann Whitney tests to compare pre-questionnaire scores between subgroups (ST, JOY, NST, SAD) to determine if participants had similar initial trust levels. Sign tests were used to compare pre- and post-questionnaire scores within the NST and SAD subgroups to assess changes in trust after interaction. Kruskal-Wallis tests were used to compare trust scores across the four experimental conditions (ST SAD, NST SAD, ST JOY, NST JOY) and to examine the influence of gender on trust within each condition. Additionally, Kruskal-Wallis tests were used to compare donation amounts across the four groups and between genders.",TRUE,Robot-emotional-display; Robot-social-attitude,Robot-emotional-display; Robot-social-attitude,,"The study manipulated the robot's emotional display by having it express either sad or joyful facial expressions and gestures while telling a story about a mechanical fault. This is explicitly stated in the paper: 'We created four (2 × 2) scenarios with regard to the combination of two emotional representations and making or not ST.' The study also manipulated the robot's social attitude by having it either start the interaction with small talk or not. This is also explicitly stated in the paper: 'For instance, with different emotions, expressed either in facial expressions or body gestures, and by making Small Talk (ST) before starting the interaction.' The results showed that the combination of small talk and sad facial expressions led to higher trust levels, indicating that both 'Robot-emotional-display' and 'Robot-social-attitude' impacted trust. There were no factors that were manipulated that did not impact trust.",,http://link.springer.com/10.1007/978-3-030-30244-3_30,"Human beings live in a society with a complex system of social-emotional relations. Trust is one key concept in this system. It can help to reduce the social complexity, mainly in those cases where it is necessary to cooperate. Thus, the area of social robotics has been studying diﬀerent approaches to perform cooperative tasks between humans and robots. Here, we examine the inﬂuence of a set of factors (gender, emotional representation, making Small Talk and embodiment) that may aﬀect the trustworthiness of a robot. The results showed that these factors inﬂuence the level of trust that people put in robots. Speciﬁcally, a social robot with embodiment telling a sad story with sad facial expression and gestures has more inﬂuence on the trust level of a female subject."
"Hashemian, Mojgan; Paradeda, Raul; Guerra, Carla; Paiva, Ana",social,2019,2,40,40,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants signed a consent form, completed a pre-interaction trust questionnaire, interacted with the Nao robot in one of four conditions (ST SAD, NST SAD, ST JOY, NST JOY), made a fictional donation, and completed a post-interaction trust questionnaire.",Participants interacted with a robot that complained about a mechanical fault and asked for a fictional donation.,Nao,Humanoid Robots,Research; Social,Social,Persuasion,minimal interaction,Participants interacted with the robot in a controlled setting.,real-world,Participants interacted with a physical robot in a lab setting.,physical,The robot was physically present during the interaction.,pre-programmed (non-adaptive),The robot followed a pre-programmed script.,Questionnaires,Human-Robot Trust Scale/Questionnaire (HRTS/HRITS),,Trust was measured using a pre- and post-interaction questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's emotional expression (sad or joyful) and the presence of small talk were manipulated to influence trust.,"The study found that participants tended to trust more in the condition without small talk, regardless of the gestures. The influence of emotional representation was not clear.","The study found that participants had difficulty perceiving the robot's emotional state, and the influence of emotional representation was not clear. There was a significant difference in the donation amount among the four groups, and also regarding the participants' gender.","Participants tended to trust more in the condition without small talk, regardless of the gestures.",The robot tells a story about a mechanical fault and asks for a fictional donation. The human participant listens to the robot and decides on a donation amount.,Mann-Whitney U; Kruskal-Wallis,"The study used U Mann Whitney tests to compare pre-questionnaire scores between subgroups (ST, NST, SAD, JOY) to determine if participants had similar initial trust levels. U Mann Whitney tests were also used to compare post-questionnaire scores within each subgroup. Kruskal-Wallis tests were used to compare trust scores across the four experimental conditions (ST SAD, NST SAD, ST JOY, NST JOY) and to examine the influence of gender on trust within each condition. Additionally, Kruskal-Wallis tests were used to compare donation amounts across the four groups and between genders. Finally, a Kruskal-Wallis test was used to compare the trust levels across the eight groups formed by combining the two experiments.",TRUE,Robot-emotional-display; Robot-social-attitude,Robot-social-attitude,Robot-emotional-display,"Similar to the first study, the second study manipulated the robot's emotional display by having it express either sad or joyful facial expressions and gestures while telling a story about a mechanical fault. This is explicitly stated in the paper: 'We created four (2 × 2) scenarios with regard to the combination of two emotional representations and making or not ST.' The study also manipulated the robot's social attitude by having it either start the interaction with small talk or not. This is also explicitly stated in the paper: 'For instance, with different emotions, expressed either in facial expressions or body gestures, and by making Small Talk (ST) before starting the interaction.' However, in this study, the results showed that participants tended to trust more in the condition without small talk, regardless of the gestures. This indicates that 'Robot-social-attitude' impacted trust, while the influence of 'Robot-emotional-display' was not clear and did not significantly impact trust. The paper states: 'The preceding paragraph entails that JOY SAD under ST, forms the same distribution, as well as JOY SAD under NST condition. Hence, regarding the combined group, which forms a non-normal distribution (D(19) = 0.119, p = .20), a significant difference exists between the two groups (U (19) = 127.0, p = .048) and the higher mean in the NST (81.6 vs 80.8) shows that participants tend to trust more in the NST, regardless of the gestures.'",,http://link.springer.com/10.1007/978-3-030-30244-3_30,"Human beings live in a society with a complex system of social-emotional relations. Trust is one key concept in this system. It can help to reduce the social complexity, mainly in those cases where it is necessary to cooperate. Thus, the area of social robotics has been studying diﬀerent approaches to perform cooperative tasks between humans and robots. Here, we examine the inﬂuence of a set of factors (gender, emotional representation, making Small Talk and embodiment) that may aﬀect the trustworthiness of a robot. The results showed that these factors inﬂuence the level of trust that people put in robots. Speciﬁcally, a social robot with embodiment telling a sad story with sad facial expression and gestures has more inﬂuence on the trust level of a female subject."
"Hägglund, Susanne; Andtfolk, Malin; Rosenberg, Sara; Wingren, Mattias; Andersson, Sören; Nyholm, Linda",Do you wanna dance? Tales of trust and driving trust factors in robot medication counseling in the pharmacy context,2024,1,6,6,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants first completed consent forms, then role-played a scenario with a robot in a simulated pharmacy, and lastly, participated in a semi-structured interview.",Participants role-played a customer purchasing emergency contraceptive pills from a robot.,Furhat,Expressive Robots,Care; Social,Social,Counseling,minimal interaction,Participants interacted verbally with the robot in a simulated pharmacy setting.,real-world,Participants interacted with a physical robot in a simulated pharmacy environment.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed script based on user input.,Behavioral Measures; Questionnaires,,Video Data; Speech Data,Trust was assessed through qualitative interviews and observations of participant behavior.,no modeling,No computational model of trust was used in this study.,Empirical HRI Studies,Physical Robot Studies,No Manipulation,"The study did not manipulate any specific factors related to trust, but rather explored the factors that influence trust in the given scenario.","The study explored factors that influence trust, but did not manipulate them to measure a direct impact on trust outcomes.","The study found that participants' trust in the robot varied considerably, with some expressing low acceptance and others seeing great potential. The robot's gaze was found to both diminish and increase trust depending on how it was perceived.","The study identified several factors driving trust in robot medication counseling, including position, autonomy, boundaries, shame, gaze, and alignment, and presented five personas representing different levels of trust.","The robot provided medication counseling for emergency contraceptive pills based on user input, and the human participant role-played a customer seeking this information.",,"The study used qualitative data from interviews, which were analyzed using inductive, reflexive thematic analysis. No statistical tests were used.",FALSE,,,,"The study did not explicitly manipulate any factors. The study explored factors influencing trust in a robot providing medication counseling, but these factors were not intentionally varied by the researchers. The study used a single robot behavior and did not compare different robot behaviors or task conditions. Therefore, no factors were manipulated in the study. The study explored the influence of factors such as position, autonomy, boundaries, shame, gaze, and alignment on trust, but these were not manipulated as independent variables. The study focused on understanding the participants' perceptions and experiences with a single robot interaction scenario.",10.3389/frobt.2024.1332110,https://www.frontiersin.org/articles/10.3389/frobt.2024.1332110/full,"Introduction:               The sustainable implementation of socially assistive robots in a pharmacy setting requires that customers trust the robot. Our aim was to explore young adults’ anticipations of and motives for trusting robot medication counseling in a high-stakes scenario.                                         Methods:               Through a co-creation approach, we co-designed a prototype application for the Furhat platform together with young adults. In-lab testing of a pharmacy scenario, where the robot provides medication counseling related to emergency contraceptive pills, was conducted to deepen our understanding of some factors driving young adults’ initial trust establishment and anticipations of interacting with a robot in a high-stakes scenario. Qualitative data from interviews with six study participants were analyzed using inductive, reflexive thematic analysis and are presented through a narrative approach.                                         Results:               We outline five tales of trust characterized by personas. A continuum of different anticipations for consulting a robot in medication counseling is presented, ranging from low to high expectations of use. Driving factors in the initial trust establishment process are position, autonomy, boundaries, shame, gaze, and alignment.                                         Discussion:               The article adds to the understanding of the dimensions of the multifaceted trust concept, of driving trust factors, and of the subsequent anticipation to trust robots in a high-stakes pharmacy context."
"Henriksen, Jesper W.; Johansen, Anders S.; Rehm, Matthias",Pilot Study for Dynamic Trust Estimation in Human-Robot Collaboration,2020,1,40,40,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a water transfer task with a robot at two different speeds. HR and GSR were recorded, and participants completed a survey after each task.",Participants collaborated with a robot to transfer water.,Sawyer,Industrial Robot Arms; Collaborative Robots (Cobots),Research,Manipulation,Object Passing,direct-contact interaction,Participants physically interacted with the robot during the water transfer task.,real-world,The study involved a real-world interaction with a physical robot.,physical,The robot was a physical robot present in the real-world interaction.,shared control (fixed rules),"The robot's speed was pre-set, and it followed a fixed sequence of actions.",Behavioral Measures; Physiological Measures; Questionnaires,Schaefer's Trust Questionnaire/Scale,Physiological Signals; Video Data,"Trust was assessed using questionnaires, physiological data, and behavioral observations.",no modeling,The study did not include any computational modeling of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The speed of the robot was manipulated to influence the perceived risk and performance of the task.,Trust was significantly lower when the robot moved at a faster speed.,Participants leaned towards the robot more during the slow task and away from the robot during the fast task.,The speed of the robot significantly impacted trust levels in the water transfer task.,"The robot transferred a cup of water to the participant, and the participant received the cup from the robot. The robot's speed was varied between 70% and 100%.",ANOVA,"The study used ANOVA to analyze the effect of robot speed on trust measurements in the water task. Additionally, ANOVA was used to compare the baseline trust measurements between the water and Lego tasks, controlling for speed.",TRUE,Robot-task-strategy,Robot-task-strategy,,"The study manipulated the speed of the robot during the water transfer task, which directly relates to the robot's task strategy. The paper states, 'the task was executed at 70% and 100% speed.' This manipulation of speed was found to significantly impact trust levels, as stated in the results: 'The statistical analysis (ANOVA) so far shows a significant difference of speed on trust measurements in the water task.' Therefore, 'Robot-task-strategy' is the appropriate category for both the manipulated factor and the factor that impacted trust. There were no other factors manipulated that were found to impact trust.",10.1145/3371382.3378327,https://dl.acm.org/doi/10.1145/3371382.3378327,"This paper reports on a pilot study for investigating the relation between observable data from users and their trust estimation in Human Robot Collaboration. Two experiments have been set up that contain different situational risks (as one of the pre-requisites for trust investigation). Here we report on one of these. Preliminary results are promising and show a correlation between risk factors, observable behavior and subjective trust estimations."
"Hergeth, Sebastian; Lorenz, Lutz; Vilimek, Roman; Krems, Josef F.",Keep Your Scanners Peeled: Gaze Behavior as a Measure of Automation Trust During Highly Automated Driving,2016,1,51,35,16,"11 participants were excluded due to simulation errors during the experiment, 5 participants were excluded because eye-tracking data could not be recorded completely or with sufficient quality",Controlled Lab Environment,within-subjects,"Participants were familiarized with the driving simulator, the automated driving system, a non-driving related task (NDRT), and take-over requests (TORs). They then completed eight NDRT sessions, with two TORs occurring during the second and fourth NDRT. Self-reported trust was collected at the beginning of each NDRT, and eye-tracking data was recorded throughout the experiment.",Participants were asked to attend to a visually demanding non-driving-related task (NDRT) on a tablet while the car was in highly automated driving mode. They were also required to take over manual control when a TOR was issued.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the automated driving system through a driving simulator and were instructed to attend to a non-driving related task.,simulation,The study used a driving simulator to create an immersive environment for the participants.,simulated,The automated driving system was simulated within the driving simulator.,fully autonomous (limited adaptation),"The automated driving system controlled the vehicle autonomously, but with limited adaptation to unexpected scenarios.",Behavioral Measures; Questionnaires,,Eye-tracking Data,Trust was assessed using self-reported questionnaires and behavioral measures (monitoring frequency) derived from eye-tracking data.,"parametric models (e.g., regression)",The study used correlations to analyze the relationship between self-reported trust and monitoring frequency.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,"The study did not directly manipulate trust, but the experimental design influenced trust through the task difficulty and the instructions given to participants, which influenced their expectations of the system.","The study found a negative relationship between self-reported trust and monitoring frequency, with higher trust associated with lower monitoring. Trust increased over the experimental session, and monitoring frequency decreased.","The study found a consistent negative relationship between self-reported trust and monitoring frequency across dispositional, situational, and learned trust. However, the relationship was not perfect, and other factors may have influenced monitoring behavior.","The study's main finding is that there is a negative relationship between self-reported automation trust and monitoring frequency during highly automated driving, suggesting that eye-tracking metrics could be used to infer trust.",The automated driving system controlled the vehicle's lateral and longitudinal movement. The human participant was instructed to attend to a non-driving related task on a tablet and to take over control when prompted by a take-over request.,Pearson correlation; t-test; Wilcoxon signed-rank test,"The study used correlation analysis to examine the relationship between dispositional, situational, and learned automation trust and monitoring frequency. Specifically, Spearman's rank correlation was used for dispositional trust, and Pearson correlations were used for situational trust. A t-test was used to compare self-reported automation trust between the first and last NDRT, and a Wilcoxon signed-rank test was used to compare monitoring frequency between the first and last NDRT. The false discovery rate was controlled for using the Benjamini and Hochberg method for multiple comparisons.",FALSE,Task-complexity; Robot-autonomy,Task-complexity; Robot-autonomy,,"The study did not explicitly manipulate trust, but the experimental design influenced trust through the task difficulty and the instructions given to participants, which influenced their expectations of the system. The task complexity was influenced by the NDRT, which was designed to be visually demanding, and the take-over requests (TORs), which introduced a change in task demands. The robot autonomy was also a factor, as the system was designed to be highly automated, but with limited adaptation to unexpected scenarios, which influenced the participants' trust in the system. The study found that trust increased over the experimental session, and monitoring frequency decreased, suggesting that both task complexity and robot autonomy influenced trust. The study did not manipulate any other factors from the list provided.",10.1177/0018720815625744,https://doi.org/10.1177/0018720815625744,"Objective:The feasibility of measuring drivers? automation trust via gaze behavior during highly automated driving was assessed with eye tracking and validated with self-reported automation trust in a driving simulator study.Background:Earlier research from other domains indicates that drivers? automation trust might be inferred from gaze behavior, such as monitoring frequency.Method:The gaze behavior and self-reported automation trust of 35 participants attending to a visually demanding non-driving-related task (NDRT) during highly automated driving was evaluated. The relationship between dispositional, situational, and learned automation trust with gaze behavior was compared.Results:Overall, there was a consistent relationship between drivers? automation trust and gaze behavior. Participants reporting higher automation trust tended to monitor the automation less frequently. Further analyses revealed that higher automation trust was associated with lower monitoring frequency of the automation during NDRTs, and an increase in trust over the experimental session was connected with a decrease in monitoring frequency.Conclusion:We suggest that (a) the current results indicate a negative relationship between drivers? self-reported automation trust and monitoring frequency, (b) gaze behavior provides a more direct measure of automation trust than other behavioral measures, and (c) with further refinement, drivers? automation trust during highly automated driving might be inferred from gaze behavior.Application:Potential applications of this research include the estimation of drivers? automation trust and reliance during highly automated driving."
"Hester, Michelle; Lee, Kevin; Dyre, Brian P.",“Driver Take Over”: A Preliminary Exploration of Driver Trust and Performance in Autonomous Vehicles,2017,1,24,24,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were randomly assigned to one of four alert conditions (no alert, sound alert, task-irrelevant voice alert, task-relevant voice alert). They completed three driving simulation trials. In the third trial, the automation failed, requiring the participant to take control. Data was collected on collision avoidance, reaction time, eyes down time, and control time.",Participants monitored a simulated automated vehicle and were required to take over manual control when the automation failed.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a simulated automated vehicle, with limited direct interaction.",simulation,The interaction took place in a driving simulation environment.,simulated,The robot was a simulated automated vehicle.,shared control (fixed rules),The automated vehicle operated autonomously but required human intervention at a specific point.,Behavioral Measures; Questionnaires,,Video Data; Performance Metrics,"Trust was assessed using behavioral measures (eyes down time, control time) and a subjective trust questionnaire.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The type of in-car alert (no alert, sound alert, task-irrelevant voice, task-relevant voice) was manipulated to influence driver awareness and ability to take over control.","There were no statistically significant differences in trust between the conditions based on the behavioral measures. However, more participants avoided collision in the task-relevant voice condition, suggesting that the type of alert can impact performance.","Although there were no statistically significant differences in trust based on the behavioral measures, there was a practical difference in collision avoidance, with the task-relevant voice condition showing the best performance. This suggests that the type of alert can impact performance even if it does not significantly affect trust as measured by the behavioral measures.","The main finding was that task-relevant voice alerts led to better collision avoidance compared to other alert types, suggesting that the content of the alert is important for driver performance during automation failures.","The automated vehicle drove along a straight road, following a lead car. The human participant monitored the automated vehicle and was required to take over control when the automation failed. The human could also use a mobile device during the simulation.",ANOVA,"Two between-subjects ANOVAs were conducted to analyze if there was a significant difference in trust between the four in-vehicle agent alert types. The dependent variables were performance (whether the participant could avoid collision) and trust (Control Time and Eyes Down Time). However, the study notes that due to the small sample size, the statistical power was limited, and no significant differences were found.",TRUE,Robot-verbal-communication-content,,Robot-verbal-communication-content,"The study manipulated the content of the verbal communication from the automated vehicle. Participants were assigned to one of four conditions: no alert, sound alert, task-irrelevant voice alert, and task-relevant voice alert. The task-irrelevant voice condition included statements unrelated to the driving task, while the task-relevant voice condition included statements related to the driving task. This manipulation directly changes the content of the robot's verbal communication, making 'Robot-verbal-communication-content' the most appropriate category. The study found no statistically significant differences in trust based on the behavioral measures (eyes down time and control time) between the conditions, indicating that the manipulation of verbal communication content did not impact trust as measured by these metrics. However, the task-relevant voice condition did lead to better collision avoidance, suggesting that the content of the alert impacted performance, but not trust as measured by the study. Therefore, 'Robot-verbal-communication-content' is included in factors_that_did_not_impact_trust, as the study explicitly states that there were no statistically significant differences in trust between the conditions based on the behavioral measures.",10.1177/1541931213601971,http://journals.sagepub.com/doi/10.1177/1541931213601971,"Automated vehicles are becoming more prominent in research and development. These automated vehicles introduce issues that have been seen in other autonomous systems such as decreases in situation awareness, complacency, and trust. Previous literature has looked at the effects of alerts and voice agents on driving performance. This preliminary study compares different in-car alerts (no alert, sound alert, task irrelevant voice alert, and task relevant voice alert) on trust and the driver’s ability to get back in-the-loop when the automation has failed. Participants were asked to monitor a simulated automated vehicle as it drove down a straight two-lane road. The main statistical results of our study show no difference in trust between the four different conditions; however, more participants avoided collision with a leading car in the task relevant voice condition in comparison to the three other conditions. These preliminary findings have important implications for the design of automated vehicles."
"Highland, Patrick; Schnell, Thomas; Woodruff, Katharine; Avdic-McIntire, Gianna",Towards Human Objective Real-Time Trust of Autonomy Measures for Combat Aviation,2023,1,9,9,0,No participants were excluded,Real-World Environment,within-subjects,"Participants flew nine live-flight scenarios in an L-29 aircraft, performing a battle manager task while monitoring an autonomous agent controlling their own-ship in a dogfight. The scenarios varied in autonomy performance (win, stalemate, loss), and data was collected using eye tracking, real-time mental workload measures, and subjective questionnaires.",Participants performed a battle manager task while simultaneously monitoring an autonomous agent controlling their own-ship in an aerial dogfight.,Unspecified,Unmanned Aerial Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants monitored the autonomous agent through a display and could disengage the autonomy.,real-world,"Participants were in a real aircraft, experiencing G-forces and visual cues.",physical,The robot was a physical aircraft controlled by a 'Wizard of Oz' method.,wizard of oz (directly controlled),The autonomous agent was simulated by a safety pilot in the front cockpit.,Behavioral Measures; Performance-Based Measures; Physiological Measures; Real-time Trust Measures; Questionnaires,,Eye-tracking Data; Physiological Signals; Performance Metrics,"Trust was assessed using a combination of behavioral, performance, physiological, and subjective measures.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The performance of the autonomous agent was manipulated to be either a win, stalemate, or loss, which influenced the perceived risk and trust of the participants.","Trust decreased when the autonomy performed poorly and increased when it performed well, with a notable hysteresis effect in some measures.","The study found a bimodal pattern in trust indicators, with trust decreasing after a rare event (own-ship destruction) and then rebuilding. There was also a hysteresis effect in the cross-check ratio of the BM task, where trust did not return to pre-rare-event levels. Some pilots overrode the autonomy even when it was not necessary, indicating that factors other than pure performance influence trust.","Objective real-time measures of trust, such as autonomy disengagement, cross-check ratio, and mental workload, can be used to assess trust in a high-risk, dynamic environment.","The robot (autonomous own-ship) engaged in a simulated dogfight, while the human participant managed a separate battle and monitored the robot's performance. The human could disengage the autonomy if they perceived a risk to the own-ship.",ANOVA; t-test; Kruskal-Wallis; Mann-Whitney U; Chi-squared,"The study used a mixed-methods within-expert evaluation pilot (nonrandomized) balanced analysis of variance (ANOVA) to analyze parametric objective data, which was verified to be of uniform normal distributions. Comparisons of specific scenarios were then made using a Student's t-test. Non-parametric subjective data was verified to be homogeneous by Kruskal-Wallis tests, and test statistic comparisons were produced by Wilcoxon tests. A Chi-square test was used to determine if autonomy disengagement was dependent on the scenario. These tests were used to assess the impact of autonomy performance (win, stalemate, loss) on various dependent variables such as autonomy disengagement, cross-check ratio, performance in the BM task, MWL, and situational risk.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the performance of the autonomous agent (the 'robot') in the dogfight scenarios. The agent's performance was varied to be either a win, stalemate, or loss. This directly influenced the success rate of the robot in the task, which is why 'Robot-accuracy' is the most appropriate category. The paper explicitly states that the performance of the AI (win, stalemate, loss) was manipulated to influence the perceived risk and trust of the participants. The results show that trust decreased when the autonomy performed poorly and increased when it performed well, indicating that 'Robot-accuracy' impacted trust. There were no other factors manipulated in the study that were found to impact trust.",10.1080/24721840.2022.2127724,https://www.tandfonline.com/doi/full/10.1080/24721840.2022.2127724,"Objective: Can objective trust indicators be measured and computed in real time in a within visual range aerial combat “dogfight” scenario consisting of a manned autonomous own-ship versus an adversary? Background: Previous research has focused on human trust of auto­ mation during simulated ground combat and human trust of civil aviation automation. Those studies largely consisted of subjective measures of trust analyzed post-hoc. Human trust of autonomy in high-risk situations such as dogfighting had not yet been studied. Method: Nine evaluation pilots participated in an experiment that consisted of nine operationally relevant live-flight vignettes during which they conducted a mission manager task while simultaneously monitoring an autonomous agent controlling their own-ship in an aerial dogfight. Eye tracking, objective real-time mental workload, and mission manager task performance were all recorded. Results: Potential indicators of trust were objectively measured in realtime as a function of autonomy disengagement, human cross-check ratio of autonomy, and real-time objective mental workload, as vali­ dated by traditional subjective trust measures. Measured trust still needs to be categorized as appropriate trust, overtrust, or undertrust in future work. Conclusion: This first-ever real-time measure of objective indicators of trust in operationally relevant live-flight paves the way for determining appropriate human trust of autonomy in high-risk situations such as dogfighting. These measures could have utility in high-risk mannedunmanned teaming applications such as working with robots and automated trading."
"Hjälmdahl, Magnus; Krupenia, Stas; Thorslund, Birgitta",Driver behaviour and driver experience of partial and fully automated truck platooning – a simulator study,2017,1,24,24,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants drove a truck simulator under three conditions: baseline (cruise control), partial automation (longitudinal control), and full automation (longitudinal and lateral control), each in three traffic situations (light, heavy, heavy with fog). Questionnaires were completed after each condition.",Participants drove a truck simulator while experiencing different levels of automation and traffic conditions.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the automated driving system in a simulator.,simulation,The study used a driving simulator to create an immersive driving experience.,simulated,The truck was simulated in a driving simulator.,shared control (fixed rules),The automation system followed pre-set rules for longitudinal and lateral control.,Questionnaires; Custom Scales,Trust in Automated Systems Scale,,Trust was measured using the Human Trust in Automated Systems Scale and Cooper-Harper Scales.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The level of automation was directly manipulated, with conditions ranging from cruise control to partial and full automation, which influenced the task difficulty.",Trust was highest in the baseline condition (cruise control) and did not differ between partial and full automation.,"The study found that partial automation led to higher workload than full automation or baseline, and that drivers rated their situation awareness higher during automated driving, despite evidence suggesting otherwise. There was a conflict between the subjective ratings of workload and the objective measures, with the subjective measures indicating higher workload for partial automation, but the objective measure (PDT) showing no significant difference.","Trust was highest in the baseline condition (cruise control) and did not differ between partial and full automation, while workload was higher for partial automation than for full automation or the baseline condition.","The human participant drove a truck in a simulator, using cruise control, partial automation (longitudinal control), or full automation (longitudinal and lateral control). The robot (simulated truck) followed a lead truck at a set distance, with varying levels of autonomy.",ANOVA; Generalized linear models,"The study used one-way repeated-measures ANOVAs to identify the effects of automation level on workload measures (DALI, ASWAT, Cooper-Harper scales) and trust measures (Global Trust, Trust subscale, Distrust subscale), and acceptance measures (AATT, QUIS). Post-hoc tests were conducted with Tukey's or Bonferroni corrections. General estimating equations (GEEs) were used to model correlated data for driving performance measures and sleepiness, with condition and situation as within-participant predictor variables and order as a covariate. Wald statistics and regression coefficients were reported.",TRUE,Robot-autonomy; Task-complexity; Task-environment,Robot-autonomy,,"The study manipulated 'Robot-autonomy' by having three conditions: baseline (cruise control), partial automation (longitudinal control), and full automation (longitudinal and lateral control). This is explicitly stated in the 'Method' section: 'In the fully automated mode, the drivers could, once the platoon had been properly coupled, relinquish both longitudinal and lateral control of the vehicle... In the partially automated mode, however, the drivers still had to steer the vehicle while the longitudinal control was automated.' The 'Task-complexity' was manipulated by varying the level of automation, which influenced the driver's workload, as stated in the 'Results' section: 'For all workload measures, partial automation produced higher workload than did the full-automation or baseline condition.' The 'Task-environment' was manipulated by having three traffic situations: light, heavy, and heavy with fog, as described in the 'Method' section: 'Each condition was simulated in three situations: light traffic, heavy traffic, and heavy traffic plus fog.' The results show that 'Robot-autonomy' impacted trust, as stated in the 'Results' section: '...trust was highest under the baseline condition, with little difference between partial and full automation.' The study did not find any factors that did not impact trust, as all manipulated factors had some effect on the measured outcomes.",10.1007/s12544-017-0222-3,http://link.springer.com/10.1007/s12544-017-0222-3,"Introduction This paper builds our knowledge of truck driver behaviour in and experience of automated truck platooning, focusing on the effect of partially and fully automated truck platoons on driver workload, trust, acceptance, performance, and sleepiness."
"Ho, Nhut T.; Sadler, Garrett G.; Hoffmann, Lauren C.; Lyons, Joseph B.; Johnson, Walter W.",Trust of a Military Automated System in an Operational Context,2017,1,36,36,0,No participants were excluded,Survey/Interview,,"Semi-structured interviews were conducted with 15 test pilots, 12 engineers, and 9 managers involved with the Auto-GCAS system. Participants were asked open-ended questions about the system's development, their involvement, opinions, and trust in the system. Interviews were audio-recorded and transcribed.",Participants were asked open-ended questions about their experiences with the Auto-GCAS system.,Unspecified,Other,Research,Evaluation,Survey/Questionnaire Completion,passive observation,"Participants were interviewed about their experiences with the system, but did not directly interact with it during the study.",media,The study involved interviews and did not include any visual or interactive elements.,hypothetical,"The robot was not physically present, and the study focused on participants' perceptions and experiences with the system.",not autonomous,The robot's actions were not directly observed or controlled during the study; the focus was on the participants' perceptions of the system.,Questionnaires,,Speech Data,"Trust was assessed through open-ended interview questions, and speech data was collected for analysis.",no modeling,The study did not use computational models to analyze trust.,Observational & Survey Studies,Qualitative Interviews,No Manipulation,The study did not manipulate any specific factors; it focused on gathering qualitative data about trust in the Auto-GCAS system.,"The study did not manipulate trust, but rather explored factors influencing trust through interviews.","The study found that trust in the Auto-GCAS system was influenced by factors such as personal history, developmental history, reputation, and nuisance avoidance. The study also highlighted the importance of human-to-human trust among stakeholders in the development of trust in the automated system.","The study found that trust in the Auto-GCAS system was influenced by a combination of technical, cultural, and organizational factors, and that human-to-human trust among stakeholders was a key factor in the development of trust in the automated system.","The robot is an automated system designed to prevent controlled flight into terrain. The human participants were interviewed about their experiences with the system, including their perceptions of its reliability and trustworthiness.",,The study employed a grounded theory approach to analyze qualitative data from semi-structured interviews. No statistical tests were used. The analysis focused on identifying themes and patterns in the interview data related to trust in the Auto-GCAS system.,FALSE,,,,"The study did not manipulate any factors. The study was observational and used semi-structured interviews to gather qualitative data about trust in the Auto-GCAS system. The researchers did not intentionally change any variables to observe their effect on trust. The study focused on understanding the factors that influence trust in the system through the experiences and perspectives of the participants. Therefore, no factors were manipulated, and no factors were found to impact or not impact trust through manipulation.",10.1037/mil0000189,https://www.tandfonline.com/doi/full/10.1037/mil0000189,"This article advances our understanding of how humans develop trust in highly automatic systems through a case study of U.S. Air Force F-16 pilots, engineers, and managers of the Automatic Ground Collision Avoidance system. The study also helps substantiate and validate theoretical trust models, and provides recommendations for future trust research with ﬁeld work."
"Hoesterey, Steffen; Onnasch, Linda",The effect of risk on trust attitude and trust behavior in interaction with information and decision automation,2023,1,82,65,17,"5 participants were excluded due to technical errors of the headmounted display (HMD), 7 participants were excluded because they were not able to proficiently conduct the task, 1 participant was excluded prior to the VR exposure because they met the criterion for acrophobia, 1 subject decided to stop the experiment after being exposed to the high altitude for the first time, 3 participants were excluded after an outlier analysis of their information sampling behavior",Controlled Lab Environment,mixed design,"Participants were introduced to a cover story about being an operator on a transmitter facility. They completed a tutorial and training trials in VR. They then completed four blocks of trials in either low or high risk conditions, alternating between the two. After the first and last block in each risk condition, participants answered questionnaires.","Participants diagnosed atmospheric conditions, decided on the correct protective alloy for a ramp, and reset an information cloud in a virtual environment. They also completed a secondary auditory reaction time task.",Unspecified,Other,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a virtual environment and automated aids, but there was no physical interaction with a robot.",simulation,Participants were immersed in a virtual reality environment using a head-mounted display.,simulated,The automated aids were presented as virtual systems within the VR environment.,pre-programmed (non-adaptive),The automated aids provided pre-programmed support without adapting to the user's behavior.,Behavioral Measures; Questionnaires; Custom Scales,Jian et al. Trust Scale; Trust in Automated Systems Scale,Performance Metrics,"Trust was assessed using questionnaires, information sampling behavior, and decision time.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the level of automation (information vs. decision automation) and the level of risk (low vs. high altitude) in a virtual environment. The automation type influenced the level of support provided, and the risk level influenced the potential consequences of errors.","Trust attitude increased over time, but was initially lower for the decision automation group. Trust behavior was higher for the decision automation group, especially in the low-risk condition. High risk attenuated the effect of automation on trust behavior.","The study found that trust attitude was initially lower for the decision automation group, but increased to the same level as the information automation group by the end of the experiment. The effect of automation on trust behavior was attenuated under high risk, suggesting that risk can buffer negative effects of higher degrees of automation. There was a habituation effect as risk perception decreased from the first to last block in both conditions.","The study's main finding is that situational risk attenuates the negative effects of higher degrees of automation on trust behavior, suggesting that previous studies may have overestimated the detrimental effects of automation in real-world scenarios.",The human participant diagnosed atmospheric conditions and applied protective alloys to a ramp in a virtual environment. The automated aid provided either information or decision support. The participant also completed a secondary auditory reaction time task.,ANOVA; t-test,"The study used a mixed design ANOVA with repeated measures and a between-subject factor to analyze task performance and trust behavior measures. Independent sample t-tests were used to analyze group differences in trust behavior within each risk condition, with a Bonferroni correction applied. Additionally, mixed ANOVAs were used to analyze the Scale of Trust in Automated Systems and the risk perception questionnaire. Temporal changes in trust attitude and risk perception were analyzed using mixed ANOVAs with a time factor.",TRUE,Robot-autonomy; Task-environment,Robot-autonomy; Task-environment,,"The study manipulated the level of automation support, which is categorized as 'Robot-autonomy'. Participants were either aided by information automation (IA), which provided information about the atmospheric conditions, or decision automation (DA), which provided a diagnosis and suggested the correct alloy. This manipulation directly affected the level of decision-making authority given to the automation. The study also manipulated the level of risk by changing the altitude in the virtual environment, which is categorized as 'Task-environment'. The risk was manipulated by setting the task at either a low altitude (0.5m) or a high altitude (70m), with the possibility of virtually falling in case of a mistake. Both 'Robot-autonomy' and 'Task-environment' were found to impact trust. The level of automation (IA vs DA) impacted trust behavior, with DA leading to less verification behavior in low-risk conditions. The level of risk also impacted trust behavior, with high risk attenuating the effect of automation on trust behavior. The paper states, 'Results revealed that trust attitude toward the automation was not affected by risk.' This indicates that while risk was manipulated, it did not impact trust attitude, but it did impact trust behavior. The paper also states, 'While trust attitude was initially lower for the decision automation, it was equally high in both groups at the end of the experiment after experiencing reliable support.' This indicates that the level of automation impacted trust attitude initially, but this effect diminished over time. However, the level of automation did impact trust behavior throughout the experiment. Therefore, both 'Robot-autonomy' and 'Task-environment' are listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1007/s10111-022-00718-y,https://link.springer.com/10.1007/s10111-022-00718-y,"Situational risk has been postulated to be one of the most important contextual factors affecting operator’s trust in automation. However, experimentally, it has received only little attention and was directly manipulated even less. To close this gap, this study used a virtual reality multi-task environment where the main task entailed making a diagnosis by assessing different parameters. Risk was manipulated via the altitude, the task was set in including the possibility of virtually falling in case of a mistake. Participants were aided either by information or decision automation. Results revealed that trust attitude toward the automation was not affected by risk. While trust attitude was initially lower for the decision automation, it was equally high in both groups at the end of the experiment after experiencing reliable support. Trust behavior was significantly higher and increased during the experiment for the decision automation supported group in the form of less automation verification behavior. However, this detrimental effect was distinctly attenuated under high risk. This implies that negative consequences of decision automation in the real world might have been overestimated by studies not incorporating risk."
"Hoffman, Guy",Evaluating Fluency in Human–Robot Collaboration,2019,1,143,104,39,"39 participants were excluded due to English proficiency, approval rate, filtering questions, and task time",Online Crowdsourcing,within-subjects,"Participants viewed five short video clips of a human-robot collaboration simulation, each followed by an eight-item fluency scale, and then completed a demographic questionnaire.",Participants watched videos of a simulated human-robot object transfer task and rated the fluency of the interaction.,Unspecified,Mobile Manipulators,Research,Manipulation,Object Passing,minimal interaction,Participants observed simulated interactions without direct physical contact.,simulation,Participants viewed a simulated human-robot interaction.,simulated,The robot was represented in a simulation.,pre-programmed (non-adaptive),The robot's actions were pre-programmed without adaptation to the user.,Questionnaires,,,Trust was assessed using a composite questionnaire.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the robot's processing delays and action times to vary the objective fluency metrics, which indirectly influenced the perceived fluency.",The study found that higher human idle time (H-IDLE) and lower functional delay (F-DEL) were correlated with higher subjective fluency ratings.,"The study found a positive correlation between human idle time and perceived fluency, which was unexpected. The robot's personality traits were not strongly correlated with the objective fluency measures.",Functional delay (F-DEL) was found to have the strongest correlation with subjective fluency perception.,"The robot and human transferred objects from one side of a shared workspace to the other, with the human placing objects on a middle table and the robot picking them up and placing them on its side.",Pearson correlation,"Pearson correlations were calculated to investigate the relationship between the objective fluency metrics (R-IDLE, H-IDLE, C-ACT, F-DEL) and the subjective fluency ratings, both as a composite score and for each individual indicator. The significance of the correlations was evaluated using a two-tailed test with a p < 0.05 threshold.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the robot's processing delays and action times, which directly influenced the objective fluency metrics (H-IDLE, F-DEL). These metrics, in turn, impacted the perceived fluency, which is related to trust. Specifically, the paper states, 'Specifically, we can vary the time it takes each agent to pick up the objects, to detect the existence of an object in each workspace location, and to drop off the objects at the various workspaces.' This manipulation of timing directly affects the robot's performance and thus falls under 'Robot-accuracy'. The study found that higher human idle time (H-IDLE) and lower functional delay (F-DEL) were correlated with higher subjective fluency ratings, indicating that the manipulation of robot timing and delays (accuracy) impacted the perceived fluency, which is related to trust. The study did not find any factors that did not impact trust.",10.1109/THMS.2019.2904558,https://ieeexplore.ieee.org/document/8678448/,"Collaborative ﬂuency is the coordinated meshing of joint activities between members of a well-synchronized team. In recent years, researchers in human–robot collaboration have been developing robots to work alongside humans aiming not only at task efﬁciency, but also at human–robot ﬂuency. As part of this effort, we have developed a number of metrics to evaluate the level of ﬂuency in human–robot shared-location teamwork. While these metrics are being used in existing research, there has been no systematic discussion on how to measure ﬂuency and how the commonly used metrics perform and compare. In this paper, we codify subjective and objective human–robot ﬂuency metrics, provide an analytical model for four objective metrics, and assess their dynamics in a turn-taking framework. We also report on a user study linking objective and subjective ﬂuency metrics and survey recent use of these metrics in the literature."
"Hoffmann, Laura; Derksen, Melanie; Kopp, Stefan","What a Pity, Pepper!: How Warmth in Robots' Language Impacts Reactions to Errors during a Collaborative Task",2020,1,81,81,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were briefed, signed consent, and then interacted with a robot for a shopping task. The robot used either machine-like or human-like language and either made an error or not. Participants then completed a survey.",Participants completed a shopping task with the assistance of a robot.,Pepper,Humanoid Robots; Expressive Robots,Social; Research,Social,Conversation,minimal interaction,Participants interacted with the robot verbally during a shopping task.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical humanoid robot.,pre-programmed (non-adaptive),The robot followed a pre-defined script and did not adapt to the user.,Questionnaires,,,Trust was measured using questionnaires with subscales for cognition-based and affect-based trust.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's language was manipulated to be either machine-like or human-like, and the robot either made an error or not, influencing perceived warmth and competence.","Human-like language mitigated the negative impact of errors on trust, while machine-like language led to lower trust after errors.","The study found that human-like language compensated for errors, which is a notable trend. The study did not find higher sympathy for an erroneous robot as reported previously.","Human-like language can compensate for negative consequences of robot errors on liking, trust, and acceptance.","The robot guided the participant through a shopping task, requesting items using either machine-like or human-like language. The participant's role was to identify and place the requested items in the robot's basket. In the error condition, the robot dropped the basket and asked to repeat the task.",ANOVA; pearson's χ 2,"Two-factorial ANOVAs were used to analyze the effects of manipulated competence (error-free/erroneous) and manipulated language warmth (machine-like/human-like) on dependent variables such as perceived warmth, perceived competence, pity, likability, cognition-based trust, affect-based trust, and intention to use the robot again. Pearson's chi-squared test was used to analyze the relationship between the experimental conditions and participants' compliance to repeat the task after an error.",TRUE,Robot-verbal-communication-style; Robot-accuracy,Robot-verbal-communication-style; Robot-accuracy,,"The study manipulated two factors: the robot's language style (machine-like vs. human-like) and the robot's accuracy (error-free vs. erroneous). The language style was manipulated by changing the tone and formality of the robot's verbal communication, which is best categorized as 'Robot-verbal-communication-style'. The robot's accuracy was manipulated by having the robot either complete the task without errors or drop the basket, which directly impacts task performance and is categorized as 'Robot-accuracy'. Both of these manipulations were found to impact trust. Specifically, human-like language mitigated the negative impact of errors on trust, while machine-like language led to lower trust after errors. The paper states, '...error-free behavior was favored over erroneous if the dialogue is machine-like, while errors do not negatively impact liking, trust and acceptance if the robot uses human-like language.' This clearly indicates that both manipulated factors influenced trust.",10.1145/3371382.3378242,https://dl.acm.org/doi/10.1145/3371382.3378242,"We investigate the impact of warmth in robots’ language on the perception of errors in a shopping assistance task (N=81) and found that error-free behavior was favored over erroneous if the dialogue is machine-like, while errors do not negatively impact liking, trust and acceptance if the robot uses human-like language. Warmth in robots’ language thus seems to mitigate negative consequences and should be considered as a crucial design aspect."
"Holland, Christopher; Perry, Grace; Neyedli, Heather F.","Calibrating Trust, Reliance and Dependence in Variable-Reliability Automation",2024,1,139,80,59,59 participants were excluded because their overall performance for initial responses was less than 56.7%,Online Crowdsourcing,mixed design,"Participants completed a color identification task with an automated aid. The reliability of the aid either increased from 50% to 100% or decreased from 100% to 50% over six blocks. Trust and self-confidence were measured via questionnaires, and dependence was measured by the proportion of times participants changed their initial response to agree with the automation's recommendation.","Participants performed a color identification task, determining if images were predominantly blue or orange, with an automated system providing recommendations.",Unspecified,Other,Research,Evaluation,Image Analysis,minimal interaction,Participants interacted with the automated system through a computer interface.,simulation,The interaction was conducted through a simulated task environment.,simulated,The automated system was presented as a simulated aid.,pre-programmed (non-adaptive),The automated system provided recommendations based on a pre-programmed reliability level.,Questionnaires,,Performance Metrics,Trust was measured using questionnaires and performance metrics were collected.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the automated system was directly manipulated, either increasing or decreasing over the course of the experiment, which influenced the system's performance and the task difficulty.",Trust remained stable despite changes in system reliability.,"There was no change in trust with changing reliability level in either group, which is contrary to previous findings. Dependence and final response performance tracked automation reliability level, with dependence and performance increasing over the experiment in the group whose automation increased in reliability and decreasing in the group whose automation decreased in reliability.","Dependence on the automated system changed independently of trust, tracking the system's reliability, while trust remained stable.","The automated system provided a recommendation on whether the image was more blue or orange. The human participant first made an initial choice, then saw the automated system's recommendation, and then made a final choice.",ANOVA; post hoc simple slopes analysis,"A 2 Group (increasing reliability vs. decreasing reliability) × 6 Block mixed ANOVA was used to analyze participants' trust, self-confidence, dependence behavior, and performance (initial response and final response). A Bonferroni correction was applied to adjust for multiple comparisons. Greenhouse-Geisser estimates were used when sphericity was violated. A post hoc simple slopes analysis was conducted to examine the relationship between the two variables under both conditions for dependence.",TRUE,Robot-accuracy,,Robot-accuracy,"The study explicitly manipulated the reliability of the automated system, which directly impacts its accuracy in providing correct recommendations. This is described in the 'Design and Procedures' section where the reliability of the automated system either increased from 50% to 100% or decreased from 100% to 50% over six blocks. This manipulation directly affects the system's performance on the task, making 'Robot-accuracy' the most appropriate category. The results section indicates that there was no significant change in trust with the changing reliability level, therefore, 'Robot-accuracy' is listed as a factor that did not impact trust.",10.1177/10711813241277531,https://journals.sagepub.com/doi/10.1177/10711813241277531,"Trust and system reliability can influence a user’s dependence on automated systems. This study aimed to investigate how increases and decreases in automation reliability affect users’ trust in these systems and how these changes in trust are associated with users’ dependence on the system. Participants completed a color identification task with the help of an automated aid, where the reliability of this aid either increased from 50% to 100% or decreased from 100% to 50% as the task progressed, depending on which group the participants were assigned to. Participants’ trust, self-confidence, and dependence on the system were measured throughout the experiment. There were no differences in trust between the two groups throughout the experiment; however, participants’ dependence behavior did follow system reliability. These findings highlight that trust is not always correlated with system reliability, and that although trust can often influence dependence, it does not always determine it."
"Holländer, Kai; Wintersberger, Philipp; Butz, Andreas",Overtrust in External Cues of Automated Vehicles: An Experimental Investigation,2019,1,18,18,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were placed in a VR environment and instructed to cross a road when they felt safe. They were exposed to an automated vehicle with an external display that either correctly or incorrectly indicated the vehicle's intentions. Participants completed 12 trials and rated their perceived safety, trust in the display, and confidence in the vehicle after each trial.","Participants were tasked with crossing a virtual road in front of an automated vehicle, making decisions based on the information displayed on the vehicle's external display.",Unspecified,Autonomous Vehicles,Research,Navigation,Street Crossing,minimal interaction,Participants interacted with a virtual vehicle in a simulated environment.,simulation,The study used a virtual reality environment to simulate the interaction.,simulated,The robot was a virtual representation of an automated vehicle.,pre-programmed (non-adaptive),The vehicle's behavior was pre-programmed and did not adapt to the participant's actions.,Behavioral Measures; Questionnaires,,,Trust was assessed using questionnaires and behavioral measures such as decision time and hesitations.,no modeling,The study did not use computational models of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the accuracy of the external car display, creating a mismatch between the displayed information and the vehicle's actual behavior. This was intended to influence trust by creating a situation where the display was unreliable.","A single malfunction of the display led to a significant decrease in perceived safety and confidence in the vehicle, but these attributes recovered quickly. Trust in the display itself did not show a significant difference between groups.","Initial trust levels were surprisingly high, and trust recovered quickly after a malfunction. Participants also tended to trust the display more than the vehicle itself. There was a conflict between the participants' reported trust and their behavior, as most participants reported that the vehicle behaved as expected, even when it did not.","A single malfunction of an external car display significantly impacts pedestrian behavior and perception of the automated system, but trust recovers quickly, indicating a potential for overtrust.","The automated vehicle either stopped or continued driving, while the human participant decided whether to cross the road based on the vehicle's external display. The human's task was to follow the sidewalk and then cross the road when they felt safe.",Kolmogorov-Smirnov; Mann-Whitney U; pearson's correlation coefficient,"The study used Kolmogorov-Smirnov tests to check for normality of data distribution. Since the data was not normally distributed, Man-Whittney-U (Wilcoxon rank-sum) tests were used to compare the two groups (match vs. mismatch) on perceived safety, trust in the display, and confidence in the vehicle. Pearson's correlation coefficient was used to assess the effect size of the p-values obtained from the Man-Whittney-U tests.",TRUE,Robot-interface-design; Robot-accuracy,Robot-accuracy,Robot-interface-design,"The study manipulated the accuracy of the external car display, which is categorized as 'Robot-accuracy' because it directly influenced the task performance (whether the vehicle stopped or not) and the information provided to the user. The display itself, which showed a green person or yellow hand, is categorized as 'Robot-interface-design' because it is an interactive element of the robot. The study found that the accuracy of the display (whether it matched the vehicle's behavior) significantly impacted trust, as a single malfunction led to a decrease in perceived safety and confidence. However, the study found that trust in the display itself did not show a significant difference between groups, indicating that while the display's accuracy impacted trust, the design of the display itself did not.",10.1145/3342197.3344528,https://dl.acm.org/doi/10.1145/3342197.3344528,"The intentions of an automated vehicle are hard to spot in the absence of eye contact with a driver or other established means of communication. External car displays have been proposed as a solution, but what if they malfunction or display misleading information? How will this influence pedestrians’ trust in the vehicle? To investigate these questions, we conducted a between-subjects study in Virtual Reality (N = 18) in which one group was exposed to erroneous displays. Our results show that participants already started with a very high degree of trust. Incorrectly communicated information led to a strong decline in trust and perceived safety, but both recovered very quickly. This was also reflected in participants’ road crossing behavior. We found that malfunctions of an external car display motivate users to ignore it and thereby aggravate the effects of overtrust. Therefore, we argue that the design of external communication should avoid misleading information and at the same time prevent the development of overtrust by design."
"Holloman, Amanda; Egbert, William; Stegman, Pierce; Cioli, Nicholas; Crawford, Chris S.",Leveraging Neurophysiological Information to Augment Interpretation of Responses to Vulnerable Robot Behaviors,2019,1,2,2,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants completed a guided learning task using Scratch with a robotic tutor, Cozmo, in either a high or low vulnerability condition. EEG data was collected during the interaction. Post-survey questions were administered after the task.",Participants used Scratch to build a birthday card for the robot.,Cozmo,Expressive Robots,Educational; Research; Social,Social,Tutoring,minimal interaction,Participants interacted with the robot through verbal instructions and a learning task.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,The robot was physically present during the interaction.,wizard of oz (directly controlled),"The robot's verbalizations were mostly pre-programmed, with a human operator manually entering utterances when necessary.",Questionnaires; Physiological Measures,,Physiological Signals,Trust was assessed using post-survey questions and EEG data.,no modeling,The study did not model trust computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot exhibited either high or low vulnerability behaviors by referring to its upcoming birthday and requesting assistance, or not mentioning it.","The study found that high vulnerability robot behaviors triggered a P100 ERP component, suggesting a neurophysiological response to the manipulation.","The study found a P100 ERP component during the high vulnerability condition, which was absent during the low vulnerability condition, aligning with previous studies linking P100 to emotional words.","High vulnerable robot behaviors may trigger detectable neurophysiological responses, specifically a P100 ERP component.",The robot provided assistance to participants in a guided learning task using Scratch. The human participant built a birthday card for the robot using the Scratch programming environment.,,"The study focused on comparing Event-Related Potentials (ERPs) between high and low robot vulnerability conditions. The analysis involved processing EEG data, filtering it, and examining the P100 ERP component at electrode site Pz. However, no specific statistical tests were mentioned. The analysis was primarily descriptive, focusing on the presence or absence of the P100 component in different conditions.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the robot's verbal communication content by having it either mention its upcoming birthday and request assistance (high vulnerability condition) or not mention it and simply instruct the participant to make a birthday card (low vulnerability condition). This manipulation directly altered the content of the robot's verbalizations, making 'Robot-verbal-communication-content' the most appropriate category. The study found that the high vulnerability condition, which involved specific verbal content, triggered a P100 ERP component, suggesting that this manipulation impacted the participants' neurophysiological responses, which are linked to trust and emotional processing. Therefore, 'Robot-verbal-communication-content' is also the factor that impacted trust. There were no other factors manipulated, and no factors that were manipulated that did not impact trust.",10.1109/HRI.2019.8673290,https://ieeexplore.ieee.org/document/8673290/,"Previous human-robot interaction (HRI) research has shown that trust, disclosure, and companionship may be inﬂuenced by a robot’s verbal behavior. Measures used to interpret these key aspects of HRI commonly include surveys, observations, and user interviews. In this preliminary work, we aim to extend previous research by exploring the use of electroencephalography (EEG) to augment our understanding of participants’ responses to vulnerable robot behaviors. We tested this method by obtaining EEG data from participants while they interacted with a robotic tutor. The robotic tutor was designed to exhibit high vulnerability (HV) or low vulnerability (LV) behaviors similar to a previous HRI study. Our preliminary results show that event-related potentials (ERPs) may provide insights into participants’ early affective processing of vulnerable robot behaviors."
"Holthausen, Brittany E.; Wintersberger, Philipp; Walker, Bruce N.; Riener, Andreas",Situational Trust Scale for Automated Driving (STS-AD): Development and Initial Validation,2020,1,303,303,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants viewed videos of an automated vehicle and completed the STS-AD after each video. Participants were assigned to one of six conditions, with the failure video appearing in different positions in the sequence of videos. Participants also completed other questionnaires before and after the video viewing.",Participants watched videos of an automated vehicle and rated their trust using the STS-AD scale after each video.,Unspecified,Autonomous Vehicles,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants passively observed videos of an automated vehicle.,media,Participants watched videos of an automated vehicle.,physical,The robot was a physical vehicle recorded in a video.,fully autonomous (limited adaptation),"The automated vehicle was presented as operating autonomously, but with limited adaptation.",Questionnaires; Custom Scales,Trust in Automation Scale (TAS),,Trust was measured using the STS-AD scale and the Trust in Automation Scale.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers manipulated the presence of a near-miss event in one of the videos to influence trust. The near-miss video was intended to decrease trust.,Trust was significantly lower after participants experienced the near miss of the pedestrian.,"The study found that the STS-AD scale was able to capture changes in situational trust due to near automation failures, as well as capture near constant levels of situational trust across situations that are highly similar. There was no significant difference in average STS-AD score between the German and the English speaking sample.","The Situational Trust Scale for Automated Driving (STS-AD) is a valid measure of situational trust in automated driving, and it is distinct from general trust in automation.","The automated vehicle drives in different environments, and the human participant watches the videos and rates their trust in the vehicle after each video.",t-test; ANOVA; t-test; t-test; cronbach's alpha; Multilevel Model,"The study used several statistical tests. A t-test was used to compare the average STS-AD scores between the German and English speaking samples. Repeated measures ANOVAs were used to determine if there were significant differences in STS-AD scores across the different videos. Paired t-tests were used to compare STS-AD scores after the near-miss incident with scores from other videos. Independent sample t-tests with Bonferroni correction were used to compare average STS-AD scores across the different experimental conditions. Cronbach's alpha was used to assess the internal consistency of the STS-AD scale. Factor analysis was used to determine the relationship between the STS-AD scale and the Trust in Automation scale, and to examine the factor structure of the STS-AD scale itself.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the presence of a near-miss event in one of the videos. This near-miss event represents a decrease in the robot's accuracy in performing its task (driving safely), as it almost hit a pedestrian. The paper states, 'To assess how well the scale is able to capture a decline in trust given the temporal development of trust formation, six videos were recorded: five consisted of typical driving situations in varying circumstances with highly reliable automation (non-failure videos); and one included an automation failure (near miss of a pedestrian on a crosswalk).' The results show that trust was significantly lower after the near-miss event, indicating that the manipulation of robot accuracy impacted trust. The paper also states, 'Overall, Situational Trust was significantly lower after participants experienced the near miss of the pedestrian'. There were no other factors explicitly manipulated in the study.",10.1145/3409120.3410637,https://dl.acm.org/doi/10.1145/3409120.3410637,"Trust is important in determining how drivers interact with automated vehicles. Overtrust has contributed to fatal accidents; and distrust can hinder successful adoption of this technology. However, existing studies on trust are often hard to compare, given the complexity of the construct and the absence of standardized measures. Further, existing trust scales often do not consider its multi-dimensionality. Another challenge is that driving is strongly context- and situation-dependent. We present the Situational Trust Scale for Automated Driving, a short questionnaire to assess different aspects of situational trust, based on the trust model proposed by Hoff and Bashir. We evaluated the scale using an online study in the US and Germany (N=303), where participants faced different videos of an automated vehicle. Results confirm the existence of situational factors as components of trust, and support the scale being a valid measure of situational trust in this automated driving context."
"Hopko, Sarah K.; Mehta, Ranjana K.",Trust in Shared-Space Collaborative Robots: Shedding Light on the Human Brain,2022,1,16,16,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a surface polishing task with a UR10 robot under varying reliability and assistance levels. Brain activity (fNIRS), subjective responses, and performance were measured. The order of assistance level was counterbalanced, but the high reliability condition always preceded the low reliability condition.",Participants controlled a UR10 robot to polish a metal surface along a defined S-shaped curve using a joystick.,UR10,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Tool Use,direct-contact interaction,Participants physically interacted with the robot to complete the task.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The study used a physical robot for the interaction.,shared control (fixed rules),"The robot provided assistance based on pre-defined rules, with the human controlling the robot's movement.",Questionnaires; Performance-Based Measures; Physiological Measures,Propensity to Trust Scales; Jian et al. Trust Scale; NASA Task Load Index (NASA-TLX),Performance Metrics; Physiological Signals,"Trust was assessed using questionnaires, performance metrics, and physiological measures (fNIRS).","parametric models (e.g., regression)","Logistic and linear regression models were used to analyze the relationship between trust, workload, and neural activity.",Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"Robot reliability and assistance levels were directly manipulated to influence trust. Reliability was manipulated by introducing errors, and assistance was manipulated by changing the level of robot control.","Lower robot reliability decreased trust, while higher assistance levels exacerbated the negative impact of unreliability on trust.","The study found that neural activation in the medial and right dorsolateral prefrontal cortex increased under unreliable robot behaviors, and that connectivity strengths throughout the brain decreased. The study also found that humans induced additional performance decrements when perturbed by robot unreliability, and these decrements were exacerbated under higher robot assistance.","Unreliable robot behavior leads to decreased trust, increased workload, and reduced human performance, accompanied by increased activation in the medial and right dorsolateral prefrontal cortex and decreased brain connectivity.","The robot moved along a predefined path, providing assistance at different levels. The human controlled the robot's movement using a joystick, and was responsible for polishing the metal surface.",t-test; rm anovas; post hoc t-tests; Logistic regression; Linear regression; t-test,"The study used a t-test to compare trust propensity between operator sexes. Repeated measures ANOVAs (RM ANOVAs) were applied to subjective questionnaire ratings (TRUST, NASA TLX) and performance metrics (accuracy, precision, efficiency) to identify the effects of operator sex, robot reliability, and assistance levels. Post hoc t-tests with Bonferroni correction were used to assess interactive effects identified by the ANOVAs. Logistic regression was performed to control for the covariate effects of workload on trust and neural activity, predicting reliability condition score using ROIs, workload, and their interaction. Linear regression was used to predict TRUST using ROIs, workload, and their interaction. Paired t-tests were used to compare average connectivity strengths between reliable and unreliable conditions, with post hoc t-tests on individual connections.",TRUE,Robot-accuracy; Robot-autonomy; Task-complexity,Robot-accuracy; Robot-autonomy,,"The study manipulated robot reliability by introducing errors during the polishing task, which directly impacted the robot's accuracy in completing the task. This is categorized as 'Robot-accuracy' because the errors directly affected the robot's performance in terms of completing the polishing task. The study also manipulated the level of robot assistance, which is categorized as 'Robot-autonomy' because it changed the degree to which the robot controlled the task, from the human controlling the entire trajectory to the robot taking over parts of the trajectory. The study also manipulated the task complexity by changing the level of assistance, which is categorized as 'Task-complexity' because it changed the cognitive demands on the participant. The results showed that both 'Robot-accuracy' (reliability) and 'Robot-autonomy' (assistance level) significantly impacted trust levels, with lower reliability and higher assistance leading to decreased trust. There were no factors that were manipulated that did not impact trust.",10.1177/00187208221109039,http://journals.sagepub.com/doi/10.1177/00187208221109039,"Background: Industry 4.0 is currently underway allowing for improved manufacturing processes that leverage the collective advantages of human and robot agents. Consideration of trust can improve the quality and safety in such shared-space human-robot collaboration environments. Objective: The use of physiological response to monitor and understand trust is currently limited due to a lack of knowledge on physiological indicators of trust. This study examines neural responses to trust within a shared-workcell human-robot collaboration task as well as discusses the use of granular and multimodal perspectives to study trust. Methods: Sixteen sex-balanced participants completed a surface ﬁnishing task in collaboration with a UR10 collaborative robot. All participants underwent robot reliability conditions and robot assistance level conditions. Brain activation and connectivity using functional near infrared spectroscopy, subjective responses, and performance were measured throughout the study. Results: Signiﬁcantly, increased neural activation was observed in response to faulty robot behavior within the medial and right dorsolateral prefrontal cortex (PFC). A similar trend was observed for the anterior PFC, primary motor cortex, and primary visual cortex. Faulty robot behavior also resulted in reduced functional connectivity strengths throughout the brain. Discussion: These ﬁndings implicate regions in the prefrontal cortex along with speciﬁc connectivity patterns as signiﬁers of distrusting conditions. The neural response may be indicative of how trust is inﬂuenced, measured, and manifested for human-robot collaboration that requires active teaming. Application: Neuroergonomic response metrics can reveal new perspectives on trust in automation that subjective responses alone are not able to provide."
"Hopko, Sarah K.; Mehta, Ranjana K.; Pagilla, Prabhakar R.",Physiological and perceptual consequences of trust in collaborative robots: An empirical investigation of human and robot factors,2023,1,16,13,3,"One male and two female participants were excluded from the HRV analysis due to missing data (i.e., equipment disconnection)",Controlled Lab Environment,within-subjects,"Participants performed a metal surface polishing task with a UR10 robot. They completed the task under two fatigue conditions (fatigued, not fatigued) and two robot reliability conditions (reliable, unreliable). The order of fatigue was counterbalanced, and the reliable condition always preceded the unreliable condition. Participants completed questionnaires and had physiological data collected.","Participants controlled a robot arm to polish a metal surface, following a predefined S-shaped trajectory. The robot provided assistance during U-turns.",UR10,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Tool Use,direct-contact interaction,Participants physically interacted with the robot to complete the polishing task.,real-world,The study involved a real-world interaction with a physical robot.,physical,The study used a physical robot for the interaction.,shared control (fixed rules),The robot provided assistance during U-turns based on fixed rules.,Physiological Measures; Questionnaires,Propensity to Trust Scales; Jian et al. Trust Scale,Physiological Signals; Performance Metrics,"Trust was assessed using questionnaires and physiological measures, including heart rate variability.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"Robot reliability was manipulated by introducing errors during the task, and cognitive fatigue was induced through a spatial n-back test. This was intended to influence trust by altering the robot's perceived performance and the operator's cognitive state.","Robot unreliability decreased perceived trust, while cognitive fatigue did not have a main effect on trust, but interacted with reliability and sex. Unreliable robot behavior also increased perceived workload and fatigue.","Females showed a greater trust breach than males during the no fatigue condition, reporting lower trust in response to unreliable automation. The LF/HF ratio was similar for both UR and R trials when fatigued, but in NF the R trials had a higher ratio. There was a trend for the interaction between fatigue and sex on perceived fatigue, where females reported higher perceived fatigue than males in the F session.","Robot unreliability causes operator-induced performance decrements, and physiological HRV responses are sensitive to trust influencers such as reliability.","The robot moved along a predefined path, assisting with U-turns, while the human controlled the lateral movements of the robot to polish a metal surface.",rm anovas; t-test,"Separate repeated measures analyses of variance (RM ANOVAs) were used to determine the effects of the independent variables (fatigue, reliability, sex) on the task performance metrics (accuracy, efficiency, and precision) and subjective responses (namely, TRUST questionnaire, fatigue, and mental effort load). A RM ANOVA was also conducted to assess the impact of fatigue and sex on propensity to trust. Additionally, separate RM ANOVAs were conducted on the HRV metrics to test the effects of fatigue, reliability, sex, and phase (early/late trials). Where necessary, post hoc analysis and t-tests were conducted to determine significant interaction effects using Bonferroni corrections.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy,Task-complexity,"The study manipulated robot reliability by introducing errors during the polishing task, which directly impacted the robot's accuracy in completing the task. This is categorized as 'Robot-accuracy' because the errors directly affected the robot's performance on the task. The study also manipulated cognitive fatigue through a spatial n-back test, which increased the cognitive demands on the participants. This is categorized as 'Task-complexity' because it altered the mental workload required for the task. The results showed that robot unreliability (manipulated through 'Robot-accuracy') significantly decreased perceived trust, while cognitive fatigue ('Task-complexity') did not have a main effect on trust, but interacted with reliability and sex. Therefore, 'Robot-accuracy' impacted trust, while 'Task-complexity' did not have a main effect on trust.",10.1016/j.apergo.2022.103863,https://linkinghub.elsevier.com/retrieve/pii/S0003687022001867,"Measuring trust is an important element of effective human-robot collaborations (HRCs). It has largely relied on subjective responses and thus cannot be readily used for adapting robots in shared operations, particularly in shared-space manufacturing applications. Additionally, whether trust in such HRCs differ under altered operator cognitive states or with sex remains unknown. This study examined the impacts of operator cognitive fatigue, robot reliability, and operator sex on trust symptoms in collaborative robots through both objective measures (i. e., performance, heart rate variability) and subjective measures (i.e., surveys). Male and female participants were recruited to perform a metal surface polishing task in partnership with a collaborative robot (UR10), in which they underwent reliability conditions (reliable, unreliable) and cognitive fatigue conditions (fatigued, not fatigued). As compared to the reliable conditions, unreliable robot manipulations resulted in perceived trust, an increase in both sympathetic and parasympathetic activity, and operator-induced reduction in task efficiency and accuracy but not precision. Cognitive fatigue was shown to correlate with higher fatigue scores and reduced task efficiency, more severely impacting females. The results highlight key interplays between operator states of fatigue, sex, and robot reliability on both subjective and objective responses of trust. These findings provide a strong foundation for future investigations on better understanding the relationship between human factors and trust in HRC as well as aid in developing more diagnostic and deployable measures of trust."
"Hopko, Sarah K.; Zhang, Yinsu; Yadav, Aakash; Pagilla, Prabhakar R.; Mehta, Ranjana K.",Brain–Behavior Relationships of Trust in Shared Space Human–Robot Collaboration,2024,1,38,38,8,"2 were removed from cortical brain imaging analysis due to device disconnection at the session, 6 were removed from eye-tracking analysis due to incomplete data or low-quality gaze samples",Controlled Lab Environment,within-subjects,"Participants performed an assembly task with a collaborative robot under reliable and unreliable conditions. They first completed practice trials, then 10 trials with a reliable robot, followed by 10 trials with an unreliable robot. Surveys were administered after each condition. Brain imaging and eye-tracking data were collected throughout the experiment.",Participants assembled a planetary gear system with the assistance of a collaborative robot.,UR10,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically interacted with the robot to assemble parts.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The robot was a physical industrial robot arm.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions without adapting to the user.,Behavioral Measures; Physiological Measures; Questionnaires,Propensity to Trust Scales; NASA Task Load Index (NASA-TLX); Situational Awareness Rating Technique (SART); Human-Robot Trust Scale/Questionnaire (HRTS/HRITS),Eye-tracking Data; Performance Metrics; Physiological Signals; Video Data,"Trust was assessed using questionnaires, behavioral measures, and physiological data.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's reliability was manipulated by introducing failures in its actions, which was intended to decrease trust.",Robot unreliability significantly reduced trust.,Males exhibited shorter fixations on the robot than females in the reliable condition. Males also made more mistakes than females. Females reported lower situation awareness and higher workload than males. The study also found that robot unreliability led to a shift towards bottom-up processing.,"Robot unreliability decreases trust and results in increased fixation on the robot, increased stationary gaze entropy, decreased gaze transition entropy, and specific neural patterns indicative of decreased top-down regulation.",The robot was responsible for picking and placing parts into the human's workspace. The human was responsible for assembling the parts into a planetary gear system.,ANOVA; Wilcoxon signed-rank test; t-test,"The study used RMANOVAs to test the main and interaction effects of reliability and sex on subjective responses, eye-tracking measures, and performance metrics. Non-parametric Wilcoxon signed-rank tests were used to assess the effects of reliability and sex on neural activation and average fixation durations. T-tests were used to assess the impact of specific cortices on functional connectivity and to test the effect of reliability on cross-cortex connectivity.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the robot's reliability by introducing failures in its actions, which directly impacted the robot's accuracy in completing its task of delivering parts. This is explicitly stated in the paper: 'Then, participants underwent 10 unreliable trials, where the robot had defined failures for an effective reliability rate of 76%'. The failures included changes in speed, drop-off location, order of parts, indicator light, and dropping parts from high location. These failures directly affected the robot's ability to perform its task accurately, thus 'Robot-accuracy' is the most appropriate category. The paper also states 'We found unreliable cobot actions significantly reduced trust, as intended (p = 0.003, μ 2 = 0.093)', indicating that the manipulation of robot accuracy impacted trust. There were no other factors manipulated in the study.",10.1145/3632149,https://dl.acm.org/doi/10.1145/3632149,"Trust in human–robot collaboration is an essential consideration that relates to operator performance, utilization, and experience. While trust’s importance is understood, the state-of-the-art methods to study trust in automation, like surveys, drastically limit the types of insights that can be made. Improvements in measuring techniques can provide a granular understanding of influencers like robot reliability and their subsequent impact on human behavior and experience. This investigation quantifies the brain–behavior relationships associated with trust manipulation in shared space human–robot collaboration to advance the scope of metrics to study trust. Thirty-eight participants, balanced by sex, were recruited to perform an assembly task with a collaborative robot under reliable and unreliable robot conditions. Brain imaging, psychological and behavioral eye-tracking, quantitative and qualitative performance, and subjective experiences were monitored. Results from this investigation identify specific information processing and cognitive strategies that result in identified trust-related behaviors that were found to be sex specific. The use of covert measurements of trust can reveal insights that humans cannot consciously report, thus shedding light on processes systematically overlooked by subjective measures. Our findings connect a trust influencer (robot reliability) to upstream cognition and downstream human behavior and are enabled by the utilization of granular metrics."
"Hu, Wan-Lin; Akash, Kumar; Jain, Neera; Reid, Tahira",Real-Time Sensing of Trust in Human-Machine Interactions,2016,1,31,23,8,"8 participants were excluded due to anomalous EEG spectrum, possibly due to bad channels or dislocation of the EEG headset",Controlled Lab Environment,within-subjects,"Participants were asked to respond to a scenario in which they would be driving a car equipped with an image processing sensor. The algorithm would detect obstacles on the road, and the participant would need to repeatedly evaluate the algorithm report. Participants had the option to choose 'trust' or 'distrust' after which they received feedback of 'correct' or 'incorrect'. The trials were divided into two categories: reliable and faulty.","Participants evaluated the reliability of an image processing algorithm in a simulated driving scenario, choosing to 'trust' or 'distrust' the algorithm's output.",Unspecified,Autonomous Vehicles,Research,Evaluation,Image Analysis,minimal interaction,"Participants interacted with a simulated system, making trust decisions based on visual feedback.",simulation,"The interaction was conducted in a simulated environment, without physical embodiment of the robot.",simulated,The robot was represented as a simulated image processing system.,pre-programmed (non-adaptive),"The algorithm's behavior was pre-programmed, with no adaptation to the user's actions.",Behavioral Measures; Physiological Measures; Real-time Trust Measures,,Physiological Signals; Performance Metrics,"Trust was assessed using behavioral responses, physiological signals (EEG and GSR), and response time.","parametric models (e.g., regression)",The study used classification algorithms to map psychophysiological signals to categorical trust levels.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the image processing algorithm was manipulated, switching between reliable and faulty trials to influence trust.",Trust decreased when the algorithm was faulty and increased when it was reliable.,The study found that high-beta band EEG measurements from the right hemisphere and parietal lobe responded most strongly to the discrepancy between reliable and faulty stimuli. Response time was also a significant feature of trust.,"Psychophysiological measurements, specifically EEG and GSR, can be used to sense trust in real-time with a mean accuracy of 71.57% using a combination of classifiers.","The robot (simulated image processing algorithm) detected obstacles, and the human participant decided whether to trust the robot's output, receiving feedback on their decision.",Discriminant analysis; cross-correlation coefficient; Discriminant analysis; linear support vector machine (svm); Logistic regression; quadratic discriminant; weighted k nearest neighbors (knn); voting,"The study used Fisher Discriminant Ratio (FDR) to select features by evaluating class separability between trust and distrust. Cross-correlation coefficients were used to assess the correlation between features. Multiple classification algorithms, including Linear Discriminant Analysis (LDA), linear Support Vector Machine (SVM), logistic regression, quadratic discriminant, and Weighted k Nearest Neighbors (kNN), were used to map the selected features to categorical trust levels. A Voting classifier, combining the results of the other classifiers, was also used. The performance of these classifiers was evaluated using 5-fold cross-validation.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the reliability of the image processing algorithm, switching between reliable and faulty trials. This directly impacts the accuracy of the robot's (simulated algorithm) performance, which is why 'Robot-accuracy' is the appropriate category. The paper states, 'In reliable trials, the algorithm correctly identified the road condition... For the faulty trials, there was a 50% probability that the algorithm incorrectly identified the road condition.' This manipulation of the algorithm's accuracy directly influenced the participants' trust levels, as they were more likely to trust the algorithm when it was reliable and distrust it when it was faulty. The study explicitly states that 'the accuracy of the algorithm was switched between reliable and faculty according to a pseudo-random binary sequence (PRBS) in order to excite all possible dynamics of the participant's trust response.' This confirms that the manipulation was intentional and directly related to the robot's performance.",,,"Human trust in automation plays an important role in successful interactions between humans and machines. To design intelligent machines that can respond to changes in human trust, real-time sensing of trust level is needed. In this paper, we describe an empirical trust sensor model that maps psychophysiological measurements to human trust level. The use of psychophysiological measurements is motivated by their ability to capture a human's response in real time. An exhaustive feature set is considered, and a rigorous statistical approach is used to determine a reduced set of ten features. Multiple classification methods are considered for mapping the reduced feature set to the categorical trust level. The results show that psychophysiological measurements can be used to sense trust in real-time. Moreover, a mean accuracy of 71.57% is achieved using a combination of classifiers to model trust level in each human subject. Future work will consider the effect of human demographics on feature selection and modeling."
"Hu, Chuan; Huang, Siwei; Zhou, Yu; Ge, Sicheng; Yi, Binlin; Zhang, Xi; Wu, Xiaodong",Dynamic and quantitative trust modeling and real-time estimation in human-machine co-driving process,2024,2,30,24,6,"6 participants were excluded because their oral ratings were opposite to the trust estimation results, indicating they did not prioritize safety and did not objectively evaluate their trust level",Controlled Lab Environment,within-subjects,"Participants drove a simulated vehicle through a 12-stage route, completing non-driving related tasks (NDRTs) and providing trust and perceived risk ratings after each stage. Physiological data was collected using ECG and GSR sensors.",Participants were required to follow a target car while completing NDRTs and providing trust and perceived risk ratings.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a driving simulator and were required to monitor the automated driving system.,simulation,Participants interacted with a driving simulator in a virtual environment.,simulated,The robot was a simulated autonomous vehicle within the driving simulator.,shared control (fixed rules),"The automated driving system followed fixed rules, and participants could intervene when they felt necessary.",Behavioral Measures; Physiological Measures; Real-time Trust Measures; Questionnaires,,Physiological Signals; Performance Metrics,"Trust was assessed using questionnaires, behavioral measures, physiological signals, and real-time trust estimation.","parametric models (e.g., regression)",A Kalman filter was used to dynamically estimate trust based on driver behavior and perceived risk.,Empirical HRI Studies,Virtual/Simulated Studies,Indirect Manipulation,"The task difficulty was indirectly manipulated through roadwork and the need to complete NDRTs, and feedback was given through the system's performance and the bonus system, influencing trust by creating a sense of risk and reward.","The study found that perceived risk negatively correlated with trust, and that the model could estimate trust with a certain degree of accuracy.","Some participants exhibited a contradiction between their oral trust ratings and their behavior, prioritizing NDRTs over safety, which led to their exclusion from the analysis.","A dynamic and quantitative trust estimation model was developed using a Kalman filter, incorporating driver behavior and perceived risk, and achieving a maximum accuracy of 74.1% in trust estimation.","The robot, an autonomous vehicle, followed a target car, while the human participant monitored the system, completed NDRTs, and provided trust and perceived risk ratings.",K-means; Pearson correlation,"The study used k-means clustering to categorize perceived risk ratings into low, middle, and high risk groups. A Pearson correlation was used to assess the relationship between driver's perceived risk ratings and average trajectory departure rate.",TRUE,Task-complexity; Task-environment,Task-complexity; Task-environment,,"The study indirectly manipulated task difficulty through roadwork and the need to complete NDRTs, which is categorized as 'Task-complexity'. The presence of roadwork also changes the driving environment, which is categorized as 'Task-environment'. The paper states that perceived risk, influenced by these factors, negatively correlated with trust, indicating that both 'Task-complexity' and 'Task-environment' impacted trust.",10.1016/j.trf.2024.08.001,https://linkinghub.elsevier.com/retrieve/pii/S1369847824002006,"The development of automated vehicles (AVs) will remain in the stage of human–machine codriving for a long time. Trust is considered as an effective foundation of the interaction between the driver and the automated driving system (ADS). Driver’s trust miscalibration, represented by under-trust and over-trust, is considered to be the potential cause of disuse and misuse of ADS, or even serious accidents. The estimation and calibration of trust are crucial to improve the safety of the driving process. This paper mainly consists of the following two aspects. Firstly, a dynamic and quantitative trust estimation model is established. A framework for trust estimation is constructed. Driver’s perceived risk and behavior features were monitored and a Kalman filter was used to dynamically and quantitatively estimate the driver’s trust. We conducted a driver-inthe-loop experiment and generated model parameters through a data-driven approach. The results demonstrated that the model exhibited precision in trust estimation, with the highest accuracy reaching 74.1%. Secondly, a reminder strategy to calibrate the over-trust of the driver is proposed based on the model from the first part. A scenario with four risky events was designed and the ADS would provide voice reminders to the driver when over-trust was detected. The results demonstrated that the reminder strategy proved to be beneficial for safety enhancement and moderate trust maintenance during the driving process. When the driver is over-trusting, the accident rates of the reminder group and the non-reminder group were 60.6% and 13.0%, respectively. Our contribution in this paper can be concluded by four points: (1) A real-time trust estimation model is proposed, which is dynamic and quantitative, considering the evolution pattern of driver’s trust and the perceived risk; (2) Mathematical modeling and machine learning methods are combined; (3) A trust-based reminder strategy that aims to enhance the safety of human–machine co-driving is designed; (4) Driver-in-loop experiment validates the effectiveness in enhancing the safety, maintaining driver’s trust and reducing trust biases in human–machine co-driving."
"Hu, Chuan; Huang, Siwei; Zhou, Yu; Ge, Sicheng; Yi, Binlin; Zhang, Xi; Wu, Xiaodong",Dynamic and quantitative trust modeling and real-time estimation in human-machine co-driving process,2024,2,30,30,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants drove a simulated vehicle through a 12-minute scenario with four risk events. One group received verbal reminders when over-trusting was detected, while the other group did not. The number of accidents was recorded.",Participants were required to follow a target car while completing arithmetic NDRTs and responding to risk events.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a driving simulator and were required to monitor the automated driving system.,simulation,Participants interacted with a driving simulator in a virtual environment.,simulated,The robot was a simulated autonomous vehicle within the driving simulator.,shared control (fixed rules),"The automated driving system followed fixed rules, and participants could intervene when they felt necessary.",Behavioral Measures; Real-time Trust Measures; Questionnaires,,"Performance Metrics; robot data (sensor data, etc.)","Trust was assessed using questionnaires, behavioral measures, and real-time trust estimation based on vehicle trajectory and driver behavior.","parametric models (e.g., regression)",A modified Kalman filter was used to estimate trust based on lane departure rate and driver behavior.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study directly manipulated feedback by providing verbal reminders to one group when over-trusting was detected, and the autonomy level was manipulated by setting up risk events that the ADS could not handle, and task difficulty was manipulated by the need to complete NDRTs while driving.","The reminder strategy effectively reduced the occurrence of accidents when drivers were over-trusting, and helped maintain trust in the system.","The non-reminder group had a significantly higher accident rate when over-trusting, while the reminder group had a lower accident rate, indicating the effectiveness of the reminder strategy.","A trust-based reminder strategy was effective in reducing accidents and maintaining moderate trust levels in human-machine co-driving, with the reminder group showing a significantly lower accident rate when over-trusting.","The robot, an autonomous vehicle, followed a target car, while the human participant monitored the system, completed arithmetic NDRTs, and intervened during risk events.",Shapiro-Wilk; t-test,"The study used the Shapiro-Wilk test to check if the experimental results conform to a normal distribution. A single-tailed two-sample t-test was used to verify the difference in mean trust levels between drivers in the reminder group and the non-reminder group, and to analyze the trust scale change measured by self-reported Likert.",TRUE,Robot-verbal-communication-content; Robot-autonomy; Task-complexity,Robot-verbal-communication-content; Robot-autonomy,Task-complexity,"The study directly manipulated feedback by providing verbal reminders to one group when over-trusting was detected, which is categorized as 'Robot-verbal-communication-content'. The autonomy level was manipulated by setting up risk events that the ADS could not handle, requiring human intervention, which is categorized as 'Robot-autonomy'. The task difficulty was also manipulated by the need to complete NDRTs while driving, which is categorized as 'Task-complexity'. The paper states that the reminder strategy reduced accidents and helped maintain trust, indicating that 'Robot-verbal-communication-content' impacted trust. The risk events that required human intervention also impacted trust, indicating that 'Robot-autonomy' impacted trust. The paper does not explicitly state that the task complexity impacted trust, but rather that the reminder strategy and risk events impacted trust. Therefore, 'Task-complexity' is listed as a manipulated factor but not as a factor that impacted trust.",10.1016/j.trf.2024.08.001,https://linkinghub.elsevier.com/retrieve/pii/S1369847824002006,"The development of automated vehicles (AVs) will remain in the stage of human–machine codriving for a long time. Trust is considered as an effective foundation of the interaction between the driver and the automated driving system (ADS). Driver’s trust miscalibration, represented by under-trust and over-trust, is considered to be the potential cause of disuse and misuse of ADS, or even serious accidents. The estimation and calibration of trust are crucial to improve the safety of the driving process. This paper mainly consists of the following two aspects. Firstly, a dynamic and quantitative trust estimation model is established. A framework for trust estimation is constructed. Driver’s perceived risk and behavior features were monitored and a Kalman filter was used to dynamically and quantitatively estimate the driver’s trust. We conducted a driver-inthe-loop experiment and generated model parameters through a data-driven approach. The results demonstrated that the model exhibited precision in trust estimation, with the highest accuracy reaching 74.1%. Secondly, a reminder strategy to calibrate the over-trust of the driver is proposed based on the model from the first part. A scenario with four risky events was designed and the ADS would provide voice reminders to the driver when over-trust was detected. The results demonstrated that the reminder strategy proved to be beneficial for safety enhancement and moderate trust maintenance during the driving process. When the driver is over-trusting, the accident rates of the reminder group and the non-reminder group were 60.6% and 13.0%, respectively. Our contribution in this paper can be concluded by four points: (1) A real-time trust estimation model is proposed, which is dynamic and quantitative, considering the evolution pattern of driver’s trust and the perceived risk; (2) Mathematical modeling and machine learning methods are combined; (3) A trust-based reminder strategy that aims to enhance the safety of human–machine co-driving is designed; (4) Driver-in-loop experiment validates the effectiveness in enhancing the safety, maintaining driver’s trust and reducing trust biases in human–machine co-driving."
"Hulcelle, Marc; Varni, Giovanna; Rollet, Nicolas; Clavel, Chloe",TURIN: A coding system for Trust in hUman Robot INteraction,2021,1,20,20,0,No participants were excluded,Controlled Lab Environment,,"Participants interacted with a NAO robot that explained paintings and then quizzed them on art. Before the quiz, participants were asked to present themselves.",Participants interacted with a robot that presented art and then quizzed them.,Nao,Humanoid Robots; Expressive Robots,Educational; Social; Research,Social,Conversation,minimal interaction,Participants interacted with the robot through verbal communication.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,wizard of oz (directly controlled),The robot was controlled in a Wizard-of-Oz setting.,Behavioral Measures,,Video Data; Speech Data; robot data,Trust was assessed through behavioral measures using video and speech data.,no modeling,"The study focused on developing a coding system for trust, not on modeling trust.",Empirical HRI Studies,Wizard-of-Oz Studies,No Manipulation,"The study did not manipulate any factors related to trust, but rather focused on developing a coding system to measure it.",,The high agreement rate on 'Mistrusting' segments compared to other categories suggests that errors in social interaction norms are easier to recognize than natural behaviors. The moderate agreement on 'Neutral' segments indicates that further work on the definition of this category is needed.,"The study presents a novel coding system, TURIN, for analyzing trust in HRI, and a preliminary analysis showed substantial agreement between expert annotators on trusting and mistrusting segments.","The robot explains paintings and then quizzes the participants on art. The human participants listen to the robot's explanations, answer questions, and engage in conversation with the robot.",cohen's kappa,The inter-rater agreement (IRA) between the experts was computed using Cohen's kappa to assess the reliability of the coding system. This test was used to measure the agreement between two expert annotators on the trust categories and the coded items within those categories.,FALSE,,,,"The study did not manipulate any factors. The focus was on developing a coding system for trust, not on manipulating variables to observe their impact on trust. The paper explicitly states, 'In this paper, we present TURIN, a coding system explicitly conceived to investigate trust dynamics in HRI.' and 'The robot was controlled in a Wizard-of-Oz setting, but the users were naive about that setting and their behaviors were unconstrained.' This indicates that no experimental manipulation was performed. The study's goal was to create a tool for measuring trust, not to test the effects of different factors on trust.",10.1109/ACII52823.2021.9597448,https://ieeexplore.ieee.org/document/9597448/,"A natural human-robot interaction (HRI) relies on the robot’s capacity to understand the users’ behaviors through psychological and sociological concepts. Users expect the robot to act in a realistic manner to create a more human-like relationship. In this context, trust is an essential concept as it determines the effectiveness of the system and its acceptance by users. The understanding of trust dynamics in HRI is still low and systematic studies of multimodal trust-related behaviors in HRI are relatively rare given the rising popularity of the topic. To bridge this gap, in this paper we present a novel coding system TURIN (Trust in hUman Robot INteraction) to study trust in HRI. A preliminary assessment of the coding system was carried out on the Vernissage dataset. Results show a signiﬁcant agreement between expert annotators."
"Hulcelle, Marc; Varni, Giovanna; Rollet, Nicolas; Clavel, Chloe",Comparing a Mentalist and an Interactionist Approach for Trust Analysis in Human-Robot Interaction,2023,1,10,10,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants engaged in three experimental phases: the robot presented paintings, participants introduced themselves, and the robot quizzed them on art. Annotators then assessed trust using two different methods.","Participants interacted with a robot that acted as an art guide, presenting paintings, asking for introductions, and quizzing them on art.",Nao,Humanoid Robots; Expressive Robots,Social; Research,Social,Conversation,minimal interaction,Participants interacted with the robot verbally in a semi-structured scenario.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed script during the interaction.,Behavioral Measures; Questionnaires,Robot Trust Scale (RTS),Video Data,Trust was assessed using the Robot Trust Scale and behavioral observations coded with the TURIN scheme.,no modeling,The study did not implement a computational model of trust.,Empirical HRI Studies,Physical Robot Studies,No Manipulation,"The study did not manipulate any specific factors related to trust, but rather compared two different methods of trust assessment.","The study did not manipulate trust, but rather analyzed how different assessment tools captured trust variations.","The study found that some RTS items were independent of TURIN trust labels, suggesting that participants' mental models of the robot's trustworthiness do not always align with their observable behaviors. There was also low inter-rater agreement for both the RTS and TURIN, highlighting the subjectivity of trust assessment.","The study compared mentalist and interactionist approaches to trust assessment, finding that they differ in orientation, generalization capability, time-framing, and scalability, and that they can complement each other.","The robot presented paintings, asked participants to introduce themselves, and then quizzed them on art. Participants responded to the robot's questions and engaged in conversation.",Shapiro-Wilk; Kruskal-Wallis; Dunn test; cohen's kappa; cramer's v correlations,"The study used a Shapiro-Wilk test to assess the normality of the RTS item scores. Since the scores were not normally distributed, a Kruskal-Wallis test was used to identify significant differences in the mean scores of RTS items based on the TURIN trust labels. Post-hoc Dunn tests with Bonferroni correction were then applied to determine which specific groups differed significantly. Cohen's Kappa was used to measure inter-rater agreement for the TURIN annotations, and Cramer's V correlations were used to measure inter-rater agreement for the RTS annotations.",FALSE,,,,"The study did not manipulate any factors related to trust. The study compared two different methods of trust assessment (mentalist and interactionist) using the same dataset. The robot's behavior was pre-programmed and consistent across all participants. Therefore, no factors were intentionally manipulated. The study focused on analyzing how different assessment tools captured trust variations, rather than manipulating trust itself. The study did not find any factors that impacted or did not impact trust because no factors were manipulated.",10.1145/3623809.3623840,https://dl.acm.org/doi/10.1145/3623809.3623840,"Trust is an important aspect of a human-robot interaction (HRI) as it mitigates the performance of many activities. Users’ trust may be impacted when robots make mistakes. To be able to properly time trust-reparation actions, robots should detect trust variations during the interaction. There are very few computational models of trust for such a task. The existing ones relied on either Psychological or Sociological theories that gave place to different definitions and analysis tools. We can distinguish two main approaches in the trust literature: the mentalist and the interactionist one. In this paper, we compare both approaches for trust detection, and explore how the adoption of two different assessment tools on an HRI dataset may lead to different results. We identify criteria that set them apart, and provide guidelines on the possibilities that each approach offers depending on the target computational model of trust."
"Hunter, Jacob G.; Ulwelling, Elise; Konishi, Matthew; Michelini, Noah; Modali, Akhil; Mendoza, Anne; Snyder, Jessie; Mehrotra, Shashank; Zheng, Zhaobo; Kumar, Anil R.; Akash, Kumar; Misu, Teruhisa; Jain, Neera; Reid, Tahira","The future of mobility-as-a-service: trust transfer across automated mobilities, from road to sidewalk",2023,1,48,48,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants completed two experimental sessions on different automated mobilities (car and sidewalk) with a 3-hour gap between sessions. Each session included a tutorial drive, a standard drive, and a proactive drive. Driving style (aggressive or defensive) was based on a screening survey. Trust was measured via questionnaires after each drive. A semi-structured interview was conducted at the end.","Participants experienced simulated drives in either an automated car or sidewalk mobility, responding to standard and conflict events while their trust was measured.",Unspecified,Autonomous Vehicles; Mobile Robots,Research,Navigation,Path Following,minimal interaction,"Participants interacted with the simulated vehicles in a virtual environment, with no physical contact.",simulation,Participants experienced the interaction through a driving simulator with a virtual environment.,simulated,The robots were simulated vehicles within a virtual environment.,wizard of oz (directly controlled),The automated driving was simulated by replaying a past researcher's drive via the 'Wizard of Oz' technique.,Questionnaires,,,Trust was measured using a questionnaire adapted from Körber (2019).,"parametric models (e.g., regression)",A linear mixed effects model was used to analyze the effect of mobility and drive types on trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the driving style (aggressive or defensive) of the automated vehicles and the type of drive (tutorial, standard, proactive) to influence trust.","Trust increased significantly from baseline to tutorial, standard, and proactive drives. The type of mobility did not significantly affect trust, but aggressive driving decreased trust in the intention of the developers.","The study found that trust increased similarly across both car and sidewalk mobilities, suggesting that trust can transfer across different types of automated vehicles. Aggressive driving decreased trust in the intention of the developers. There was a conflict in the paper where it was stated that the vehicles were 100% reliable, but also that participants did not perceive them as 100% reliable.","Trust in automated vehicles evolves similarly across different mobility types, and prior experience with an automated car can positively influence trust in a novel automated mobility, particularly regarding the intention of the developers.","The robot (simulated vehicle) navigated a virtual environment, responding to standard and conflict events. The human participant observed the simulation and completed trust questionnaires after each drive.",Mixed-effects model; ANOVA; tukey pairwise comparisons; Shapiro-Wilk; t-test,"A linear mixed effects model was used to analyze the effect of mobility and drive types on trust, with participant as a random factor. An omnibus ANOVA was performed to identify significant factors influencing trust, followed by Tukey pairwise comparisons to understand the relations between levels. Shapiro-Wilk test was used to check for normality. Two-sample t-tests were used to compare trust in the sidewalk mobility between participants who experienced the car mobility first and those who did not.",TRUE,Robot-autonomy; Robot-task-strategy; Task-environment,Robot-task-strategy,Robot-autonomy; Task-environment,"The researchers manipulated the driving style of the automated vehicles (aggressive or defensive), which is categorized as 'Robot-task-strategy' because it changes the way the robot completes the task without affecting the success rate. The type of drive (tutorial, standard, proactive) also influenced the robot's behavior, which is categorized as 'Robot-autonomy' because it changes the level of decision-making the robot exhibits. The study also used two different mobility types (car and sidewalk), which is categorized as 'Task-environment' because it changes the context in which the task is performed. The 'Robot-task-strategy' (driving style) was found to impact trust, as aggressive driving decreased trust in the intention of the developers, while defensive driving increased trust. The 'Robot-autonomy' (drive type) and 'Task-environment' (mobility type) were not found to have a significant impact on trust, as trust increased similarly across both car and sidewalk mobilities and across different drive types.",10.3389/fpsyg.2023.1129583,https://www.frontiersin.org/articles/10.3389/fpsyg.2023.1129583/full,"While trust in different types of automated vehicles has been a major focus for researchers and vehicle manufacturers, few studies have explored how people trust automated vehicles that are not cars, nor how their trust may transfer across different mobilities enabled with automation. To address this objective, a dual mobility study was designed to measure how trust in an automated vehicle with a familiar form factor—a car—compares to, and influences, trust in a novel automated vehicle—termed sidewalk mobility. A mixed-method approach involving both surveys and a semi-structured interview was used to characterize trust in these automated mobilities. Results found that the type of mobility had little to no effect on the different dimensions of trust that were studied, suggesting that trust can grow and evolve across different mobilities when the user is unfamiliar with a novel automated driving-enabled (AD-enabled) mobility. These results have important implications for the design of novel mobilities."
"Hussein, Aya; Elsawah, Sondoss; Abbass, Hussein A.",The reliability and transparency bases of trust in human-swarm interaction: principles and implications,2020,1,32,32,0,No participants were excluded,Controlled Lab Environment,within-subjects,Participants completed four 10-minute scenarios with varying reliability and transparency conditions. The order of scenarios was randomized. Participants were given a 2-minute warm-up before each scenario. Trust was measured every 2 minutes during the simulation.,Participants supervised a swarm of UAVs tasked with collecting target objects in a grid-based environment. They could accept or verify the swarm's recommendations on whether to collect an object.,Unspecified,Unmanned Aerial Vehicles (UAVs); Swarm Robots,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the swarm through a simulation, making decisions based on the swarm's recommendations.",simulation,"The interaction was conducted in a simulated environment, providing a visual representation of the swarm and the task.",simulated,The robots were represented as simulated UAVs within the virtual environment.,shared control (fixed rules),"The swarm operated autonomously using a flocking algorithm, but the human could accept or verify the swarm's recommendations.",Questionnaires,Jian et al. Trust Scale,,"Trust was measured using a short questionnaire adapted from Jian et al. Trust Scale, administered every two minutes during the simulation.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The reliability of the swarm's recommendations (70% or 90% correct) and the transparency of the swarm's decision-making process (with or without information on the number of UAVs detecting an object) were directly manipulated to influence trust.,"Both reliability and transparency had a positive effect on trust. Reliability-based trust was associated with increased reliance and decreased correct rejection rates, while transparency-based trust was associated with increased reliance and increased correct rejection rates.","The study found that reliability-based trust was negatively correlated with correct rejection rates, while transparency-based trust was positively correlated with these rates. This suggests that transparency can mitigate the negative effects of high reliability on human complacency.","Reliability and transparency have distinct effects on trust calibration, with reliability-based trust leading to increased reliance and complacency, and transparency-based trust leading to improved proper reliance.","The swarm of UAVs autonomously navigated the environment, detected objects, and made recommendations to the human on whether to collect them. The human's role was to accept or verify these recommendations to maximize the mission score.",ANOVA; lmm,"The study used a multivariate analysis of variance (MANOVA) to examine the effects of reliability, transparency, and their interaction on subjects' response variables. Linear mixed models (LMM) analyses were conducted to examine the effects of reliability, transparency, and their interaction on trust, reliance rate, correct acceptance, correct rejection, team accuracy, response time, and human contribution to performance. LMM was also used to study the change of these variables over time. Additionally, partial correlations were calculated to examine the relationship between trust and other response variables, controlling for either reliability or transparency.",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-accuracy; Robot-verbal-communication-content,,"The study manipulated two factors: the reliability of the swarm's recommendations (70% or 90% correct), which falls under 'Robot-accuracy' because it directly affects the success rate of the swarm's actions, and the transparency of the swarm's decision-making process (with or without information on the number of UAVs detecting an object), which falls under 'Robot-verbal-communication-content' because it changes the information communicated to the user. The paper explicitly states that both reliability and transparency had a positive effect on trust, therefore both 'Robot-accuracy' and 'Robot-verbal-communication-content' are included in 'factors_that_impacted_trust'. There were no factors that were manipulated that did not impact trust, so 'factors_that_did_not_impact_trust' is empty. The paper states, 'The results indicate that trust, whether it is reliability- or transparency-based, indicates high reliance rates and shorter response times.' and 'The analysis showed statistically significant positive effects of reliability (F(1, 96) ¼ 58.32, p < .001, partial g 2 ¼ :38) and transparency (Fð1, 96Þ ¼ 7:61, p ¼ :007, partial g 2 ¼ :07) but not for their interaction (Fð1, 96Þ ¼ 0:89, p ¼ :35).'",10.1080/00140139.2020.1764112,https://www.tandfonline.com/doi/full/10.1080/00140139.2020.1764112,"Automation reliability and transparency are key factors for trust calibration and as such can have distinct effects on human reliance behaviour and mission performance. One question that remains unexplored is: what are the implications of reliability and transparency on trust calibration for human-swarm interaction? We investigate this research question in the context of human-swarm interaction, as swarm systems are becoming more popular for their robustness and versatility. Thirty-two participants performed swarm-based tasks under different reliability and transparency conditions. The results indicate that trust, whether it is reliability- or transparency-based, indicates high reliance rates and shorter response times. Reliability-based trust is negatively correlated with correct rejection rates while transparency-based trust is positively correlated with these rates. We conclude that reliability and transparency have distinct effects on trust calibration."
"Inbar, Ohad; Meyer, Joachim",Manners Matter: Trust in Robotic Peacekeepers,2015,1,30,17,13,13 participants were excluded due to technical problems,Educational Setting,mixed design,"Participants viewed static images of a robot interacting with different people and answered questions about the robot's threat, fairness, function, and correctness.",Participants evaluated a robot's behavior in access-control scenarios based on static images.,Unspecified,Humanoid Robots,Social,Evaluation,Rating,passive observation,Participants observed static images of robot interactions.,media,The interaction was presented through static images.,simulated,The robot was presented as a 3D model in static images.,pre-programmed (non-adaptive),The robot's behavior was pre-programmed and did not adapt to the interaction.,Questionnaires,,,Trust was assessed using a questionnaire with Likert scale questions.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's politeness was manipulated to influence perceptions of threat, fairness, function, and correctness. Participants were also given a brief introduction about the robot's role.","The robot's politeness significantly affected perceptions of threat, fairness, function, and correctness. Impolite behavior led to higher threat perception and lower ratings of fairness, function, and correctness.","The study found that the robot's politeness was the most significant factor influencing trust, overshadowing the effects of age and gender of the person interacting with the robot. This was unexpected, as the researchers anticipated that politeness towards older people would have a larger impact.","The robot's politeness is a crucial determinant of people's perception of peacekeeping robots, affecting the extent to which people view the robot's actions as correct, efficient and fair.","The robot was depicted performing an access-control task, inspecting people entering a building. The human participants observed static images of these interactions and rated the robot's behavior.",Multilevel mixed-effects model,"The study used mixed within-between subject analyses to analyze the four dependent variables (Threat, Fairness, Function, and Correctness). The analysis included the respondent's gender as a between-subject variable, and the robot behavior, the age of the person interacting with the robot, and the gender of the person interacting with the robot as within-subject independent variables. The purpose was to determine the effect of these independent variables on the dependent variables.",TRUE,Robot-social-attitude,Robot-social-attitude,,"The study manipulated the robot's politeness (polite or impolite behavior) while interacting with individuals. This directly corresponds to the 'Robot-social-attitude' category, as it involves changes in the robot's social approach and demeanor. The paper states, 'applying polite or impolite behavior' and 'The strong effects of the robot's politeness show that the robot's attitude is the first and most important determinant of our participants' view of the robot.' This clearly indicates that the robot's social attitude was manipulated and had a significant impact on trust. The study also manipulated the age and gender of the person interacting with the robot, but these factors did not significantly impact trust, so they are not included in the factors that impacted trust. The paper states, 'Age and gender of the people interacting with the robot had no effect on participants' impressions of the robot.' Therefore, these factors are not included in the factors that impacted trust.",10.1177/1541931215591038,http://journals.sagepub.com/doi/10.1177/1541931215591038,"The 'intuitive' trust people feel when encountering robots in public spaces is a key determinant of their interactions with the systems. To study the trust we presented subjects with static images of a robot performing an access-control task, interacting with younger and older male and female civilians, applying polite or impolite behavior. Our results showed strong effects of the robot's behavior. Age and gender of the people interacting with the robot had no significant effect on participants' impressions of the robot's attributes. This preliminary study shows that politeness may be a crucial determinant of people's perception of peacekeeping robots."
"Javaid, Misbah; Estivill-Castro, Vladimir; Hexel, Rene",Manners Matter: Trust in Robotic Peacekeepers,2020,1,34,34,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants interacted with a robot to complete a decision-making task involving traffic scenarios. The robot provided explanations, and participants could change their answers based on the robot's input. The study had two conditions: one where the robot made no errors and one where the robot made an error and then corrected itself. Participants completed questionnaires before and after the interaction.","Participants were asked to make decisions about traffic scenarios in collaboration with a robot, and they could change their answers after hearing the robot's explanations.",Unspecified,Expressive Robots,Research; Social,Evaluation,Image Analysis,minimal interaction,"Participants interacted with the robot verbally and by selecting answers, but there was no physical touch.",real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,shared control (fixed rules),"The robot provided explanations based on pre-programmed rules, and the human could change their answer.",Behavioral Measures; Questionnaires,Human-Robot Trust Scale/Questionnaire (HRTS/HRITS); Negative Attitude towards Robots Scale (NARS),Performance Metrics,"Trust was measured using questionnaires and behavioral measures, specifically the participants' conformation to the robot's answers.",no modeling,"The study did not use computational models of trust, only statistical analysis.",Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's behavior was manipulated by having it either provide correct explanations or make an error and then correct itself, which was intended to influence trust.","Trust increased in both conditions, but the increase was higher when the robot did not make an error. The error-justification and correction policy mitigated the negative impact of the error.","Participants showed a willingness to conform to the robot's answers, even when they were initially confident in their own choices. The error-justification and correction policy was effective in mitigating the negative impact of the robot's errors.","Explanations from a robot, especially when combined with error-justification and correction, can reduce negative attitudes and increase trust in human-robot interactions.","The robot provided explanations for traffic scenarios, and the human participant selected an answer and could change it after hearing the robot's explanation. The robot also made an error in one condition and then corrected itself.",t-test; Shapiro-Wilk; Wilcoxon signed-rank test,"The study used a Shapiro-Wilk test to check for normality of data distribution. Parametric paired sample t-tests were used to analyze the effect of robot explanations on trust levels by comparing pre- and post-interaction trust scores in both the no-error and error-justification conditions. Non-parametric Wilcoxon signed-rank tests were used to analyze the change in negative attitude towards robots (NARS subscales S1 and S2) before and after interaction, as the data was not normally distributed. Additionally, a Wilcoxon test was used to compare the conformation scores between the two conditions.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the content of the robot's verbal communication by having it provide either correct explanations or incorrect explanations followed by a correction. This manipulation was intended to influence trust. The paper states, 'We manipulated the robot to make a mistake by generating Wrong Explanations 1 and immediately correct itself with a sophisticated justification for the error.' This clearly indicates a manipulation of the content of the robot's verbal communication. The study found that the robot's explanations, especially when combined with error-justification and correction, impacted trust levels. The paper states, 'Overall, the results provide strong support to our Hypothesis 1, by showing a decrease in the ratings of the negative attitude of human participants and an increase in the level of trust towards the robot.' and 'We found this has an impact and it was reflected during the decision-making task, where the human participants accepted and conformed more with the robot's decisions as compared to their answers.' This shows that the manipulation of the robot's verbal communication content impacted trust. There were no other factors manipulated in the study.",,,"When humans interact with robots in daily life, each human has a diﬀerent attitude towards robots that may directly aﬀect humanrobot trustworthy relationships. By attitude, we mean any mental disposition matured through experience. A negative attitude is the psychological factors that prevent a human from interacting with robots in daily life and also creates a hurdle for a human to build trust in a robot. In this paper, we hypothesise that explanations from a robot that contain “Decision-Transparency” and “Error-Justiﬁcation and Correction” policy can help in reducing humans’ negative attitude towards robots and facilitate smooth interaction. Explanations, speciﬁcally communicated in human-understandable terms can create a signiﬁcant diﬀerence. To analyse the profound impact of explanations from a robot, we conducted an Experimental Study with 34 human participants by performing a decisionmaking task in collaboration with a real robot. Objective assessment, i.e. facial expressions and eye contact with the robot signalled a decrease in the negative attitude of human participants towards the robot. We also found that human participants trusted and conformed more with the robot’s decisions (communicated in terms of explanations), as compared to their own decisions. Meanwhile, subjective measures (Negative Attitude toward Robots Scale (NARS), Human-Robot Trust Scale (HRT) questionnaires) also reported that, after having interaction with the robot, humans’ trust in the robot increased and negative attitude signiﬁcantly reduced. Our ﬁndings suggest new implications for the establishment of smooth human-robot trustworthy relationships."
"Javaid, Misbah; Estivill-Castro, Vladimir; Hexel, Rene",Enhancing Humans Trust and Perception of Robots Through Explanations,2020,1,33,33,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants played a domino game with a robot, first without explanations, then with explanations, and completed questionnaires before and after each game session.",Participants played a team-based domino game with a robot partner against a team of two humans.,Unspecified,Social Robots,Social; Research,Game,Cooperative Game,minimal interaction,"Participants interacted with the robot through a game, receiving verbal explanations.",real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,fully autonomous (limited adaptation),"The robot made decisions autonomously during the game, with limited adaptation.",Questionnaires; Behavioral Measures,Human-Robot Trust Scale/Questionnaire (HRTS/HRITS); Godspeed Questionnaire,Video Data; Performance Metrics,Trust was measured using questionnaires and behavioral observations.,no modeling,"Trust was not modeled computationally, only descriptive statistics were used.",Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot provided explanations of its decisions, which was intended to increase transparency and trust.",Explanations increased human participants' trust in the robot.,"The anthropomorphism ratings of the robot decreased after interaction, while trust and other perception ratings increased.",Explanations from a robot increased human trust and improved the perception of the robot's attributes.,"The robot played the domino game autonomously, making decisions and providing explanations, while the human participant played as a teammate, making their own moves.",cronbach's α; Shapiro-Wilk; t-test; Pearson correlation,The study used Cronbach's alpha to assess the internal reliability of the Human-Robot Trust and Godspeed questionnaires. The Shapiro-Wilk test was used to check for normality of the data. Paired sample t-tests were used to compare the levels of trust and perception of robot attributes before and after interaction with the robot. Pearson's correlation was used to analyze the correlation between trust and perception of the robot attributes.,TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study explicitly manipulated the content of the robot's verbal communication by providing explanations of its decisions. The paper states, 'We performed a user study investigating the effect of the explanations from a robot on humans’ trust.' and 'The independent variable is the explanations of the robot at the beginning of the first game and after the end of the match.' This clearly indicates that the presence or absence of explanations was the manipulated factor. The explanations provided transparency and justification for the robot's actions, which falls under the 'Robot-verbal-communication-content' category. The results showed that the explanations increased trust, as stated in the paper: 'Results from our preliminary analysis strongly support our Hypothesis 1 by indicating that explanations increased human participants' trust in the robot.' Therefore, 'Robot-verbal-communication-content' is also the factor that impacted trust. There were no other factors manipulated in the study.",,,"To integrate robots into humans’ environment, robots need to make their decision-making process transparent to increase humans’ trust in robots. Explanations from a robot are a promising way to express “how” a decision is made and “why” the decision made is the best. We performed a user study investigating the effect of the explanations from a robot on humans’ trust. Our setting consists of an interactive game-playing environment (the partial information game Domino), in which the robot partners with a human to form a team. Since in the game there are two adversarial teams, the robot plays two roles: the already mentioned partner with a human in a team, but also as an adversary facing the second team of two humans. The robot’s explanations are provided in human-understandable terms. Explanations from the robot not only provide insight into the robot’s decision-making process, but also help in improving humans’ learning of the task. We evaluated the human participants’ implicit trust in the robot by performing multi-modal scrutiny i.e., recording observations of facial expressions and affective states during the game-play sessions. We also used questionnaires to measure participants’ explicit trust and perception of the robot attributes. Our results show that the human participants considered the robot with explanations’ ability as a trustworthy team-mate. We conclude explanations can be used as an effective communication modality for robots to earn humans’ trust in social environments."
"Jayaraman, Suresh Kumaar; Creech, Chandler; Robert Jr., Lionel P.; Tilbury, Dawn M.; Yang, X. Jessie; Pradhan, Anuj K.; Tsui, Katherine M.",Trust in AV: An Uncertainty Reduction Model of AV-Pedestrian Interactions,2018,1,30,30,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants experienced six conditions in a virtual reality environment, acting as pedestrians crossing a street with approaching autonomous vehicles. The conditions varied by AV driving behavior (defensive, normal, aggressive) and crosswalk type (signalized, unsignalized). Attitudinal and behavioral measures were collected.",Participants were tasked with moving an object across a street at a mid-block crossing with several approaching autonomous vehicles.,Unspecified,Autonomous Vehicles,Research,Navigation,Street Crossing,minimal interaction,Participants interacted with the AV in a virtual environment.,simulation,The study used an immersive virtual reality environment.,simulated,The autonomous vehicles were simulated in the virtual environment.,pre-programmed (non-adaptive),The AV's behavior was pre-programmed based on the pedestrian's position.,Behavioral Measures; Questionnaires,,Performance Metrics,Trust was measured using questionnaires and behavioral measures.,"parametric models (e.g., regression)",A mixed linear model was used to analyze the data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the AV's driving behavior (defensive, normal, aggressive) and the type of crosswalk (signalized, unsignalized) to influence trust.","Aggressive driving decreased trust, while signalized crosswalks increased trust. The impact of aggressive driving on trust was moderated by the type of crosswalk.","The study found that the impact of aggressive driving on trust was significantly reduced at signalized crosswalks, suggesting that environmental cues can moderate the effects of robot behavior on trust.","The study found that aggressive driving decreases trust in autonomous vehicles, and this effect is moderated by the type of crosswalk.","Participants, as pedestrians, moved an object across a street while autonomous vehicles approached. The AVs varied their driving behavior, and the crossing was either signalized or unsignalized.",f statistic; mixed linear model,"The study used an F statistic to perform a manipulation check, ensuring that the driving conditions varied the participant's perception of aggressive driving. A mixed linear model was then employed to analyze the data, accounting for the non-independence of the data. This model tested the hypotheses related to the impact of aggressive driving and crosswalk type on trust, and the relationship between trust and trusting behaviors. Control variables such as age, driving experience, propensity to trust automation, and simulation sickness were included in the model.",TRUE,Robot-task-strategy; Task-environment,Robot-task-strategy; Task-environment,,"The study manipulated the AV's driving behavior (defensive, normal, aggressive) which is categorized as 'Robot-task-strategy' because it changes the way the robot completes the task of navigating the environment, without directly influencing the success of the task itself (the AV still navigates). The study also manipulated the type of crosswalk (signalized, unsignalized), which is categorized as 'Task-environment' because it changes the working conditions of the task. The results showed that both the AV's driving behavior and the type of crosswalk impacted trust, therefore both are listed in 'factors_that_impacted_trust'. There were no factors that were manipulated that did not impact trust.",10.1145/3173386.3177073,https://dl.acm.org/doi/10.1145/3173386.3177073,"Autonomous vehicles (AVs) have the potential to improve road safety. Trust in AVs, especially among pedestrians, is vital to alleviate public skepticism. Yet much of the research has focused on trust between the AV and its driver/passengers. To address this shortcoming, we examined the interactions between AVs and pedestrians using uncertainty reduction theory (URT). We empirically verified this model with a user study in an immersive virtual reality environment (IVE). The study manipulated two factors: AV driving behavior (defensive, normal and aggressive) and the traffic situation (signalized and unsignalized). Results suggest that the impact of aggressive driving on trust in AVs depends on the type of crosswalk. At signalized crosswalks the AV’s driving behavior had little impact on trust, but at unsignalized crosswalks the AV’s driving behavior was a major determinant of trust. Our findings shed new insights on trust between AVs and pedestrians."
"Jel?nek, Matou_; Fischer, Kerstin",Trust regulation in Social Robotics: From Violation to Repair,2023,1,24,14,10,10 participants were excluded due to technical problems with the robot,Controlled Lab Environment,within-subjects,"Participants played a collaborative game with a robot, with five different trust calibration strategies implemented in different sections of the game. Trust was measured after each section.",Participants played a modified version of the game Activity with a robot partner.,JD Humanoid,Humanoid Robots,Research; Social,Game,Cooperative Game,minimal interaction,Participants interacted with the robot verbally and through a collaborative game.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,The study used a physical humanoid robot.,wizard of oz (directly controlled),The robot was controlled using the Wizard-of-Oz approach.,Multidimensional Measures; Questionnaires,Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using the Multi-Dimensional Measure of Trust (MDMT) questionnaire after each experimental section.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's behavior was manipulated to demonstrate proficiency, situation awareness, transparency, trust violation, and trust repair, which was intended to influence the participants' trust levels.","The study found that trust could be effectively manipulated, with trust violation decreasing trust and trust repair increasing it. The robot was also perceived as more benevolent when demonstrating situation awareness.","The study found that moral trust was significantly impacted by the trust violation, while performance trust remained relatively stable. The robot was perceived as more benevolent when using strategies of shared context.","The subjective level of trust can be effectively manipulated over the course of one human-robot interaction, including violating and repairing trust.","The robot performed pantomimes or described words, and the human participant had to guess the word. The robot's behavior was manipulated across different sections of the game to influence trust.",ANOVA; t-test,"A one-way repeated measures ANOVA was used to examine the effect of five different trust modification approaches on the perceived level of trust. Follow-up student t-tests were used to analyze the differences between specific experimental sections, particularly to compare the trust violation condition with the initial stage and the trust repair condition with the trust violation condition. Additionally, t-tests were used to analyze the differences in trust factors (performance trust and moral trust) after the trust violation attempt and to compare benevolence between the shared context and proficiency stages.",TRUE,Robot-verbal-communication-content; Robot-social-attitude; Robot-accuracy,Robot-verbal-communication-content; Robot-social-attitude,Robot-accuracy,"The study manipulated several aspects of the robot's behavior to influence trust. 'Robot-verbal-communication-content' was manipulated through the robot's utterances, which were designed to demonstrate proficiency, situation awareness, transparency, and later, to violate trust and then repair it through apologies. For example, in the proficiency section (S1), the robot used utterances that demonstrated its ability to solve the problem. In the transparency section (S3), the robot informed the participant about its limitations. In the trust violation section (S4), the robot broke the work agreement. In the trust repair section (S5), the robot apologized. 'Robot-social-attitude' was manipulated through the robot's actions, such as being mean or playing against the human teammate in the trust violation section (S4), and showing remorse in the trust repair section (S5). 'Robot-accuracy' was manipulated through the robot's performance in the game, specifically by intentionally making errors during the trust violation phase (S4). The results showed that the manipulations of 'Robot-verbal-communication-content' and 'Robot-social-attitude' significantly impacted trust levels, with trust violation decreasing trust and trust repair increasing it. However, while the robot's accuracy was manipulated, the study found that performance trust remained relatively stable after the trust violation attempt, indicating that the manipulation of 'Robot-accuracy' did not significantly impact trust levels as measured by the MDMT questionnaire. The study explicitly states that the robot's behavior was manipulated to demonstrate proficiency, situation awareness, transparency, trust violation, and trust repair, which was intended to influence the participants' trust levels.",,,"While trust in human-robot interaction is increasingly recognized as necessary for the implementation of social robots, our understanding of regulating trust in human-robot interaction is yet limited. In the current experiment, we evaluated different approaches to trust calibration in human-robot interaction. The within-subject experimental approach utilized five different strategies for trust calibration: proficiency, situation awareness, transparency, trust violation, and trust repair. We implemented these interventions into a within-subject experiment where participants (N=24) teamed up with a social robot and played a collaborative game. The level of trust was measured after each section using the Multi-Dimensional Measure of Trust (MDMT) scale. As expected, the interventions have a significant effect on i) violating and ii) repairing the level of trust throughout the interaction. Consequently, the robot demonstrating situation awareness was perceived as significantly more benevolent than the baseline."
"Jensen, Theodore",The Role of Behavioral Anthropomorphism in Human-Automation Trust Calibration,2020,1,158,130,28,"27 participants were excluded for incorrectly answering attention check questions, 1 participant was excluded for not understanding the scoring mechanism",Online Crowdsourcing,between-subjects,Participants played an online game where they collaborated with an automated system to classify images across 5 rounds. They allocated images to the automation before each round and received feedback after each round. They completed a post-gameplay survey.,Participants classified images as 'Dangerous' or 'Not Dangerous' and allocated a number of images to an automated system for classification.,Unspecified,Other,Research,Game,Cooperative Game,minimal interaction,Participants interacted with the automated system through a computer interface.,simulation,The interaction was through a simulated online game environment.,simulated,The automated system was represented through text and a visual interface.,pre-programmed (non-adaptive),The automated system performed its task based on pre-set parameters without adapting to user behavior.,Questionnaires; Behavioral Measures; Performance-Based Measures,Godspeed Questionnaire; Mayer and Davis' Trust/Trustworthiness Scales (1999),Performance Metrics,"Trust was measured using questionnaires, behavioral measures of allocation, and performance metrics.","parametric models (e.g., regression)",The study used ANOVA and repeated measures ANOVA to analyze the relationship between experimental conditions and trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the automated system and its communication style were manipulated to influence perceptions of anthropomorphism and trust. The system's reliability was set to either 60% or 90% accuracy, and the communication style was either machinelike or humanlike.","Reliability significantly influenced trust appropriateness, with low reliability leading to overtrust and high reliability leading to undertrust. Communication style influenced perceptions of benevolence, but did not significantly impact trust appropriateness.","The study found that high reliability participants tended to undertrust the automation, while low reliability participants tended to overtrust it. The communication style did not significantly influence trust appropriateness, which was contrary to the initial hypotheses. The high reliability system was consistently perceived as more humanlike than the low reliability system.","The study found that while communication style and reliability influenced perceptions of anthropomorphism, only reliability significantly influenced the appropriateness of participants' trust in the automation.","The automated system classified images with a set accuracy, and the human allocated a number of images to the automated system for classification. The human also manually classified a subset of the images.",Chi-squared; Fisher's exact test; Kruskal-Wallis; wilcoxon signed-ranks tests; Mann-Whitney U; ANCOVA; repeated measures analysis of variance (rm-anova); Multivariate Analysis of Variance; ANOVA; t-test,"The study used a variety of statistical tests to analyze the data. Chi-square and Fisher's Exact tests were used to check for group differences in demographics. Kruskal-Wallis tests were used to check for group differences in age, education, and gaming frequency. Wilcoxon Signed-Ranks tests were used to compare perceived accuracy and speed of manual vs. automated identification. A Mann-Whitney U-test was used to compare perceived reliability between groups. ANCOVAs were used to examine the effects of reliability and communication style on perceived anthropomorphism, controlling for individual differences. A repeated measures ANOVA was used to analyze the effect of reliability and communication style on trust appropriateness across the 5 rounds of gameplay. MANOVA was used to analyze the effect of reliability and communication style on perceived ability, integrity, and benevolence. Follow-up ANOVAs were used to examine the main effects on each of the trust dimensions. One sample t-tests were used to determine if the mean trust appropriateness in each round was significantly different from 0.",TRUE,Robot-verbal-communication-style; Robot-accuracy,Robot-accuracy,Robot-verbal-communication-style,"The study manipulated two factors: the communication style of the automated system and its reliability (accuracy). 'Robot-verbal-communication-style' was chosen because the study explicitly altered the tone and style of the messages from the automated system, using either machinelike or humanlike language. This manipulation directly relates to how the content was communicated, not the content itself. 'Robot-accuracy' was chosen because the study manipulated the percentage of images that the system correctly identified (60% or 90%), which directly impacts the system's performance on the task. The study found that 'Robot-accuracy' significantly influenced trust appropriateness, with low reliability leading to overtrust and high reliability leading to undertrust. However, 'Robot-verbal-communication-style' did not significantly impact trust appropriateness, although it did influence perceptions of benevolence. Therefore, 'Robot-accuracy' is listed as a factor that impacted trust, and 'Robot-verbal-communication-style' is listed as a factor that did not impact trust.",,,"Trust has been identiﬁed as a critical factor in the success and safety of interaction with automated systems. Researchers have referred to “trust calibration” as an apt design goal– user trust should be at an appropriate level given a system’s reliability. One factor in user trust is the degree to which a system is perceived as humanlike, or anthropomorphic. However, relevant prior work does not explicitly characterize trust appropriateness, and generally considers visual rather than behavioral anthropomorphism. To investigate the role of humanlike system behavior in trust calibration, we conducted a 2 (communication style: machinelike, humanlike) × 2 (reliability: low, high) between-subject study online where participants collaborated alongside an Automated Target Detection (ATD) system to classify a set of images in 5 rounds of gameplay. Participants chose how many images to allocate to the automation before each round, where appropriate trust was deﬁned by a number of images that optimized performance. We found that communication style and reliability inﬂuenced perceptions of anthropomorphism and trustworthiness. Low and high reliability participants demonstrated overtrust and undertrust, respectively. The implications of our ﬁndings for the design and research of automated and autonomous systems are discussed in the paper."
"Jensen, Laura U.; Winther, Trine S.; Jorgensen, Rasmus; Hellestrup, Didde M.; Jensen, Lars C.",Maintaining trust while fixated to a rehabilitative robot,2016,1,14,14,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants interacted with a robot arm twice, once holding the arm and once fixated to it, while the robot guided their arm through preset coordinates. Video recordings and open-ended interviews were used for data collection.",Participants' arms were guided by a robot arm through a series of preset coordinates in a 3D space.,UR5,Industrial Robot Arms,Research; Care,Manipulation,Object Passing,direct-contact interaction,Participants physically interacted with the robot by having their arm guided by the robot.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot arm.,pre-programmed (non-adaptive),The robot followed a pre-set path without adapting to the user.,Behavioral Measures; Questionnaires,,Video Data; Speech Data,Trust was assessed through behavioral observations and post-experiment interviews.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated whether participants were holding or fixated to the robot arm, and the robot's position in relation to the participant's personal space.","Participants did not feel more insecure when fixated to the robot, but showed more fear when the robot moved into their intimate or outer workspace.","The study found that participants did not feel more insecure when fixated to the robot, which was contrary to the expected result. Participants displayed more fear when the robot moved into their intimate or outer workspace, regardless of fixation condition.","Participants generally felt safe with the robot, but displayed more fear when the robot moved into their intimate or outer workspace.",The robot arm moved the participant's arm through a series of preset coordinates in a 3D space. The participant either held onto the robot arm or was fixated to it.,t-test; Kruskal-Wallis,"The study used Welch's T-tests to compare the effect of fixation (fixated vs. not fixated) within each of the three workspace groups (intimate, normal, and outer). A Kruskal Wallis test was then used to examine the effect of fixation across all workspaces, and to determine if there were differences in fear responses across the different workspaces, regardless of fixation condition. The Kruskal Wallis test was used because the data was not normally distributed.",TRUE,Robot-nonverbal-communication; Task-environment,Task-environment,Robot-nonverbal-communication,"The study manipulated whether participants were holding or fixated to the robot arm, which is a change in the robot's physical interaction with the participant and thus classified as 'Robot-nonverbal-communication'. The study also manipulated the robot's position in relation to the participant's personal space (intimate, normal, outer), which is a change in the 'Task-environment'. The results showed that the robot's position in relation to the participant's personal space ('Task-environment') impacted trust, with participants displaying more fear when the robot moved into their intimate or outer workspace. However, the fixation condition ('Robot-nonverbal-communication') did not significantly impact feelings of insecurity, as participants did not feel more insecure when fixated to the robot.",10.1109/HRI.2016.7451797,http://ieeexplore.ieee.org/document/7451797/,"This paper investigates the trust relationship between humans and a rehabilitation robot, the RoboTrainer. We present a study in which participants let the robot guide their arms through a series of preset coordinates in a 3D space. Each participant interact with the robot twice, one time where participants hold on to the robotic arm, and a second time where participants are ﬁxated to the robotic arm. Our ﬁndings show that in general participants did not feel more insecure when ﬁxated to the robot. However, when the robot arm moves close to participants and enter their intimate space, or when the robot moves out into an outer position participants display signiﬁcantly more signs of fear opposed to when the robot arm is in a normal position."
"Jensen, Theodore; Albayram, Yusuf; Khan, Mohammad Maifi Hasan; Fahim, Md Abdullah Al; Buck, Ross; Coman, Emil",The Apple Does Fall Far from the Tree: User Separation of a System from its Developers in Human-Automation Trust Repair,2019,1,264,147,117,"13 participants were excluded due to incorrectly answering attention check questions, 7 participants were excluded for allocating all 20 images to the automation in each of the final 4 rounds, 97 participants were excluded for failing source and blame manipulation checks",Online Crowdsourcing,between-subjects,"Participants played an online game where they collaborated with an automated system in an image classification task across 5 rounds; after each round, the system displayed a feedback message attributing blame for errors either internally, pseudo-externally, or externally; participants then completed a survey on their perceptions of the system.","Participants classified images of vehicles as 'Dangerous' or 'Not Dangerous' with the help of an automated system, and chose how many images to allocate to the automation in each round.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Game,Cooperative Game,minimal interaction,"Participants interacted with the automated system through a computer interface, allocating images to the system and receiving feedback.",simulation,The interaction was through a simulated online game environment.,simulated,The robot was represented as an automated system within the game interface.,pre-programmed (non-adaptive),The automated system's behavior was pre-programmed with fixed reliability and feedback messages.,Behavioral Measures; Questionnaires; Custom Scales,Jian et al. Trust Scale; Mayer and Davis' Trust/Trustworthiness Scales (1999),Performance Metrics,"Trust was measured using questionnaires, custom scales, and behavioral measures of image allocation.","parametric models (e.g., regression)",The study used ANOVA to analyze the effects of reliability and blame on trust measures.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers manipulated the reliability of the automated system and the attribution of blame for errors in feedback messages to influence trust.,High reliability led to greater behavioral and subjective trust; internal blame led to more positive perceptions of the system's integrity and benevolence than pseudo-external blame.,"The high rate of source failure in pseudo-external conditions and blame failure in internal blame conditions may have resulted from the explicit mention of developers and the implicit nature of internal blame, respectively. The effect of blame on perceived integrity was nearly three times the size of the effect of reliability.","The study found that users distinguish between an automated system and its developers when evaluating trust, with internal blame leading to more positive perceptions of the system than pseudo-external blame.","The automated system ostensibly identified images of vehicles as 'Dangerous' or 'Not Dangerous', while the human participant manually inspected some images and allocated the remaining images to the automated system.",Mann-Whitney U; Chi-squared; Fisher's exact test; Kruskal-Wallis; ANOVA; ANOVA,"The study used a variety of statistical tests to analyze the data. A Mann-Whitney U-test was used to compare perceived reliability between groups. Chi-Square tests, Fisher Exact Tests, and Kruskal-Wallis Tests were used to ensure demographic similarity between groups. ANOVA was used to analyze the effects of reliability and blame on total allocation, first calibration, trust in automation, ability, integrity, and benevolence. A MANOVA was used to analyze the combined effects of reliability and blame on the perceived trustworthiness characteristics (ability, integrity, and benevolence). Post-hoc pairwise comparisons were done using Tukey's HSD.",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-accuracy; Robot-verbal-communication-content,,"The study manipulated the reliability of the automated system, which directly impacted its accuracy in identifying images, thus influencing task performance metrics. This is categorized as 'Robot-accuracy'. The study also manipulated the content of the feedback messages, specifically the attribution of blame for errors (internal, pseudo-external, or external). This is categorized as 'Robot-verbal-communication-content' because it changes what information is communicated to the user. The results showed that both the reliability (accuracy) and the blame attribution (communication content) impacted trust. Specifically, high reliability led to greater trust, and internal blame led to more positive perceptions of the system than pseudo-external blame. Therefore, both 'Robot-accuracy' and 'Robot-verbal-communication-content' are listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1145/3322276.3322349,https://dl.acm.org/doi/10.1145/3322276.3322349,"To promote safe and effective human-computer interactions, researchers have begun studying mechanisms for “trust repair” in response to automated system errors. The extent to which users distinguish between a system and the system’s developers may be an important factor in the efﬁcacy of trust repair messages. To investigate this, we conducted a 2 (reliability) x 3 (blame) between-group, factorial study. Participants interacted with a high or low reliability automated system that attributed blame for errors internally (“I was not able...”), pseudo-externally (“The developers were not able...”), or externally (“A third-party algorithm that I used was not able...”). We found that pseudo-external blame and internal blame inﬂuenced subjective trust differently, suggesting that the system and its developers represent distinct trustees. We discuss the implications of our ﬁndings for the design and study of humanautomation trust repair."
"Jessup, Sarah A.; Schneider, Tamera R.; Alarcon, Gene M.; Ryan, Tyler J.; Capiola, August",The Measurement of the Propensity to Trust Automation,2019,1,55,55,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed propensity to trust surveys, then played a modified investor/dictator game with a NAO robot, believing it was their partner. They were told the robot was making decisions based on self-preservation algorithms. Participants completed a perceived trustworthiness questionnaire after the practice rounds, and their loan amounts in the first round were used as a measure of behavioral trust.","Participants played a modified investor/dictator game where they loaned money to a NAO robot, who was the 'runner' in a maze task. The robot always returned the promised amount of money.",Nao,Humanoid Robots; Expressive Robots,Research; Social,Game,Economic Game,minimal interaction,"Participants interacted with the robot through a computer interface, observing its actions in a virtual maze.",simulation,The interaction was presented through a computer simulation of a maze environment.,physical,"Participants were introduced to a physical Nao robot, but the interaction was mediated through a computer interface.",pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the participant's behavior.,Behavioral Measures; Questionnaires; Custom Scales,Trust in Automated Systems Scale; Propensity to Trust Scales,,"Trust was measured using questionnaires, a custom scale, and behavioral data from the investment game.","parametric models (e.g., regression)",The study used regression and discriminant function analysis to examine the relationship between propensity to trust and trust behaviors.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the framing of the task by telling participants the robot was using self-preservation algorithms, and the robot's behavior was pre-programmed to always return the promised amount of money. The study also manipulated the wording of the propensity to trust scale.",The adapted propensity to trust scale was a better predictor of both perceived trustworthiness and behavioral trust compared to the original scale. The robot's consistent behavior likely influenced trust.,"The adapted propensity to trust scale was more reliable and predictive of both perceived trustworthiness and behavioral trust than the original scale. The CPRS did not predict perceived trustworthiness or behavioral trust, which is inconsistent with some past research.",Context-specific measures of propensity to trust automation are more reliable and predictive of perceived trustworthiness and behavioral trust than general measures.,"The robot acted as a 'runner' in a virtual maze, and the human participant acted as a 'banker', loaning money to the robot. The robot always returned the promised amount of money. The human participant chose the loan amount.",Pearson correlation; spearman's rank-order correlations; Discriminant analysis; hierarchical regression,"The study used Pearson's product-moment correlations to examine the relationship between propensity to trust measures and perceived trustworthiness. Spearman's rank-order correlations were used to assess the relationship between propensity to trust measures and behavioral trust (loan amounts). Discriminant analyses were employed to determine if propensity to trust measures could predict the category of loan amount (small, medium, or large). Hierarchical regression was used to test if the adapted propensity to trust scale better predicted perceived trustworthiness and behavioral trust compared to other scales, controlling for the other measures.",,,,,,,http://link.springer.com/10.1007/978-3-030-21565-1_32,"In recent years, there has been a focus on not just how people work with automation, but how humans interact with and rely on automation in human-automation teams. Few studies have examined how propensity to trust in automation inﬂuences trust behaviors. Of the published studies, there are inconsistencies in how propensity to trust automation is conceptualized and thus measured. Research on attitudes and intentions has discerned that reliability and validity of measures can be increased by using language that is more speciﬁc for a particular context, which reduces respondent ambiguity and increases the ability to predict behavior. This study examined traditional measures of propensity to trust automation, and whether adapting measures could enhance our ability to predict beliefs about automation trustworthiness (perceived trustworthiness) and behaving in a trusting manner when interacting with automation (behavioral trust). Participants (N = 55) completed three propensity to trust in automation surveys including Propensity to Trust Technology, an adapted propensity measure, and the Complacency-Potential Rating Scale. Participants played a modiﬁed investor/dictator game, where people thought they were teaming with a NAO robot. This study demonstrated that compared to a more generally-worded measure, the context-speciﬁc measure of propensity to trust automation was more reliable and better predicted perceived trustworthiness and behavioral trust. Furthermore, the adapted measure was the only signiﬁcant predictor of both beliefs about the trustworthiness of the automation and the actual trusting behaviors of participants. By decreasing the ambiguity of measures of propensity to trust automation, the reliability and predictive validity are increased."
"Johanson, Deborah; Ahn, Ho Seok; Goswami, Rishab; Saegusa, Kazuki; Broadbent, Elizabeth",The Effects of Healthcare Robot Empathy Statements and Head Nodding on Trust and Satisfaction: A Video Study,2023,1,100,100,0,No participants were excluded,Online Crowdsourcing,mixed design,"Participants viewed an initial video of a robot interacting with a patient, then completed a questionnaire. They were then randomized to view one of four videos with different robot empathy and head nodding conditions, and completed the same questionnaire again.",Participants watched videos of a robot interacting with a patient and answered questionnaires about their perceptions of the robot.,Nao,Humanoid Robots,Care; Research; Social,Social,Social Perception,passive observation,Participants passively observed the robot through video recordings.,media,Participants watched video recordings of the robot interaction.,physical,"The robot was a physical robot, but participants only viewed it in videos.",pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires; Custom Scales,Jian et al. Trust Scale,,Trust was measured using a questionnaire and a custom scale.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's verbal empathy statements and head nodding behavior were manipulated to influence perceptions of empathy and trust.,"Verbal empathy statements increased trust, while head nodding had no significant effect on trust.","The study found that verbal empathy statements significantly increased trust and satisfaction, while head nodding had no significant effect, which was unexpected given previous research on human head nodding.","The use of empathetic statements by a healthcare robot significantly increased participant perceptions of robot empathy, trust, and satisfaction, and reduced robot distrust.","The robot acted as a nurse, asking the patient about their health and offering to book a health check. The human participant watched the video and answered questionnaires.",ANOVA; loglinear analysis,"Four 2x2x2 ANOVAs were conducted with time-point as a repeated measures variable and head-nodding and empathy statements as between subjects' factors. A three-way loglinear analysis was used to analyze the question of whether participants would want to interact with the robot face-to-face, with head-nodding and empathetic-statements as factors, at each of the two time-points.",TRUE,Robot-verbal-communication-content; Robot-nonverbal-communication,Robot-verbal-communication-content,Robot-nonverbal-communication,"The study manipulated the robot's verbal communication by having it use either empathetic or neutral statements, which falls under 'Robot-verbal-communication-content'. The robot's head nodding behavior was also manipulated, which is a physical movement and thus falls under 'Robot-nonverbal-communication'. The results showed that the use of empathetic statements significantly increased trust, while head nodding had no significant effect on trust. Therefore, 'Robot-verbal-communication-content' is listed as a factor that impacted trust, and 'Robot-nonverbal-communication' is listed as a factor that did not impact trust.",10.1145/3549534,https://dl.acm.org/doi/10.1145/3549534,"Clinical empathy has been associated with many positive outcomes, including patient trust and satisfaction. Physicians can demonstrate clinical empathy through verbal statements and non-verbal behaviors, such as head nodding. The use of verbal and non-verbal empathy behaviors by healthcare robots may also positively affect patient outcomes. The current study examined whether the use of robot verbal empathy statements and head nodding during a video recorded interaction between a healthcare robot and patient improved participant trust and satisfaction. One hundred participants took part in the experiment, online through Amazon Mechanical Turk. They were randoimnized to watch one of four videos depicting an interaction with a `patient' and a Nao robot that (1) either made empathetic or neutral statements, and (2) either nodded its head when listening to the patient or did not. Results showed that the use of empathetic statements by the healthcare robot significantly increased participant perceptions of robot empathy, trust and satisfaction, and reduced robot distrust. No significant findings were revealed in relation to robot head nodding. The positive effects of empathy statements support the model of Robot-Patient Communication, which theorizes that robot use of recommended clinical empathy behaviors can improve patient outcomes. The effects of healthcare robot nodding behavior needs to be further investigated."
"Kaber, David B.; Onal, Emrah; Endsley, Mica R.","Design of automation for telerobots and the effect on performance, operator situation awareness, and subjective workload",2000,1,10,10,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were trained on a telerobot simulation across 5 days, starting with direct teleoperation and progressing to different levels of automation. They performed a materials handling task, with automation failures introduced in some trials. Situation awareness was measured using SAGAT queries, and workload was assessed using NASA-TLX.","Participants controlled a simulated telerobot to perform safety tests on plutonium storage containers, including removing containers, weighing them, and leak testing.",Unspecified,Industrial Robot Arms,Industrial; Research,Manipulation,Remote Manipulation,minimal interaction,Participants interacted with the robot through a simulation interface.,simulation,The interaction was conducted in a high-fidelity simulation environment.,simulated,The robot was a simulated representation within the virtual environment.,shared control (fixed rules),"The robot operated with varying levels of autonomy, from direct teleoperation to full automation, with fixed rules for shared control.",Questionnaires; Performance-Based Measures,NASA Task Load Index (NASA-TLX),Performance Metrics,Trust was assessed using performance metrics and the NASA-TLX questionnaire.,no modeling,Trust was not modeled computationally in this study.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The level of automation was directly manipulated, which influenced the level of human control and the difficulty of the task, and feedback was provided through error messages.","Trust was not directly measured, but the study found that lower levels of automation led to better performance during failure modes, suggesting a potential link between control and trust.","The study found that higher levels of automation improved performance under normal conditions but reduced performance during failure modes. There was a trade-off between performance and situation awareness, with higher automation leading to lower SA.","Intermediate levels of automation maintain operator situation awareness and improve performance during automation failures, compared to full automation.","The robot performed a simulated nuclear materials handling task, including moving containers and performing tests. The human controlled the robot through a graphical interface, with varying levels of automation.",ANOVA; Tukey HSD,"The study used ANOVA to analyze the effects of Level of Automation (LOA) on several dependent variables: time-to-container completion, number of collisions, and time-to-system recovery from failure. Tukey's HSD post-hoc tests were used to determine significant differences between specific LOA conditions when the ANOVA showed a significant main effect. Additionally, a correlation analysis was conducted between Level 3 SA and overall NASA-TLX scores.",TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,,"The study explicitly manipulated the level of automation (LOA), which directly corresponds to 'Robot-autonomy'. The different LOAs ranged from full manual control with some computer assistance (Action Support) to full computer control (Full Automation), with intermediate levels involving varying degrees of shared control. This manipulation of autonomy is described throughout the paper, particularly in the 'Experimental Design' section where the five LOAs are defined. The study also implicitly manipulated 'Task-complexity' as the different levels of automation changed the cognitive demands on the participants. For example, under Action Support, participants had to manually control the robot, which is more complex than simply monitoring the robot under Full Automation. The results section shows that the level of automation (Robot-autonomy) impacted performance during failure modes, with lower levels of automation leading to better performance, suggesting a link between control and trust. The study did not find any factors that did not impact trust, as the study did not directly measure trust, but rather inferred it from performance and workload measures.",10.1002/1520-6564(200023)10:4<409::AID-HFM4>3.0.CO;2-V,https://onlinelibrary.wiley.com/doi/10.1002/1520-6564(200023)10:4<409::AID-HFM4>3.0.CO;2-V,"In this article we review and assess human-centered level of automation (LOA), an alternate approach to traditional, technology-centered design of automation in dynamic-control systems. The objective of human-controlled LOA is to improve human-machine performance by taking into account both operator and technological capabilities. Automation literature has shown that traditional automation can lead to problems in operator situation awareness (SA) due to the out-of-the (control) loop performance problem, which may lead to a negative impact on overall systems performance. Herein we address a standing paucity of research into LOA to deal with these problems. Various schemes of generic control system function allocations were developed to establish a LOA taxonomy. The functions allocated to a human operator, a computer, or both, included monitoring system variables, generating process plans, selecting an “optimal” plan and implementing the plan. Five different function allocation schemes, or LOAs, were empirically investigated as to their usefulness for enhancing telerobot system performance and operator SA, as well as reducing workload. Human participants participated in experimental trials involving a high fidelity, interactive simulation of a telerobot performing nuclear materials handling at the various LOAs. Automation failures were attributed to various simulated system deficiencies necessitating operator detection and correction to return to functioning at an automated mode. Operator performance at each LOA, and during the failure periods, was evaluated. Operator SA was measured using the Situation Awareness Global Assessment Technique, and perceived workload was measured using the NASA-Task Load Index. Results demonstrated improvements in human-machine system performance at higher LOAs (levels involving greater computer control of system functions) along with lower operator subjective workload. However, under the same conditions, operator SA was reduced for certain types of system problems and reaction time to, and performance during, automation failures was substantially lower. Performance during automation failure was best when participants had been functioning at lower, intermediate LOAs (levels involving greater human control of system functions). © 2000 John Wiley & Sons, Inc."
"Kaltenbach1, Elizabeth; Dolgov, Igor",On the Dual Nature of Transparency and Reliability: Rethinking Factors that Shape Trust in Automation,2017,1,72,72,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants completed a training block followed by two experimental blocks with different transparency levels, and completed trust questionnaires before and after each block.","Participants controlled a simulated coffee production machine, adjusting temperature and pressure to maximize output.",Unspecified,Other,Industrial,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated system through a computer interface.,simulation,The interaction was through a computer simulation of a coffee production machine.,simulated,The robot was represented as a simulated system on a computer screen.,shared control (fixed rules),"The system automated some aspects of the task, but the user controlled the pressure and release of water.",Questionnaires,Trust in Automated Systems Scale; Human-Computer Trust Scale/Questionnaire (HCT/HCTM),,Trust was measured using two questionnaires.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers manipulated the reliability of the system and the transparency of the system display to influence trust.,"High transparency combined with low reliability led to lower trust, while low transparency masked the effects of low reliability.","The study found that high transparency combined with low reliability led to lower trust, which is contrary to some previous findings. The interaction between transparency and reliability was a key finding.","High transparency revealed low reliability, leading to lower trust, while low transparency masked the effects of low reliability.","The simulated system controlled the coffee production process, and the human monitored the temperature and pressure, releasing the water when both were in the optimal range.",ANOVA; ANCOVA; ANCOVA,"The study used several statistical tests. First, two ANOVAs were conducted to investigate order effects on trust scale measures. Then, a repeated measures ANOVA and correlation analyses were used to determine covariates from the PANAS-X scores. Finally, a MANCOVA was used to analyze the groupings of the Human Computer Trust Scale, and two ANCOVAs were used to analyze the positive and negative groupings of the Trust in Automated Systems Scale. These tests examined the effects of reliability and transparency on trust, while controlling for affect.",TRUE,Robot-interface-design; Robot-accuracy,Robot-interface-design; Robot-accuracy,,"The study explicitly manipulated two factors: the transparency of the system display (single-line vs. multi-line), which is categorized as 'Robot-interface-design' because it involves changes to the interactive elements of the system's display, and the reliability of the system (65% vs. 95%), which is categorized as 'Robot-accuracy' because it directly affects the success rate of the system's actions. The paper states, 'In the current experiment, transparency of the system was manipulated via varying the number of lines of information in the system history display... Reliability was manipulated in stage 4 simply by having the system execute the participants' commands with either 95% or 65% of the time.' The results showed that both of these factors impacted trust. Specifically, high transparency combined with low reliability led to lower trust, while low transparency masked the effects of low reliability. The paper states, 'When system reliability was low, high transparency allowed participants to peer into the inner workings of the system and potentially identify poor reliability in stage 4 automation, whereas low transparency may have masked this fact.' This indicates that both 'Robot-interface-design' and 'Robot-accuracy' influenced trust levels.",10.1177/1541931213601558,http://journals.sagepub.com/doi/10.1177/1541931213601558,"Prior literature has found that increasing system reliability and transparency can positively impact operators’ trust of automated systems; however, these factors are typically confounded. In the present study, we separated these factors by manipulating different stages of automation. Participants engaged in a simulated coffee manufacturing task using an interface with differing levels of reliability (65% or 95%) and transparency (one line or multiple lines of system display). The Human Computer Trust Scale (HCTS) and the Trust in Automated Systems Scale (TAS) were used to measure trust. When examining scores on TAS items with a positive-valence, we novelly observed that transparency interacted with reliability, such that high transparency and low reliability negatively impacted trust in the system. Alternatively, trust was not negatively affected by poor reliability when transparency was low, due to trivial cost of corrective behaviors that compensated for poor reliability and lack of system history understanding by the operators."
"Kamaraj, Amudha V.; Lee, Joonbum; Parker, Jah’inaya; Domeyer, Joshua E.; Liu, Shu-Yuan; Lee, John D.",Bimodal Trust: High and Low Trust in Vehicle Automation Influence Response to Automation Errors,2023,1,24,23,1,1 participant was excluded because they did not respond to the automation error by pressing the brake pedal within the intersection bounds,Controlled Lab Environment,within-subjects,"Participants completed practice drives, then two manual drives, followed by three automated drives with reliable automation, and a final automated drive with an error. They were asked to press the brake or gas pedal if they felt the automation was too fast or slow, respectively. Subjective trust was assessed via surveys after each automated drive.",Participants monitored automated driving and pressed pedals to indicate if the automation was too fast or slow. They also responded to an automation error by pressing the brake pedal.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the automation through pedal presses and monitoring the driving simulation.,simulation,Participants experienced the interaction through a driving simulator.,simulated,The robot was a simulated vehicle within the driving simulator.,fully autonomous (limited adaptation),The vehicle operated autonomously but with limited adaptation to the user.,Behavioral Measures; Questionnaires,Muir's Trust Questionnaire,Performance Metrics,Trust was measured using pedal presses and subjective questionnaires.,"parametric models (e.g., regression)",Regression mixture models were used to analyze the relationship between pedal presses and response time.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The automation's behavior was manipulated by varying the driving style (conservative, moderate, aggressive) and introducing an error where the vehicle failed to stop. This was intended to influence trust by exposing participants to reliable automation followed by an error.","The study found that drivers with higher trust (less pedal presses) were slower to respond to the automation error, indicating overreliance. Lower trust (more pedal presses) was associated with quicker responses.","The study revealed a bimodal distribution of response times to the automation error, suggesting that drivers responded differently based on their trust levels. The study also found that subjective trust ratings were higher for late responders.","Drivers with high trust in reliable automation were slower to respond to automation errors, indicating overreliance.","The robot (simulated vehicle) drove autonomously, and the human monitored the driving and pressed the brake or gas pedal if they felt the automation was too fast or slow. The human also had to respond to an automation error by pressing the brake pedal.",regression mixture models; ANOVA; t-test,"The study used regression mixture models to identify subgroups in the data based on response time to automation errors and pedal presses (brake and gas). Linear models were then applied to each cluster to examine the relationship between pedal press time and response time. Finally, a Welch Two Sample t-test was used to compare the mean subjective trust ratings between early and late responders.",TRUE,Robot-accuracy; Robot-task-strategy,Robot-accuracy,Robot-task-strategy,"The study manipulated 'Robot-accuracy' by introducing an automation error where the vehicle failed to stop at a stop-controlled intersection. This directly impacted the robot's performance on the task and was intended to influence trust by exposing participants to reliable automation followed by an error. The study also manipulated 'Robot-task-strategy' by varying the driving style (conservative, moderate, aggressive). While this manipulation was intended to influence trust, the results showed that the driving style did not significantly impact the response time to the error, and thus did not directly impact trust. The key finding was that drivers with higher trust (less pedal presses) were slower to respond to the automation error, indicating overreliance. The different driving styles were implemented by changing the speed profiles of the vehicle, which is a change in the task strategy, but not the success rate of the task. The error was the only factor that directly impacted the success rate of the task.",10.1177/21695067231196244,https://journals.sagepub.com/doi/10.1177/21695067231196244,"Extended exposure to reliable automation may lead to overreliance as evidenced by poor responses to auto-mation errors. Individual differences in trust may also influence responses. We investigated how these factors affect response to automation errors in a driving simulator study comprised of stop-controlled and uncon-trolled intersections. Drivers experienced reliable vehicle automation during six drives where they indicated if they felt the automation was going too slow or too fast by pressing the accelerator or brake pedal. Engage-ment via pedal presses did not affect the automation but offered an objective measure of trust in automation. In the final drive, an error occurred where the vehicle failed to stop at a stop-controlled intersection. Drivers’ response to the error was inferred from brake presses. Mixture models showed bimodal response times and revealed that drivers with high trust were less likely to respond to automation errors than drivers with low trust."
"Kaniarasu, Poornima; Steinfeld, Aaron M.",Effects of blame on trust in human robot interaction,2014,1,12,12,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants remotely operated a robot through a course with different blame attribution strategies. They experienced three different robot personalities that assigned blame to the user, the robot itself, or the team after errors. Participants completed questionnaires after each run and ranked the personalities based on trust and likeability.","Participants were asked to remotely operate a robot through a course, following direction labels and avoiding obstacles.",iRobot ATRV-JR,Mobile Robots; UGVs,Research,Navigation,Remote Navigation,minimal interaction,"Participants interacted with the robot through a user interface, controlling its movement remotely.",simulation,The interaction was through a user interface displaying video feeds and a map of the course.,physical,"The robot was a physical robot, but the interaction was remote through a user interface.",shared control (fixed rules),"The robot had two modes: fully autonomous and assisted, where the user had manual control.",Questionnaires; Real-time Trust Measures,Muir's Trust Questionnaire,Performance Metrics,"Trust was measured using a questionnaire and real-time user input, with performance metrics also collected.",no modeling,Trust was not modeled computationally; the study used statistical analysis.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's blame attribution strategy was manipulated, with the robot assigning blame to the user, itself, or the team after errors. This was intended to affect user trust.",The introduction of blame attribution by the robot lowered user trust. Self-blame boosted likeability but did not increase trust.,"Participants liked the self-blaming robot the most, but did not necessarily trust it more. Some participants swore at the robot when it blamed them, while others thanked the robot when it gave them credit.","The introduction of blame attribution by the robot lowers user trust, regardless of the target of the blame.","The robot moved through a course, reading direction labels and avoiding obstacles. The human remotely controlled the robot, ensuring it followed the correct path and intervened when necessary.",ANOVA; Tukey HSD,"The study used a two-way ANOVA to analyze the effect of blame attribution on user trust, comparing the current study's data with baseline data from previous studies. Post hoc comparisons were conducted using Tukey's HSD test to determine specific differences between the groups. Additionally, an ANOVA was used to analyze the number of wrong turns across different blame conditions and compared to baseline data.",TRUE,Robot-verbal-communication-content; Robot-social-attitude,Robot-verbal-communication-content,Robot-social-attitude,"The study manipulated the robot's blame attribution strategy, which is a form of verbal communication content. The robot assigned blame to the user, itself, or the team after errors, which directly changes the content of what the robot communicates. This is why 'Robot-verbal-communication-content' is chosen. The study also manipulated the robot's personality by having it express different blame attribution strategies (user-blame, self-blame, team-blame), which can be seen as a change in social attitude. This is why 'Robot-social-attitude' is chosen. The results showed that the blame attribution strategy (content of the robot's communication) impacted user trust, as the introduction of blame lowered trust compared to a baseline. However, while the self-blaming robot was liked more, it did not significantly increase trust compared to the other blame conditions, indicating that the change in social attitude did not have a significant impact on trust. Therefore, 'Robot-verbal-communication-content' is listed as a factor that impacted trust, and 'Robot-social-attitude' is listed as a factor that did not impact trust.",10.1109/ROMAN.2014.6926359,http://ieeexplore.ieee.org/document/6926359/,"Trust in automation is a crucial ingredient for successful human robot interaction. Both human related and robot related factors influence the user’s trust on the robot and it is challenging to characterize each of these factors and study how they affect human trust. In this study we try to understand how blame attribution after an error impacts user trust. Three different robot personalities were implemented, each assigning blame to either of the user, the robot itself, or the human-robot team. Our study results confirm that blame attribution impacts human trust in robots."
"Karli, Ulas Berk; Cao, Shiye; Huang, Chien-Ming","""What If It Is Wrong"": Effects of Power Dynamics and Trust Repair Strategy on Trust and Compliance in HRI",2023,1,39,32,7,"1 participant was excluded for failing the robot error check, 5 participants were excluded for providing incorrect robot authority rating, 1 participant was excluded for unspecified reasons",Controlled Lab Environment,between-subjects,"Participants completed a personality questionnaire, then completed two cooking trials with a robot, one with and one without verbal trust repair. After each trial, participants filled out a trust questionnaire. Finally, participants were interviewed about their experience.","Participants collaborated with a robot in a cooking task, where the robot provided ingredients and instructions.",UR5,Industrial Robot Arms,Research,Manipulation,Cooking/Food Preparation,minimal interaction,Participants interacted with the robot through verbal instructions and receiving ingredients.,real-world,Participants interacted with a physical robot in a lab setting.,physical,A physical robot was used in the study.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions without adapting to the user.,Behavioral Measures; Questionnaires,Trust Perception Scale - HRI; Big Five Inventory Scale,,Trust was measured using a questionnaire and behavioral measures.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's role (supervisor or subordinate) and trust repair strategy (promise or explanation) were manipulated to influence trust.,Promise was more effective at repairing trust than explanation. Supervisor robots with verbal trust repair were perceived as more trustworthy than subordinate robots with verbal trust repair. Participants were more likely to comply with the supervisor robot even when it was wrong.,Participants were more likely to verbally react to errors from the supervisor robot. Some participants complied with the supervisor robot even when they noticed the error.,"Promise is a more effective trust repair strategy than explanation, and supervisor robots with verbal trust repair are perceived as more trustworthy than subordinate robots with verbal trust repair.","The robot provided ingredients for a cooking task, either by giving instructions (supervisor) or waiting for requests (subordinate). The human participant followed the recipe and used the ingredients provided by the robot.",ANOVA; t-test; likelihood ratio test; Logistic regression,"The study used a two-way ANOVA to examine the main effects of power dynamics and trust repair strategy on user trust and changes in trust. Welch's t-tests were used to compare means of robot authority ratings and perceived trust between groups. Likelihood ratio tests were used in contingency analysis to explore the effect of power dynamics on improper compliance and user reactions to errors. Finally, a binary logistic regression model was used to analyze the effect of power dynamics and trust repair strategy on positive social reactions to the robot's mitigation attempt.",TRUE,Robot-verbal-communication-content; Robot-autonomy,Robot-verbal-communication-content,Robot-autonomy,"The study manipulated the robot's verbal communication content by varying the trust repair strategy (promise vs. explanation) after an error. This is explicitly stated in the 'Trust Repair Strategy Manipulation' section: 'In the experimental trial, the robot also attempts to verbally mitigate the lost trust due to its error through promise or explanation.' The robot's role was also manipulated to create different power dynamics (supervisor vs. subordinate), which is a manipulation of the robot's autonomy. The 'Power Dynamics Manipulation' section describes how the supervisor robot provides step-by-step instructions and takes initiative, while the subordinate robot waits for requests. The results showed that the verbal communication content (promise vs. explanation) impacted trust, as promise was more effective at repairing trust than explanation. However, the power dynamic manipulation (supervisor vs. subordinate) alone did not significantly impact user perceived trust in the robot during the control trial, indicating that the robot's autonomy did not directly impact trust on its own.",10.1145/3568162.3576964,https://dl.acm.org/doi/10.1145/3568162.3576964,"Robotic systems designed to work alongside people are susceptible to technical and unexpected errors. Prior work has investigated a variety of strategies aimed at repairing people’s trust in the robot after its erroneous operations. In this work, we explore the efect of post-error trust repair strategies (promise and explanation) on people’s trust in the robot under varying power dynamics (supervisor and subordinate robot). Our results show that, regardless of the power dynamics, promise is more efective at repairing user trust than explanation. Moreover, people found a supervisor robot with verbal trust repair to be more trustworthy than a subordinate robot with verbal trust repair. Our results further reveal that people are prone to complying with the supervisor robot even if it is wrong. We discuss the ethical concerns in the use of supervisor robot and potential interventions to prevent improper compliance in users for more productive human-robot collaboration."
"Kaur, Kanwaldeep; Rampersad, Giselle",Trust in driverless cars: Investigating key factors influencing the adoption of driverless cars,2018,1,101,101,0,No participants were excluded,Online Crowdsourcing,,Participants completed an online survey with 10 questions and sub-questions about driverless car adoption.,Participants answered questions about their willingness to adopt driverless cars in various scenarios.,Unspecified,Autonomous Vehicles,Other: Transportation,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about driverless cars and scenarios.,media,The interaction was based on text descriptions of driverless cars.,hypothetical,"The robot was only described in text, with no visual representation.",fully autonomous (limited adaptation),"The driverless car was described as operating autonomously, but the level of adaptation was not fully specified.",Questionnaires,,,Trust was measured using a questionnaire.,no modeling,Trust was not modeled computationally.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,"The study did not manipulate any specific factors related to trust, but rather assessed trust through a survey.","The study found that performance expectation, reliability, trust, privacy and security all positively influence the adoption of driverless cars.","The study found that people were more willing to adopt driverless cars in closed environments, for parking, and on highways where drivers can take over control. They were less willing to adopt driverless cars with no driver controls, in areas with high pedestrian traffic, and for transporting children.","The study found that performance expectancy, reliability, security, privacy, and trust all positively influence the adoption of driverless cars.","The robot (driverless car) was described as transporting people, and the human participant was asked to respond to questions about their willingness to use the car in different scenarios.",Multilevel Model,"The study used confirmatory factor analysis (CFA) to determine the impact of various factors on the adoption of driverless cars. Prior to CFA, data was checked for normality using skewness and kurtosis tests. Reliability was evaluated using coefficient alpha, and construct reliabilities were calculated using standardized item loadings and error measurement. Convergent validity was also assessed. The CFA was used to test the hypotheses about the influence of performance expectation, reliability, trust, privacy, and security on the adoption of driverless cars.",FALSE,,,,"The study did not manipulate any factors. The study was a survey that assessed the influence of performance expectation, reliability, trust, privacy, and security on the adoption of driverless cars. The participants were asked about their willingness to adopt driverless cars in different scenarios, but these scenarios were not manipulated as part of the study design. Therefore, no factors were manipulated.",10.1016/j.jengtecman.2018.04.006,https://www.sciencedirect.com/science/article/pii/S0923474817304253,"Driverless cars are seen as one of the key disruptors in the next technology revolution. However, the main barrier to adoption is the lack of public trust. The purpose of this study is to investigate the key factors influencing the adoption of driverless cars. Drawing on quantitative evidence, the study found that the ability of the driverless car to meet performance expectations and its reliability were important adoption determinants. Significant concerns included privacy (autonomy, location tracking and surveillance) and security (from hackers). The paper provides implications for firms developing the next generation of car features and early implementation sites."
"Kelch, Yngve; Kluge, Annette; Kunold, Laura",Would you Trust a Robot that Distrusts you?,2024,1,194,194,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of six conditions. The robot verbally disclosed either high trust, distrust, or no trust towards the participants. After two planned robotic errors, the participants either received an apology or not. Trust was assessed at four time points during the simulation.",Participants sorted boxes together with a humanoid robot in a virtual warehouse environment.,Unspecified,Humanoid Robots,Research,Manipulation,Sorting/Arranging,minimal interaction,Participants interacted with a virtual robot in a simulated environment.,simulation,The interaction took place in a virtual warehouse environment.,simulated,The robot was a digital representation of a humanoid robot.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Multidimensional Measures; Questionnaires,Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using the MDMT questionnaire at four time points.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's verbal trust disclosure (high trust, distrust, or no trust) was manipulated to influence human trust.","Disclosing distrust decreased human trust, while disclosing high trust did not significantly affect trust.","The study found that distrust disclosure significantly decreased trust, while high trust disclosure did not significantly increase trust, possibly due to a ceiling effect.","Disclosing distrust by a robot negatively impacts human trust, while disclosing high trust does not significantly differ from no disclosure.","The robot picked boxes, and the human checked the serial numbers and confirmed or rejected the robot's selections.",ANOVA; t-test,A one-way ANOVA was used to check for baseline differences in trust and sympathy before the trust disclosure manipulation. Independent t-tests were then used to compare the mean trust and sympathy ratings between the high trust and distrust disclosure groups with the control group after the trust disclosure manipulation. The purpose was to determine the impact of the robot's trust disclosure on human trust and sympathy.,TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the robot's verbal communication by having it disclose either high trust ('I trust you'), distrust ('I do not trust you'), or no trust (control condition) towards the participant. This manipulation directly altered the content of the robot's verbal communication, making 'Robot-verbal-communication-content' the most appropriate category. The results showed that distrust disclosure significantly decreased human trust, while high trust disclosure did not significantly impact trust. Therefore, 'Robot-verbal-communication-content' is listed as a factor that impacted trust. There were no other factors manipulated in the study.",10.1145/3610978.3640757,https://dl.acm.org/doi/10.1145/3610978.3640757,"Trust is an important antecedent to successful human-robot collaboration and is conceptualized as a reciprocal process. The current study evaluated whether a robot could influence participants’ trust levels through the disclosure of its own trust. We conducted a 3 (trust disclosure) × 2 (trust repair) betweensubjects experiment in which N = 194 participants sorted boxes together with a humanoid robot that either disclosed distrust, high trust, or no trust information (control condition) towards the participant in a virtual warehouse environment. Additionally, participants either received an apology or did not (control) in response to a subsequent robotic error. Our analyses so far reveal that the disclosure of distrust from a robot towards a human indeed negatively impacts human trust in the robot. However, the disclosure of high trust did not significantly differ from no disclosure. Our findings contribute to a better understanding of reciprocal trust calibration and highlight the importance to consider trust as a reciprocal interaction. Moreover, our findings illustrate that disclosing distrust by a robot might serve as a promising strategy to proactively mitigate overtrust."
"Kessler, Theresa T.; Larios, Cintya; Walker, Tiffani; Yerdon, Valarie; Hancock, P. A.",A Comparison of Trust Measures in Human–Robot Interaction Scenarios,2017,1,58,58,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants completed demographic and NARS questionnaires, then the Trust in Automation Scale and the Human Robot Trust Scale (pre). They were trained on the task, which involved assisting the robot in scanning the area for threats. The robot either malfunctioned or did not, and provided high or low levels of information. Participants then completed the trust scales again.",Participants assisted a robot in scanning an area for threats.,Lego Mindstorms EV3,Mobile Robots,Research,Supervision,Monitoring,minimal interaction,"Participants assisted the robot with its task, but there was no physical touch.",real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,shared control (fixed rules),"The robot operated independently but with fixed rules, and required assistance from the participant when it encountered problems.",Questionnaires,Trust in Automation Scale (TAS); Human-Robot Trust Scale/Questionnaire (HRTS/HRITS); Negative Attitude towards Robots Scale (NARS),,Trust was measured using two questionnaires.,no modeling,The study did not use any computational models of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers manipulated the presence of a robot malfunction, the level of information provided to the participant, and the level of autonomy of the robot when a malfunction occurred, to see how these factors affected trust.","The Trust in Automation Scale showed higher trust when no malfunction was present, while the Human Robot Trust Scale showed higher trust when high levels of information were provided during a malfunction. The two scales yielded conflicting results.","The two trust scales yielded conflicting results, with the Trust in Automation Scale showing higher trust when no malfunction was present, and the Human Robot Trust Scale showing higher trust when high levels of information were provided during a malfunction. This suggests that the scales may be measuring different constructs.",The Human Robot Trust Scale and the Trust in Automation Scale measure different constructs and cannot be used interchangeably.,"The robot moved around a defined area, scanning for threats, and notified the participant of its status. The human participant assisted the robot if it encountered problems.",ANCOVA,"Two separate analyses of covariance (ANCOVA) were performed to examine the effects of malfunction presence, transparency levels, and autonomy levels on changes in trust scores, as measured by the Trust in Automation Scale and the Human Robot Trust Scale. The Negative Attitudes Toward Robots Scale (NARS) was used as a covariate in both analyses. The first ANCOVA used the Trust in Automation Scale as the dependent variable, and the second ANCOVA used the Human Robot Trust Scale as the dependent variable. The purpose was to determine how the manipulated factors influenced the two different trust scales.",TRUE,Robot-verbal-communication-content; Robot-accuracy; Robot-autonomy,Robot-verbal-communication-content; Robot-accuracy,Robot-autonomy,"The study manipulated three factors: the level of information provided by the robot (transparency), the presence or absence of a robot malfunction (accuracy), and the level of autonomy the robot had when a malfunction occurred. The level of information provided by the robot is categorized as 'Robot-verbal-communication-content' because it directly relates to the content of the information communicated to the participant about the robot's status. The presence or absence of a malfunction is categorized as 'Robot-accuracy' because it directly impacts the robot's performance on the task. The level of autonomy is categorized as 'Robot-autonomy' because it relates to the robot's decision-making authority when a malfunction occurred. The results showed that the level of information and the presence of a malfunction impacted trust, as measured by the two scales. Specifically, the Trust in Automation Scale showed higher trust when no malfunction was present, while the Human Robot Trust Scale showed higher trust when high levels of information were provided during a malfunction. The level of autonomy did not significantly impact trust, as stated in the results section: 'There were no significant effects pertaining to the autonomy level of the robot.' and 'There were no significant differences contingent upon the autonomy level of the robot.'",,http://link.springer.com/10.1007/978-3-319-41959-6_29,"When studying Human-Robot Interaction (HRI), measures of trust are often employed. Trust is essential in HRI, as inappropriate levels of trust result in misuse, abuse, or disuse of that robot. Some measures of trust specifically target automation, while others specifically target HRI. Although robots are a type of automation, it is unclear which of the broader factors that define automation are shared by robots. However, measurements of trust in automation and trust in robots should theoretically still yield similar results. We examined an HRI scenario using 1) an automation trust scale and 2) a robotic trust scale. Findings indicated conflicting results coming from these respective trust scales. It may well be that these two trust scales examine separate constructs and are therefore not interchangeable. This discord shows us that future evaluations are required to identify scale appropriate context applications for either automation or robotic operations."
"Khalid, Halimahtun M.; Shiung, Liew Wei; Nooralishahi, Parham; Rasool, Zeeshan; Helander, Martin G.; Kiong, Loo Chu; Ai-vyrn, Chin",Exploring Psycho-Physiological Correlates to Trust: Implications for Human-Robot-Human Interaction,2016,1,42,40,2,2 subjects were dropped due to incomplete data,Controlled Lab Environment,within-subjects,"Participants completed three tasks: evaluating videos of human-robot interaction, communicating with a partner using a guided script, and acting out predetermined dialogs in remote rooms. The order of tasks was counterbalanced.","Participants evaluated videos of human-robot interaction, engaged in guided dialogs, and acted out scripted dialogs.",Unspecified,Humanoid Robots,Research,Social,Conversation,minimal interaction,"Participants interacted through guided and scripted dialogs, with some observation of human-robot interaction videos.",media,"The study used videos of human-robot interaction and remote communication, providing a visual but not fully immersive experience.",physical,"The study involved videos of physical robots, but the participants did not directly interact with them.",not autonomous,The robots in the videos were not described as acting autonomously.,Custom Scales; Physiological Measures,,Physiological Signals; Speech Data; Video Data,"Trust was assessed using subjective scales and physiological measures, including facial expressions, voice, and heart rate.","deep learning (e.g., neural networks, reinforcement learning)",A neuro-fuzzy neural network was used to model the relationship between objective features and subjective trust scores.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the type of dialog (guided or scripted), the interaction medium (face-to-face or remote), and the robot's behavior through videos.","The study found that objective features such as facial expressions, voice, and heart rate could be used to predict trust with 67% accuracy. Gender differences in voice also impacted trust.","The study found that certain facial expressions (sad, anger, fear) and heart rate features were significant for trust estimation. Gender differences in voice were also found to impact trust.","The study demonstrated that a neuro-fuzzy model could estimate trust from voice, facial expressions, and heart rate with 67% accuracy.","The robot was presented in videos, and the human participants evaluated these videos, engaged in guided dialogs, and acted out scripted dialogs with another human participant.",Factor analysis; Pearson correlation; ANOVA; cronbach's alpha,"The study used exploratory factor analysis and correlation to analyze the subjective trust data. MANOVA was used to test the effect of gender on subjective trust measures (ability, benevolence, integrity) and objective voice features. Cronbach's Alpha was used to assess the internal consistency of the subjective trust scale.",TRUE,Robot-verbal-communication-content; Task-environment,Robot-verbal-communication-content,,"The study manipulated the type of dialog (guided or scripted), which falls under 'Robot-verbal-communication-content' as it changes what is being communicated. The study also manipulated the interaction medium (face-to-face or remote), which is categorized as 'Task-environment' because it changes the working conditions. The study found that gender differences in voice impacted trust, which is related to the content of the communication, thus 'Robot-verbal-communication-content' is listed as a factor that impacted trust. The study did not find any factors that did not impact trust.",10.1177/1541931213601160,https://doi.org/10.1177/1541931213601160,"This paper describes a methodology for exploring trust using psychological (subjective) and physiological (objective) correlates to trust. The aim was to explore trust using natural dialogs of real-world scenarios that embed fifteen subjective measures. The goal was to apply the method in modeling human-robot-human interaction, involving three types of androids and to predict trust. Two forms of dialogs were employed: a guided script and a predetermined dialog representing three social scenarios. Objective features included facial expressions, voice and heart rate. Subjective trust measures comprised ability, benevolence and integrity. A repeated measures experimental design was employed. Forty-two subjects participated in the study. The data was analyzed using exploratory factor analysis and correlation. Multiple neuro-fuzzy models were trained using the data set and combined as an ensemble using evolutionary algorithms. The final ensemble estimated trust with 67% accuracy. The implications of the findings and limitations of the method are discussed."
"Khalid, H. M.; Liew, W. S.; Helander, M. G.; Loo, C. K.",Prediction of trust in scripted dialogs using neuro-fuzzy method,2016,1,40,40,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were placed in separate rooms and interacted via Skype, acting out scripted dialogs for two scenarios (business and fire rescue). Subjective trust was measured using a line-by-line scale, and objective data was collected via video and audio recordings.",Participants acted out scripted dialogs in two different scenarios: a business scenario and a fire rescue scenario. They rated their trust level after each line of dialog.,Unspecified,Other,Research,Social,Conversation,minimal interaction,"Participants interacted through a scripted dialog via Skype, with no physical contact.",media,"Participants interacted via video conferencing, which provided a visual representation of the interaction.",hypothetical,The robot was not physically present; the interaction was based on a scripted dialog.,not autonomous,"The robot's actions were simulated through a scripted dialog, with no autonomous behavior.",Custom Scales; Physiological Measures,,Physiological Signals; Speech Data; Video Data,"Trust was measured using subjective ratings of Ability, Benevolence, and Integrity, along with objective physiological measures from facial expressions, voice, and heart rate.","deep learning (e.g., neural networks, reinforcement learning)",A neuro-fuzzy classifier was used to predict trust based on subjective and objective measures.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,The study manipulated the scenario context by using two different scripted dialogs (business and fire rescue) to influence the perceived importance of trust factors.,"The fire rescue scenario showed higher predictive accuracy for benevolence and integrity compared to the business scenario, suggesting that scenario context influences trust.","The study found that the fire rescue scenario led to better trust prediction for benevolence and integrity than the business scenario, while ability was similar across both scenarios. This suggests that the context of the interaction significantly impacts trust.","Trust prediction is enhanced by combining subjective psychological measures with objective physiological features, and the type of scenario significantly impacts the predictive power of trust measures.","Participants acted out scripted dialogs, with one participant initiating the interaction and the other responding. The human participants evaluated their trust level after each line of dialog.",neuro-fuzzy classifier; tenfold cross validation; genetic algorithm; plurality voting method,"The study used a neuro-fuzzy classifier based on the fuzzy extreme learning machine to predict trust. The subjective trust scores were divided into three classes (Low, Neutral, High Trust). The neuro-fuzzy classifier was trained and tested using a tenfold cross-validation method. A genetic algorithm was used for classifier selection to create an ensemble of classifiers. A plurality voting method combined the decision outputs of the selected neuro-fuzzy classifiers. This process was repeated for six data subsets based on dialog and trust attributes.",TRUE,Task-environment,Task-environment,,"The study manipulated the scenario context by using two different scripted dialogs (business and fire rescue). This manipulation is best categorized as 'Task-environment' because it changes the working conditions and context of the interaction. The paper states, 'The difference in application scenarios was expected to reflect the importance of trust factors in each application domain.' The results showed that the fire rescue scenario led to better trust prediction for benevolence and integrity than the business scenario, while ability was similar across both scenarios. This indicates that the 'Task-environment' had an impact on trust. The paper states, 'The results revealed that the 'benevolence' trust attribute in the fire rescue scenario contributed 98.9% to accuracy in trust prediction... This finding is in contrast to the business scenario, where 'benevolence' contributed 63.4%... The 'ability' attribute was similar for both scenarios...'. This shows that the task environment impacted trust differently for different trust attributes. There were no factors that were manipulated that did not impact trust.",10.1109/IEEM.2016.7798139,http://ieeexplore.ieee.org/document/7798139/,"Design of effective communication dialogs can help to determine trust between humans. This paper reports an experimental study to explore and predict trust using two different scripted dialogs. The first dialog represented a business scenario, while the second was a fire rescue. Trust was measured using subjective psychological criteria of Ability, Benevolence and Integrity which were embedded in the dialog. These were correlated with objective physiological features of human facial expressions, voiced speech and camera-based heart rate. Using neurofuzzy method, the subjective trust data was analyzed separately. The results showed that subjective measures of Benevolence and Integrity for fire rescue scenario could predict trust better than the business scenario. The power of prediction was almost identical for the Ability measure. The findings suggest that trust is associated with the type of scenario, and that both subjective and objective measures are important in predicting trust."
"Khalid, Halimahtun; Liew, Wei Shiung; Voong, Bin Sheng; Helander, Martin",Creativity in Measuring Trust in Human-Robot Interaction Using Interactive Dialogs,2019,1,48,48,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Subjects were briefed, trained, and then performed dialog tasks with different robots in different scenarios. They evaluated the robot on trust attributes during the interaction. Physiological data was also collected.","Participants engaged in scripted dialogs with a robot in different scenarios (Business, Disaster, Healthcare) and evaluated the robot's trustworthiness.",Unspecified,Humanoid Robots; Virtual/Simulated Robots,Research; Social,Social,Conversation,minimal interaction,Participants interacted with the robot through scripted dialogs.,real-world,Participants interacted with physical robots in a lab setting.,physical,The study used physical humanoid robots and virtual agents.,wizard of oz (directly controlled),The robot's actions were controlled through a pre-scripted dialog.,Custom Scales; Physiological Measures; Multidimensional Measures,,Video Data; Physiological Signals; Speech Data,"Trust was measured using subjective ratings, physiological data, and a multidimensional approach.","deep learning (e.g., neural networks, reinforcement learning)",A neural network ensemble method was used to predict trust from physiological measurements.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The context of use was manipulated through different dialog scenarios, and the robot's behavior was controlled through the dialog scripts.","Context of use significantly affected trust, with 'Integrity' and 'Ability' being important in Business and Disaster scenarios. The robots were perceived as more trustworthy in Business and Healthcare contexts for 'Ability' attributes, and Disaster and Healthcare contexts for 'Benevolence' attributes. Subjects distrusted the robots in the Disaster context, especially for Integrity.",The study found that the 'Informed' attribute had low scores across all factors and may be removed from future consideration. The 'Likeable' and 'Responsible' attributes were also removed to maintain balance in the number of attributes per category. The physical robots in the HRI experiment were considered more trustworthy than the virtual avatar in the HRHI experiment for some attributes.,"Trust can be measured using interactive dialogs that represent different contexts of use, and situational context influences trust between humans and humanoid robots. The estimation of trust was about 83% accurate when using this creative approach.","The robot engaged in scripted dialogs with human participants, acting as an intermediary between two human subjects. The human participants evaluated the robot's trustworthiness based on the dialog.",cronbach's alpha; Factor analysis; ANOVA; Multivariate Analysis of Variance; Kruskal-Wallis,"Cronbach's Alpha was used to assess the reliability of the trust attributes. Exploratory factor analysis was performed to analyze the relationships between trust attributes. Univariate ANOVA was used to determine the effect of context of use on composite trust scores. One-way MANOVA was used to analyze the effects of context of use on the Ability, Benevolence, and Integrity (ABI) components of trust, and individual trust attributes. Kruskal-Wallis tests were used to determine the effects of the factors on psychological trust measures.",TRUE,Task-environment; Robot-verbal-communication-content,Task-environment; Robot-verbal-communication-content,,"The study manipulated the context of use through different scenarios (Business, Disaster, and Healthcare), which falls under 'Task-environment' as it changes the working conditions and the overall setting of the interaction. The dialog scripts themselves, which were designed to portray different trust attributes, represent a manipulation of 'Robot-verbal-communication-content' as the content of the robot's communication was altered to convey different aspects of trustworthiness. The paper explicitly states that the context of use had a significant effect on trust, as well as the specific trust attributes embedded in the dialogs, indicating that both 'Task-environment' and 'Robot-verbal-communication-content' impacted trust. There were no factors explicitly stated as not impacting trust.",,http://link.springer.com/10.1007/978-3-319-96071-5_119,"The measurement of human trust in humanoid robots in humanrobot interaction requires novel approaches that can predict trust effectively. We present a method that mapped subjective measures (i.e. general trust, psychological) to objective measures (i.e. physiological) to predict trust. We designed interactive dialogs that represent real world service scenarios of Business, Disaster, and Healthcare. The dialogs embedded ﬁfteen trust attributes of Ability, Benevolence and Integrity (ABI) in the communication dialogs. The ABI measures were mapped to physiological measures of facial expressions, voiced speech and camera-based heart rate. Forty-eight subjects comprising 24 males and 24 females aged between 18 to 36 years participated in the experiment. Half of the subjects were Malays and half were Chinese. Three humanoid robots represented full bodied, partial bodied and virtual agents. The experimental design was a within-subjects design. Each subject was tested on all robots in all scenarios. Subjects scored trust on an online scale that ranged from 0 to 7 points. The subjective data was analyzed using Univariate and Oneway MANOVA. The results found the humanoids to be trustworthy in different service tasks. The attributes of ‘Integrity’ and ‘Ability’ trust components are important in Business and Disaster scenarios. The estimation of trust was about 83% accurate when using this creative approach. In conclusion, humanoid robots can interact with humans using dialogs that are representative of real world communication."
"Khastgir, Siddartha; Birrell, Stewart; Dhadyalla, Gunwant; Jennings, Paul",Calibrating trust through knowledge: Introducing the concept of informed safety for automation in vehicles,2018,1,56,48,8,8 participants were excluded due to simulator sickness and technical issues,Controlled Lab Environment,mixed design,"Participants completed two driving simulator runs, with knowledge about the automation system provided before the second run for two of the three groups. Participants were asked to press an emergency stop button when they felt the automated system could not handle a situation. The control group experienced two different automated systems without any knowledge provided.",Participants were asked to sit in the front passenger seat of a driving simulator and press an emergency stop button when they perceived a hazardous situation that the automated system could not handle. They were also asked to maximize their score by spending time in automated mode and avoiding crashes.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the automated vehicle in a driving simulator by pressing an emergency stop button.,simulation,The study used a driving simulator to create an immersive environment for the participants.,simulated,The robot was a simulated autonomous vehicle within a driving simulator.,fully autonomous (limited adaptation),"The automated vehicle operated autonomously, but with limited adaptation to unexpected scenarios.",Custom Scales,,Performance Metrics,Trust was measured using a custom rating scale and performance metrics such as false presses and accidents.,no modeling,"The study did not use computational modeling of trust, only descriptive statistics.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The study manipulated the automation capability (low vs. high) and provided static knowledge about the system's capabilities and limitations to influence trust. The knowledge was provided via a script.,"Providing static knowledge increased trust in the system for both low and high capability automation. However, the number of false presses increased for low capability automation and decreased for high capability automation. The number of accidents decreased for both groups.","The study found that providing static knowledge increased trust in the system, but also increased false presses for low capability automation. This suggests that while knowledge can increase trust, it may not always lead to optimal behavior. The control group showed no change in trust levels due to experience, supporting the hypothesis that the change in trust was due to the knowledge provided.","Providing static knowledge about the capabilities and limitations of an automated system increases trust in the system, regardless of the system's actual capability.","The robot (simulated autonomous vehicle) drove along a predefined route, and the human participant monitored the system and pressed an emergency stop button when they perceived a hazardous situation. The human was also asked to maximize their score by spending time in automated mode and avoiding crashes.",ANOVA; t-test,"The study used repeated measures ANOVA to analyze the effect of knowledge about automation capabilities on 'trust in the system' and 'trust with the system', with automation capability as a between-subject factor and knowledge as a within-subject factor. A separate ANOVA was conducted on the control group to assess the effect of experience on trust ratings. Paired-sample t-tests were used to assess the significance of the change in the number of false presses and accidents with the introduction of knowledge for both low and high capability automation groups.",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-verbal-communication-content,Robot-accuracy,"The study manipulated the automation capability (low vs. high), which directly impacts the robot's accuracy in handling hazardous events, thus 'Robot-accuracy'. The study also manipulated the knowledge provided to the participants about the system's capabilities and limitations through a prepared script, which is a form of 'Robot-verbal-communication-content'. The results showed that providing static knowledge (verbal communication content) significantly impacted trust in the system, while the automation capability (robot accuracy) did not have a significant interaction effect on trust when knowledge was provided. The study explicitly states that 'The introduction of knowledge about the automation capabilities and limitations had a highly significant statistical effect on the level of ""trust in the system""' and 'The introduction of knowledge didn't have an interaction effect with automation capability'. This indicates that the knowledge provided had a direct impact on trust, while the level of automation capability did not have a significant impact on trust when knowledge was provided. Therefore, 'Robot-verbal-communication-content' impacted trust, while 'Robot-accuracy' did not have a significant impact on trust when knowledge was provided.",10.1016/j.trc.2018.07.001,https://linkinghub.elsevier.com/retrieve/pii/S0968090X18309252,"There has been an increasing focus on the development of automation in vehicles due its many potential benefits like safety, improved traffic efficiency, reduced emissions etc. One of the key factors influencing public acceptance of automated vehicle technologies is their level of trust. Development of trust is a dynamic process and needs to be calibrated to the correct levels for safe deployment to ensure appropriate use of such systems. One of the factors influencing trust is the knowledge provided to the driver about the system’s true capabilities and limitations. After a 56 participants driving simulator study, the authors found that with the introduction of knowledge about the true capabilities and limitations of the automated system, trust in the automated system increased as compared to when no knowledge was provided about the system. Participants experienced two different types of automated systems: low capability automated system and high capability automated system. Interestingly, with the introduction of knowledge, the average trust levels for both low and high capability automated systems were similar. Based on the experimental results, the authors introduce the concept of informed safety, i.e., informing the drivers about the safety limits of the automated system to enable them to calibrate their trust in the system to an appropriate level."
"Khavas, Zahra Rezaei; Perkins, Russell; Ahmadzadeh, S. Reza; Robinette, Paul",Moral-Trust Violation vs Performance-Trust Violation by a Robot: Which Hurts More?,2021,1,100,100,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants played a search game with a robot, where they decided whether to integrate or discard the robot's score after each round. There were two conditions: performance-trust violation and moral-trust violation. Participants were randomly assigned to one condition.",Participants played a search game where they had to find targets hidden in an area. They decided whether to integrate or discard the robot's score after each round.,Unspecified,Other,Research,Game,Cooperative Game,minimal interaction,Participants interacted with the robot through an online game.,simulation,The interaction was through a simulated online game environment.,simulated,The robot was represented in a simulated online game.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires; Behavioral Measures,,Performance Metrics,"Trust was assessed using questionnaires and behavioral measures, with performance metrics collected for trust modeling.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's behavior was manipulated to either pick red targets (performance violation) or pink targets (moral violation), with the same magnitude of score change.",The study aims to investigate if moral-trust violation would affect human trust differently from a performance-trust violation with the same magnitude.,,The study aims to investigate if moral-trust violation would affect human trust differently from a performance-trust violation with the same magnitude.,"The robot searches for targets in a game environment, and the human decides whether to integrate or discard the robot's score. The robot picks either red targets (performance violation) or pink targets (moral violation).",,"The paper describes the experimental design and data collection methods but does not mention any specific statistical tests that were used to analyze the data. Therefore, the statistical test is marked as N/A.",TRUE,Robot-morality; Robot-accuracy,Robot-morality; Robot-accuracy,,"The study manipulated the robot's behavior to create two conditions: a performance-trust violation and a moral-trust violation. In the performance-trust violation condition, the robot picked red targets, which subtracted points from the team's total score (TPS), thus manipulating 'Robot-accuracy' by affecting the robot's performance. In the moral-trust violation condition, the robot picked pink targets, which subtracted points from the TPS and added them to an individual performance score (IPS), thus manipulating 'Robot-morality' by introducing a moral violation. The paper states that the magnitude of the score change was the same in both conditions, but the type of violation was different. The study aims to investigate if these two types of violations affect human trust differently, indicating that both 'Robot-morality' and 'Robot-accuracy' are expected to impact trust. The paper explicitly states that the robot's behavior was manipulated to either pick red targets (performance violation) or pink targets (moral violation), with the same magnitude of score change. This direct manipulation of the robot's actions to induce different types of trust violations is why 'Robot-morality' and 'Robot-accuracy' are chosen as manipulated factors. The study aims to investigate if moral-trust violation would affect human trust differently from a performance-trust violation with the same magnitude, indicating that both factors are expected to impact trust.",,http://arxiv.org/abs/2110.04418,"In recent years a modern conceptualization of trust in human-robot interaction (HRI) was introduced by Ullman et al.(Ullman and Malle 2018). This new conceptualization of trust suggested that trust between humans and robots is multidimensional, incorporating both performance aspects (i.e., similar to the trust in human-automation interaction) and moral aspects (i.e., similar to the trust in human-human interaction). But how does a robot violating each of these different aspects of trust affect human trust in a robot? How does trust in robots change when a robot commits a moral-trust violation compared to a performance-trust violation? And whether physiological signals have the potential to be used for assessing gain/loss of each of these two trust aspects in a human. We aim to design an experiment to study the effects of performance-trust violation and moral-trust violation separately in a search and rescue task. We want to see whether two failures of a robot with equal magnitudes would affect human trust differently if one failure is due to a performancetrust violation and the other is a moral-trust violation."
"Khavas, Zahra Rezaei; Majdi, Amin; Ahmadzadeh, S. Reza; Robinette, Paul",Human Trust After Drone Failure: Study of the Effects of Drone Type and Failure Type on Human-Drone Trust,2023,2,60,57,3,3 participants were excluded due to not responding to the manipulation check questions correctly,Online Crowdsourcing,between-subjects,"Participants read a scenario, watched videos of two drones, chose a preferred drone, and completed trust questionnaires.",Participants were asked to choose a drone to assist them in a mapping mission after observing videos of the drones.,Elios Drone; Mavic Drone,Unmanned Aerial Vehicles (UAVs),Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants observed videos of the drones.,media,Participants watched videos of the drones performing a task.,physical,Participants observed videos of physical drones.,pre-programmed (non-adaptive),The drones followed a pre-programmed path in the videos.,Questionnaires; Behavioral Measures,Checklist for Trust between People and Automation; Human-Computer Trust Scale/Questionnaire (HCT/HCTM),,Trust was measured using questionnaires and drone selection.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The type of drone (caged vs. uncaged) and the outcome of a collision (drone falling vs. continuing) were manipulated to influence trust.,"The cage-protected drone, which continued after a collision, was trusted more than the uncaged drone, which fell after a collision.","Participants showed a strong preference for the cage-protected drone, citing its ability to continue after a collision as a key factor.",A cage-protected drone that continues after a collision is trusted more than an uncaged drone that falls after a collision.,"The robot (drone) performed a mapping task, and the human participant observed the drone's performance and chose a preferred drone.",Binomial test; t-test,"The study used a binomial test to determine if the number of participants who chose the Elios drone was significantly higher than those who chose the Mavic drone. It also used a binomial test to determine if the number of reasons related to the non-catastrophic consequences of the Elios collision was significantly higher than other reasons. T-tests were used to compare the mean trust scores between the Elios and Mavic drones on both the HTCM and Jian's questionnaires, and to compare individual questionnaire items.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the robot's (drone's) accuracy by showing two different outcomes after a collision: one where the drone continued to function (Elios) and one where the drone fell (Mavic). This manipulation directly impacted the perceived reliability and capability of the drone, which is a core aspect of robot accuracy. The paper states, 'In this video, the Elios drone collides with a pillar sometime after the start of the mapping operation. However, this collision does not cause the drone to fall... In this video, the Mavic drone collided with a pillar sometime after the start of the mapping operation, and immediately after the collision, it fell to the ground.' This difference in outcome directly relates to the robot's ability to perform the task, thus it is classified as Robot-accuracy. The results showed that the drone that continued after the collision was trusted more, indicating that this manipulation of accuracy impacted trust.",10.1109/UR57808.2023.10202489,https://ieeexplore.ieee.org/document/10202489/,"Advances in the navigation and autonomy of drones, along with significant advances in manufacturing microprocessors and sensor systems, caused a growing trend in deploying drones. In the near future, we should expect to see drones collaborating with humans in various applications. When we talk about human-robot collaboration, we should always consider that trust is a major element that should be taken into consideration, especially in high-risk situations. In this study, we perform two experiments using a similar methodology to compare the effects of different drone failures on human trust. We compare the effects of failures with different catastrophic levels on human trust. In our first experiment, we tested the effects of a drone collision with an obstacle on human trust. The experiment results showed that the collisions of cage-protected drones affect human trust less negatively than unprotected drones. In our second experiment, we compared the effects of a drone collision with a wall with drone connection loss on human trust. The experiment results showed that a drone’s connection loss affects human trust more negatively than a drone collision when the drone possesses a protective cage. Our results can help guide robot designers in building/programming robots for diverse scenarios by showing what features a human operator values most."
"Khavas, Zahra Rezaei; Majdi, Amin; Ahmadzadeh, S. Reza; Robinette, Paul",Human Trust After Drone Failure: Study of the Effects of Drone Type and Failure Type on Human-Drone Trust,2023,2,60,57,3,3 participants were excluded due to not responding to the manipulation check questions correctly,Online Crowdsourcing,between-subjects,"Participants read a scenario, watched videos of a drone with two different failure types, chose a preferred failure type, and completed trust questionnaires.",Participants were asked to choose which software they would prefer to control their drone assistant after observing videos of the drone experiencing different failure types.,Elios Drone,Unmanned Aerial Vehicles (UAVs),Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants observed videos of the drone experiencing different failure types.,media,Participants watched videos of the drone experiencing different failure types.,physical,Participants observed videos of a physical drone.,pre-programmed (non-adaptive),The drone followed a pre-programmed path in the videos.,Questionnaires; Behavioral Measures,Checklist for Trust between People and Automation; Human-Computer Trust Scale/Questionnaire (HCT/HCTM),,Trust was measured using questionnaires and drone software selection.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The type of failure (collision vs. connection loss) was manipulated to influence trust.,The drone experiencing a collision was trusted more than the drone experiencing a connection loss.,"Participants showed a strong preference for the collision case, citing the catastrophic nature of connection loss as a key factor.","A drone experiencing a collision is trusted more than a drone experiencing a connection loss, as connection loss is perceived as more catastrophic.","The robot (drone) performed a mapping task, and the human participant observed the drone's performance and chose a preferred software to control the drone.",Binomial test; t-test,"The study used a binomial test to determine if the number of participants who chose the Collision Case was significantly higher than those who chose the Connection Loss Case. It also used a binomial test to determine if the number of reasons related to the non-catastrophic consequences of the collision was significantly higher than other reasons. T-tests were used to compare the mean trust scores between the Collision Case and Connection Loss Case on both the HTCM and Jian's questionnaires, and to compare individual questionnaire items.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the robot's (drone's) accuracy by showing two different failure types: a collision and a connection loss. Although the drone itself was the same, the different failure types resulted in different task performance outcomes. The paper states, 'In this video, sometime after the start of the mapping operation, the drone faces a communication problem with the base, and the signals received by the drone get weakened. The drone remains suspended in the air for a while. Then the drone completely loses its connection with the controller.' and 'This video is similar to the Elios Collision video in the drone characteristics experiment videos. However, the difference is that we cut the video a few seconds after the drone's collision with the pillar, and the drone's attempt to regain its balance and resume the mission is not shown in this video.' The connection loss is a more severe failure that prevents the drone from completing the task, while the collision, even though it is a failure, does not necessarily prevent the drone from completing the task. This difference in outcome directly relates to the robot's ability to perform the task, thus it is classified as Robot-accuracy. The results showed that the collision case was trusted more, indicating that this manipulation of accuracy impacted trust.",10.1109/UR57808.2023.10202489,https://ieeexplore.ieee.org/document/10202489/,"Advances in the navigation and autonomy of drones, along with significant advances in manufacturing microprocessors and sensor systems, caused a growing trend in deploying drones. In the near future, we should expect to see drones collaborating with humans in various applications. When we talk about human-robot collaboration, we should always consider that trust is a major element that should be taken into consideration, especially in high-risk situations. In this study, we perform two experiments using a similar methodology to compare the effects of different drone failures on human trust. We compare the effects of failures with different catastrophic levels on human trust. In our first experiment, we tested the effects of a drone collision with an obstacle on human trust. The experiment results showed that the collisions of cage-protected drones affect human trust less negatively than unprotected drones. In our second experiment, we compared the effects of a drone collision with a wall with drone connection loss on human trust. The experiment results showed that a drone’s connection loss affects human trust more negatively than a drone collision when the drone possesses a protective cage. Our results can help guide robot designers in building/programming robots for diverse scenarios by showing what features a human operator values most."
"Kim, Ki Joon; Park, Eunil; Sundar, S. Shyam; del Pobil, Angel P.",The effects of immersive tendency and need to belong on human-robot interaction,2012,1,20,20,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants completed pre-experiment questionnaires, interacted with a Nao robot in casual conversation, and then completed post-experiment questionnaires.",Participants engaged in casual conversations with the robot about their school life and responded to the robot's questions from a script.,Nao,Humanoid Robots; Expressive Robots,Social; Research,Social,Conversation,minimal interaction,Participants had a verbal interaction with the robot in a lab setting.,real-world,Participants interacted with a physical robot in a lab environment.,physical,Participants interacted with a physical Nao robot.,pre-programmed (non-adaptive),The robot followed a pre-selected script during the interaction.,Questionnaires,,,Trust was measured using a post-experiment questionnaire.,no modeling,No computational model of trust was used in this study.,Empirical HRI Studies,Physical Robot Studies,Indirect Manipulation,"The study indirectly manipulated trust by categorizing participants based on their immersive tendency and need to belong, which influenced their expectations and roles in the interaction.",Participants with higher immersive tendency and need to belong showed greater trust towards the robot.,The study found that individual differences in immersive tendency and need to belong significantly impact trust in human-robot interaction.,Individuals with higher immersive tendency and need to belong showed greater trust towards the robot.,"The robot engaged in casual conversations with participants, asking questions from a pre-selected script, while participants responded to the robot's questions.",ANOVA,"A series of 2x2 factorial analyses of variance (ANOVAs) were used to examine the effects of immersive tendency (high vs. low) and need to belong (high vs. low) on the dependent variables: social presence, attachment, trust, and relationship satisfaction. The ANOVAs tested for main effects of immersive tendency and need to belong, as well as any interaction effects between these two factors.",TRUE,Other,Other,,"The study manipulated participant characteristics, specifically 'immersive tendency' and 'need to belong', by categorizing participants into high and low groups based on their pre-experiment questionnaire scores. This manipulation is not directly related to the robot's behavior or task, but rather to the participant's dispositional tendencies. Therefore, it does not fit into any of the existing categories. The manipulation of these participant characteristics impacted trust, as participants with higher immersive tendency and need to belong showed greater trust towards the robot. Since this is not a manipulation of the robot or task, it is classified as 'Other'. The specific factor being manipulated is participant's dispositional tendencies (immersive tendency and need to belong). This does not fit into any existing category because it is not a manipulation of the robot's behavior, the task, or the environment, but rather a manipulation of the participants' characteristics. A suggested category for future classification would be 'Participant-characteristics'.",10.1145/2157689.2157758,http://dl.acm.org/citation.cfm?doid=2157689.2157758,"Do individual differences in dispositional behavioral tendencies, such as immersive tendency and need to belong, play a significant role in human-robot interaction? To answer this question, the present study conducted a 2 x 2 between-subjects experiment to examine the effects of immersive tendency (high vs. low) and need to belong (high vs. low) on individuals’ perceptions of a social robot. Preliminary data analyses revealed that participants with a higher level of immersive tendency and need to belong showed greater attachment and trust towards the robot, and were more satisfied with their relationship with the robot than participants with a lower level of immersive tendency and need to belong. In addition, participants with a higher level of immersive tendency experienced greater feelings of social presence. Implications of notable findings are discussed."
"Kim, Hyunjin",Trustworthiness of unmanned automated subway services and its effects on passengers’ anxiety and fear,2019,1,458,303,155,155 participants were excluded due to incomplete data,Real-World Environment,within-subjects,"Participants completed a questionnaire after reading two scenarios about subway incidents. The questionnaire included items measuring trustworthiness, trust, certainty, coping potential, and anxiety/fear.",Participants rated their anxiety and fear levels in response to two scenarios involving subway incidents.,Unspecified,Autonomous Vehicles,Other: Transportation,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the scenarios.,media,The interaction was based on written scenarios.,hypothetical,The robot was only described in text.,fully autonomous (limited adaptation),"The subway system operates autonomously, but with limited adaptation to unexpected situations.",Questionnaires; Custom Scales,Mayer and Davis' Trust/Trustworthiness Scales (1999); Dirks and Ferrin (2002); Roseman et al. (1996); Ellsworth and Scherer (2009),,Trust was measured using questionnaires and custom scales.,"parametric models (e.g., regression)","The study used multiple regression analysis and structural equation modeling to analyze the relationships between trust, trustworthiness, certainty, coping potential, and anxiety/fear.",Observational & Survey Studies,Quantitative Surveys,Indirect Manipulation,"The scenarios presented different types of subway incidents, indirectly influencing participants' perceptions of the system's trustworthiness and their expectations about its performance.","The study found that both automation and service trustworthiness negatively affected anxiety and fear, mediated by certainty and coping potential. The specific impact of each factor varied depending on the scenario.","The study found that service trustworthiness directly affected certainty in the delayed train scenario, while automation trustworthiness directly affected certainty in the door-related incident scenario. This suggests that the perceived cause of the incident influences which aspect of trustworthiness is most salient.","The trustworthiness of automated subway services, encompassing both automation and service aspects, negatively affects passengers' anxiety and fear, mediated by their sense of certainty and coping potential.","Participants read two scenarios describing subway incidents and then completed a questionnaire assessing their anxiety, fear, certainty, coping potential, and perceptions of the subway's trustworthiness. The robot (automated subway) was not directly present, but was the subject of the scenarios.",Factor analysis; Linear regression; Partial least squares; cronbach's alpha; Pearson correlation,"The study used exploratory factor analysis (EFA) to identify underlying factors of automated subway trustworthiness. Multiple regression analysis was used to examine the relationship between the identified trustworthiness factors and trust in automated subway services. Structural equation modeling (SEM) was employed to test the hypothesized relationships between trustworthiness, certainty, coping potential, and anxiety/fear in response to two different scenarios. Cronbach's alpha was used to assess the reliability of the scales. Pearson correlation coefficient was used to assess the correlation between automation and service trustworthiness.",TRUE,Task-environment,Task-environment,,"The study presented participants with two different scenarios describing subway incidents: a train suddenly stopping and blacking out, and a train door closing and leaving a baby in a stroller behind. These scenarios represent different 'Task-environment' conditions, as they describe different situations that could occur within the subway system. The study found that the type of scenario (i.e., the specific task environment) influenced which aspect of trustworthiness (automation vs. service) was most salient and how it impacted trust, certainty, coping potential, and anxiety/fear. Specifically, service trustworthiness directly affected certainty in the delayed train scenario, while automation trustworthiness directly affected certainty in the door-related incident scenario. This indicates that the manipulated 'Task-environment' had a direct impact on how participants perceived and trusted the system.",10.1016/j.trf.2019.07.014,https://linkinghub.elsevier.com/retrieve/pii/S1369847818308489,"Commuters worldwide can now travel to work using unmanned subway services while tourists may travel to holiday destinations on planes that employ automatic ﬂight control equipment. More recently, autonomous buses have made their debut in several cities. Technological advances in transport services, however, are accompanied by challenges to passengers’ perceived safety and the acceptability of the technology. While passengers’ anxiety and fear appear to reduce their acceptance of new technologies, trust has been shown to positively affect their acceptance of automation and autonomous vehicles. In this vein, the present study investigates the trustworthiness of automated subway services in Korea with regard to passenger anxiety and fear. First, automated subway-speciﬁc trustworthiness factors are identiﬁed through exploratory factor analysis. Subsequently, the effects of the identiﬁed factors on passenger anxiety and fear are examined using structural equation modeling. The results indicate that attributes that contribute to the trustworthiness of automated subways are grouped into automation- and service-related factors: ‘automation trustworthiness’ and ‘service trustworthiness’, both of which appear to negatively affect passengers’ anxiety and fear, mediated by the passengers’ sense of certainty and coping potential. The negative effects of trustworthiness on anxiety and fear indicate that strengthening of the trustworthiness factors must be accompanied with technological advances in transportation. The implications for strengthening the trustworthiness of unmanned automated subways and other transport services employing autonomous features are presented, correspondingly, in this study."
"Kim, Wonjoon; Kim, Nayoung; Lyons, Joseph B.; Nam, Chang S.",Factors affecting trust in high-vulnerability human-robot interaction contexts: A structural equation modelling approach,2020,1,233,233,0,No participants were excluded,Online Crowdsourcing,between-subjects,Participants watched a video of a security robot interacting with people and then completed a survey.,Participants watched a video of a security robot and then answered questions about their perceptions of the robot.,Baxter,Humanoid Robots; Collaborative Robots,Other: Security,Evaluation,Survey/Questionnaire Completion,passive observation,Participants watched a video of the robot interacting with others.,media,Participants watched a video of a robot interacting with people.,physical,The robot in the video was a real Baxter robot.,wizard of oz (directly controlled),"The robot's actions were triggered by a robot engineer, but depicted as autonomous in the video.",Questionnaires; Custom Scales,Mayer and Davis' Trust/Trustworthiness Scales (1999); Reliance Intention Scale,,"Trust was measured using a 10-item scale and trustworthiness was measured using scales for ability, benevolence, and integrity.","parametric models (e.g., regression)",Structural equation modeling was used to analyze the relationships between trust and other variables.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"Participants were given one of three transparency scripts that described the robot's programming as being benevolent towards the visitor, the protected, or the robot itself, which influenced their expectations of the robot.","The study found that trustworthiness, particularly ability, had the greatest effect on trust. Integrity was found to mediate the relationships between robot-and human-related metrics and trustworthiness. Human-likeness had an indirect effect on trust through perceived intelligence. High PAS was also found to have a positive correlation with trust.","The study found that integrity had no direct effect on trust, but had indirect effects, and that integrity mediated the relationship between other metrics of trustworthiness. Also, human-likeness had an indirect effect on trust through perceived intelligence.","The study found that ability was the most important factor affecting trust in HRI, and that integrity mediated the relationships between robot-and human-related metrics and trustworthiness.",Participants watched a video of a security robot interacting with people and then completed a survey about their perceptions of the robot. The robot was shown scanning badges and either granting or denying access to a secure area.,Structural equation modeling; cronbach's alpha (ca); Confirmatory factor analysis; Pearson correlation,"The study used structural equation modeling (SEM) to analyze the relationships between trust and other variables such as trustworthiness, human-likeness, intelligence, perfect automation schema (PAS), and affect. Cronbach's alpha (CA) was used to assess the internal consistency of the scales. Confirmation factor analysis (CFA) was used to examine the construct validity of the variables. Correlation analysis was performed to identify variables with statistically significant correlations with trust.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the transparency script provided to participants before they watched the video. The scripts described the robot's programming as being benevolent towards the visitor, the protected, or the robot itself. This manipulation directly altered the content of the information communicated to the participants about the robot's priorities and intentions, which falls under the category of 'Robot-verbal-communication-content'. The study found that the manipulation of the transparency script, which influenced the perceived trustworthiness of the robot, had an impact on trust. The study did not find any other manipulated factors that did not impact trust.",10.1016/j.apergo.2020.103056,https://linkinghub.elsevier.com/retrieve/pii/S0003687020300132,"The current research proposed and tested a structural equation model (SEM) that describes hypothesized re­ lationships among factors affecting trust in human-robot interaction (HRI) such as trustworthiness, humanlikeness, intelligence, perfect automation schema (PAS), and affect. A video stimulus depicting an autonomous guard robot interacting with humans was employed as a stimulus via Amazon’s Mechanical Turk to recruit 233 participants. Human-related and robot-related metrics were found to affect trustworthiness that subsequently affected trust. In particular, ability (as a trustworthiness facet) was a dominant factor affecting trust in HRI. Integrity was found to mediate the relationships between robot- and human-related metrics and trustworthiness. This study also showed a correlation between intelligence and trustworthiness, as well as between PAS and trustworthiness. The findings of the present study have significant implications for both theory and practice on factors and levels that affect trust in HRI."
"Kim, Byung Hyung; Ho Kwak, Ji; Kim, Minuk; Jo, Sungho",Affect-driven Robot Behavior Learning System using EEG Signals for Less Negative Feelings and More Positive Outcomes,2021,1,24,24,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were divided into closed-loop and open-loop groups. They completed training, evaluation, and testing phases. In the testing phase, participants performed three tasks involving a robot arm grasping and delivering objects, with the closed-loop group's feedback influencing the robot's behavior.","Participants interacted with a robot arm that grasped and delivered objects. They performed three tasks: filling a container, emptying a container, and catching a snack. The closed-loop group's emotional feedback influenced the robot's velocity.",IRB 14000 Yumi,Industrial Robot Arms; Collaborative Robots (Cobots),Research,Manipulation,Object Passing,direct-contact interaction,Participants directly interacted with the robot by receiving objects from it.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The study used a physical robot arm for the interaction.,shared control (adaptive),The robot adapted its velocity based on the user's emotional feedback in the closed-loop condition.,Physiological Measures; Performance-Based Measures,,Physiological Signals; Performance Metrics,Trust was assessed using EEG signals and task completion times.,"parametric models (e.g., regression)",The study used a linear function to adjust the robot's velocity based on the user's emotional feedback.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's velocity was directly manipulated based on the user's emotional feedback in the closed-loop condition, influencing its behavior and performance.","The closed-loop system led to increased valence and faster task completion times, suggesting improved trust and comfort with the robot.","The closed-loop group showed a strong negative correlation between task completion time and valence, indicating that positive feelings led to faster task completion. The open-loop group did not show this correlation.","A closed-loop affective system, where the robot adapts its behavior based on user's emotional feedback, resulted in better affective outcomes and task performance compared to an open-loop system.","The robot arm grasped and delivered containers to the participant. The participant filled, emptied, or caught a snack from the container. The robot's velocity was adjusted based on the user's emotional feedback in the closed-loop condition.",Spearman correlation; Fisher's exact test,"The study used Spearman's rank correlation to analyze the relationship between task completion times and valence values for both the closed-loop and open-loop groups. Specifically, it examined the correlation between these variables during the 'catching' and 'giving' stages of the tasks. Fisher's method was used to combine p-values from multiple correlation tests across different EEG frequency bands and electrodes to determine the overall significance of the correlation between EEG signals and valence ratings.",TRUE,Robot-autonomy; Robot-accuracy,Robot-autonomy; Robot-accuracy,,"The study manipulated 'Robot-autonomy' by implementing a closed-loop system where the robot's velocity was adjusted based on the user's emotional feedback, contrasting it with an open-loop system where the robot's velocity was not influenced by user feedback. This is described in the paper as 'In a closed-loop system, each user provides an emotional response Z(m) = (z 1 , z 2 , . . . , z t ) as a feedback for the robot behavior' and 'the arm velocity was changed or maintained constant after receiving feedback from the main controller'. The study also manipulated 'Robot-accuracy' because the robot's velocity changes directly impacted the task completion time and success rate, as described in the paper: 'A robot increases s i+1 smoothly in the next trial when a user feels positively (z i > y th = 3.5) against the current robot behavior in trial i' and 'If the velocity value s is too fast, the object was easily dropped from containers, so more time was required to finish their goals'. The closed-loop system, which adapted the robot's velocity based on user feedback, led to improved task performance and positive feelings, indicating that both 'Robot-autonomy' and 'Robot-accuracy' impacted trust. The paper states 'We observed the closedloop mechanism exhibited greater improvements in completed time and valence than the open-loop system' and 'These results imply that participants perceived using the closed-loop affective system to be more productive (=faster completed time) and comfortable (=less negative feeling) than using the open-loop affective system when performing the iterative tasks'.",10.1109/IROS51168.2021.9636451,https://ieeexplore.ieee.org/document/9636451/,"Learning from human feedback using eventrelated electroencephalography (EEG) signals has attracted extensive attention recently owing to their intuitive communication ability by decoding user intentions. However, this approach requires users to perform speciﬁed tasks and their success or failure. In addition, the amount of attention needed for decisionmaking increases with the task difﬁculty, decreasing human feedback quality over time because of fatigue. Consequently, this can reduce the interaction quality and can even cause interaction breakdowns. To overcome these limitations and enable the interaction of robots with higher complexity tasks, we propose a closed-loop control system that learns affective responses to robot behaviors and provides natural feedback to optimize robot parameters for smoothing the next action. Experimental results demonstrate our affect-driven closedloop control system yielded better affective outcomes and task performance than an open-loop system with correlated neuroscientiﬁc characteristics of EEG signals, thus enhancing the quality of human-robot interaction."
"Kio, Onoise G.; Yuan, Mingfeng; Allison, Robert S.; Shan, Jinjun",Performance-based Data-driven Assessment of Trust,2024,1,39,37,2,Corrupt flight data from two operators was excluded,Controlled Lab Environment,within-subjects,"Participants were trained to use a teleoperated drone to find targets. They then completed three 3-minute runs of the target-finding task. Finally, they assessed other operators' performance based on simulations of the drone's flight paths.",Participants teleoperated a drone to find alphanumeric targets in a defined area.,Ryze Tech Tello,Unmanned Aerial Vehicles (UAVs),Research,Navigation,Remote Navigation,minimal interaction,Participants interacted with the drone through a remote control interface.,simulation,Participants observed simulations of the drone's flight paths.,physical,"The study used a physical drone, but participants interacted with it remotely.",wizard of oz (directly controlled),The drone was directly controlled by the participants.,Questionnaires; Performance-Based Measures,,"Performance Metrics; robot data (sensor data, etc.)",Trust was assessed using subjective ratings and performance data.,"parametric models (e.g., regression)",Linear mixed-effects and logistic regression models were used to analyze the relationship between performance and trust.,Empirical HRI Studies,Physical Robot Studies,No Manipulation,"The study did not directly manipulate trust, but the task difficulty and the performance of the operators influenced trust ratings.","Subjective trust ratings were significantly related to actual performance for non-animated simulations, but not for animated simulations. The drone's position density and path length showed potential for use in binary categorization of performance.","Operators were unable to make accurate performance assessments of their peers by observing animated and non-animated simulations. The drone's position density was more effective in distinguishing better-than-average from lower-than-average performers than path length. There was a significant relationship between trust and performance for non-animated simulations, but not for animated simulations.","Flight path data from a teleoperated drone can be used to assess operator performance using position density and path length, but subjective assessments of performance were not accurate.","The human operator teleoperated a drone to find alphanumeric targets. The drone moved through the environment based on the operator's commands, and the operator used the drone's camera feed to locate the targets.",ANOVA; fisher's least significant distance post hoc test; Linear regression; Logistic regression,The study used Repeated Measures ANOVA (rANOVA) and Fisher's LSD post hoc test to analyze the differences in the number of targets found across the three runs. Linear mixed-effects regression was used to examine the relationship between subjective performance assessments (estimated speed and trust) and actual operator performance. Logistic regression was used to model the relationship between drone trajectory length and position density with operator performance (categorized as below or above average).,FALSE,Task-complexity,,,"The study did not explicitly manipulate any factors to influence trust. However, the task complexity implicitly changed across the three runs as participants gained experience. This is evidenced by the statement 'Their average performance on the task also improved significantly after 2 runs.' This indicates that the task became less complex for the participants as they progressed through the experiment. While the study did not manipulate the task complexity directly, the change in performance across runs indicates that the task became less complex for the participants as they gained experience. The study did not manipulate any other factors from the list provided.",10.1109/ICHMS59971.2024.10555633,https://ieeexplore.ieee.org/document/10555633/,"The likelihood of an agent’s success in achieving its goals and subgoals under specific operational constraints is an important factor in performance-based trust. Trust management systems are being proposed to monitor performance and other trust-related system variables to improve operational efficiency. We present in this paper an exploratory study on the feasibility of assessing the performance of drone operators in a target-finding task through subjective and objective evaluations of the drone’s flight data. Thirty-nine participants were trained in using a teleoperated drone to find specific alphanumeric targets arranged on the laboratory floor and thereafter performed the task three times in separate three-minute runs. Following this, they made subjective assessments of other operators’ performance based on viewing the flight paths flown. Linear mixed effects modelling of subjective performance assessments showed either non-significant or weak relationships between subjective assessments and actual operator performance. Binomial logistic regression modelling showed significant relationships between features of the drone’s flight pattern and operator performance. These results could inform the development of data-driven, performance-based trust management systems for use in collaborative human and machine environments."
"Kircher, Katja; Larsson, Annika; Hultgren, Jonas Andersson",Tactical Driving Behavior With Different Levels of Automation,2014,1,31,30,2,"1 participant had to terminate the study early because of technical difficulties, 1 driver experienced nausea and quitted before driving the last condition",Controlled Lab Environment,within-subjects,"Participants drove a simulator in four conditions: manual, intentional car following (ICF), adaptive cruise control (ACC), and ACC with adaptive steering (ACC-AS). They encountered three traffic events. Trust was measured with a questionnaire after the first two drives and after the final condition.",Participants drove a car simulator and reacted to different traffic events in different automation conditions.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the simulated vehicle in a driving simulator.,simulation,The study used a driving simulator to create an immersive driving experience.,simulated,The robot was a simulated vehicle within the driving simulator.,shared control (fixed rules),"The automation systems (ACC and ACC-AS) operated with fixed rules, and the driver could intervene.",Questionnaires,,Eye-tracking Data; Performance Metrics,"Trust was measured using a questionnaire, and eye-tracking and performance data were collected.",no modeling,Trust was not modeled computationally; only descriptive statistics were used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The level of automation was manipulated by using different driving conditions (manual, ICF, ACC, ACC-AS), which influenced the driver's control and expectations.","Trust was higher for ACC than ACC-AS, but no association between trust levels and behavior was found.","Drivers adapted their behavior to the automation, making informed choices about when to let the automation handle the situation and when to intervene. There was a trend for more braking in manual conditions, and higher brake jerks in automated conditions. Drivers also had higher trust in ACC than ACC-AS.","Drivers adapt their tactical behavior based on the type of automation, making informed choices about when to let the automation handle the driving or when to switch it off.","The robot (simulated vehicle) maintained speed and distance to the car ahead, and in the ACC-AS condition, also provided lateral control. The human participant drove the car, monitored the automation, and intervened when necessary.",ANOVA; Chi-squared; paired samples t-tests; Pearson correlation; t-test,The study used ANOVA to compare means of headway and brake jerk across different driving conditions. Chi-squared tests were used to analyze the distribution of braking behavior and lane exceedances across conditions. Paired samples t-tests were used to compare THW at different points in the curve event. Pearson's r was used to assess the correlation between brake and lateral jerks. T-tests were used to compare trust scores between automation systems and to compare brake and lateral jerks between ACC and ACC-AS conditions.,TRUE,Robot-autonomy; Task-complexity,,Robot-autonomy,"The study manipulated the level of automation in the driving task, which directly corresponds to 'Robot-autonomy'. Participants experienced four conditions: manual driving, intentional car following (ICF), adaptive cruise control (ACC), and ACC with adaptive steering (ACC-AS). These conditions represent different levels of automation and control delegation, thus manipulating the robot's autonomy. The task complexity was also implicitly manipulated as the different automation conditions changed the cognitive demands on the driver. For example, in manual driving, the driver is responsible for all aspects of control, while in ACC-AS, the system handles both longitudinal and lateral control, thus reducing the complexity of the task for the driver. The study found that trust levels were higher for ACC than ACC-AS, but no association between trust levels and behavior was found. Therefore, while the level of automation was manipulated, it did not directly impact the behavioral outcomes related to trust. The study explicitly states, 'no statistically significant association between the level of trust in the system and the likelihood to brake or steer could be found in any of the events.' This indicates that while trust was measured, the manipulation of autonomy did not result in a measurable impact on trust-related behaviors.",10.1109/TITS.2013.2277725,http://ieeexplore.ieee.org/document/6642088/,"This paper investigated how different types of automation affect tactical driving behavior, depending on trust in the system. Previous research indicates that drivers wait for automation to act, delegating the monitoring of trafﬁc situations. This would be especially true for those who have more trust in automation. Behavioral and gaze data from 30 participants driving an advanced simulator were recorded in four driving conditions, namely, manual driving, intentional car following, adaptive cruise control (ACC), and ACC with adaptive steering. Measures of trust in the systems were recorded with a questionnaire. Three fairly common trafﬁc events requiring a driver response were analyzed. Trust in automation was high among the participants, and no associations between trust levels and behavior could be found. Drivers seem to make informed choices on when to let the automation handle a situation and when to switch it off manually or via the vehicle controls. If drivers did not expect the system to be able to handle the situation, they usually resumed control before the automation reached its limits. If the automation was expected to be able to deal with the situation, control was usually not resumed. In addition, situations were dealt with in a tactically different manner with automation than without. Controlling the car with automation systems is thus accepted by drivers as being a different undertaking than driving in manual mode."
"Kluy, Lina; Roesler, Eileen",Working with Industrial Cobots: The Influence of Reliability and Transparency on Perception and Trust,2021,1,181,124,56,56 participants were excluded due to not meeting the inclusion criteria or incomplete data sets,Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of four conditions (high/low reliability and high/low transparency), completed questionnaires regarding control variables, watched a video of a robot performing an assembly task, and then completed questionnaires regarding dependent variables and a manipulation check.","Participants watched a video of a robot assembling a pedal car seat and were asked to imagine collaborating with the robot, then they rated the robot on several scales.",KUKA IIWA,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial,Evaluation,Survey/Questionnaire Completion,passive observation,Participants observed a video of the robot performing a task.,media,Participants watched a video of a simulated human-robot interaction.,simulated,The robot was presented in a video simulation.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions.,Questionnaires; Custom Scales,Godspeed Questionnaire; Human-Robot Trust Scale/Questionnaire (HRTS/HRITS),,Trust was measured using a questionnaire and a single item question.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"Reliability was manipulated by varying the robot's error rate, and transparency was manipulated by providing additional information about the system and process.","Higher reliability led to increased trust, and the effect of reliability on trust was more pronounced for low transparent robots. Transparency alone did not have a significant effect on trust.","The study found that transparency did not have a mitigating effect on trust for low reliability robots, and that transparency may even decrease trust for highly reliable robots. This is contrary to some previous findings.","Reliability positively affects trust, and this effect is more pronounced when transparency is low. Transparency alone did not have a significant effect on trust.",The robot picked a seat base based on color and held it for the human to fasten a nut and mount the plastic seat. The human observed the robot's actions and completed questionnaires.,ANOVA; ANOVA; Scheffe post-hoc,"The study used a 2x2 between-subjects ANOVA to analyze the manipulation check, examining the effect of reliability and transparency on perceived reliability. Additionally, a 2x2 between-subjects ANOVA was used to analyze the effects of reliability and transparency on the dependent variables: perceived safety, intelligence, likeability, and trust (measured by a trust scale and a single item). Post-hoc tests with Bonferroni-Holm correction were used to further investigate significant interaction effects found in the ANOVA for the trust scale.",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-accuracy,Robot-verbal-communication-content,"The study manipulated robot reliability by varying the robot's error rate in picking the correct colored seat base (66% vs 100% accuracy), which directly impacts task performance, thus 'Robot-accuracy' is chosen. The study also manipulated transparency by providing additional information about the system and process, which is a change in what is communicated, thus 'Robot-verbal-communication-content' is chosen. The results showed that 'Robot-accuracy' (reliability) had a significant impact on trust, with higher reliability leading to increased trust. However, 'Robot-verbal-communication-content' (transparency) alone did not have a significant effect on trust, although it did interact with reliability. Therefore, 'Robot-accuracy' is listed as a factor that impacted trust, and 'Robot-verbal-communication-content' is listed as a factor that did not impact trust on its own.",10.1177/1071181321651110,https://journals.sagepub.com/doi/10.1177/1071181321651110,"Industrial human-robot collaboration (HRC) is not yet widely spread but on the rise. This development raises the question about properties collaborative robots (cobots) need, to enable a pleasant and smooth interaction. Therefore, this study investigated the influence of transparency and reliability on perception of and trust towards cobots. A video-enhanced online study with 124 participants was conducted. Transparency was provided through the presentation of differing information, and reliability was manipulated through differing error rates. The results showed a positive effect of transparency on perceived safety and intelligence. Reliability had a positive effect on perceived intelligence, likeability and trust. The effect of reliability on trust was more pronounced for low transparent robots. The results indicate the relevance of carefully selected information to counteract negative effects of failures. Future research should transfer the study design into a real-life experiment with more fine-grained levels of transparency and reliability."
"Knutzen, Kathrin; Weidner, Florian; Broll, Wolfgang",Talk to me!: exploring stereoscopic 3D anthropomorphic virtual assistants in automated vehicles,2019,1,56,56,0,No participants were excluded,Controlled Lab Environment,between-subjects,Participants drove in a simulator with either a 2D or S3D virtual assistant. Distrust was gradually evoked by a car in front and fog. The virtual assistant provided feedback on why it did not overtake. The drive ended when the car was overtaken or after 200 seconds.,Participants were asked to drive in a conditionally automated driving simulator and decide when to take over control from the virtual assistant.,Unspecified,Autonomous Vehicles,Social; Research,Supervision,Monitoring,minimal interaction,Participants interacted with a virtual assistant in a driving simulator.,simulation,The study used a driving simulator to create an immersive experience.,simulated,The robot was a virtual assistant displayed on a screen.,shared control (fixed rules),"The virtual assistant made decisions based on pre-set rules, but the human could take over control.",Behavioral Measures; Questionnaires,,Performance Metrics,Trust was measured using self-reported questionnaires and behavioral measures such as control transition times.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The study manipulated the visualization modality of the virtual assistant (2D vs. S3D) to see how it affected trust and driving behavior.,"The S3D virtual assistant led to higher anthropomorphism and safer overtaking behavior, but no significant differences in self-reported trust were found.","The study found that the S3D virtual assistant led to safer overtaking behavior, but did not significantly increase self-reported trust. This suggests that the visual presentation of the assistant may influence behavior more than explicit trust ratings.","The implementation of a stereoscopic 3D virtual assistant impacts overtaking behavior, potentially fostering safety-consciousness and mitigating inappropriate usage of the system.",The virtual assistant provided information about the driving situation and reasons for not overtaking. The human participant monitored the driving situation and decided when to take over control from the virtual assistant.,Mann-Whitney U; t-test; Chi-squared,"The study used Mann-Whitney-U tests to analyze control transition times and anthropomorphism, as these were not normally distributed. Independent sample t-tests were used to analyze self-reported trust. Chi-square tests were used to analyze overtaking behavior. These tests were used to compare the effects of 2D and S3D virtual assistants on trust, anthropomorphism, and driving behavior.",TRUE,Robot-interface-design,Robot-interface-design,,"The study manipulated the visualization modality of the virtual assistant, presenting it in either 2D or stereoscopic 3D (S3D). This manipulation directly relates to the 'Robot-interface-design' category, as it involves changes to the visual display of the robot. The paper states, 'In a driving simulator experiment with a between-group design (n = 56), participants experienced either an anthropomorphic 2D or stereoscopic 3D virtual assistant.' The results showed that the S3D virtual assistant led to safer overtaking behavior, indicating an impact on trust-related behavior, even though self-reported trust was not significantly different. The paper states, 'Data on overtaking behavior show that a S3D VA can lead to safer overtaking behavior'. Therefore, the 'Robot-interface-design' is listed as a factor that impacted trust. There were no other factors that were manipulated in the study.",10.1145/3349263.3351503,https://dl.acm.org/doi/10.1145/3349263.3351503,"Trust in self-driving cars is considered to ensure appropriate usage of such vehicles. Representing the automation system as an anthropomorphic virtual assistant that communicates the system’s behavior could help to build and maintain trust. To explore such assistants, a prototype of an anthropomorphic virtual assistant was designed and integrated in a conditionally automated vehicle. In a driving simulator experiment with a between-group design (n = 56), participants experienced either an anthropomorphic 2D or stereoscopic 3D virtual assistant. While driving, distrust towards the automation system was gradually evoked. We analyzed trust and driving behavior. The current study yielded ﬁndings that the implementation of stereoscopic 3D impacts overtaking behavior, potentially fosters safety-consciousness and mitigates inappropriate usage of the system. We further report on the design of the virtual assistant."
"Kohn, Spencer C.; Quinn, Daniel; Pak, Richard; de Visser, Ewart J.; Shaw, Tyler H.",Trust Repair Strategies with Self-Driving Vehicles: An Exploratory Study,2018,2,60,60,0,No participants were excluded,Online Crowdsourcing,mixed design,"Participants completed demographics, personality, and trust questionnaires, then watched videos of driving scenarios with either a human or self-driving car, and rated performance, trustworthiness, and severity after each video.","Participants watched videos of driving scenarios and rated the performance, trustworthiness, and severity of the driver.",Unspecified,Autonomous Vehicles,Research,Evaluation,Rating,passive observation,Participants watched videos of driving scenarios.,media,Participants viewed videos of driving scenarios.,simulated,The robot was represented in a video simulation.,pre-programmed (non-adaptive),The self-driving car followed a pre-programmed driving path.,Questionnaires,Propensity to Trust Machines scale,,Trust was measured using a questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The study manipulated the type of driver (human or self-driving) and the type of driving error to see how these factors affected trust.,"The study found no significant difference in trust ratings between human and self-driving car errors, but error type significantly affected trust.","The study found that participants did not rate errors committed by human drivers and self-driving cars differently, which is contrary to some theories about how people perceive automation.",Participants rated errors committed by self-driving cars similarly to errors committed by human drivers.,"Participants watched videos of driving scenarios and rated the performance, trustworthiness, and severity of the driver. The robot (self-driving car) performed driving actions, and the human participant evaluated the robot's performance.",ANOVA; ANOVA,"A repeated measures MANOVA was used to analyze the subjective evaluations of performance, trustworthiness, and severity, with driver type as a between-subjects variable and error type as a within-subjects variable. Univariate analyses were then conducted for each subjective measure to follow up on the effect of driver type. Greenhouse-Geisser corrections were applied to analyses of the error conditions.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the type of driving error (e.g., u-turn, left turn, delayed start, overshooting) which directly impacts the robot's (self-driving car) accuracy in performing the driving task. The paper states, 'Experiment 1 manipulated error type and driver types to determine whether driver type (human versus self-driving) affected how participants assessed errors.' The different error types directly influenced the performance of the driving task, thus 'Robot-accuracy' is the most appropriate category. While the study also manipulated the driver type (human vs. self-driving), this did not impact trust ratings, so it is not included in the factors that impacted trust. The study found a significant effect of error type on trust, performance, and severity ratings, indicating that the manipulated errors (Robot-accuracy) impacted trust.",10.1177/1541931218621254,http://journals.sagepub.com/doi/10.1177/1541931218621254,"Trust is important for any relationship, especially so with self-driving vehicles: passengers must trust these vehicles with their life. Given the criticality of maintaining passenger’s trust, yet the dearth of self-driving trust repair research relative to the growth of the self-driving industry, we conducted two studies to better understand how people view errors committed by self-driving cars, as well as what types of trust repair efforts may be viable for use by self-driving cars. Experiment 1 manipulated error type and driver types to determine whether driver type (human versus self-driving) affected how participants assessed errors. Results indicate that errors committed by both driver types are not assessed differently. Given the similarity, experiment 2 focused on self-driving cars, using a wide variety of trust repair efforts to confirm human-human research and determine which repairs were most effective at mitigating the effect of violations on trust. We confirmed the pattern of trust repairs in human-human research, and found that some apologies were more effective at repairing trust than some denials. These findings help focus future research, while providing broad guidance as to potential methods for approaching trust repair with self-driving cars."
"Kohn, Spencer C.; Quinn, Daniel; Pak, Richard; de Visser, Ewart J.; Shaw, Tyler H.",Trust Repair Strategies with Self-Driving Vehicles: An Exploratory Study,2018,2,52,52,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants read text vignettes describing self-driving car errors, followed by a trust repair strategy, and then rated trust, competence, and severity.","Participants read vignettes of self-driving car errors and trust repair strategies, then rated trust, competence, and severity.",Unspecified,Autonomous Vehicles,Research,Evaluation,Rating,passive observation,Participants read text vignettes describing self-driving car errors.,media,Participants viewed diagrams and text vignettes.,hypothetical,The robot was described in text and diagrams.,pre-programmed (non-adaptive),The self-driving car was described as performing pre-programmed actions.,Questionnaires,,,Trust was measured using a questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the type of trust repair strategy (apologies, denials, etc.) following a self-driving car error to see how these affected trust.","Timed apologies were more effective at repairing trust than denials or no repair, replicating findings from human-human research.","The study found that some apologies were more effective than some denials at repairing trust, which mirrors findings from human-human research.",Apologies were more effective at repairing trust after a self-driving car error than denials or no repair.,"Participants read text vignettes describing self-driving car errors and trust repair strategies, then rated trust, competence, and severity. The robot (self-driving car) performed driving actions, and the human participant evaluated the robot's performance and the effectiveness of the repair strategy.",ANOVA; ANOVA; Bonferroni correction,"A repeated measures MANOVA was used to analyze the subjective evaluations of trust, competence, and severity, with the 10 types of repairs as a within-subjects variable. Univariate analyses with Greenhouse-Geisser corrections were conducted on the individual subjective measures. Pairwise comparisons of repair strategies for trust and competence were also conducted using a corrected alpha of .005.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the type of trust repair strategy (apologies, denials, etc.) presented to participants after a self-driving car error. The paper states, 'The second study manipulated a wide range of active trust repair strategies, that is, behavioral remedial actions to restore trust following undesirable actions.' These repair strategies are presented as text-based interventions, which directly changes the content of the robot's (self-driving car) verbal communication. Therefore, 'Robot-verbal-communication-content' is the most appropriate category. The study found that timed apologies were more effective at repairing trust than denials or no repair, indicating that the manipulated communication content impacted trust. The study did not find any factors that did not impact trust.",10.1177/1541931218621254,http://journals.sagepub.com/doi/10.1177/1541931218621254,"Trust is important for any relationship, especially so with self-driving vehicles: passengers must trust these vehicles with their life. Given the criticality of maintaining passenger’s trust, yet the dearth of self-driving trust repair research relative to the growth of the self-driving industry, we conducted two studies to better understand how people view errors committed by self-driving cars, as well as what types of trust repair efforts may be viable for use by self-driving cars. Experiment 1 manipulated error type and driver types to determine whether driver type (human versus self-driving) affected how participants assessed errors. Results indicate that errors committed by both driver types are not assessed differently. Given the similarity, experiment 2 focused on self-driving cars, using a wide variety of trust repair efforts to confirm human-human research and determine which repairs were most effective at mitigating the effect of violations on trust. We confirmed the pattern of trust repairs in human-human research, and found that some apologies were more effective at repairing trust than some denials. These findings help focus future research, while providing broad guidance as to potential methods for approaching trust repair with self-driving cars."
"Kopp, Tobias; Baumgartner, Marco; Kinkel, Steffen",Success factors for introducing industrial human-robot interaction in practice: an empirically driven framework,2021,1,81,81,0,No participants were excluded,Online Crowdsourcing,between-subjects,Participants were randomly assigned to one of three scenarios describing a company introducing a cobot for different reasons. They then rated the importance of various success factors for the HRI solution.,Participants rated the importance of various success factors for the HRI solution in a given scenario.,Unspecified,Collaborative Robots (Cobots),Industrial,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the robot and the interaction scenario.,media,The interaction was based on written descriptions of the robot and scenarios.,hypothetical,"The robot was only described in text, without any visual representation.",not autonomous,"The robot's actions were only described in text, without any real autonomy.",Questionnaires,,,Trust was measured using a questionnaire with a Likert scale.,no modeling,Trust was measured but not modeled computationally.,Observational & Survey Studies,Quantitative Surveys,Indirect Manipulation,"Participants were presented with different scenarios framing the reason for introducing a cobot, which indirectly influenced their perception of the importance of different success factors.","The study found that the perceived importance of success factors was largely independent of the goal associated with cobot introduction, suggesting that the framing had minimal impact on trust outcomes.","The study found that financial factors were frequently mentioned in free-text fields but were not rated as highly important as other factors, such as occupational safety and trust in the robot. The study also found that the perceived importance of success factors was largely independent of the goal associated with cobot introduction.","The study identified key success factors for introducing industrial collaborative robots, including occupational safety, addressing fear of job loss, and ensuring trust in the robot, and found that these factors are largely independent of the specific goals of the company.","The robot's actions were described in text, and the human participant's task was to read the scenario and rate the importance of various success factors.",ANOVA; ANOVA; Spearman correlation,"The study used a one-way ANOVA to check for differences in realism ratings across the three scenarios. A one-way MANOVA was used to examine whether the importance of success factors depended on the goal of cobot introduction (independent variable with three levels), with the evaluated success factors as dependent variables. Additionally, Spearman's correlation was used to compare the distribution of industrial sectors in the sample with the distribution in the German economy.",TRUE,Task-environment,,Task-environment,"The study manipulated the task environment by presenting participants with one of three different scenarios describing a company introducing a cobot for different reasons (increasing job attractiveness, increasing flexibility, or increasing productivity). Although the scenarios were designed to influence the perceived importance of success factors, the study found that the perceived importance of success factors was largely independent of the goal associated with cobot introduction. Therefore, while the task environment was manipulated, it did not impact trust outcomes. The manipulation is classified as 'Task-environment' because it changes the context in which the participants are evaluating the success factors, which is a change to the working conditions.",10.1007/s00170-020-06398-0,http://link.springer.com/10.1007/s00170-020-06398-0,"Human-robot interaction (HRI) promises to be a means whereby manufacturing companies will be able to address current challenges like a higher demand for customization. However, despite comparably low costs, there are only few applications in practice. To date, it remains unclear which factors facilitate or hinder the successful introduction of industrial collaborative robots (cobots). In a three-step approach, we first developed a comprehensive two-dimensional framework covering three separate phases and four essential components for human-robot working systems. Secondly, we reviewed related literature to identify relevant success factors. Thirdly, in an online survey we asked leading representatives of German manufacturing companies (n = 81) to assess the importance of these factors from a practical point of view. The results reveal that besides technology-related factors like occupational safety and appropriate cobot configuration, employee-centered factors like the fear of job loss and ensuring an appropriate level of trust in the robot are considered important. However, company representatives seem to underestimate the impact of subtle measures to increase employee acceptance which could be incorporated into internal communication strategies prior to and during the introduction of cobots. Comparative analysis based on three distinct application scenarios suggests that most success factors’ practical importance is independent of the motivation for implementing HRI. Furthermore, answers from practitioners in free-text fields reveal that success factors which intuitively come to their mind such as financial factors are not necessarily perceived most important. Finally, we argue for more application-oriented research that focuses on practically relevant factors to guide HRI research, inform cobot development, and support companies in overcoming apparent barriers."
"Kopp, Tobias; Baumgartner, Marco; Kinkel, Steffen","“It's not Paul, it's a robot”: The impact of linguistic framing and the evolution of trust and distrust in a collaborative robot during a human-robot interaction",2023,1,80,68,12,"12 data sets were excluded due to unforeseen malfunctions of the cobot during interaction, which required the experimenter to intervene",Controlled Lab Environment,mixed design,"Participants completed questionnaires before and after reading a framing text, and after a short interaction with a cobot. The interaction involved assembling gear transmissions with the robot's assistance.",Participants assembled three gear transmissions using 3D-printed parts with the help of a collaborative robot.,UR5,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically collaborated with the robot to assemble gear transmissions.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,shared control (fixed rules),The robot performed pre-programmed actions and responded to button presses from the participant.,Questionnaires; Multidimensional Measures,Trust in Automation Scale (TAS); Godspeed Questionnaire; Negative Attitude towards Robots Scale (NARS),Video Data,Trust was measured using questionnaires and video data was collected.,no modeling,No computational model of trust was used; descriptive statistics were used.,Empirical HRI Studies,Physical Robot Studies,Indirect Manipulation,"The robot was framed as either human-like or machine-like through textual descriptions, influencing the perceived human-likeness and expectations of the robot.","The framing manipulation did not significantly affect trust, but trust increased after the interaction. Distrust decreased after framing and after the interaction.","The framing manipulation was ineffective in influencing the perceived human-likeness of the robot, and trust increased during the interaction, while distrust decreased. The study also found that students had more positive attitudes towards robots than factory workers in a previous study.","Linguistic framing of a cobot's human-likeness did not significantly affect trust in a sample of students, but trust increased and distrust decreased after a real-life interaction with the robot.",The robot provided gear wheels and held a component for the participant to tighten screws. The human assembled the gear transmissions using the parts provided by the robot and tightened the screws.,ANOVA; t-test; Mann-Whitney U; Wilcoxon rank sum,"The study used a mixed analysis of variance (ANOVA) design to analyze the pre-post measurement of trust and distrust, with a between-subject factor of anthropomorphic vs. machine-like framing. Independent t-tests were used to compare the perceived human-likeness of the robot between the two framing conditions. Mann-Whitney U tests were used as a non-parametric alternative to t-tests for free-text data. Wilcoxon tests were used to analyze the time-dependent evolution of trust and distrust across the three measurement points (pre-framing, post-framing, post-interaction).",TRUE,Robot-verbal-communication-content,,Robot-verbal-communication-content,"The study manipulated the linguistic framing of the robot through textual descriptions, presenting it as either human-like (named 'Paul', using personal pronouns, active voice, and anthropomorphic vocabulary) or machine-like (named 'UR-5', using impersonal terms, passive voice, and technical vocabulary). This manipulation directly altered the content of the verbal communication about the robot, thus fitting the 'Robot-verbal-communication-content' category. The study found that this framing manipulation did not significantly affect trust, so 'Robot-verbal-communication-content' is listed under 'factors_that_did_not_impact_trust'. There were no factors that impacted trust due to the manipulation.",10.1016/j.ijhcs.2023.103095,https://linkinghub.elsevier.com/retrieve/pii/S1071581923001040,"An appropriate level of human-robot trust is widely acknowledged as a crucial factor for successful human-robot interaction (HRI). Previous research has addressed anthropomorphic design but mostly neglected linguistic framing as a means to influence trust by altering the perceived human-likeness of a robot. Similarly, timedependant patterns of trust and distrust during an HRI have rarely been investigated, hindering the develop­ ment of a coherent theoretical framework on framing effects, their formation conditions, and their relation to trust and distrust evolution in the HRI context. A previous online study suggested that linguistic framing mod­ ulates inexperienced factory workers’ trust in a collaborative robot at the workplace. This article presents a follow-up study with a sample of students and an realistic HRI setting. Despite using similar framing stimuli, the study failed to replicate significant framing effects on trust, suggesting their context-sensitivity and dependence on individual characteristics of the recipients of the framing. Additionally, a significant increase in trust and decrease in distrust during the HRI highlights the need for time-dependant considerations. The results call for a more fine-grained investigation of linguistic framing effects as well as of trust and distrust evolution in order to develop a comprehensive theoretical framework applicable to the HRI context."
"Kox, E. S.; Kerstholt, J. H.; Hueting, T. F.; De Vries, P. W.",Trust repair in human-agent teams: the effectiveness of explanations and expressing regret,2021,1,66,66,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants played a first-person shooter game where they received advice from a virtual robot; after a trust violation, the robot offered an apology with varying components (regret, explanation, both, or neither); trust and advice acceptance were measured at three time points.",Participants were tasked with navigating a virtual environment and following the advice of a virtual robot to avoid enemies.,Unspecified,Humanoid Robots,Research; Social,Game,Cooperative Game,minimal interaction,"Participants interacted with a virtual robot in a game environment, receiving auditory messages and providing responses via questionnaires.",simulation,The interaction took place in a simulated first-person shooter game environment.,simulated,The robot was represented graphically as a virtual robot in the game environment.,wizard of oz (directly controlled),The robot's actions were controlled by an experimenter using the Wizard of Oz method.,Questionnaires; Behavioral Measures,Godspeed Questionnaire,,Trust was measured using a questionnaire with subdimensions and a single-item measure of advice acceptance.,no modeling,"The study did not use computational modeling of trust, relying on statistical analysis of the collected data.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's apology was manipulated by varying the presence of an expression of regret and an explanation, which was intended to influence trust after a trust violation.","Trust significantly recovered only when the apology included an expression of regret, and this effect was stronger when an explanation was also provided.","The groups that received an apology including an expression of regret showed steeper declines in trust in response to the trust violation than the groups that did not, leading to counterintuitive outcomes where the 'neither regret nor explanation' condition scored higher on final trust than the 'both regret and explanation' condition. However, the results show a steeper increase in trust in the trust repair phase when the agent provided an expression of regret.","Apologies including an expression of regret were most effective in repairing trust after a trust violation, and this effect was stronger when an explanation was added.",The robot provided auditory advice to the human participant about whether to take shelter or continue moving in a virtual game environment; the human participant followed the advice and completed questionnaires.,ANOVA; LSD post-hoc; Pearson correlation,"The study used Repeated-Measures ANOVA to analyze the effects of time, regret, and explanation on advice taking and trust. LSD post-hoc tests were used to further investigate significant main effects and interactions found in the ANOVAs. Additionally, correlations were calculated to examine the relationships between trust, advice taking, and other variables such as perceived anthropomorphism, likeability, intelligence, and usefulness.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the content of the robot's verbal communication after a trust violation. Specifically, the robot either provided an explanation for the error, expressed regret, both, or neither. This manipulation directly relates to the content of the robot's message, making 'Robot-verbal-communication-content' the most appropriate category. The results showed that the presence of regret in the robot's communication significantly impacted trust repair, and this effect was stronger when an explanation was also provided. Therefore, 'Robot-verbal-communication-content' is also listed as a factor that impacted trust. There were no other factors manipulated, and no factors that were manipulated that did not impact trust.",10.1007/s10458-021-09515-9,https://link.springer.com/10.1007/s10458-021-09515-9,"The role of intelligent agents becomes more social as they are expected to act in direct interaction, involvement and/or interdependency with humans and other artificial entities, as in Human-Agent Teams (HAT). The highly interdependent and dynamic nature of teamwork demands correctly calibrated trust among team members. Trust violations are an inevitable aspect of the cycle of trust and since repairing damaged trust proves to be more difficult than building trust initially, effective trust repair strategies are needed to ensure durable and successful team performance. The aim of this study was to explore the effectiveness of different trust repair strategies from an intelligent agent by measuring the development of human trust and advice taking in a Human-Agent Teaming task. Data for this study were obtained using a task environment resembling a first-person shooter game. Participants carried out a mission in collaboration with their artificial team member. A trust violation was provoked when the agent failed to detect an approaching enemy. After this, the agent offered one of four trust repair strategies, composed of the apology components explanation and expression of regret (either one alone, both or neither). Our results indicated that expressing regret was crucial for effective trust repair. After trust declined due to the violation by the agent, trust only significantly recovered when an expression of regret was included in the apology. This effect was stronger when an explanation was added. In this context, the intelligent agent was the most effective in its attempt of rebuilding trust when it provided an apology that was both affective, and informational. Finally, the implications of our findings for the design and study of Human-Agent trust repair are discussed."
"Körber, Moritz; Baseler, Eva; Bengler, Klaus",Introduction matters: Manipulating trust in automation and reliance in automated driving,2018,1,40,40,3,3 participants were excluded due to unreasonable high values in take-over time and minTTC,Controlled Lab Environment,between-subjects,"Participants were divided into two groups (Trust Lowered and Trust Promoted) and received different introductory information about an automated driving system. They then experienced three driving situations in a simulator, with a non-driving related task (NDRT) between each situation. Trust was measured using a questionnaire at three time points.",Participants drove in a simulated automated vehicle and were required to take over control in a critical situation. They also had the option to take over in two non-critical situations. Participants also engaged in a non-driving related task (NDRT) between the driving situations.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the automated driving system in a simulator, with limited physical interaction.",simulation,The study used a driving simulator to create an immersive environment for the participants.,simulated,The automated vehicle was simulated in a driving simulator.,shared control (fixed rules),"The automated driving system operated independently but with fixed rules, requiring human take-over in certain situations.",Questionnaires; Behavioral Measures; Performance-Based Measures,Trust in Automation Scale (TAS); Affinity for Technological Interaction (ATI) Scale,Eye-tracking Data; Performance Metrics,"Trust was assessed using questionnaires, behavioral measures (take-over interventions), and performance metrics (take-over time, collisions), and eye-tracking data was collected.","parametric models (e.g., regression)",The study used Bayesian parameter estimation and Bayesian hypothesis tests to analyze the relationship between trust and other variables.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated introductory information about the automated driving system to influence participants' expectations and perceived reliability, which in turn affected their trust.","Trust was higher in the Trust Promoted group after the introductory drive, but this effect diminished after the experimental drive. The manipulation influenced reliance behavior and take-over performance.","The initial manipulation of trust through introductory information had a temporary effect, with trust levels converging after the experimental drive. Participants with higher trust were more likely to collide in the critical take-over situation.","The study demonstrated that prior information influences trust in automation, which in turn affects reliance behavior, monitoring, and take-over performance in automated driving.","The robot (automated vehicle) maintained lane and speed, and the human monitored the system and took over control when necessary. The human also engaged in a non-driving related task (NDRT) during automated driving.",bayesian parameter estimation; bayesian hypothesis tests; bayes factors; t-test; cronbach's alpha; odds ratio; relative risk,"The study used Bayesian parameter estimation and Bayesian hypothesis tests, including Bayes Factors, to analyze the data. These methods were used to quantify evidence for the alternative model over the null model. Independent samples t-tests were also used, and their results were compared to Bayes Factors. Cronbach's alpha was used to assess the internal consistency of the Affinity for Technology questionnaire. Odds ratios and relative risks were calculated to compare the likelihood of interventions and collisions between the two groups. The analysis also included correlations between trust and various measures, such as gaze behavior, take-over time, and take-over quality.",TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-verbal-communication-content; Robot-accuracy,,"The study manipulated the introductory information about the automated driving system, which falls under 'Robot-verbal-communication-content'. The Trust Promoted group received information suggesting high reliability and competence, while the Trust Lowered group received information highlighting potential limitations and the need for human oversight. This manipulation was done through text and video. Specifically, the Trust Lowered group saw a non-critical take-over in the video and was explicitly reminded of their responsibility, while the Trust Promoted group did not. Additionally, the introductory drive involved a critical take-over for the Trust Lowered group and a non-critical take-over for the Trust Promoted group, which is a manipulation of 'Robot-accuracy' as it directly influenced the perceived reliability of the system. The study found that both the introductory information and the introductory drive (which influenced the perceived reliability of the system) impacted trust levels, as evidenced by the differences in trust scores after the introductory drive. The initial manipulation of trust through introductory information had a temporary effect, with trust levels converging after the experimental drive, indicating that the manipulation of 'Robot-verbal-communication-content' and 'Robot-accuracy' impacted trust.",10.1016/j.apergo.2017.07.006,https://linkinghub.elsevier.com/retrieve/pii/S0003687017301606,"Trust in automation is a key determinant for the adoption of automated systems and their appropriate use. Therefore, it constitutes an essential research area for the introduction of automated vehicles to road trafﬁc. In this study, we investigated the inﬂuence of trust promoting (Trust promoted group) and trust lowering (Trust lowered group) introductory information on reported trust, reliance behavior and takeover performance. Forty participants encountered three situations in a 17-min highway drive in a conditionally automated vehicle (SAE Level 3). Situation 1 and Situation 3 were non-critical situations where a take-over was optional. Situation 2 represented a critical situation where a take-over was necessary to avoid a collision. A non-driving-related task (NDRT) was presented between the situations to record the allocation of visual attention. Participants reporting a higher trust level spent less time looking at the road or instrument cluster and more time looking at the NDRT. The manipulation of introductory information resulted in medium differences in reported trust and inﬂuenced participants' reliance behavior. Participants of the Trust promoted group looked less at the road or instrument cluster and more at the NDRT. The odds of participants of the Trust promoted group to overrule the automated driving system in the non-critical situations were 3.65 times (Situation 1) to 5 times (Situation 3) higher. In Situation 2, the Trust promoted group's mean take-over time was extended by 1154 ms and the mean minimum time-to-collision was 933 ms shorter. Six participants from the Trust promoted group compared to no participant of the Trust lowered group collided with the obstacle. The results demonstrate that the individual trust level inﬂuences how much drivers monitor the environment while performing an NDRT. Introductory information inﬂuences this trust level, reliance on an automated driving system, and if a critical take-over situation can be successfully solved."
"Körber, Moritz",Theoretical Considerations and Development of a Questionnaire to Measure Trust in Automation,2019,2,94,94,0,No participants were excluded,Online Crowdsourcing,,"Participants watched two videos of an automated driving system, then completed a questionnaire.",Participants watched videos of an automated driving system and completed a questionnaire.,Unspecified,Autonomous Vehicles,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants watched videos of an automated driving system.,media,Participants watched videos of an automated driving system.,simulated,The robot was presented in videos.,pre-programmed (non-adaptive),The automated driving system operated based on pre-programmed behavior.,Questionnaires,,,Trust was measured using a questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,No factors were manipulated in this study.,,,This study was used for item analysis of a new questionnaire.,Participants watched videos of an automated driving system and completed a questionnaire.,item analysis; internal consistency; omega coefficients,"This study focused on item analysis of a newly developed questionnaire. Item difficulty, standard deviation, item-total correlation, internal consistency, overlap with other items, and response quote were used to eliminate items. The internal consistency of the scales was assessed using omega coefficients.",FALSE,,,,"This study did not manipulate any factors. The study was focused on item analysis of a newly developed questionnaire. Participants watched videos and completed a questionnaire, but there was no manipulation of any variables to influence trust. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust.",,http://link.springer.com/10.1007/978-3-319-96074-6_2,"The increasing number of interactions with automated systems has sparked the interest of researchers in trust in automation because it predicts not only whether but also how an operator interacts with an automation. In this work, a theoretical model of trust in automation is established and the development and evaluation of a corresponding questionnaire (Trust in Automation, TiA) are described."
"Körber, Moritz",Theoretical Considerations and Development of a Questionnaire to Measure Trust in Automation,2019,2,58,58,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants watched a video of a conditionally automated highway drive, either with a reliable or non-reliable system, and then completed a questionnaire.",Participants watched a video of an automated driving system and completed a questionnaire.,Unspecified,Autonomous Vehicles,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants watched videos of an automated driving system.,media,Participants watched videos of an automated driving system.,simulated,The robot was presented in videos.,pre-programmed (non-adaptive),The automated driving system operated based on pre-programmed behavior.,Questionnaires,,,Trust was measured using a questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the automated driving system was manipulated by showing either a perfectly functioning system or one with a takeover request, which was intended to influence trust.",Participants in the reliable condition reported higher trust ratings than those in the non-reliable condition.,The subscale Trust in Automation was more sensitive to group differences than the full questionnaire.,The study provided preliminary evidence for the factor structure of the trust questionnaire and showed that reliability manipulation influenced trust ratings.,Participants watched a video of an automated driving system and completed a questionnaire.,t-test; Factor analysis; Chi-squared; Root Mean Square Error; tucker-lewis index; omega coefficients,"This study used a t-test to compare the reliability ratings between the reliable and non-reliable conditions. An exploratory factor analysis (EFA) was conducted to assess the factor structure of the questionnaire. Fit indices from confirmatory factor analysis (CFA), including the chi-squared test, RMSEA, and Tucker-Lewis Index (TLI), were reported to evaluate the model fit. Omega coefficients were used to assess the internal consistency of the scales.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the reliability of the automated driving system by showing either a perfectly functioning system or one with a takeover request. This manipulation directly impacts the accuracy of the robot's performance, as the non-reliable condition introduces an error (takeover request) that the reliable condition does not have. Therefore, 'Robot-accuracy' is the most appropriate category. The paper explicitly states that participants in the reliable condition reported higher trust ratings than those in the non-reliable condition, indicating that the manipulation of 'Robot-accuracy' impacted trust. No other factors were manipulated, and therefore, no factors did not impact trust.",,http://link.springer.com/10.1007/978-3-319-96074-6_2,"The increasing number of interactions with automated systems has sparked the interest of researchers in trust in automation because it predicts not only whether but also how an operator interacts with an automation. In this work, a theoretical model of trust in automation is established and the development and evaluation of a corresponding questionnaire (Trust in Automation, TiA) are described."
"Krantz, Amandus; Balkenius, Christian; Johansson, Birger",Using Speech to Reduce Loss of Trust in Humanoid Social Robots,2022,1,227,227,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants viewed videos of a robot exhibiting faulty or non-faulty gaze behaviors. In Experiment 1, the robot also spoke, while in Experiment 2 it did not. Participants then evaluated the robot's trustworthiness, likeability, animacy, and perceived intelligence.",Participants were asked to evaluate their perception of the robot's trustworthiness after watching a video of the robot.,Epi,Humanoid Robots,Research,Evaluation,Trustworthiness Rating,passive observation,Participants passively observed the robot through videos.,media,Participants watched videos of the robot.,physical,"The robot was a physical robot, but presented in a video.",pre-programmed (non-adaptive),The robot's behaviors were pre-recorded and not adaptive.,Questionnaires,Trust Perception Scale - HRI; Godspeed Questionnaire; Negative Attitude towards Robots Scale (NARS),,Trust was measured using questionnaires before and after the interaction.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's gaze behavior was manipulated to be either faulty or non-faulty, and the presence of speech was also manipulated. The task was framed differently in the two experiments.","Faulty behavior reduced trust when the robot did not speak, but this loss was mitigated when the robot spoke. Non-faulty behavior did not significantly affect trust regardless of speech.","The main finding was that speech mitigated the loss of trust due to faulty behavior. The study also found that speaking robots were perceived as more intelligent, likeable, and animated.",A humanoid robot with the ability to speak does not suffer the same loss of trust when displaying faulty behavior as a robot without the ability to speak.,"The robot either looked at an object correctly or incorrectly, and in one condition, it also spoke about the object. The human participant watched the video and then rated the robot's trustworthiness.",Mann-Whitney U; cronbach's alpha,"The study used Mann-Whitney U tests to compare trust levels between different conditions (faulty vs. non-faulty gaze behavior, and between experiments). Cronbach's Alpha was used to assess the internal consistency of the Godspeed questionnaires.",TRUE,Robot-nonverbal-communication; Robot-verbal-communication-content,Robot-nonverbal-communication; Robot-verbal-communication-content,,"The study manipulated the robot's gaze behavior (faulty vs. non-faulty), which falls under 'Robot-nonverbal-communication' as it involves physical movements of the robot's head. Additionally, the presence or absence of speech was manipulated, which is categorized as 'Robot-verbal-communication-content' because it changes what the robot communicates (either facts about the object or nothing). The paper states, 'In Experiment 1, once the gaze behaviour had been displayed, the robot would play a pre-recorded audio file of a computerized voice presenting a number of facts about the object that had been displayed.' and 'Experiment 2 had no speech component'. Both of these manipulations impacted trust. The paper states, 'In Experiment 1, no significant difference was found between the faulty and non-faulty conditions (Mann-Whitney U, p = 0.179). However, once the speech of the robot was removed in Experiment 2, a significant difference was found (Mann-Whitney U, p < 0.05).' This indicates that both the gaze behavior and the presence of speech influenced trust levels. The paper also states, 'the combined results of the two experiments show that, if the robot behaves in a non-faulty manner, unsurprisingly, trust in the robot remains largely unaffected, regardless of whether it can speak. However, once the robot is perceived as being faulty, having the ability to speak seems to reduce the resulting loss of trust, making the faulty robot appear about as trustworthy as the non-faulty ones.' This further supports that both factors impacted trust.",,http://arxiv.org/abs/2208.13688,"We present data from two online human-robot interaction experiments where 227 participants viewed videos of a humanoid robot exhibiting faulty or non-faulty behaviours while either remaining mute or speaking. The participants were asked to evaluate their perception of the robot’s trustworthiness, as well as its likeability, animacy, and perceived intelligence. The results show that, while a non-faulty robot achieves the highest trust, an apparently faulty robot that can speak manages to almost completely mitigate the loss of trust that is otherwise seen with faulty behaviour. We theorize that this mitigation is correlated with the increase in perceived intelligence that is also seen when speech is present."
"Kraus, Johannes Maria; Nothdurft, Florian; Hock, Philipp; Scholz, David; Minker, Wolfgang; Baumann, Martin",Human After All: Effects of Mere Presence and Social Interaction of a Humanoid Robot as a Co-Driver in Automated Driving,2016,1,16,16,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants completed a pre-test questionnaire, practiced with the driving simulator, received information about the system, completed three experimental trials, completed a post-test questionnaire, and were debriefed.",Participants drove in an automated mode on a foggy rural road and decided whether to leave automation on or switch to manual mode when approaching a slow car.,Nao,Humanoid Robots,Research; Social,Social,Conversation,minimal interaction,Participants interacted with the robot through a driving simulator and verbal instructions.,simulation,Participants interacted with a driving simulator in a controlled lab environment.,physical,The robot was physically present in the driving simulator setup.,wizard of oz (directly controlled),The robot's speech and actions were controlled by a wizard.,Questionnaires,Jian et al. Trust Scale,,Trust was measured using a pre- and post-test questionnaire.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The study manipulated the presence of a physical robot and its social interaction to influence trust.,The presence of a robot and additional social interaction increased trust in the automated driving system.,"The Social NAO condition showed a significantly higher increase in trust compared to the Voice Only condition, with medium effect sizes.",The introduction of a humanoid robot with social interaction significantly increased trust in an automated driving system compared to a voice-only system.,"The robot provided driving-related information and engaged in small talk, while the human drove in an automated mode and decided when to switch to manual mode.",Kruskal-Wallis; wilcoxon's test,"The study used the non-parametric Kruskal-Wallis test to examine overall differences in trust increase between the three study groups (Voice Only, NAO, and Social NAO). Wilcoxon's test was then used for pairwise comparisons between the groups to determine which specific group differences were significant. The analysis focused on the change in trust scores from pre-test to post-test.",TRUE,Robot-nonverbal-communication; Robot-verbal-communication-content; Robot-social-attitude,Robot-nonverbal-communication; Robot-social-attitude,Robot-verbal-communication-content,"The study manipulated the presence of a physical robot (NAO) and its social interaction. The 'Voice Only' condition had no physical robot, while the 'NAO' condition had the robot present, which constitutes a change in 'Robot-nonverbal-communication' through the addition of physical presence, body movements, gestures, and gaze direction. The 'Social NAO' condition added 'small talk' to the 'NAO' condition, which is a change in 'Robot-social-attitude' as it introduces a more social and engaging interaction style. The content of the driving-related information was the same across all conditions, which is why 'Robot-verbal-communication-content' is included as a manipulated factor, but it did not impact trust as the content was the same across all conditions. The study found that the presence of the robot and the additional social interaction significantly increased trust, indicating that 'Robot-nonverbal-communication' and 'Robot-social-attitude' impacted trust. The 'Robot-verbal-communication-content' did not impact trust as the content was the same across all conditions.",10.1145/3004323.3004338,https://dl.acm.org/doi/10.1145/3004323.3004338,"In automated cars social agents could facilitate trust generation. We conducted a driving simulator study in which we manipulated the human-likeness of a spokendialogue system (SDS). In a simple rural road overtaking paradigm users either interacted with a SDS with no physical presence (Voice Only), with an SDS represented by the humanoid robot NAO providing road advice (NAO) or with a NAO with additional social interaction (Social NAO). The Social NAO condition showed a significantly higher trust as compared to the Voice Only condition. All group differences were associated with medium effect sizes. This study provides some evidence that anthropomorphic features increase trust in automated driving systems."
"Kraus, Johannes Maria; Forster, Yannick; Hergeth, Sebastian; Baumann, Martin",Two Routes to Trust Calibration: Effects of Reliability and Brand Information on Trust in Automation,2019,1,623,519,104,Participants were excluded if the manipulation checks indicated poor memory for the study stimuli (wrong OEM brand selected; for low reliability a value of ≥3; for high reliability a value of ≤3) or if their processing time indicated speeding (half the time of the sample median (<24min)).,Online Crowdsourcing,between-subjects,"Participants were presented with one-pagers containing brand and reliability information, then rated their trust in the system. They then watched four videos of the system in use and rated their trust again, followed by personality questionnaires. Finally, they were informed that the function test reports were fictional.","Participants read descriptions of an automated driving system, watched videos of it in use, and completed questionnaires about their trust in the system and their personality traits.",Unspecified,Autonomous Vehicles,Other: Automated driving,Evaluation,Survey/Questionnaire Completion,passive observation,Participants watched videos of the automated driving system.,media,Participants watched videos of the automated driving system in use.,simulated,The robot was presented through videos.,fully autonomous (limited adaptation),"The automated driving system was described as operating autonomously, but with limited adaptation.",Questionnaires,Automation Trust Scale (ATS),,Trust was measured using the Automation Trust Scale (ATS) questionnaire.,"parametric models (e.g., regression)",Data was analyzed using ANOVA and correlations.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the reliability of the automated driving system and the brand reputation of the manufacturer to influence trust. Reliability was manipulated by providing information about the number of malfunctions, and brand reputation was manipulated by presenting different OEM brands.",Trust was significantly lower for systems described as unreliable and for brands with below-average reputation. The effect of brand information was weaker and more prone to change over time compared to reliability information.,"The study found that the effect of OEM brand information on trust was weaker and more prone to change over time compared to reliability information. Also, no significant trust differences were found for the above average brand reputation group.","Both reliability and brand information influenced trust in an automated driving system prior to any actual interaction, with reliability having a stronger and more lasting effect.","Participants read descriptions of an automated driving system, watched videos of it in use, and completed questionnaires about their trust in the system and their personality traits. The robot was a simulated automated driving system.",ANOVA; Pearson correlation,The study used a two-way ANOVA to examine the differences in trust ratings between study groups based on reliability and OEM brand. Correlations were used to assess the relationships between personality traits and trust in automation.,TRUE,Robot-accuracy; Robot-aesthetics,Robot-accuracy; Robot-aesthetics,,"The study manipulated the reliability of the automated driving system by providing information about the number of malfunctions, which directly impacts the system's accuracy and performance, thus 'Robot-accuracy'. The study also manipulated the brand reputation of the manufacturer by presenting different OEM brands, which is a manipulation of the visual appeal and perception of the system, thus 'Robot-aesthetics'. Both of these manipulations were found to significantly impact trust levels, as stated in the paper: 'Both heuristic brand information and reliability information influenced trust in automation.' and 'a CAD function introduced as unreliable with a higher number of system failures was trusted significantly less as compared to one that was described as reliable with a small number of failures' and 'Study participants significantly trusted OEM brands with a below average reputation less than those with a reputation in the average range'. Therefore, both 'Robot-accuracy' and 'Robot-aesthetics' are listed as factors that impacted trust. No factors were found to not impact trust.",10.4018/IJMHCI.2019070101,http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/IJMHCI.2019070101,"Trust calibration takes place prior to and during system interaction along the available information. In an online study N = 519 participants were introduced to a conditionally automated driving (CAD) system and received different a priori information about the automation’s reliability (low vs high) and brand of the CAD system (below average vs average vs above average reputation). Trust was measured three times during the study. Additionally, need for cognition (NFC) and other personality traits were assessed. Both heuristic brand information and reliability information influenced trust in automation. In line with the Elaboration Likelihood Model (ELM), participants with high NFC relied on the reliability information more than those with lower NFC. In terms of personality traits, materialism, the regulatory focus and the perfect automation scheme predicted trust in automation. These findings show that a priori information can influence a driver’s trust in CAD and that such information is interpreted individually."
"Kraus, Johannes","Scared to Trust? – Predicting Trust in Highly Automated Driving by Depressiveness, Negative Self-Evaluations and State Anxiety",2020,1,49,47,2,2 participants were excluded due to automation error and non-compliance with study instructions,Controlled Lab Environment,,"Participants completed questionnaires on personality and emotional states, then were introduced to a driving simulator with an automated driving system. After a practice trial in manual mode, they used the automated system for about 5 minutes and then rated their trust in the system.","Participants drove a simulated car, first manually and then using an automated driving system.",Unspecified,Autonomous Vehicles,Research,Navigation,Path Following,minimal interaction,Participants interacted with a simulated automated driving system.,simulation,Participants used a driving simulator to interact with the automated driving system.,simulated,The robot was a simulated automated driving system within a driving simulator.,shared control (fixed rules),"The automated driving system operated with fixed rules, with the participant able to take over control.",Questionnaires,Automation Trust Scale (ATS),,Trust was measured using a shortened version of the Automation Trust Scale.,"parametric models (e.g., regression)","Path modeling was used to test the hypothesized relationships between personality variables, anxiety, and trust.",Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,"The study did not manipulate any specific factors related to trust, but measured the relationship between personality, anxiety, and trust.",,"The study found a significant negative correlation between state anxiety and trust in automation, and that self-esteem and self-efficacy were positively correlated with trust. Depression was indirectly related to trust through anxiety. Locus of control did not show a significant association to trust in automation.","State anxiety significantly predicts lower trust in an automated driving system, and this relationship is mediated by personality traits such as self-esteem and self-efficacy.","The robot, a simulated automated driving system, controlled the vehicle's speed and lane position on a motorway. The human participant was responsible for monitoring the system and taking over control when necessary.",Pearson correlation; Partial least squares; Generalized linear models,"The study used Pearson correlations to examine the bivariate relationships between variables such as state anxiety, trust in automation, and personality traits. Path modeling was employed to test the hypothesized mediation effects of anxiety on the relationship between personality variables (depressiveness, self-esteem, self-efficacy, locus of control) and trust in automation. General linear models were used to rule out any biases from experimental manipulations on the results.",FALSE,,,,"The study did not manipulate any factors related to the robot or the task. The study measured the relationship between personality traits, anxiety, and trust in an automated driving system. There was a manipulation of transparency of the automation, but this was not in the scope of this research and was controlled for in the analysis. Therefore, no factors were manipulated in relation to trust.",,,"The advantages of automated driving can only come fully into play if these systems are used in an appropriate way, which means that they are neither used in situations they are not designed for (misuse) nor used in a too restricted manner (disuse). Trust in automation has been found to be an essential psychological basis for appropriate interaction with automated systems. Well-balanced system use requires a calibrated level of trust in correspondence with the actual ability of an automated system. As for these far-reaching implications of trust for safe and efﬁcient system use, the psychological processes, in which trust is dynamically calibrated prior and during the use of automated technology, need to be understood. At this point, only a restricted body of research investigated the role of personality and emotional states for the formation of trust in automated systems. In this research, the role of the personality variables depressiveness, self-efﬁcacy, self-esteem, and locus of control for the experience of anxiety before the ﬁrst experience with a highly automated driving system were investigated. Additionally, the relationship of the investigated personality variables and anxiety to subsequent formation of trust in automation was investigated. In a driving simulator study, personality variables and anxiety were measured before the interaction with an automated system. Trust in the system was measured after participants drove with the system for a while. Trust in the system was signiﬁcantly predicted by state anxiety and the personality characteristics self-esteem and selfefﬁcacy. The relationships of self-esteem and self-efﬁcacy were mediated by state anxiety as supported by signiﬁcant speciﬁc indirect effects. While for depression the direct relationship with trust in automation was not found to be signiﬁcant, an indirect effect through the experience of anxiety was supported. Locus of control did not show a signiﬁcant association to trust in automation. The reported ﬁndings support the importance of considering individual differences in negative self-evaluations and anxiety when being introduced to a new automated system for individual differences in trust in automation. Implications for future research as well as implications for the design of automated technology in general and automated driving systems are discussed."
"Kraus, Johannes; Scholz, David; Stiegemeier, Dina; Baumann, Martin","The More You Know: Trust Dynamics and Calibration in Highly Automated Driving and the Effects of Take-Overs, System Malfunction, and System Transparency",2020,2,31,31,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants completed a driving simulator task with a highly automated vehicle, experiencing take-overs and a potential system malfunction. Trust was measured repeatedly using self-report questionnaires.",Participants were instructed to drive safely in a driving simulator while also playing a Tetris game on a tablet during automated driving.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the automated vehicle in a driving simulator.,simulation,The study used a driving simulator to create an immersive environment.,simulated,The automated vehicle was simulated in the driving simulator.,shared control (fixed rules),"The automated vehicle operated autonomously but with fixed rules, and the human could take over control.",Questionnaires,Jian et al. Trust Scale,,Trust was measured using a shortened version of the Jian et al. Trust Scale.,"parametric models (e.g., regression)",The study used ANOVA and polynomial contrasts to analyze trust data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers manipulated the occurrence of a system malfunction and take-over requests to influence trust.,"Trust increased over time during error-free operation, decreased temporarily after take-overs and malfunctions, and recovered with continued error-free operation.","A second take-over request did not lead to a subsequent trust recovery, unlike the first take-over request. The study also found that a previous experience of a system malfunction did not make a difference in the subsequent trust reduction after a second take-over request.","Trust in an automated vehicle increases during error-free operation, decreases temporarily after take-overs and malfunctions, and recovers with continued error-free operation.","The robot drove autonomously on a highway, performing overtaking maneuvers. The human supervised the automated driving and played a Tetris game during automated driving.",ANOVA; t-test; polynomial contrasts,"The study used repeated-measures ANOVA to assess the effect of time on trust in the error-free condition. T-tests were used to compare trust levels at specific time points. Polynomial contrast analyses were employed to examine the shape of trust development over time, specifically looking for linear, quadratic, and cubic trends in relation to take-overs and malfunctions. Correlations between beliefs (reliability, predictability, competence) and trust were also investigated.",TRUE,Robot-autonomy; Robot-accuracy,Robot-autonomy; Robot-accuracy,,"The study manipulated 'Robot-autonomy' by including take-over requests (TORs), which forced the human to take control from the automated system, and by having the system switch between manual and automated modes. This is described in the paper as 'system-initiated take-over' and 'control from automation can be taken over at all times'. The study also manipulated 'Robot-accuracy' by introducing a system malfunction in one group, which is described as 'a malfunction during the drive (MF+)'. The paper states that 'take-overs led to a temporary decrease in trust, as did malfunctions', indicating that both factors impacted trust. The paper also states that 'a second take-over request did not lead to a subsequent trust recovery, unlike the first take-over request' and 'a previous experience of a system malfunction did not make a difference in the subsequent trust reduction after a second take-over request', which further supports the impact of these factors on trust.",10.1177/0018720819853686,http://journals.sagepub.com/doi/10.1177/0018720819853686,"Objective: This paper presents a theoretical model and two simulator studies on the psychological processes during early trust calibration in automated vehicles. Background: The positive outcomes of automation can only reach their full potential if a calibrated level of trust is achieved. In this process, information on system capabilities and limitations plays a crucial role. Method: In two simulator experiments, trust was repeatedly measured during an automated drive. In Study 1, all participants in a two-group experiment experienced a system-initiated take-over, and the occurrence of a system malfunction was manipulated. In Study 2 in a 2 × 2 between-subject design, system transparency was manipulated as an additional factor. Results: Trust was found to increase during the first interactions progressively. In Study 1, take-overs led to a temporary decrease in trust, as did malfunctions in both studies. Interestingly, trust was reestablished in the course of interaction for take-overs and malfunctions. In Study 2, the high transparency condition did not show a temporary decline in trust after a malfunction. Conclusion: Trust is calibrated along provided information prior to and during the initial drive with an automated vehicle. The experience of take-overs and malfunctions leads to a temporary decline in trust that was recovered in the course of error-free interaction. The temporary decrease can be prevented by providing transparent information prior to system interaction."
"Kraus, Johannes; Scholz, David; Stiegemeier, Dina; Baumann, Martin","The More You Know: Trust Dynamics and Calibration in Highly Automated Driving and the Effects of Take-Overs, System Malfunction, and System Transparency",2020,2,50,47,3,"1 participant was excluded due to a technical error, 2 additional participants were excluded as they did not comply with study instructions",Controlled Lab Environment,mixed design,"Participants completed a driving simulator task with a highly automated vehicle, experiencing a potential system malfunction. System transparency was manipulated. Trust was measured repeatedly using self-report questionnaires.",Participants were instructed to drive safely in a driving simulator while also playing an Angry Birds game on a tablet during automated driving.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the automated vehicle in a driving simulator.,simulation,The study used a driving simulator to create an immersive environment.,simulated,The automated vehicle was simulated in the driving simulator.,shared control (fixed rules),"The automated vehicle operated autonomously but with fixed rules, and the human could take over control.",Questionnaires,Jian et al. Trust Scale,,Trust was measured using a shortened version of the Jian et al. Trust Scale.,"parametric models (e.g., regression)",The study used ANOVA and polynomial contrasts to analyze trust data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers manipulated the occurrence of a system malfunction and system transparency to influence trust.,"Trust increased over time during error-free operation. A malfunction led to a temporary decrease in trust in the low transparency group, but not in the high transparency group. Trust recovered with continued error-free operation.",The study found that a priori information about the causes and characteristics of a malfunction eliminated the decrease in trust in case of a system malfunction.,A priori information about the causes and characteristics of a malfunction eliminated the decrease in trust in case of a system malfunction.,"The robot drove autonomously on a highway, performing overtaking maneuvers. The human supervised the automated driving and played an Angry Birds game during automated driving.",ANOVA; t-test; polynomial contrasts,"The study used repeated-measures ANOVA to assess the effect of time on trust in the error-free condition. T-tests were used to compare trust levels at specific time points and to check the success of the transparency manipulation. Polynomial contrast analyses were employed to examine the shape of trust development over time, specifically looking for linear and quadratic trends in relation to malfunctions. Correlations between beliefs (reliability, predictability, competence) and trust were also investigated.",TRUE,Robot-autonomy; Robot-accuracy; Robot-verbal-communication-content,Robot-accuracy; Robot-verbal-communication-content,Robot-autonomy,"The study manipulated 'Robot-autonomy' by having the system switch between manual and automated modes, but unlike Study 1, there were no take-over requests during the experimental drive. The study manipulated 'Robot-accuracy' by introducing a system malfunction in some groups, described as 'the second group experienced a malfunction during the drive (MF+)'. The study also manipulated 'Robot-verbal-communication-content' by providing information about the reasons for system malfunctions and the safe mode to the high transparency groups, described as 'Participants in the high transparency groups (HT) received information about malfunctions prior to the practice trials'. The paper states that 'a malfunction led to a temporary decrease in trust in the low transparency group, but not in the high transparency group', indicating that both 'Robot-accuracy' and 'Robot-verbal-communication-content' impacted trust. The paper also states that 'all TORs were removed' in Study 2, which means that 'Robot-autonomy' did not impact trust in this study.",10.1177/0018720819853686,http://journals.sagepub.com/doi/10.1177/0018720819853686,"Objective: This paper presents a theoretical model and two simulator studies on the psychological processes during early trust calibration in automated vehicles. Background: The positive outcomes of automation can only reach their full potential if a calibrated level of trust is achieved. In this process, information on system capabilities and limitations plays a crucial role. Method: In two simulator experiments, trust was repeatedly measured during an automated drive. In Study 1, all participants in a two-group experiment experienced a system-initiated take-over, and the occurrence of a system malfunction was manipulated. In Study 2 in a 2 × 2 between-subject design, system transparency was manipulated as an additional factor. Results: Trust was found to increase during the first interactions progressively. In Study 1, take-overs led to a temporary decrease in trust, as did malfunctions in both studies. Interestingly, trust was reestablished in the course of interaction for take-overs and malfunctions. In Study 2, the high transparency condition did not show a temporary decline in trust after a malfunction. Conclusion: Trust is calibrated along provided information prior to and during the initial drive with an automated vehicle. The experience of take-overs and malfunctions leads to a temporary decline in trust that was recovered in the course of error-free interaction. The temporary decrease can be prevented by providing transparent information prior to system interaction."
"Kraus, Matthias; Wagner, Nicolas; Untereiner, Nico; Minker, Wolfgang",Including Social Expectations for Trustworthy Proactive Human-Robot Dialogue,2022,1,200,163,37,some participants were excluded due to violations of the study instructions and technical errors,Online Crowdsourcing,mixed design,"Participants were briefed, completed a pre-test questionnaire, watched interactive videos of robot scenarios, and completed questionnaires after scenarios 4 and 6.",Participants watched interactive videos of a robot performing household tasks and made choices that influenced the robot's behavior.,TIAGo,Mobile Manipulators; Service and Assistive Robots,Social; Care; Research,Social,Conversation,minimal interaction,"Participants interacted with the robot through interactive videos, making choices that influenced the robot's behavior.",media,Participants watched interactive videos of the robot performing tasks in a simulated domestic environment.,physical,"The robot was a physical robot, but the interaction was mediated through video.",wizard of oz (directly controlled),The robot's actions were controlled by a human operator using the Wizard-of-Oz paradigm.,Behavioral Measures; Questionnaires,Trust in Automated Systems Scale; Propensity to Trust Scales; Negative Attitude towards Robots Scale (NARS),Performance Metrics,Trust was measured using questionnaires and behavioral measures of compliance with the robot's actions.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's proactive dialogue strategy was manipulated to be either static (none, notification, suggestion, intervention) or adaptive based on social expectations, influencing the perceived trust and acceptance.","The adaptive proactive strategy increased trust and competence compared to static strategies, particularly cognition-based trust. The intervention strategy also increased trust and satisfaction.",The adaptive strategy showed the highest scores for overall trust and competence. The intervention strategy also increased trust and satisfaction. The None strategy showed a decrease in perceived competence. There was a mismatch between objective and subjective acceptance ratings.,"Adapting the proactive dialogue with regard to user expectations increases trust in the robot, particularly cognition-based trust.","The robot performed household tasks such as tidying up and fetching items, while the human participant watched videos and made choices that influenced the robot's behavior.",ANOVA; welch's anova; t-test; spearman's ρ,"The study used a multivariate analysis of variance (ANOVA) to check for confounding variables and manipulation checks, and a mixed ANOVA to analyze the independent variables at different time steps. Welch's ANOVA was used to examine the influence of proactive dialogue on trust. Paired t-tests were applied to examine the influence of the degree of proactive behavior between and after the experiment. Spearman's ρ was used to assess the correlations between a user's propensity to trust and the trust-related concepts.",TRUE,Robot-verbal-communication-content; Robot-autonomy; Robot-adaptability,Robot-verbal-communication-content; Robot-autonomy; Robot-adaptability,,"The study manipulated the robot's proactive dialogue strategy, which directly changes the content of the robot's verbal communication (e.g., 'None', 'Notification', 'Suggestion', 'Intervention', and 'Adaptive'). This falls under 'Robot-verbal-communication-content' because the core manipulation is about what the robot says and how it communicates its actions. The study also manipulated the level of autonomy the robot exhibited, ranging from reactive (None) to fully autonomous (Intervention), which directly relates to 'Robot-autonomy'. The 'Adaptive' condition further manipulated the robot's ability to change its level of proactivity based on social expectations, which is a form of 'Robot-adaptability'. The results showed that the adaptive strategy, which varied the robot's proactivity based on social expectations, significantly increased trust and competence compared to static strategies. The intervention strategy also increased trust and satisfaction. Therefore, 'Robot-verbal-communication-content', 'Robot-autonomy', and 'Robot-adaptability' were all found to impact trust. There were no factors that were manipulated that did not impact trust.",10.1145/3503252.3531294,https://dl.acm.org/doi/10.1145/3503252.3531294,"Trust forms an important factor in human-robot interaction and is highly influencing the success or failure of a mixed team of humans and machines. Similarly, to human-human teamwork, communication and proactivity are one of the keys to task success and efficiency. However, the level of proactive robot behaviour needs to be adapted to a dynamically changing social environment. Otherwise, it may be perceived as counterproductive and the robot’s assistance may not be accepted. For this reason, this work investigates the design of a socially-adaptive proactive dialogue strategy and its effects on humans’ trust and acceptance towards the robot. The strategy is implemented in a human-like household assistance robot that helps in the execution of domestic tasks, such as tidying up or fetch-and-carry tasks. For evaluation of the strategy, users interact with the robot while watching interactive videos of the robots in six different task scenarios. Here, the adaptive proactive behaviour of the robot is compared to four different levels of static proactivity: None, Notification, Suggestion, and Intervention. The results show that proactive robot behaviour that adapts to the social expectations of a user has a significant effect on the perceived trust in the system. Here, it is shown that a robot expressing socially-adaptive proactivity is perceived as more competent and reliable than a non-adaptive robot. Based on these results, important implications for the design of future robotic assistants at home are described."
"Kraus, Johannes Maria; Merger, Julia; Gröner, Felix; Pätz, Jessica",'Sorry' Says the Robot: The Tendency to Anthropomorphize and Technology Affinity Affect Trust in Repair Strategies after Error,2023,1,665,604,61,"27 participants were excluded for not participating seriously, 18 participants were excluded for speeding, 24 participants were excluded for flatlining on more than one scale with inverted items",Online Crowdsourcing,mixed design,"Participants first completed questionnaires on demographics and personal dispositions, then watched videos of a robot packing lunch bags, including one error and one of six trust repair strategies. In the within-subjects part, participants evaluated all six trust repair strategies in a randomized order.",Participants were asked to supervise a robot packing lunch bags and evaluate different robot responses after an error.,Lio,Mobile Manipulators,Research,Supervision,Monitoring,passive observation,Participants watched videos of the robot performing a task.,media,Participants watched videos of the robot performing a task.,physical,The robot was a physical robot shown in videos.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions.,Questionnaires,Trust in Automation Scale (TAS); Negative Attitude towards Robots Scale (NARS),,Trust was measured using questionnaires.,"parametric models (e.g., regression)",The study used t-tests and correlations to analyze the data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's responses after an error were manipulated by providing different trust repair strategies, including apologies, explanations, and denial, which influenced how participants perceived the robot's behavior and the task.","Apologies and technical explanations increased trust, while denial decreased trust. Anthropomorphic explanations did not increase trust.","The study found that a technical explanation increased trust more than a simple apology, while an anthropomorphic explanation did not increase trust. The study also found that user dispositions such as ANTEN and TA affected trust in specific repair strategies.",A technical explanation following an apology is more effective at repairing trust after a robot error than a simple apology or an anthropomorphic explanation.,"The robot was programmed to pack lunch bags, and the human participant was asked to supervise the robot and evaluate its responses after an error.",t-test; Pearson correlation; ANOVA,"The study used t-tests to compare the means of different trust repair strategies, examining the differences in trust, liking, acceptance, and intention to use. Correlations were used to examine the relationship between personality traits (ANTEN, TA, NFC, NARS) and trust. ANOVA was mentioned as a test that is robust against deviations from normal distributions, but the study primarily used t-tests for comparisons. The purpose of the statistical analysis was to determine the effectiveness of different trust repair strategies and the influence of user dispositions on trust.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the robot's verbal communication content by providing different trust repair strategies after an error. These strategies included apologies, technical explanations, anthropomorphic explanations, and denial of responsibility. The paper explicitly states that the different explanations and apologies were manipulated to see how they affected trust. The results showed that technical explanations and apologies increased trust, while denial decreased trust. Anthropomorphic explanations did not increase trust. Therefore, 'Robot-verbal-communication-content' is the most appropriate category as it directly relates to the content of the robot's communication and its impact on trust. No other factors were explicitly manipulated.",10.1145/3568294.3580122,https://dl.acm.org/doi/10.1145/3568294.3580122,"This research investigates how six different trust repair strategies (apologies, explanations and denial) of a robot packing lunch bags affect trust after an error and how user dispositions predict trust in the repair strategies. In an online experiment, the perceived trustworthiness was assessed in a within-subjects design (N = 604) in which all strategies were evaluated in direct comparison. Higher trustworthiness of an apology (vs. no apology) and of a technical explanation for an error (vs. an empty and an anthropomorphic explanation) was found. In line with theoretical considerations, user personality was found to be associated with trust in specific strategies. A higher tendency to anthropomorphize technology was associated with higher trust in an anthropomorphic explanation and technological affinity was associated with a higher trust in the technical explanation. Taken together, personalization of trust repair strategies is a promising direction for individualized design to foster trustworthy human-robot interaction."
"Kraus, Johannes; Miller, Linda; Klumpp, Marielène; Babel, Franziska; Scholz, David; Merger, Julia; Baumann, Martin",On the Role of Beliefs and Trust for the Intention to Use Service Robots: An Integrated Trustworthiness Beliefs Model for Robot Acceptance,2023,1,400,400,80,"17 participants were excluded due to a processing time that was too short, 38 participants were excluded due to no variance, 25 participants were excluded due to multivariate outliers",Online Crowdsourcing,mixed design,"Participants completed demographic questionnaires, then answered questions about their beliefs, trust, and intention to use regarding service robots in general. They were then presented with seven specific examples of service robots and indicated their trust. Finally, they were introduced to an assistance robot prototype and answered questions about it, with manipulations of application area and autonomy level.","Participants evaluated their perceptions of service robots in general and a specific assistance robot, and completed questionnaires about their beliefs, trust, and intention to use.",Unspecified,Service and Assistive Robots,Other: Assistance tasks,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the robot and interaction scenarios.,media,The interaction was based on written descriptions of the robot and scenarios.,hypothetical,"The robot was described through text and a sketch, without physical presence.",shared control (fixed rules),The robot's autonomy level was manipulated with descriptions of either fully autonomous or partly autonomous functioning.,Questionnaires,LETRAS-G,,Trust was measured using questionnaires.,"parametric models (e.g., regression)",The study used regression analyses and structural equation modeling to model trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The study manipulated the application area (public vs. private) and the robot's autonomy level (partly vs. fully automated) to see how these factors influenced trust.,The effect of social influence on trust was higher in the public setting than in the private setting. The effect of perceived behavioral control on trust did not differ between autonomy levels.,"The study found that understandability was negatively related to trust in the full model, which was unexpected. This effect disappeared in the reduced model, suggesting a possible suppressing effect. Also, the direct effect of social influence on the intention to use suggests that other mediating variables may be present at the attitude level.","The study found that trust in service robots as a general category predicts trust in specific robots, and that this specific trust mediates the effect of general trust on the intention to use. The study also found that competence, effort expectancy, and social influence are key predictors of trust.","Participants read descriptions of service robots and a specific assistance robot, and then completed questionnaires about their beliefs, trust, and intention to use. The robot was described as performing assistance tasks, such as carrying objects, tidying up, and storing objects.",Linear regression; Mediation analysis; Structural equation modeling; Moderation analysis; multiple group cfas; ANOVA; Generalized linear models,"The study used a combination of regression analyses to examine the relationships between variables, mediation analyses to test indirect effects, structural equation modeling (SEM) to validate the proposed model, moderation analyses to investigate the influence of situational variables, multiple group CFAs to test for moderation, ANOVAs to check for mean differences between experimental groups, and general linear models to test for group effects. These tests were used to analyze the relationships between beliefs, trust, and intention to use service robots, and to explore the moderating effects of application area and robot autonomy.",TRUE,Robot-autonomy; Task-environment,Task-environment,Robot-autonomy,"The study manipulated the robot's autonomy level by describing it as either fully autonomous or partly autonomous, which falls under the 'Robot-autonomy' category as it directly changes the decision authority of the robot. The study also manipulated the application area of the robot, describing it as operating in either a public or private setting. This manipulation of the context in which the task is performed is best categorized as 'Task-environment'. The study found that the effect of social influence on trust was higher in the public setting than in the private setting, indicating that the 'Task-environment' manipulation impacted trust. The study did not find a significant difference in the effect of perceived behavioral control on trust between the different autonomy levels, indicating that the 'Robot-autonomy' manipulation did not impact trust.",10.1007/s12369-022-00952-4,https://link.springer.com/10.1007/s12369-022-00952-4,"With the increasing abilities of robots, the prediction of user decisions needs to go beyond the usability perspective, for example, by integrating distinctive beliefs and trust. In an online study (N = 400), ﬁrst, the relationship between general trust in service robots and trust in a speciﬁc robot was investigated, supporting the role of general trust as a starting point for trust formation. On this basis, it was explored—both for general acceptance of service robots and acceptance of a speciﬁc robot—if technology acceptance models can be meaningfully complemented by speciﬁc beliefs from the theory of planned behavior (TPB) and trust literature to enhance understanding of robot adoption. First, models integrating all belief groups were ﬁtted, providing essential variance predictions at both levels (general and speciﬁc) and a mediation of beliefs via trust to the intention to use. The omission of the performance expectancy and reliability belief was compensated for by more distinctive beliefs. In the ﬁnal model (TB-RAM), effort expectancy and competence predicted trust at the general level. For a speciﬁc robot, competence and social inﬂuence predicted trust. Moreover, the effect of social inﬂuence on trust was moderated by the robot’s application area (public > private), supporting situation-speciﬁc belief relevance in robot adoption. Taken together, in line with the TPB, these ﬁndings support a mediation cascade from beliefs via trust to the intention to use. Furthermore, an incorporation of distinctive instead of broad beliefs is promising for increasing the explanatory and practical value of acceptance modeling."
"Kuhnert, Barbara; Ragni, Marco; Lindner, Felix",The gap between human's attitude towards robots in general and human's expectation of an ideal everyday life robot,2017,1,28,28,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants completed an online questionnaire with four sections, including a Semantic Differential Scale to assess attitudes towards robots in general and expectations of an ideal personal robot. They also ranked robot images based on likability and eeriness, and answered open-ended questions.",Participants completed a questionnaire including a Semantic Differential Scale to rate their attitude towards robots in general and their expectations for an ideal personal robot. They also ranked images of robots and human-robot interaction scenes based on likability and eeriness.,Unspecified,Humanoid Robots; Industrial Robot Arms; Android Robots; Mobile Robots,Social; Care; Industrial,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read and responded to questions about robots and their expectations.,media,"The interaction was based on text descriptions and questionnaires, without any visual or interactive elements.",hypothetical,"The robots were described in text and images, but no physical robots were present.",not autonomous,"The robots were described in text and images, but no actions were carried out by a robot.",Questionnaires; Custom Scales,Negative Attitude towards Robots Scale (NARS),,Trust was assessed using a Semantic Differential Scale and statements from the Negative Attitudes toward Robots Scale.,no modeling,The study did not use any computational models of trust.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,"The study did not manipulate any factors related to trust, but rather measured existing attitudes and expectations.",,"The study found that participants' attitudes towards robots in general differed significantly from their expectations of an ideal personal robot. Male participants showed more differences in their attitudes and expectations than female participants. Participants with a technical background wished for their ideal robot to be more intelligent and cautious, while those with a non-technical background focused on personality features.","Participants' attitudes towards robots in general differ significantly from their expectations of an ideal personal robot, highlighting the need for user-adapted robot development.","Participants completed an online questionnaire, rating their attitudes towards robots in general and their expectations for an ideal personal robot using a Semantic Differential Scale. They also ranked images of robots and human-robot interaction scenes based on likability and eeriness.",Mann-Whitney U; wilcoxon signed-sum test,"The study used Mann-Whitney-U tests to compare differences between groups (e.g., technical vs. non-technical background, male vs. female) regarding their interest in science fiction, robots, and prior contact with robots. Wilcoxon signed-sum tests were used to compare within-subject differences in attitudes towards robots in general versus expectations of an ideal personal robot, for the overall group and subgroups based on technical background and gender. The False Discovery Rate (FDR) method was used to correct for multiple comparisons.",FALSE,,,,"The study did not manipulate any factors related to trust. The study measured pre-existing attitudes and expectations towards robots using a questionnaire with a Semantic Differential Scale and ranking tasks. The study design did not involve any intentional manipulation of robot behavior, characteristics, or task parameters. Therefore, no factors were manipulated.",10.1109/ROMAN.2017.8172441,http://ieeexplore.ieee.org/document/8172441/,"Acceptance, trust and a successful deployment of robots in human? everyday life depends both on technical implementation and on psychological aspects. The current work identifies essential features and characteristics that have the potential to increase the acceptance of robots, reduce prejudices and improve the development of appropriate and suitable robots with target-group specific features and characteristics. We present the first part of a major research project that aims to develop a valid and reliable toolkit for an encompassing measurement of human? attitude towards robots. By means of the Semantic Differential Scale a comparison of human? attitude towards robots in general and human? expectation of an ideal, personal everyday life robot has been performed. Results reveal major differences between these two concepts and the demand of robots adapted to human? requirements."
"Kundinger, Thomas; Wintersberger, Philipp; Riener, Andreas",(Over)Trust in Automated Driving: The Sleeping Pill of Tomorrow?,2019,1,30,30,0,No participants were excluded,Real-World Environment,within-subjects,"Participants completed a trust scale before and after a 45-minute ride in an automated vehicle, and rated their drowsiness every 5 minutes during the ride.",Participants monitored the driving environment during a 45-minute ride in an automated vehicle.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants monitored the automated vehicle and were instructed to be ready to take over control.,real-world,Participants experienced a real-world ride in an automated vehicle.,physical,Participants interacted with a physical automated vehicle.,shared control (fixed rules),The vehicle operated autonomously but required human monitoring and potential intervention.,Questionnaires; Custom Scales,Jian et al. Trust Scale,Video Data; Physiological Signals,"Trust was measured using a questionnaire before and after the ride, and drowsiness was measured during the ride.","parametric models (e.g., regression)",Linear regression was used to analyze the relationship between drowsiness and trust.,Empirical HRI Studies,Physical Robot Studies,Indirect Manipulation,"The study exposed participants to a real automated vehicle, which influenced their trust levels and drowsiness.","Trust in the automated vehicle increased significantly after the ride, and higher trust was correlated with increased drowsiness.","The study found a significant positive correlation between increased trust and increased drowsiness, suggesting that overtrust in automation may lead to reduced vigilance.","Initial exposure to an automated vehicle significantly increases trust, and higher trust is correlated with increased drowsiness.","The robot (automated vehicle) drove along a predefined route, while the human participant monitored the driving environment and rated their drowsiness.",wilcoxon signed ranked tests; Mann-Whitney U; Linear regression,"Wilcoxon signed ranked tests were used to evaluate within-subjects effects on trust and distrust ratings before and after the ride. Mann-Whitney U tests were used to evaluate between-subjects effects with respect to gender and age groups on drowsiness. Linear regression was used to quantify the effect of the monitoring task on drowsiness by calculating the slope of the KSS ratings over time, and to assess the correlation between drowsiness increase and trust ratings after the drive.",FALSE,Task-environment,Task-environment,,"The study did not explicitly manipulate any factors to influence trust. However, the study design exposed participants to a real-world automated driving environment, which implicitly influenced their trust levels. The 'Task-environment' category is chosen because the study involved a real-world driving scenario on a test track, which is a change to the working conditions. The paper states, 'To evaluate a) how trust is affected by initial system exposure and b) if a connection between subjective trust and drowsiness exists, we conducted a real driving study on a test track.' This indicates that the real-world environment was a key aspect of the study design, and it was found to impact trust levels, as the results showed a significant increase in trust after the ride. No other factors were explicitly manipulated or found to not impact trust.",10.1145/3290607.3312869,https://dl.acm.org/doi/10.1145/3290607.3312869,"Both overtrust in technology and drowsy driving are safety-critical issues. Monitoring a system is a tedious task and overtrust in technology might also influence drivers’ vigilance, what in turn could multiply the negative impact of both issues. The aim of this study was to investigate if trust in automation affects drowsiness. 30 participants in two age groups conducted a 45-minute ride in a level-2 vehicle on a real test track. Trust was assessed before and after the ride with a subjective trust scale. Drowsiness was captured during the experiment using the Karolinska Sleepiness Scale. Results depict, that even a short initial system exposure significantly increases trust in automated driving. Drivers who trust the automated vehicles more show larger signs of drowsiness what may negatively impact the monitoring behavior. Drowsiness detection is important for automated vehicles, and the behavior of drowsy drivers might help to infer trust in an unobtrusively way."
"Lagomarsino, Marta; Lorenzini, Marta; Balatti, Pietro; Momi, Elena De; Ajoudani, Arash",Pick the Right Co-Worker: Online Assessment of Cognitive Ergonomics in Human-Robot Collaborative Assembly,2022,1,14,14,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a collaborative assembly task in three different scenarios: human-human collaboration, human-robot interaction with low transparency, and human-robot collaboration with high transparency. Physiological data (ECG, GSR) and subjective questionnaires were collected. Participants were divided into two groups based on their prior experience with robots.","Participants completed a collaborative assembly task, receiving components from either a human or a robot assistant.",Franka Emika Panda,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically collaborated with a robot or human assistant to complete an assembly task.,real-world,The study was conducted in a real-world lab setting with physical robots and components.,physical,Participants interacted with a physical robot in the study.,shared control (fixed rules),"The robot operated with fixed rules, with some actions triggered by the human, and some actions performed autonomously.",Questionnaires; Behavioral Measures; Physiological Measures,NASA Task Load Index (NASA-TLX); N/A,Video Data; Physiological Signals; Performance Metrics,"Trust was assessed using questionnaires, behavioral measures (gaze tracking), and physiological measures (ECG, GSR).",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The level of robot autonomy and transparency was manipulated across three conditions, influencing the robot's behavior and feedback to the user, and the task difficulty was implicitly changed by the level of robot assistance.","Trust was found to be higher when the robot provided more transparency and feedback, and when the robot had a higher level of autonomy. Naive participants showed a significant trust diminution with the robot's introduction, but this was mitigated with increased transparency.",The study found that participants with prior experience in robotics did not show the same increase in cognitive load when interacting with the robot as naive participants. The study also found that the mental effort score computed online by the framework was positively correlated with the LF/HF ratio derived from ECG signal.,"Increased transparency and feedback from the robot during collaborative assembly tasks reduces cognitive load and increases trust, particularly for users without prior experience with robots.","The robot provided components to the human for assembly. The human assembled the components, requested components from the robot, and monitored the robot's actions.",Friedman test; Wilcoxon rank sum; Kruskal-Wallis; Spearman correlation,"The study used Friedman tests with repeated measures to assess the impact of different experimental conditions on the LF/HF ratio derived from ECG signals, SCL and SCR from GSR, and mental effort scores. Wilcoxon tests with Bonferroni correction were used for post-hoc comparisons of the LF/HF ratio. Kruskal-Wallis tests were used to determine differences in trust scores and collaboration burden across conditions. Spearman's rank-order correlation was used to assess the relationship between the mental effort score and the LF/HF ratio, as well as between trust factors and subjective trust scale results.",TRUE,Robot-autonomy; Robot-verbal-communication-content,Robot-autonomy; Robot-verbal-communication-content,,"The study manipulated the level of robot autonomy and transparency. In the HRI condition, the robot's movements were controlled by the user via buttons, and the robot provided no feedback. In the HRC condition, the robot performed more tasks autonomously and provided feedback on its actions. This manipulation of autonomy is described in the paper: 'HRI-C sessions featured different levels of robot assistance and degrees of transparency into the robot's autonomous status.' The paper also states that 'In scenario (b), the robot neither informs the user on the receiving of the part request nor provides any feedback about its state. Hence, the degree of interaction is extremely low, and there is no transparency about what the robot is doing.' and 'In scenario (c), the robotic agent can perform more tasks in autonomy, but it gives feedback on the monitor about the action currently served (e.g. grasping an object).' This difference in feedback constitutes a manipulation of 'Robot-verbal-communication-content' as it changes what information is communicated to the user. The results showed that increased transparency and autonomy led to higher trust, as stated in the paper: 'Specifically, naïve participants reported a significant trust diminution with the robot's introduction... but this was mitigated with increased transparency.' and 'The underlying hypothesis was that high robot assistance level and greater transparency into the robot's autonomous status decreases the cognitive load and increases the trust of the human partner'. Therefore, both 'Robot-autonomy' and 'Robot-verbal-communication-content' impacted trust.",10.1109/TCDS.2022.3182811,https://ieeexplore.ieee.org/document/9795908/,"Human-robot collaborative assembly systems enhance the efficiency and productivity of the workplace but may increase the workers’ cognitive demand. This paper proposes an online and quantitative framework to assess the cognitive workload induced by the interaction with a co-worker, either a human operator or an industrial collaborative robot with different control strategies. The approach monitors the operator’s attention distribution and upper-body kinematics benefiting from the input images of a low-cost stereo camera and cutting-edge artificial intelligence algorithms (i.e. head pose estimation and skeleton tracking). Three experimental scenarios with variations in workstation features and interaction modalities were designed to test the performance of our online method against state-of-theart offline measurements. Results proved that our vision-based cognitive load assessment has the potential to be integrated into the new generation of collaborative robotic technologies. The latter would enable human cognitive state monitoring and robot control strategy adaptation for improving human comfort, ergonomics, and trust in automation."
"Lagomarsino, Marta; Lorenzini, Marta; Constable, Merryn Dale; De Momi, Elena; Becchio, Cristina; Ajoudani, Arash",Maximising Coefficiency of Human-Robot Handovers Through Reinforcement Learning,2023,1,12,12,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a handover task 50 times with a robot, while the robot adapted its behavior using reinforcement learning. The robot adjusted object orientation, interaction distance, and velocity. Human motion was tracked using a wearable suit, and subjective questionnaires were administered after the experiment.",Participants received a mug from a robot and placed it under a coffee machine.,Franka Emika Panda,Industrial Robot Arms; Collaborative Robots (Cobots),Research,Manipulation,Object Passing,direct-contact interaction,Participants physically interacted with the robot during a handover task.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,A physical robot was used in the experiment.,shared control (adaptive),The robot adapted its behavior based on real-time interaction with the human.,Questionnaires; Custom Scales,Charalambous trust scores,"Video Data; Performance Metrics; robot data (sensor data, etc.)","Trust was assessed using questionnaires and custom scales, with video, performance, and robot sensor data collected.","deep learning (e.g., neural networks, reinforcement learning)",Reinforcement learning was used to adapt the robot's behavior based on a coefficiency score.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's object orientation, interaction distance, and velocity were manipulated to influence human comfort and robot efficiency, which in turn affected trust.","The robot's adaptive behavior, based on the coefficiency model, improved perceived appropriateness of motion, cooperation safety, and reliability, leading to an overall increase in trust.","Two participants exhibited behaviors that deviated from the expected patterns, which prevented the system from appropriately learning. One participant was overfocused, and the other was distracted when the robot was more legible.","The study demonstrated that a robot acting coefficiently, by considering both human comfort and its own efficiency, can improve human perceived comfort and foster trust.",The robot picked up a mug and handed it to the human. The human grasped the mug and placed it under a coffee machine.,Wilcoxon signed-rank test,"The Wilcoxon signed-rank test was used to compare the efficiency metrics (human ergonomic cost and human-robot coefficiency cost) when the robot used the learned parameters versus all other iterations. It was also used to assess the effect of each interaction parameter (mug orientation, distance, and robot velocity) on the reward of the reinforcement learning algorithm. Additionally, it was used to compare the coefficiency score with preferred parameters.",TRUE,Robot-autonomy; Robot-task-strategy; Robot-adaptability,Robot-autonomy; Robot-adaptability,Robot-task-strategy,"The study manipulated the robot's behavior by adapting its object orientation, interaction distance, and velocity using reinforcement learning, which falls under 'Robot-autonomy' as the robot was making decisions on how to perform the handover. The robot's task strategy was also manipulated as the robot was changing its approach to the handover, but this did not directly impact trust. The robot's ability to learn and adapt to user preferences is a key manipulation, which is classified as 'Robot-adaptability'. The paper states that the robot's adaptive behavior, based on the coefficiency model, improved perceived appropriateness of motion, cooperation safety, and reliability, leading to an overall increase in trust. This indicates that 'Robot-autonomy' and 'Robot-adaptability' impacted trust. The robot's task strategy was manipulated, but the paper does not indicate that this manipulation directly impacted trust, only that the robot was changing its approach to the handover. Therefore, 'Robot-task-strategy' is listed as a factor that was manipulated but did not impact trust.",10.1109/LRA.2023.3280752,https://ieeexplore.ieee.org/document/10137876/,"Handing objects to humans is an essential capability for collaborative robots. Previous research works on human-robot handovers focus on facilitating the performance of the human partner and possibly minimising the physical effort needed to grasp the object. However, altruistic robot behaviours may result in protracted and awkward robot motions, contributing to unpleasant sensations by the human partner and affecting perceived safety and social acceptance. This letter investigates whether transferring the cognitive science principle that “humans act coefﬁciently as a group” (i.e. simultaneously maximising the beneﬁts of all agents involved) to human-robot cooperative tasks promotes a more seamless and natural interaction. Human-robot coefﬁciency is ﬁrst modelled by identifying implicit indicators of human comfort and discomfort as well as calculating the robot energy consumption in performing the desired trajectory. We then present a reinforcement learning approach that uses the human-robot coefﬁciency score as reward to adapt and learn online the combination of robot interaction parameters that maximises such coefﬁciency. Results proved that by acting coefﬁciently the robot could meet the individual preferences of most subjects involved in the experiments, improve the human perceived comfort, and foster trust in the robotic partner."
"Lane, Sophie; Esterwood, Connor; Kulic, Dana; Robinson, Nicole",Robots That Use Physical Repair Strategies After Repeated Errors to Mitigate Trust Decline in Human-Robot Interaction: A Repeated Measures Experiment,2024,1,47,34,13,13 participants were excluded due to robot problems,Controlled Lab Environment,mixed design,"Participants completed a safety induction and demonstration of the robot, then completed initial trust surveys. They then completed six collaborative building tasks with the robot, where the robot made errors in two of the tasks. The robot then initiated a repair strategy based on the assigned condition. Participants completed mid-study surveys after each task and final trust surveys and an interview at the end.","Participants and a robot collaborated on six block-building tasks, where the robot made errors in two of the tasks and then initiated a repair strategy.",Unspecified,Industrial Robot Arms,Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically collaborated with the robot on a block-building task.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,shared control (fixed rules),The robot operated independently but followed pre-determined rules for the task and repair strategies.,Questionnaires; Multidimensional Measures,Trust Perception Scale - HRI; Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using questionnaires and a multidimensional measure of trust.,no modeling,"Trust was not modeled computationally, and only descriptive statistics were used.",Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's behavior was manipulated by introducing errors and then implementing different repair strategies (no repair, offered repair, automatic repair), which influenced participants' expectations of the robot's performance.","Automatic physical repair was the only condition that successfully restored trust after a robot error, while the no-repair condition experienced a significant decrease in trust. The offered repair condition did not improve trust and may have confused participants.","Participants were hesitant to associate moral terms with the robot, but this hesitancy may reduce over time. The offered repair condition performed worse than the no-repair condition, which was unexpected. The speed of the robot was a significant factor in participants' perception of trust.","Automatic physical repair was effective in restoring trust after a robot error, while offered repair was not. The immediacy of the automatic repair was more successful in maintaining positive perceptions of trust.","The robot placed colored blocks in a specific order, and the human participant was tasked with ensuring the robot placed the blocks correctly. The robot made errors in two of the tasks, and then either apologized and asked the participant to fix the error (no repair), apologized and offered to fix the error (offered repair), or apologized and automatically fixed the error (automatic repair).",ANOVA; t-test; mixed linear model,"A two-way between subjects ANOVA was used to analyze the change in scores from the pre-post surveys (TPS and MDMT). Post-hoc analysis included pairwise t-tests with Bonferroni adjustment to compare conditions within time and time within condition. A mixed linear model was used to analyze the mid-study survey data, investigating interaction effects between repair condition and time. The ANOVA was used to assess the effect of repair condition and time on trust scores, while the t-tests were used for pairwise comparisons to identify specific differences. The mixed linear model was used to examine how trust changed over time and whether this change differed across the repair conditions.",TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-accuracy,Robot-verbal-communication-content,"The study manipulated 'Robot-verbal-communication-content' by having the robot either apologize and ask the participant to fix the error (no repair), apologize and offer to fix the error (offered repair), or apologize and automatically fix the error (automatic repair). This is a manipulation of the content of the robot's communication. The study also manipulated 'Robot-accuracy' by having the robot make errors in two of the tasks, and then either not repair the error, offer to repair the error, or automatically repair the error. This directly impacts the robot's performance on the task. The results showed that the 'Robot-accuracy' manipulation, specifically the automatic repair condition, significantly impacted trust, while the different communication contents did not have a significant impact on trust, as the offered repair condition performed worse than the no-repair condition. The automatic repair was the only condition that successfully restored trust after a robot error, while the no-repair condition experienced a significant decrease in trust. The offered repair condition did not improve trust and may have confused participants. Therefore, 'Robot-accuracy' impacted trust, while 'Robot-verbal-communication-content' did not.",,,"Robots are inherently imperfect, and collaborating with an error-prone robotic teammate can deteriorate perceptions of trust and the willingness of users to continue working with the robot. Evidence-based trust repair strategies can be implemented into a robot’s design to mitigate the decline of trust in human-robot relationships following errors. It is not yet clear what trust repair strategies are most effective. To address this shortcoming, this study investigates two novel trust repair strategies: offered and automatic physical repair. A between-subjects repeated measures study was performed to determine the extent to which each type of physical trust repair was successful in restoring participants’ perceptions of trust. The results indicated that, where the no-repair condition experienced a significant decrease in trust score, only the automatic repair was consistently successful in bypassing the trust decline. Detailed analysis showed that participants from the offered repair condition did not view the robot as providing the appropriate information, meaning that the offer itself may have confused them. Participants’ response rate to the Multi-Dimensional Measure of Trust also revealed that users were less willing to associate moral terms with robotic teammates, though this hesitancy may reduce over time. These results contribute to research on human-robot trust repair by uncovering that physical repair is effective when it is automatic, but not when it is offered. This finding will help to further elucidate what repair strategies work to mitigate trust decline and thus help inform robot design."
"Large, David R.; Burnett, Gary; Morris, Andrew; Muthumani, Arun; Matthias, Rebecca",A Longitudinal Simulator Study to Explore Drivers’ Behaviour During Highly-Automated Driving,2018,1,20,20,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a driving task in a simulator, experiencing both manual and automated control. They were instructed to take over manual control when prompted by a visual cue. Participants completed a trust questionnaire and a situational awareness questionnaire after each driving session. Video recordings were also collected.","Participants drove a simulated vehicle, alternating between manual and automated control, and were required to take over manual control when prompted.",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the automated system through a driving simulator, with limited physical interaction.",simulation,The study used a driving simulator to create an interactive virtual environment.,simulated,The robot was a simulated autonomous vehicle within the driving simulator.,shared control (fixed rules),The vehicle operated autonomously but required the human to take over control at specific times.,Questionnaires; Real-time Trust Measures,Trust in Automation Scale (TAS); Situational Awareness Rating Technique (SART),Video Data,"Trust was measured using questionnaires and real-time measures, with video data collected for analysis.",no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the level of autonomy by switching between automated and manual control, and the task difficulty by requiring manual takeover at specific times. The visual cue acted as a form of feedback manipulation.","The study found that trust in the automated system was generally high, but decreased when manual takeover was required. The visual cue indicating the need for manual takeover influenced trust.","Participants generally trusted the automated system, but their trust decreased when they were required to take over manual control. Some participants also reported that they were not always aware of the need to take over control, which may have impacted their trust.","The study found that drivers generally trust automated driving systems, but their trust can be negatively impacted when they are required to take over manual control.","The robot (simulated autonomous vehicle) drove the car automatically, and the human participant monitored the driving and took over manual control when prompted by a visual cue. The human also completed questionnaires after each driving session.",,No statistical tests were explicitly mentioned in the paper. The analysis focused on qualitative observations of participant behavior and subjective ratings from questionnaires.,TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,,"The study manipulated 'Robot-autonomy' by switching between automated and manual control of the vehicle. This is explicitly stated in the paper: 'Participants were encouraged to act as they might in such a vehicle...During periods of automa-tion, participants were quickly engrossed by their chosen activities...Consistently high subjective ratings of trust suggest that drivers were unperturbed by the novelty of highly-automated driving and generally willing to allow the vehicle to assume control'. The study also implicitly manipulated 'Task-complexity' by requiring manual takeover at specific times, which increased the cognitive load on the driver. This is supported by the text: 'During periods of automa-tion, participants were quickly engrossed by their chosen activities...and required postural adaptation...Consistently high subjective ratings of trust suggest that drivers were unperturbed by the novelty of highly-automated driving and generally willing to allow the vehicle to assume control; ratings of situational awareness varied considerably indicating mixed opinions.' The paper also states that 'Participants generally trusted the automated system, but their trust decreased when they were required to take over manual control.' This indicates that the manipulation of 'Robot-autonomy' impacted trust, while the manipulation of 'Task-complexity' was not explicitly stated to have impacted trust, but rather situational awareness.",,http://link.springer.com/10.1007/978-3-319-60441-1_57,"Six experienced drivers each undertook five 30-min journeys (portrayed as 'daily commutes' i.e. one on each of five consecutive weekdays) in a medium-fidelity driving-simulator engineered to mimic a highly-automated vehicle. Participants were encouraged to act as they might in such a vehicle by bringing with them their own objects/devices to use. During periods of automa-tion, participants were quickly engrossed by their chosen activities, many of which had strong visual, manual and cognitive elements, and required postural adaptation (e.g. moving/reclining the driver's seat); the steering wheel was typically used to support objects/devices. Consistently high subjective ratings of trust suggest that drivers were unperturbed by the novelty of highly-automated driving and generally willing to allow the vehicle to assume control; ratings of situational awareness varied considerably indicating mixed opinions. Qualitative results are discussed in the context of the re-design of vehicles to enable safe and comfortable engagement with secondary activities during high-automation."
"Law, Edith; Cai, Vicky; Liu, Qi Feng; Sasy, Sajin; Goh, Joslin; Blidaru, Alex; Kulic, Dana",A Wizard-of-Oz study of curiosity in human-robot interaction,2017,1,36,35,1,1 participant was excluded because she deviated from the procedure,Controlled Lab Environment,between-subjects,"Participants were introduced to the study, completed a pre-study questionnaire, interacted with the robot, completed a post-study questionnaire, and participated in a short interview.",Participants interacted with a robot to identify objects and determine their recyclability.,Unspecified,Mobile Robots; Service and Assistive Robots,Research,Manipulation,Sorting/Arranging,minimal interaction,Participants interacted with the robot by presenting objects and receiving verbal and visual feedback.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,wizard of oz (directly controlled),The robot's responses were directly controlled by a human operator.,Behavioral Measures; Questionnaires,,,Trust was measured using questionnaires and behavioral measures such as compliance with the robot's recommendations.,"parametric models (e.g., regression)","Logistic and Poisson regression models were used to analyze the relationship between surprise, trust, and other variables.",Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's object recognition responses were manipulated to be either predictable or unpredictable, influencing user expectations and surprise.",Unpredictable robot responses decreased trust and compliance with the robot's recommendations.,"Participants with AI knowledge were more surprised by the robot's responses, and male participants experienced less surprise than female participants. The study also found that the juxtaposition of responses with different levels of specificity and correctness contributed to surprise.",Surprising robot responses increased curiosity and engagement but decreased trust in the robot's recommendations.,"The robot provided object recognition and recyclability recommendations, while the human presented objects, decided whether to trust the robot's recommendation, and disposed of the object.",proportional odds model; Linear regression; Poisson regression; Logistic regression; Wald test; t-test,"The study used a variety of statistical tests to analyze the relationship between surprise, curiosity, engagement, and trust. Proportional odds models were used for ordinal response variables like per-object surprise. Linear regression models were used to investigate factors influencing the proportion of time spent on testing. Poisson regression models were used for count data, such as the number of objects presented. Logistic regression models were used for binary response variables, such as compliance and trust in the robot's recommendations. The Wald test was used to investigate the significance of effects for the proportional odds models, and t-tests were used for the logistic and Poisson regression models. Model averaging was used to select the best explanatory variables for each model, and the Akaike Information Criterion was used to assess model fit. These tests were used to examine the impact of manipulated robot responses and user characteristics on the dependent variables.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the robot's object recognition responses to be either predictable or unpredictable, which directly impacts the accuracy of the robot's performance. This manipulation is described in the 'Framework' section where the researchers explain how they manipulated robot responses to be more or less surprising based on the pilot study. The 'Experiment' section further details the two conditions: low surprise (predictable responses) and high surprise (unpredictable responses). The 'Results' section shows that the type of response (correct-specific, incorrect-nonsense, etc.) influenced the level of surprise experienced by participants, and that this surprise, in turn, impacted trust. Specifically, the 'Results' section states that participants were significantly less likely to trust Recyclo if they were in the high surprise condition or if they were more surprised on a per-object level. This indicates that the manipulation of robot accuracy (through predictable vs. unpredictable responses) directly impacted trust. The study did not manipulate any other factors from the provided list.",10.1109/ROMAN.2017.8172365,http://ieeexplore.ieee.org/document/8172365/,"Service robots are becoming a widespread tool for assisting humans in scientiﬁc, industrial and even domestic settings. Yet, our understanding of how to motivate and sustain interactions between human users and robots remains limited. In this work, we conducted a study to investigate how surprising robot behaviour evokes curiosity and inﬂuences trust and engagement in the context of participants interacting with Recyclo, a service robot for providing recycling recommendations. In a Wizard-of-Oz experiment, 36 participants were asked to interact with Recyclo to recognize and sort a variety of objects, and were given object recognition responses that were either unsurprising or surprising. Results show that surprise gave rise to information seeking behavior indicative of curiosity, while having a positive inﬂuence on engagement and negative inﬂuence on trust."
"Law, Theresa; de Leeuw, Josh; Long, John H.",How Movements of a Non-Humanoid Robot Affect Emotional Perceptions and Trust,2020,1,84,84,0,No participants were excluded,Educational Setting,between-subjects,"Participants played two rounds of the ultimatum game with a Cozmo robot that moved in either a positive or negative manner. In each round, participants made an offer of candy to the robot, and the robot either accepted or rejected the offer. Participants rated the robot's emotional valence and their expectation of the robot accepting their offer.",Participants played the ultimatum game with a robot.,Cozmo,Expressive Robots,Research; Social,Game,Economic Game,minimal interaction,Participants interacted with the robot in a structured game setting with limited physical contact.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the participant's behavior.,Behavioral Measures,,Speech Data,"Trust was measured by participants' expectation of the robot accepting their offer, and qualitative data was collected through verbal responses.",no modeling,Trust was not modeled computationally; statistical analysis was used to analyze the data.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's movement style (positive or negative) was directly manipulated to influence participants' perceptions of its emotional state and their expectations of its behavior in the ultimatum game.,"The robot's movement and bid response did not significantly affect participants' trust, as measured by their expectation of the robot accepting their offer. However, the robot's non-social behaviors led to increased anthropomorphism.","The study found that negative movements and rejection of offers led to increased anthropomorphism, with participants attributing emotional explanations to the robot's behavior. There was a conflict between the hypothesis that trust would be affected by the robot's behavior and the results, which showed no significant effect on trust.","The study's main finding is that a non-humanoid robot's movement can influence human perception of its emotional state, and that non-social behaviors lead to increased anthropomorphism.","The robot moved in either a positive or negative manner, and either accepted or rejected the participant's offer in the ultimatum game. The human participant made an offer of candy to the robot and stated their expectation of the robot accepting their offer.",ordered-probit model; Logistic regression,"The study used an ordered-probit model to analyze the Likert scale data of participants' emotional valence ratings of the robot, examining the effect of robot movement and bid response on these ratings. Additionally, logistic regression models were used to predict participants' expectations of the robot accepting their bid, using robot movement, emotional valence ratings, and bid response as predictors. The purpose of these tests was to assess the impact of robot behavior on emotional perception and trust.",TRUE,Robot-nonverbal-communication; Robot-emotional-display; Teaming,,Robot-nonverbal-communication; Robot-emotional-display; Teaming,"The study manipulated the robot's movement style (positive or negative) which falls under 'Robot-nonverbal-communication'. The robot's eye expressions and overall demeanor were also manipulated to convey different emotional states, which is categorized as 'Robot-emotional-display'. The study also involved a game where the human and robot were in a transactional setting, which is categorized as 'Teaming'. The results showed that while the robot's movement and emotional display affected participants' perceptions of the robot's emotional state, these manipulations did not significantly impact their trust in the robot, as measured by their expectation of the robot accepting their offer. Therefore, 'Robot-nonverbal-communication', 'Robot-emotional-display', and 'Teaming' are listed under 'factors_that_did_not_impact_trust'.",10.1007/s12369-020-00711-3,http://link.springer.com/10.1007/s12369-020-00711-3,"The way that we move often carries emotional meaning that the people with whom we interact are adept at detecting. Humans treat the movements of robots similarly, attributing the same emotions when the robots move in ways that are analogous to emotionally-charged human movements. However, this HRI work has primarily been done on humanoid or animal-shaped robots. In this paper, we examined whether this effect would hold when people observed the movements of a non-humanoid robot, Cozmo. Moreover, the attribution of emotional stance to another agent is key in the process of predicting the behavior of the other (Eivers AR et al. in Br J Dev Psychol 28:499–504, 2010). This process is laid bare in transactional scenarios where the predicted level of trust guides the humans behavior. The ultimatum game is a transactional framework that we have adapted to allow us to test in stages how humans predict and react to the behavior of the robot. We performed a study in which people played two rounds of the ultimatum game with a non-humanoid robot that moved in either a positive or negative manner. We found that in both rounds people in the Positive Movement condition rated Cozmo’s emotional valence as higher than those in the Negative Movement condition. In the second round, after Cozmo had responded to the ﬁrst offers that the participants made, Cozmo’s bid response was a signiﬁcant factor in the Positive Movement condition, in which participants whose ﬁrst bids were rejected by Cozmo rated its emotional valence as lower than those whose bids were accepted by Cozmo. There was not an effect of movement on trust. We also ran a series of exploratory analyses to explore how various factors affected participants’ reasonings about Cozmo’s behavior, and found that unexpected, non-social behaviors (such as moving in a negative manner or rejecting a participant’s offer) lead to an increase in anthropomorphic behavior explanations."
"Law, Theresa; Chita-Tegmark, Meia; Scheutz, Matthias","The Interplay Between Emotional Intelligence, Trust, and Gender in Human–Robot Interaction: A Vignette-Based Study",2020,2,200,198,2,2 participants were excluded due to not passing the attention check,Online Crowdsourcing,between-subjects,"Participants were recruited from Amazon Mechanical Turk, provided informed consent, filled out demographic information and a self-report EI questionnaire, watched one of eight vignette videos, answered an attention-check question, rated the robot's EI, and then indicated their trust in the robot.","Participants watched a video of a human and robot coworker interacting with a supervisor, and then rated the robot's emotional intelligence and trustworthiness.",Willow Garage PR2,Mobile Manipulators,Industrial; Research,Evaluation,Rating,passive observation,Participants passively observed a video of a robot interacting with a human.,media,Participants watched a video of a robot interaction.,physical,The robot was physically present in the video.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the participant.,Questionnaires,,,Trust was measured using a 4-item questionnaire.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's emotional intelligence (high vs. low) was manipulated through its dialogue, and the robot's gender was manipulated through its name and voice, and the interaction was presented either as text or spoken dialogue.","Trust was higher for robots exhibiting high EI, and male robots were trusted more than female robots. Surprisingly, robots presented in text format were trusted more than those presented with a voice.","The finding that robots presented in text format were trusted more than those presented with a voice was unexpected. There was a conflict in the results, where the initial ANOVA did not find a significant effect of the vignette presentation style, but the ANCOVA did.","Trust in the robot was influenced by the level of the robot's EI, and gender stereotypical expectations related to EI were transferred to trust.","The robot reacted to a human coworker being reprimanded by a supervisor, either empathetically or non-empathetically. The human participant observed this interaction and then rated the robot.",ANOVA; ANCOVA,"A one-way ANOVA was used to confirm the effectiveness of the EI manipulation by comparing EI ratings between the low and high EI conditions. A 2x2x2 ANOVA was conducted to examine the effects of EI manipulation, robot gender, and vignette presentation style on trust in the robot. An ANCOVA was then used to further investigate the effects of participant gender and age on trust, adding these as independent variables and a covariate to the ANOVA model.",TRUE,Robot-emotional-display; Robot-verbal-communication-style,Robot-emotional-display; Robot-verbal-communication-style,,"The study manipulated the robot's emotional intelligence (EI) through its dialogue, which directly affects the robot's emotional display. The robot was either empathetic (high EI) or non-empathetic (low EI) towards the human coworker. This manipulation is categorized as 'Robot-emotional-display' because it involves changes in the robot's expression of empathy, a key component of emotional intelligence. Additionally, the study manipulated the presentation style of the dialogue (text vs. voice), which is categorized as 'Robot-verbal-communication-style' because it changes how the robot's communication is perceived (e.g., tone, expressiveness). The results showed that both the robot's emotional display and the presentation style impacted trust. The robot's gender was also manipulated, but this is not a factor in the list provided.",10.1007/s12369-020-00624-1,http://link.springer.com/10.1007/s12369-020-00624-1,"As robots begin to enter roles in which they work closely with human teammates or peers, it is critical to understand how people trust them based on how they interpret the robot’s behavior. In this paper we investigated the interplay between trust in a robot and people’s perceptions of the robot’s emotional intelligence. We used a vignette-based method to explore the following questions: (1) Do subjects perceive differences in robot EI, and is their trust in the robot inﬂuenced by differences in the robot’s reliability and capability? (2) Does a robot’s EI inﬂuence how much it is trusted and conversely does a robot’s capability and reliability inﬂuence how emotionally intelligent it is perceived to be? (3) Do people trust male and female robots differently when the robots exhibit different levels of EI or different levels of capability and reliability, and do gender stereotypical expectations related to EI transfer to trust?; (4) Does focusing on the robot’s EI increase one’s trust in the robot? (5) Is the interplay between trust, EI and gender the same for different levels of evoked social presence and human-likeness (i.e., when the interaction is presented in different modalities, text or spoken dialogue when the robot’s voice is actually heard)? We found that trust in the robot was inﬂuenced by the level of the robot’s EI (p < .001) and that gender stereotypical expectations related to EI were transferred to trust (p = .006), but gender effects on trust disappeared when only capability and reliability (robot’s trustworthiness) were manipulated but not the robot’s EI (p = .103). Surprisingly, we found that people trusted the robot more when the interaction was presented in text format (p = .024), going against our hypothesis that spoken dialogue would evoke more social presence and thus bolster EI perception and instill more trust. We suggest that this effect might be due to people’s expectations of a more expressive and human-like voice. Finally, we also found that people’s trust ratings in the robot were higher when they were made to notice and think about the robot’s EI, by answering EI questionnaires prior to trust questionnaires (p = .022). We discuss the implications of our ﬁndings for robot design and HRI research."
"Law, Theresa; Chita-Tegmark, Meia; Scheutz, Matthias","The Interplay Between Emotional Intelligence, Trust, and Gender in Human–Robot Interaction: A Vignette-Based Study",2020,2,439,421,18,18 participants did not pass the attention check,Online Crowdsourcing,between-subjects,"Participants were recruited from Amazon Mechanical Turk, provided informed consent, watched one of eight vignette videos, answered an attention-check question, and then rated the robot's EI and trust, with the order of the questionnaires counterbalanced.","Participants watched a video of a human and robot coworker interacting with a supervisor, and then rated the robot's emotional intelligence and trustworthiness.",Willow Garage PR2,Mobile Manipulators,Industrial; Research,Evaluation,Rating,passive observation,Participants passively observed a video of a robot interacting with a human.,media,Participants watched a video of a robot interaction.,physical,The robot was physically present in the video.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the participant.,Questionnaires; Multidimensional Measures,N/A; Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using a 4-item questionnaire and the Multidimensional Measure of Trust.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's trustworthiness (high vs. low capability and reliability) was manipulated through its dialogue, the interaction was presented either as text or spoken dialogue, and the order of the EI and trust questionnaires was counterbalanced.",Trust was higher for robots with high capability and reliability. Robots presented in text format were trusted more than those presented with a voice. Participants who answered EI questions first trusted the robot more.,The finding that robots presented in text format were trusted more than those presented with a voice was unexpected. There was an order effect in which participants who were asked the EI questions first trusted the robot more.,"Manipulating trust did not affect the participants' perceptions of the robot's EI, and gender effects on trust were not seen when trust was manipulated. The order of the questionnaires affected trust ratings.","The robot reacted to a human coworker being reprimanded by a supervisor, either indicating high or low capability and reliability. The human participant observed this interaction and then rated the robot.",ANOVA; ANCOVA,"Two ANOVAs were conducted using subscales of the Multidimensional Measure of Trust (MDMT) to verify the effectiveness of the trust manipulation. A 2x2 ANOVA was used to examine the effects of the trust manipulation and robot gender on the perception of the robot's EI. A 2x2x2x2 ANOVA was used to investigate the effects of the trust manipulation, robot gender, vignette presentation style, and questionnaire order on trust in the robot. An ANCOVA was used to explore potential effects of participant gender and age on trust.",TRUE,Robot-accuracy; Robot-verbal-communication-style,Robot-accuracy; Robot-verbal-communication-style,,"In this study, the researchers manipulated the robot's trustworthiness by varying its capability and reliability, which directly impacts the robot's performance accuracy. In the high trust condition, the robot's algorithm was 98% accurate, while in the low trust condition, it was 65% accurate. This is categorized as 'Robot-accuracy' because it directly affects the robot's ability to perform the task correctly. The study also manipulated the presentation style of the dialogue (text vs. voice), which is categorized as 'Robot-verbal-communication-style' because it changes how the robot's communication is perceived (e.g., tone, expressiveness). The results showed that both the robot's accuracy and the presentation style impacted trust. The order of the questionnaires also impacted trust, but this is not a factor in the list provided. The robot's gender was also manipulated, but this is not a factor in the list provided.",10.1007/s12369-020-00624-1,http://link.springer.com/10.1007/s12369-020-00624-1,"As robots begin to enter roles in which they work closely with human teammates or peers, it is critical to understand how people trust them based on how they interpret the robot’s behavior. In this paper we investigated the interplay between trust in a robot and people’s perceptions of the robot’s emotional intelligence. We used a vignette-based method to explore the following questions: (1) Do subjects perceive differences in robot EI, and is their trust in the robot inﬂuenced by differences in the robot’s reliability and capability? (2) Does a robot’s EI inﬂuence how much it is trusted and conversely does a robot’s capability and reliability inﬂuence how emotionally intelligent it is perceived to be? (3) Do people trust male and female robots differently when the robots exhibit different levels of EI or different levels of capability and reliability, and do gender stereotypical expectations related to EI transfer to trust?; (4) Does focusing on the robot’s EI increase one’s trust in the robot? (5) Is the interplay between trust, EI and gender the same for different levels of evoked social presence and human-likeness (i.e., when the interaction is presented in different modalities, text or spoken dialogue when the robot’s voice is actually heard)? We found that trust in the robot was inﬂuenced by the level of the robot’s EI (p < .001) and that gender stereotypical expectations related to EI were transferred to trust (p = .006), but gender effects on trust disappeared when only capability and reliability (robot’s trustworthiness) were manipulated but not the robot’s EI (p = .103). Surprisingly, we found that people trusted the robot more when the interaction was presented in text format (p = .024), going against our hypothesis that spoken dialogue would evoke more social presence and thus bolster EI perception and instill more trust. We suggest that this effect might be due to people’s expectations of a more expressive and human-like voice. Finally, we also found that people’s trust ratings in the robot were higher when they were made to notice and think about the robot’s EI, by answering EI questionnaires prior to trust questionnaires (p = .022). We discuss the implications of our ﬁndings for robot design and HRI research."
"Law, Theresa; Malle, Bertram F.; Scheutz, Matthias",A Touching Connection: How Observing Robotic Touch Can Affect Human Trust in a Robot,2021,3,600,538,49,49 participants failed the manipulation check,Online Crowdsourcing,between-subjects,"Participants watched a warm-up video, then a video of a human-robot interaction, and then completed questionnaires.",Participants watched a video of a robot interacting with a human and rated their impressions of the robot.,Willow Garage PR2,Mobile Manipulators,Industrial; Research,Evaluation,Rating,passive observation,Participants passively observed a video of a robot interacting with a human.,media,Participants watched a video of a human-robot interaction.,physical,The robot was a physical robot shown in a video.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Multidimensional Measures; Questionnaires,Multi-Dimensional Measure of Trust (MDMT),,Trust was assessed using the Multidimensional Measure of Trust questionnaire.,"parametric models (e.g., regression)","Mediation analysis was used to model the relationship between touch, behavior impressions, and trust.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot either touched the human on the shoulder or did not, and the robot's attitude and gender were also varied.","Observed touch increased overall trust, especially for the sincere and ethical trust aspects, and led people to perceive the robot as more comforting, but also more inappropriate.","The robot's touch elicited an impression that came with an interesting tension between being perceived as comforting and being perceived as somewhat odd and inappropriate. There was also a three-way interaction between Robot Gender, Actor Gender, and Robot Attitude, but post-hoc tests revealed no specific significant comparisons.","Observing a robot touch a human on the shoulder affected people's impressions of the robot's behavior and, in turn, their trust in the robot.","The robot stood next to a human who was entering information on a computer, and then either touched the human on the shoulder or did not.",ANOVA; Principal component analysis; Mediation analysis; Linear regression,"Between-subjects ANOVAs were conducted to analyze the effects of Touch, Attitude, Robot Gender, and Actor Gender on trust and behavior impressions. Principal Component Analysis (PCA) was used to identify underlying components of the behavior impression variables. Mediation analysis was used to examine the relationship between touch, behavior impressions (specifically comforting), and trust. Multiple regression analysis was used to predict trust from the set of behavior impression variables.",TRUE,Robot-nonverbal-communication; Robot-social-attitude; Robot-verbal-communication-style,Robot-nonverbal-communication; Robot-social-attitude,Robot-verbal-communication-style,"The study manipulated whether the robot touched the human on the shoulder (Robot-nonverbal-communication), the robot's attitude (positive, neutral, negative) when speaking to the human (Robot-social-attitude), and the robot's voice (male or female) (Robot-verbal-communication-style). The touch and attitude manipulations impacted trust, while the robot's voice did not have a significant impact on trust.",10.1007/s12369-020-00729-7,https://link.springer.com/10.1007/s12369-020-00729-7,"As robots begin to occupy our social spaces, touch will increasingly become part of human–robot interactions. This paper examines the impact of observing a robot touch a human on trust in that robot. In three online studies, observers watched short videos of human–robot interactions and provided a series of judgments about the robot, which either did or did not touch the human on the shoulder. Trust was measured using a recently introduced multi-dimensional instrument, which assesses people’s trust in a robot as being capable, reliable, sincere, and/or ethical. The ﬁrst study showed that observed robot touch increased overall trust in the robot, especially for the sincere and ethical trust aspects, and led people to perceive the robot as more comforting, but also more inappropriate. A second study replicated the general pattern, even with a handshake preceding the touch; but in the context of the handshake the touch was seen as more inappropriate. A third study examined the joint impact of a handshake, touch, and information about the robot’s designed function. In the context of such information, observed touch was seen as even more inappropriate, which in turn decreased trust."
"Law, Theresa; Malle, Bertram F.; Scheutz, Matthias",A Touching Connection: How Observing Robotic Touch Can Affect Human Trust in a Robot,2021,3,60,51,9,9 participants failed the manipulation check question,Online Crowdsourcing,between-subjects,"Participants saw an image of a handshake, then watched a video of a human-robot interaction, and then completed questionnaires.",Participants watched a video of a robot interacting with a human and rated their impressions of the robot.,Willow Garage PR2,Mobile Manipulators,Industrial; Research,Evaluation,Rating,passive observation,Participants passively observed a video of a robot interacting with a human.,media,Participants watched a video of a human-robot interaction.,physical,The robot was a physical robot shown in a video.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Multidimensional Measures; Questionnaires,Multi-Dimensional Measure of Trust (MDMT),,Trust was assessed using the Multidimensional Measure of Trust questionnaire.,"parametric models (e.g., regression)","Mediation analysis was used to model the relationship between touch, behavior impressions, and trust.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot either touched the human on the shoulder or did not, and participants saw an image of a handshake before the video.","The touch was seen as more surprising and inappropriate in the context of the handshake, but the effect on trust was not significant.","The effect sizes on all dependent variables were larger than in Experiment I, making the handshake seem, if anything, to backfire. The touch was seen as even more surprising and inappropriate in the context of the handshake.","A robot touching a human elicits contrasting reactions, being perceived as comforting but also surprising and inappropriate, especially when presented with the interaction as a formal relationship.","The robot stood next to a human who was entering information on a computer, and then either touched the human on the shoulder or did not. Participants saw an image of a handshake before the video.",ANOVA; Mediation analysis; Linear regression,"ANOVAs were used to examine the effect of Touch on trust and behavior impressions. Mediation analysis was used to examine the relationship between touch, behavior impressions (specifically comforting), and trust. Multiple regression analysis was used to predict trust from the set of behavior impression variables.",TRUE,Robot-nonverbal-communication,Robot-nonverbal-communication,,"This study manipulated whether the robot touched the human on the shoulder (Robot-nonverbal-communication). The touch manipulation impacted trust, although not significantly, but the effect size was larger than in Study 1. The study also included a handshake image before the video, but this was not a manipulated factor, it was present in all conditions.",10.1007/s12369-020-00729-7,https://link.springer.com/10.1007/s12369-020-00729-7,"As robots begin to occupy our social spaces, touch will increasingly become part of human–robot interactions. This paper examines the impact of observing a robot touch a human on trust in that robot. In three online studies, observers watched short videos of human–robot interactions and provided a series of judgments about the robot, which either did or did not touch the human on the shoulder. Trust was measured using a recently introduced multi-dimensional instrument, which assesses people’s trust in a robot as being capable, reliable, sincere, and/or ethical. The ﬁrst study showed that observed robot touch increased overall trust in the robot, especially for the sincere and ethical trust aspects, and led people to perceive the robot as more comforting, but also more inappropriate. A second study replicated the general pattern, even with a handshake preceding the touch; but in the context of the handshake the touch was seen as more inappropriate. A third study examined the joint impact of a handshake, touch, and information about the robot’s designed function. In the context of such information, observed touch was seen as even more inappropriate, which in turn decreased trust."
"Law, Theresa; Malle, Bertram F.; Scheutz, Matthias",A Touching Connection: How Observing Robotic Touch Can Affect Human Trust in a Robot,2021,3,361,335,26,26 people failed the manipulation check question,Online Crowdsourcing,between-subjects,"Participants were assigned to a baseline, customer-focused, or performance-focused condition, and saw a handshake image or not, then watched a video of a human-robot interaction, and then completed questionnaires.",Participants watched a video of a robot interacting with a human and rated their impressions of the robot.,Willow Garage PR2,Mobile Manipulators,Industrial; Research,Evaluation,Rating,passive observation,Participants passively observed a video of a robot interacting with a human.,media,Participants watched a video of a human-robot interaction.,physical,The robot was a physical robot shown in a video.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Multidimensional Measures; Questionnaires,Multi-Dimensional Measure of Trust (MDMT),,Trust was assessed using the Multidimensional Measure of Trust questionnaire.,"parametric models (e.g., regression)","Mediation analysis was used to model the relationship between touch, behavior impressions, and trust.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot either touched the human on the shoulder or did not, participants saw an image of a handshake or not, and were told the robot was designed to be customer-focused, performance-focused, or given no information.","In the designed function conditions, the robot touch became detrimental for trust, and this effect was entirely explained by increased perceptions of inappropriateness.","In the baseline condition, touch had nearly no impact on perceptions of being comforting and no impact on trust. In the designed function conditions, the robot touch became detrimental for trust. The designed-function manipulation did not have a negative impact by itself; only when paired with the robot touch did it decrease trust and worsen behavior impressions.","In the designed function conditions, the robot touch became detrimental for trust, and this effect was entirely explained by increased perceptions of inappropriateness.","The robot stood next to a human who was entering information on a computer, and then either touched the human on the shoulder or did not. Participants saw an image of a handshake or not, and were told the robot was designed to be customer-focused, performance-focused, or given no information.",ANOVA; Mediation analysis; Linear regression,"ANOVAs were used to examine the effects of Touch, Handshake, and Function on trust and behavior impressions. Mediation analysis was used to examine the relationship between touch, perceptions of inappropriateness, and trust in the designed function conditions. Multiple regression analysis was used to predict trust from the set of behavior impression variables.",TRUE,Robot-nonverbal-communication; Robot-verbal-communication-content,Robot-nonverbal-communication; Robot-verbal-communication-content,,"This study manipulated whether the robot touched the human on the shoulder (Robot-nonverbal-communication), and the information provided about the robot's designed function (customer-focused, performance-focused, or no information) (Robot-verbal-communication-content). Both the touch and the designed function information impacted trust. The study also included a handshake image before the video, but this was not a manipulated factor, it was present in some conditions and not in others.",10.1007/s12369-020-00729-7,https://link.springer.com/10.1007/s12369-020-00729-7,"As robots begin to occupy our social spaces, touch will increasingly become part of human–robot interactions. This paper examines the impact of observing a robot touch a human on trust in that robot. In three online studies, observers watched short videos of human–robot interactions and provided a series of judgments about the robot, which either did or did not touch the human on the shoulder. Trust was measured using a recently introduced multi-dimensional instrument, which assesses people’s trust in a robot as being capable, reliable, sincere, and/or ethical. The ﬁrst study showed that observed robot touch increased overall trust in the robot, especially for the sincere and ethical trust aspects, and led people to perceive the robot as more comforting, but also more inappropriate. A second study replicated the general pattern, even with a handshake preceding the touch; but in the context of the handshake the touch was seen as more inappropriate. A third study examined the joint impact of a handshake, touch, and information about the robot’s designed function. In the context of such information, observed touch was seen as even more inappropriate, which in turn decreased trust."
"Law, Theresa; Chita-Tegmark, Meia; Scheutz, Matthias",Trust Transfer in Robots between Task Environments,2024,3,63,51,12,"12 participants were excluded due to technical issues, participant confusion or nausea, or prior participation in a similar study",Controlled Lab Environment,between-subjects,"Participants were assigned to either a control group or a functioning robot group. The functioning robot group interacted with a robot in a spaceship environment before being moved to a warehouse environment. The control group only interacted with the robot in the warehouse environment. In the warehouse, participants were asked to rate the robot's capability and trust for various tasks.","Participants rated the robot's capability and trust for various tasks in a virtual warehouse environment, and decided whether to assign the tasks to the robot or complete them themselves.",Unspecified,Mobile Manipulators,Research,Evaluation,Rating,minimal interaction,Participants interacted with a virtual robot in a simulated environment.,simulation,Participants were immersed in a virtual reality environment.,simulated,The robot was a virtual representation in a VR environment.,wizard of oz (directly controlled),The robot's actions were controlled by a researcher in an adjacent room.,Custom Scales; Behavioral Measures,,Eye-tracking Data,"Trust was measured using custom Likert scales and task assignment choices, and eye-tracking data was collected.",no modeling,No computational model of trust was used; the study used statistical tests to analyze the data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"Participants were either exposed to a functioning robot in a spaceship environment or not, influencing their expectations and perceptions of the robot's performance in a subsequent warehouse environment.",Seeing the robot perform in the spaceship environment only affected participants' perceptions of how much they trusted the robot to fix broken equipment.,"The hypothesis that seeing the robot perform in one environment would lead to heightened ratings of capability and trust was not supported, except for the task of fixing broken equipment. Participants in the control condition spent more time looking at task-relevant objects.","Prior experience with a robot in one environment did not significantly affect trust and capability ratings in a new environment, except for a specific task related to the robot's visible tool.",The robot was tasked with fixing broken tubes in the spaceship environment and was presented as a teammate in the warehouse environment. The human participant was tasked with marking rocks and radiation zones in the spaceship environment and rating the robot's capability and trust for various tasks in the warehouse environment.,Shapiro-Wilk; Kruskal-Wallis; Chi-squared; Linear regression; ANOVA,The study used a Shapiro-Wilk test to check for normality of data. Kruskal-Wallis tests were used to compare the average capability and trust ratings between the control and functioning robot conditions. Chi-squared tests were used to compare the number of tasks assigned to the robot or human between conditions. Linear regression models were used to assess whether the number of tubes fixed predicted average capability and trust ratings. A one-way ANOVA was used to compare the amount of time participants spent looking at task-relevant objects between conditions.,TRUE,Robot-accuracy; Task-environment,Robot-accuracy,Task-environment,"The study manipulated the robot's accuracy by having a 'functioning robot' condition where the robot performed tasks in a spaceship environment before the warehouse environment, and a control condition where the robot was not seen in the spaceship environment. This manipulation of the robot's prior performance directly relates to 'Robot-accuracy' as it influences the perceived reliability of the robot. The study also manipulated the 'Task-environment' by having participants interact with the robot in a spaceship environment before the warehouse environment, or only in the warehouse environment. The results showed that the robot's prior performance in the spaceship environment (Robot-accuracy) only impacted trust for the task of fixing broken equipment, but the change in environment (Task-environment) did not have a significant impact on trust for most tasks.",10.1145/3686038.3686061,https://dl.acm.org/doi/10.1145/3686038.3686061,"Trust and capability transfer between tasks and environments is common in human-human interactions. For human-robot interactions it is unclear how a robot’s performance of a task in one environment affects humans predictions about the robot’s performance of another related or unrelated task in a different environment. When making assessments about a robot’s task capabilities, three main sources of information are pertinent: the human’s “default mental model” of robots, the robot’s appearance, and the robot’s performance. We hypothesized that past task performance would be the most salient information source, and that participants who saw the robot perform tasks in one environment would transfer their assumptions about the robot’s capability to a new environment with new tasks. However, the results of our first study did not support this hypothesis. We then performed a second study to exclude the possibility that because the robot worked well in the first environment, it did not supply any salient, different information from the participants’ default mental model of robots (that robots are functional, etc.). If this hypothesis was correct, a faulty robot in the first environment would be rated significantly lower at the tasks in the second environment. However, the results did not support the second hypothesis either. We then conducted a third study investigating whether the tasks themselves or the environment had a stronger effect on trust assessments. The results showed that because individual judgments varied dramatically no systematic trust and task transfer result can be obtained. The upshot for HRI is that trust and task transfer are solely dependent on the individual’s background and judgment rather than on task or environmental properties."
"Law, Theresa; Chita-Tegmark, Meia; Scheutz, Matthias",Trust Transfer in Robots between Task Environments,2024,3,33,29,4,4 participants were excluded due to technical issues or participant confusion or nausea,Controlled Lab Environment,between-subjects,"Participants were assigned to one of three conditions: control, functioning robot, or faulty robot. The functioning and faulty robot groups interacted with a robot in a spaceship environment before being moved to a warehouse environment. The control group only interacted with the robot in the warehouse environment. In the warehouse, participants were asked to rate the robot's capability and trust for various tasks.","Participants rated the robot's capability and trust for various tasks in a virtual warehouse environment, and decided whether to assign the tasks to the robot or complete them themselves.",Unspecified,Mobile Manipulators,Research,Evaluation,Rating,minimal interaction,Participants interacted with a virtual robot in a simulated environment.,simulation,Participants were immersed in a virtual reality environment.,simulated,The robot was a virtual representation in a VR environment.,wizard of oz (directly controlled),The robot's actions were controlled by a researcher in an adjacent room.,Custom Scales; Behavioral Measures,,,Trust was measured using custom Likert scales and task assignment choices.,no modeling,No computational model of trust was used; the study used statistical tests to analyze the data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"Participants were exposed to either a functioning or faulty robot in a spaceship environment, or no robot, influencing their expectations and perceptions of the robot's performance in a subsequent warehouse environment.","The faulty robot did not result in lower capability, trust, and assignment ratings compared to the functioning robot or control conditions, except for the task of fixing broken equipment.",The hypothesis that a faulty robot would lead to lower trust ratings was not supported. Participants who fixed fewer tubes rated the robot's capability as higher.,"The robot's performance in the first environment did not significantly affect trust and capability ratings in a new environment, even when the robot was faulty, except for a specific task related to the robot's visible tool.",The robot was tasked with fixing broken tubes in the spaceship environment and was presented as a teammate in the warehouse environment. The human participant was tasked with marking rocks and radiation zones in the spaceship environment and rating the robot's capability and trust for various tasks in the warehouse environment.,Kruskal-Wallis; Chi-squared; Linear regression,"The study used Kruskal-Wallis tests to compare the average capability and trust ratings between the control, functioning robot, and faulty robot conditions. Chi-squared tests were used to compare the number of tasks assigned to the robot or human between conditions. Linear regression models were used to assess whether the number of tubes fixed predicted average capability, trust ratings, and task assignment.",TRUE,Robot-accuracy; Task-environment,Robot-accuracy,Task-environment,"This study manipulated 'Robot-accuracy' by having three conditions: a control group, a functioning robot group, and a faulty robot group. The functioning and faulty robot groups interacted with the robot in a spaceship environment before the warehouse environment, with the faulty robot group experiencing errors during the spaceship task. This directly manipulates the robot's perceived reliability and performance. The study also manipulated the 'Task-environment' by having participants interact with the robot in a spaceship environment before the warehouse environment, or only in the warehouse environment. The results showed that the robot's prior performance (Robot-accuracy), whether functioning or faulty, only impacted trust for the task of fixing broken equipment, but the change in environment (Task-environment) did not have a significant impact on trust for most tasks.",10.1145/3686038.3686061,https://dl.acm.org/doi/10.1145/3686038.3686061,"Trust and capability transfer between tasks and environments is common in human-human interactions. For human-robot interactions it is unclear how a robot’s performance of a task in one environment affects humans predictions about the robot’s performance of another related or unrelated task in a different environment. When making assessments about a robot’s task capabilities, three main sources of information are pertinent: the human’s “default mental model” of robots, the robot’s appearance, and the robot’s performance. We hypothesized that past task performance would be the most salient information source, and that participants who saw the robot perform tasks in one environment would transfer their assumptions about the robot’s capability to a new environment with new tasks. However, the results of our first study did not support this hypothesis. We then performed a second study to exclude the possibility that because the robot worked well in the first environment, it did not supply any salient, different information from the participants’ default mental model of robots (that robots are functional, etc.). If this hypothesis was correct, a faulty robot in the first environment would be rated significantly lower at the tasks in the second environment. However, the results did not support the second hypothesis either. We then conducted a third study investigating whether the tasks themselves or the environment had a stronger effect on trust assessments. The results showed that because individual judgments varied dramatically no systematic trust and task transfer result can be obtained. The upshot for HRI is that trust and task transfer are solely dependent on the individual’s background and judgment rather than on task or environmental properties."
"Law, Theresa; Chita-Tegmark, Meia; Scheutz, Matthias",Trust Transfer in Robots between Task Environments,2024,3,52,52,0,No participants were excluded,Online Crowdsourcing,within-subjects,Participants watched videos of a robot completing one task and then rated the robot's capability and trust for a different task. They then rated the similarity between the two tasks.,"Participants rated the robot's capability and trust for various tasks after watching a video of the robot performing a different task, and then rated the similarity between the two tasks.",Unspecified,Mobile Manipulators,Research,Evaluation,Rating,passive observation,Participants passively observed videos of the robot performing tasks.,media,Participants watched videos of the robot in a virtual environment.,simulated,The robot was a virtual representation in a video.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and not adaptive to the user.,Custom Scales,,,Trust was measured using custom Likert scales.,no modeling,No computational model of trust was used; the study used statistical tests to analyze the data.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,"The study assessed trust without intentionally altering any specific factors, focusing on the relationship between task similarity and trust ratings.","There was no overall correlation between task similarity ratings and trust scores, but there were significant individual differences.","There was no overall correlation between task similarity and trust, but there were significant individual differences in how participants related task similarity to trust.","Trust transfer across tasks and environments is not uniform across subjects, but seems to depend essentially on individual factors.",The robot was shown performing various tasks in a virtual warehouse environment. The human participant was tasked with rating the robot's capability and trust for different tasks and rating the similarity between the tasks.,Linear regression,The study used linear regressions to assess the effect of task similarity ratings on capability and trust perceptions. It also checked for the effect of gender and age.,FALSE,,,,"This study did not manipulate any factors related to the robot or the task. Participants passively observed videos of the robot performing tasks and then rated the robot's capability and trust for different tasks. The study focused on the relationship between task similarity and trust ratings without any intentional manipulation of the robot's behavior or the environment. Therefore, no factors were manipulated.",10.1145/3686038.3686061,https://dl.acm.org/doi/10.1145/3686038.3686061,"Trust and capability transfer between tasks and environments is common in human-human interactions. For human-robot interactions it is unclear how a robot’s performance of a task in one environment affects humans predictions about the robot’s performance of another related or unrelated task in a different environment. When making assessments about a robot’s task capabilities, three main sources of information are pertinent: the human’s “default mental model” of robots, the robot’s appearance, and the robot’s performance. We hypothesized that past task performance would be the most salient information source, and that participants who saw the robot perform tasks in one environment would transfer their assumptions about the robot’s capability to a new environment with new tasks. However, the results of our first study did not support this hypothesis. We then performed a second study to exclude the possibility that because the robot worked well in the first environment, it did not supply any salient, different information from the participants’ default mental model of robots (that robots are functional, etc.). If this hypothesis was correct, a faulty robot in the first environment would be rated significantly lower at the tasks in the second environment. However, the results did not support the second hypothesis either. We then conducted a third study investigating whether the tasks themselves or the environment had a stronger effect on trust assessments. The results showed that because individual judgments varied dramatically no systematic trust and task transfer result can be obtained. The upshot for HRI is that trust and task transfer are solely dependent on the individual’s background and judgment rather than on task or environmental properties."
"Lawson-Guidigbe, Clarisse; Louveton, Nicolas; Amokrane-Ferka, Kahina; LeBlanc, Benoît; Andre, Jean-Marc",Impact of Visual Embodiment on Trust for a Self-driving Car Virtual Agent: A Survey Study and Design Recommendations,2020,1,146,146,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants completed an online survey where they were asked to imagine being in a self-driving car with a virtual assistant, then they rated different visual embodiments of the assistant in a critical driving scenario.","Participants rated the perceived anthropomorphism, liking, and self-reported trust towards different visual embodiments of a virtual assistant in a self-driving car.",Unspecified,Autonomous Vehicles,Social,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the interaction scenario.,media,Participants viewed static images of the virtual assistant.,simulated,The robot was represented by static images.,fully autonomous (limited adaptation),"The virtual assistant was described as fully autonomous in handling the driving task, but its actions were not adaptive.",Questionnaires,,,Trust was measured using a questionnaire.,no modeling,Trust was not modeled computationally.,Observational & Survey Studies,Quantitative Surveys,Direct Manipulation,"The visual appearance of the virtual assistant was manipulated by showing different models (human, animal, mechanical, abstract) to see how it affected trust.","The Mechanical Human model, followed by the Human and Abstract models, elicited the highest trust scores, while Animal and Mechanical Animal models elicited the lowest trust scores.","The study found that human-like and mechanical-human visual embodiments were more trusted than animal-like embodiments, which is a notable trend in the context of virtual assistants.","The study found that the Mechanical Human model was the most suitable visual embodiment for a virtual assistant in an autonomous driving context, followed by Human and Abstract models.",The human participant imagined being in a self-driving car and rated the trustworthiness of different visual embodiments of a virtual assistant that was described as handling the driving task.,,"The study calculated a composite trust score by averaging nine items from a questionnaire (Cronbach's alpha = 0.97). The analysis focused on comparing the mean trust scores across different visual embodiment models (Mechanical Human, Human, Abstract, Animal, and Mechanical Animal) to determine which models elicited the highest and lowest trust. No specific statistical tests like t-tests or ANOVA were explicitly mentioned, but the analysis involved comparing means and distributions of trust scores across different conditions.",TRUE,Robot-aesthetics,Robot-aesthetics,,"The study manipulated the visual appearance of the virtual assistant by presenting different models (human, animal, mechanical, abstract). This falls under 'Robot-aesthetics' because it directly relates to the visual appeal and design of the robot's representation. The paper explicitly states, 'In this study, we assessed the trustworthiness of different models of visual embodiment such as abstract, human, animal, mechanical, etc.' and 'Then they are presented with each of the 5 visual embodiment models...selected in the picture sorting procedure.' The results showed that different visual embodiments elicited different levels of trust, indicating that 'Robot-aesthetics' impacted trust. The paper states, 'Our preliminary results are pointing to the Mechanical Human model followed by the Human and Abstract to be the most suitable embodiment models...while Animal and Mechanical Animal models must be avoided.' This shows that the manipulation of the visual appearance had a direct impact on trust. There were no other factors manipulated in the study.",,http://link.springer.com/10.1007/978-3-030-50732-9_51,"Designing trust-based in-car interfaces is critical for the adoption of self-driving cars. Indeed, latest studies revealed that a vast majority of drivers are not willing to trust this technology."
"Lazanyi, Kornelia",Do you trust your car?,2016,1,238,238,0,No participants were excluded,Survey/Interview,,Participants completed a questionnaire online or in pen and paper format with the help of trained interviewers.,Participants evaluated statements connected to the automated breaking system on a five-point Likert scale and rated their level of trust in the system on a scale of 0 to 10.,Unspecified,Autonomous Vehicles,Other: Automated breaking system,Evaluation,Survey/Questionnaire Completion,passive observation,"Participants did not interact with the system directly, but rather answered questions about it.",media,The interaction was based on text descriptions of the system.,hypothetical,"The robot was described in text, without any visual representation.",pre-programmed (non-adaptive),The automated breaking system operates based on pre-set rules.,Questionnaires; Custom Scales,,,Trust was measured using a custom scale and a questionnaire.,no modeling,The study did not model trust computationally.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,"The study did not manipulate any factors related to trust, but rather assessed existing trust levels.","The study found that drivers generally trusted the automated breaking system, with longer experience in the industry correlating with higher trust. Female drivers were more cautious when evaluating risks.","Female drivers were more cautious when evaluating the risks of the automated breaking system, and drivers with more experience in the industry trusted the system more.","Drivers generally trust the automated breaking system, and their trust is influenced by experience and gender.",The robot (automated breaking system) is described as a system that can automatically brake the vehicle. The human participant's role is to evaluate the system's features and rate their trust in it.,t-test; independent sample's t-test,"The study used independent sample's t-tests to compare the means of different groups. Specifically, one t-test was used to compare the average work experience of male and female drivers, and another t-test was used to compare the responses of drivers who rated their trust in the automated breaking system as 8 or higher with those who rated it lower. The purpose of these tests was to identify significant differences between groups in terms of their trust and attitudes towards the automated breaking system.",FALSE,,,,"The study did not manipulate any factors. The research focused on assessing existing trust levels in an automated breaking system among truck drivers. The study explored the relationship between trust and factors such as driving experience and gender, but these were not manipulated by the researchers. The study used a survey to gather data on drivers' perceptions and trust levels, without any experimental manipulation of the system or the driving conditions. Therefore, no factors were manipulated.",10.1109/CINTI.2016.7846424,http://ieeexplore.ieee.org/document/7846424/,"Automation has become increasingly common not only in complex, technical systems, but in everyday situations as well. The success of such automated systems is only to a certain extent dependent on their technical features, the market penetration will be determined by the extent, how much people accept and trust these systems to perform safely. Present paper endeavours to explore safety and connected phenomena in the mirror of the semi-autonomous cars’ and their drivers’ co-functioning – the relation of trust."
"Lazanyi, Kornelia","Generation Z and Y – are they different, when it comes to trust in robots?",2019,1,547,426,121,"Respondents born before 1977 or after 2012 were omitted, Respondents born in 1994 and 1995 were also left out",Online Crowdsourcing,between-subjects,"An online questionnaire was administered to participants, including questions about demographics, trust, and social relations. Data from participants outside the target age ranges were excluded.",Participants completed an online questionnaire about their trust and social relations.,Unspecified,Other,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants responded to questions about trust in robots and AI without direct interaction.,media,The study used text-based questions without any visual or interactive elements.,hypothetical,The study involved hypothetical robots described in the questionnaire.,not autonomous,"The robots were described in the questionnaire, but no autonomous actions were performed.",Questionnaires,,,Trust was assessed using a questionnaire with Likert scale questions.,no modeling,The study did not use any computational model of trust.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study did not manipulate any factors related to trust; it measured existing attitudes.,,"GenZ respondents reported having larger social networks but lower levels of trust in those networks compared to GenY. GenZ also trusted robots more than virtual entities, suggesting a preference for physical embodiment.","GenZ is more willing to engage with robots than GenY, and this is not due to loneliness but rather a preference for personal contact and physical embodiment.","Participants completed an online questionnaire about their trust in robots and AI, and their social relations. The robot was not physically present, and the interaction was purely hypothetical.",independent samples t test; Pearson correlation,"The study used an independent samples T-test to compare demographic differences (other than age) between the GenY and GenZ samples, and to compare the means of responses to statements related to everyday interactions. Correlation analysis was used to examine the relationship between gender and trust in robots, and between trust in robots and other factors describing the respondents' inclination towards others.",FALSE,,,,"The study did not manipulate any factors. It was an observational study that surveyed participants about their trust in robots and AI. The study compared responses between Gen Y and Gen Z, but this was not a manipulation, rather a comparison of pre-existing attitudes. The study did not manipulate any aspect of the robot or the task. Therefore, no factors were manipulated, and no factors impacted or did not impact trust due to manipulation.",10.1109/INES46365.2019.9109508,https://ieeexplore.ieee.org/document/9109508/,"Readiness is still a hot topic in the 21st century, but it is not about modern ICT technology any more, but about artificial intelligence (AI) and robots in specific. Robots are the new industrial revolution. They can extend human sensory, psychomotor, and cognitive capabilities, and for this, they are feared, or even abhorred by some. The new generations, however, seem to be much more eager to engage with them. Present paper intends to analyse international literature about young generations’ attitudes towards AI and robots and presents a primary research about Hungarian young people’s attitude towards robots. The aim of the paper is to test the statements of international literature against a suitable sample of Y and Z generation people, to figure out, whether there are real significant differences between the two generations, when it comes to robots and trust in them. On the basis of the data presented in the paper generation Z is much more willing to engage with robots, than generation Y. However, this attitude is not generated by their “loneliness”, but originates from their need for personal contact. Hence, the research did not only succeed in identifying differences between the two generations under scrutiny, but pinpointed a difference that is rather neglected, or misinterpreted in international literature."
"Lee, Wen-Hwa; Lin, Ching-Wen; Shih, Kuang-Heng","A technology acceptance model for the perception of restaurant service robots for trust, interactivity, and output quality",2018,1,400,382,18,18 surveys were deemed invalid,Survey/Interview,,"Participants were given a paper-based survey to complete. The survey used a 5-point Likert scale and included questions about perceived usefulness, perceived ease of use, attitude, behavioral intentions, trust, interactivity, and output quality.",Participants completed a survey about their perceptions of restaurant service robots.,Unspecified,Service and Assistive Robots,Social; Other,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the robot in the survey.,media,The interaction was based on written descriptions of the robot.,hypothetical,The robot was only described in text.,not autonomous,"The robot's actions were only described in the survey, with no actual robot present.",Questionnaires,,,Trust was measured using a questionnaire.,no modeling,Trust was measured but not modeled computationally.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study assessed trust without intentionally altering any specific factors.,,"The study found that trust and output quality significantly affect perceived usefulness, and interactivity has a positive influence on perceived ease of use. However, trust did not have a significant impact on perceived ease of use, and interactivity did not have a significant impact on perceived usefulness.","The attitude of restaurant supervisors towards restaurant service robots positively influenced their acceptance of the robots, and this attitude was influenced by perceived usefulness and perceived ease of use.",The robot is described as a service robot in a restaurant setting. The human participant is a restaurant manager or deputy manager who is asked to complete a survey about their perceptions of the robot.,descriptive statistics; Multilevel Model; Structural equation modeling,"The study used descriptive statistics to analyze demographic data and means. Confirmatory factor analysis (CFA) was applied to assess the validity, reliability, and consistency of the research model. Structural equation modeling (SEM) was used to explore the path analysis of the relationships between the variables and the degree of acceptance of restaurant robots.",FALSE,,,,"The study did not manipulate any factors. It was an observational study using a survey to assess the relationships between trust, interactivity, output quality, perceived usefulness, perceived ease of use, attitude, and acceptance of restaurant service robots. The participants were asked to complete a survey about their perceptions of restaurant service robots, but there was no manipulation of any variables by the researchers. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust due to manipulation.",,,"Robotics has entered food services for orders and deliveries. Robots not only attract customer’s attention, but also solve the problems of human resource shortages, training, and turnover rates. For these reasons, restaurant robots may become staple in future culinary experiences. The present study attempts to study the perception of restaurant robots using technology acceptance model (TAM) with trust, interactivity, and quality of output as the variables. The interviewed subjects are general managers or deputy managers in more than 100 restaurants in Taiwan. The results indicate that attitude positively influences acceptance with perceived usefulness and perceived ease of use positively nudges attitude. Perceived ease of use significantly increases perceived usefulness. Trust and the quality of output significantly affect perceived usefulness, as does interaction has a positive influence on perceived ease of use. Finally, we hope this research result will shed some light on the future implementation of robotics in restaurant services."
"Lee, Jae-Gil; Kim, Ki Joon; Lee, Sangwon; Shin, Dong-Hee",Can Autonomous Vehicles Be Safe and Trustworthy? Effects of Appearance and Autonomy of Unmanned Driving Systems,2015,1,89,89,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants observed a remote-controlled car driven by an artificial agent with varying appearance and autonomy levels, and then completed a questionnaire.",Participants observed the car's driving performance and style and rated the car's general performance.,Nao,Humanoid Robots; Mobile Robots,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants observed the robot car driving without direct interaction.,real-world,Participants observed a physical robot car in a lab setting.,physical,The robot was physically present in the experiment.,shared control (fixed rules),"The robot's autonomy was manipulated, with some conditions requiring human input to start/stop.",Questionnaires; Multidimensional Measures,,,Trust was measured using a multidimensional questionnaire.,"parametric models (e.g., regression)",The study used regression analysis to examine the relationships between variables.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The appearance of the driving agent (human-like vs. gadget-like) and its autonomy level (high vs. low) were directly manipulated to influence social presence and trust.,"Human-like appearance and high autonomy increased perceived intelligence, safety, and trust, with social presence mediating these effects.","The study found that appearance had a main effect on affective trust, while autonomy had a main effect on both affective and cognitive trust. There was also a significant interaction effect between appearance and autonomy on affective trust, where high autonomy increased affective trust for the gadget-like agent but not for the human-like agent.","Human-like appearance and high autonomy in an artificial driving agent increase social presence, which in turn positively affects perceived intelligence, safety, and trust.","The robot car drove along a predefined path, and the human participant observed the car and completed a questionnaire about their perceptions of the system.",t-test; Pearson correlation; ANOVA; one-way analyses of variance; Bootstrapping,"The study used independent sample t-tests to validate the anthropomorphic cues. Pearson's r correlation analyses were conducted to examine the relationship between social presence and the measured variables. A 2x2 multivariate analysis of variance (MANOVA) was used to examine the main effects of appearance and autonomy on all measured variables. One-way analyses of variance (ANOVAs) were then conducted to examine the effects of the independent variables on each measured variable. Finally, Preacher and Hayes's bootstrapping procedure was used to examine the mediating effects of social presence on the relationship between anthropomorphism and other measured variables.",TRUE,Robot-aesthetics; Robot-autonomy,Robot-aesthetics; Robot-autonomy,,"The study manipulated the appearance of the driving agent, using a NAO robot for the human-like condition and an iPhone for the gadget-like condition. This is categorized as 'Robot-aesthetics' because it directly changes the visual appeal and form of the robot. The study also manipulated the level of autonomy, where in the high-autonomy condition, the robot started and stopped the car without participant input, while in the low-autonomy condition, participants had to press a button to start and stop the car. This is categorized as 'Robot-autonomy' because it changes the decision-making authority of the robot. Both 'Robot-aesthetics' and 'Robot-autonomy' were found to impact trust. The paper states, 'The results indicated that human-like appearance and high autonomy were more effective in eliciting positive perceptions of the agent.' and 'Participants perceived that the car driven by the driving agent with high autonomy to be inducing greater social presence...and both cognitively...and affectively...trustworthy.' and 'Participants perceived the car driven by the humanlike agent to be inducing greater social presence...and to be more intelligent...safe...and affectively trustworthy.' There were no factors that were manipulated that did not impact trust.",10.1080/10447318.2015.1070547,http://www.tandfonline.com/doi/full/10.1080/10447318.2015.1070547,"Although autonomous vehicles are increasingly becoming a reality, eliminating human intervention from driving may imply significant safety and trust-related concerns. To address this issue from a psychological perspective, this study applies layers of anthropomorphic cues to an artificial driving agent and explicates the process in which these cues promote positive evaluations and perceptions of an unmanned driving system. In a between-subjects factorial experiment (N = 89) consisting of three unmanned driving scenarios, participants interacted with an artificial driving agent with different levels of anthropomorphic cues induced by the variations in appearance (human-like vs. gadget-like) and autonomy (high vs. low) of the agent. The results indicated that human-like appearance and high autonomy were more effective in eliciting positive perceptions of the agent. In addition, a mediation analysis revealed that the greater level of anthropomorphism induced by human-like appearance and high autonomy in the agent evoked the feelings of social presence, which in turn positively affected the perceived intelligence and safety of and trust in the agent, suggesting that the extent to which users perceive the driving agent as intelligent, safe, and trustworthy is largely determined by the feelings of social presence experienced during their interaction."
"Lee, Jieun; Yamani, Yusuke; Itoh, Makoto",Revisiting Trust in Machines: Examining Human–Machine Trust Using a Reprogrammed Pasteurizer Task,2018,1,12,12,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were trained on a pasteurization system, then completed nine experimental sessions with manipulated display and control properties, and filled out questionnaires after each run.","Participants operated a simulated pasteurization plant, adjusting pump and heating systems to maximize pasteurization output.",Unspecified,Other,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulation of a pasteurization plant.,simulation,The interaction was through a computer simulation of a pasteurization plant.,simulated,The robot was a simulated pasteurization plant.,shared control (fixed rules),The system operated automatically but participants could switch to manual control.,Questionnaires,Muir's Trust Questionnaire,,Trust was measured using a questionnaire adapted from Muir & Moray (1996).,"parametric models (e.g., regression)",Stepwise multiple regression was used to model the relationship between trust dimensions.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The accuracy of the pump's response and the display of the pump rate were manipulated to be exact, have a constant error, or have a variable error, influencing user expectations and perceived performance.",The study aims to examine how these manipulations affect the development of trust in the system.,"The study aims to replicate and challenge the findings of Muir and Moray (1996), which found that faith was the best predictor of trust early in the interaction, contrary to their initial hypothesis.","The study hypothesizes that faith governs overall trust early in the interaction with the automated system, then dependability, and finally predictability.","The human participant monitors and controls a simulated pasteurization plant, adjusting pump and heating systems, while the system automatically adjusts the flow rate in automatic mode.",Linear regression,"A stepwise multiple regression was used to examine the relationship between trust dimensions (faith, dependability, and predictability) and overall trust. The variables were entered in the order of faith, then dependability, and then predictability. Interaction terms between faith and dependability, and faith and predictability were also explored.",TRUE,Robot-accuracy; Robot-interface-design,,,"The study manipulated the accuracy of the pump's response (Robot-accuracy) by having it be exact, have a constant error, or have a variable error. This directly impacts the performance of the system and the user's ability to achieve the task goal. The study also manipulated the display of the pump rate (Robot-interface-design) to be accurate, have a constant error, or have a variable error. This affects the information presented to the user about the system's state. The paper does not explicitly state which of these factors impacted trust, but it does state that the study aims to examine how these manipulations affect the development of trust in the system. Therefore, we cannot definitively say which factors impacted or did not impact trust based on the provided text.",10.1177/1541931218621400,http://journals.sagepub.com/doi/10.1177/1541931218621400,"Automated technologies have brought a number of benefits to professional domains, expanding the area in which humans can perform optimally in complex work environments. Human–automation trust has become an important aspect when designing acceptable automated systems considering general users who have no comprehensive knowledge of the systems. Muir and Moray (1996) proposed a model of human–machine trust incorporating predictability, dependability, and faith as predictors of overall trust in machines. Though Muir and Moray (1996) predicted that trust in machines grows from predictability, then dependability, and finally faith, their results suggested the opposite. This study will reexamine their theoretical framework and test which of the three dimensions governs initial trust in automation. Participants will be trained to operate a simulated pasteurization plant, as in Muir and Moray (1996), and they will be asked to maximize system performance in the pasteurizing task. We hypothesized that faith governs overall trust early in the interaction with the automated system, then dependability, and finally predictability as lay automation users become more familiar with the system. We attempt to replicate the results of Muir and Moray (1996) and argue that their model should be revised for trust development for general automation users."
"Lee, Joshua; Fong, Jeffrey; Kok, Bing Cai; Soh, Harold","Getting to Know One Another: Calibrating Intent, Capabilities and Trust for Human-Robot Collaboration",2020,1,28,24,4,4 participants were removed due to a consistency check question failure,Controlled Lab Environment,between-subjects,"Participants were briefed about the task, completed a practice round with a human demonstrator, and then performed 5 consecutive rounds of the task with a real robot. Participants received a bonus for completing the shopping list.","Participants collaborated with a robot to complete a shopping task, where they had to pick up specific items from a table.",Fetch Robot,Mobile Manipulators,Research,Manipulation,Object Passing,direct-contact interaction,Participants physically interacted with the robot to complete the task.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,Participants interacted with a physical robot.,shared control (adaptive),The robot adapted its behavior based on the interaction with the human.,Questionnaires,,,Trust was measured using subjective questionnaires.,POMDP,Trust was modeled using a Partially Observable Markov Decision Process.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's behavior was manipulated using the TICC-MCP algorithm, which aimed to calibrate the robot's and human's beliefs about each other's capabilities, and the task was designed to be challenging due to the limited horizon and imperfect capabilities of both agents. Participants were also told about their own capabilities and the shopping list.",The TICC-MCP algorithm led to higher average task rewards and induced higher trust in the robot.,"The calibration scores for both human and robot capabilities showed an increasing trend, suggesting that the TICC-MCP robot effectively learned about the participants' capabilities and influenced their beliefs about its own capabilities.",Calibrating mutual beliefs of intention and capability resulted in both higher team performance and higher human trust.,"The robot and human collaborated to pick up items from a table to complete a shopping list. The robot moved first, followed by the human. The human could indicate when they wanted to pick up an item but were unable to, and the robot could also communicate its capabilities.",,"The paper mentions that the TICC-MCP algorithm led to significantly higher average task rewards in the final three rounds, and that the mean calibration scores for both ψ and φ show an increasing trend. However, it does not explicitly mention any specific statistical tests used to determine statistical significance. Therefore, the statistical test is marked as N/A.",TRUE,Robot-adaptability; Robot-accuracy; Task-complexity,Robot-adaptability; Robot-accuracy,,"The study manipulated the robot's behavior using the TICC-MCP algorithm, which aimed to calibrate the robot's and human's beliefs about each other's capabilities. This falls under 'Robot-adaptability' because the robot's behavior adapted based on the interaction with the human, learning about their capabilities and adjusting its own actions. The robot's ability to pick up items was also manipulated, with the robot having a higher probability of picking up 'good' yellow cups than the human, and being unable to pick up sweets, which falls under 'Robot-accuracy' as it directly influences task performance. The task was designed to be challenging due to the limited horizon and imperfect capabilities of both agents, which is a manipulation of 'Task-complexity'. The paper explicitly states that the TICC-MCP algorithm led to higher average task rewards and induced higher trust in the robot, indicating that 'Robot-adaptability' and 'Robot-accuracy' impacted trust. There is no mention of any of the manipulated factors not impacting trust.",,http://arxiv.org/abs/2008.00699,"Common experience suggests that agents who know each other well are better able to work together. In this work, we address the problem of calibrating intention and capabilities in human-robot collaboration. In particular, we focus on scenarios where the robot is attempting to assist a human who is unable to directly communicate her intent. Moreover, both agents may have differing capabilities that are unknown to one another. We adopt a decision-theoretic approach and propose the TICC-POMDP for modeling this setting, with an associated online solver. Experiments show our approach leads to better team performance both in simulation and in a real-world study with human subjects."
"Lee, Jieun; Abe, Genya; Sato, Kenji; Itoh, Makoto",Impacts of System Transparency and System Failure on Driver Trust During Partially Automated Driving,2020,1,28,28,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants received generic information about vehicle automation, then either detailed or less detailed information. They completed a practice drive, followed by four trials with trust questionnaires after each trial. A system failure occurred in the third trial.",Participants experienced a partially automated driving scenario in a simulator and were instructed to resume control when prompted by the automation.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a driving simulator and were required to intervene when prompted.,simulation,The study used a driving simulator to create an interactive virtual environment.,simulated,The robot was a simulated automated vehicle within the driving simulator.,shared control (fixed rules),"The automated vehicle operated independently but with fixed rules, requiring human intervention at specific points.",Questionnaires,Muir's Trust Questionnaire,,Trust was measured using a questionnaire with an 11-point Likert scale.,no modeling,"The study did not use computational modeling of trust, relying on statistical analysis of questionnaire data.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The study manipulated system transparency (detailed vs. less information) and the type of system failure (limits vs. malfunction) to influence trust. The detailed information provided prior knowledge of the system's limitations and reasons for intervention.,"System malfunction led to a greater decrease in trust compared to system limits. Detailed information helped maintain trust after system limits, and subsequent flawless automation recovered trust.","The study found that system malfunction led to a greater decrease in trust than system limits, and that detailed information about the system helped maintain trust after a system limit failure. The study also found that subsequent flawless automation could recover trust after a failure.","System malfunction led to a greater decrease in trust compared to system limits, and detailed information about the system helped maintain trust after a system limit failure.","The robot (automated vehicle) controlled the driving task, and the human was required to monitor the system and intervene when prompted. The human resumed control of the vehicle when the automation issued a short notice.",ANOVA; ANOVA,"The study used a 2x2x3 repeated measures ANOVA to analyze the impact of system transparency (detailed vs. less), reason of system failure (system-limits vs. system-malfunction), and measurement point (before intervention, after intervention, after flawless automation) on driver trust. Additionally, 2x2 ANOVAs were used to analyze the differences in trust ratings between the trials, specifically before and after the intervention, and after the intervention and after flawless automation, examining the main effects of system failure and system transparency.",TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-accuracy,Robot-verbal-communication-content,"The study manipulated system transparency by providing either detailed or less detailed information about the reasons for system intervention, which falls under 'Robot-verbal-communication-content' because it changes what information is communicated to the user. The study also manipulated the type of system failure (system-limits vs. system-malfunction), which directly impacts the robot's performance and success rate, thus categorized as 'Robot-accuracy'. The results showed that the type of system failure ('Robot-accuracy') significantly impacted trust, with system malfunctions leading to a greater decrease in trust compared to system limits. However, the level of system transparency ('Robot-verbal-communication-content') did not have a significant impact on trust change from before to after the intervention, although it did help maintain trust after a system limit failure. Therefore, 'Robot-accuracy' is listed as a factor that impacted trust, and 'Robot-verbal-communication-content' is listed as a factor that did not impact trust.",10.1109/ICHMS49158.2020.9209322,https://ieeexplore.ieee.org/document/9209322/,"The objective of this study is to explore changes of trust by a situation where drivers need to intervene. Trust in automation is a key determinant for appropriate interaction between drivers and the system. System transparency and types of system failure influence shaping trust in a supervisory control. Subjective ratings of trust were collected to examine the impact of two factors: system transparency (Detailed vs. Less) and system failure (by Limits vs. Malfunction) in a driving simulator study in which drivers experienced a partially automated vehicle. We examined trust ratings at three points: before and after driver intervention in the automated vehicle, and after subsequent experience of flawless automated driving. Our result found that system transparency did not have significant impacts on trust change from before to after the intervention. System-malfunction led trust reduction compared to those of before the intervention, whilst system-limits did not influence trust. The subsequent experience recovered decreased trust, in addition, when the system-limit occurred to drivers who have detailed information about the system, trust prompted in spite of the intervention. The present finding has implications for automation design to achieve the appropriate level of trust."
"Lee, Jieun; Abe, Genya; Sato, Kenji; Itoh, Makoto",Developing human-machine trust: Impacts of prior instruction and automation failure on driver trust in partially automated vehicles,2021,1,56,56,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were given instructions about a driving automation system, completed a practice drive, and then completed four trials with the system. Trust questionnaires were administered after the instructions, practice drive, and each trial. Participants experienced either a system limitation or malfunction in the third trial.",Participants were asked to use a simulated partial driving automation system and resume control when the system issued a short notice.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a driving simulator and a partial driving automation system.,simulation,Participants interacted with a simulated driving environment.,simulated,The robot was a simulated driving automation system.,shared control (fixed rules),"The driving automation system operated autonomously but with fixed rules, and the driver could disengage the automation.",Questionnaires,Muir's Trust Questionnaire,,"Trust was measured using a questionnaire with items about predictability, dependability, faith, and overall trust.","parametric models (e.g., regression)","Bayesian ANOVAs and regressions were used to analyze the effects of knowledge level, automation failure type, and measurement point on trust.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers manipulated prior information about the automation (detailed vs. less) and the type of automation failure (limitation vs. malfunction) to influence trust.,"Trust increased with experience of the automation, decreased temporarily after automation failure, and was rebuilt by subsequent flawless automation. Dependability was the most dominant belief of drivers' trust throughout the whole experiment.","The study found that dependability was the most dominant belief of drivers' trust throughout the whole experiment, regardless of their knowledge level. The study also found that the type of automation failure did not have a significant effect on trust, which was contrary to the researchers' expectations.","Driver trust in partial driving automation increases with experience, decreases temporarily after automation failure, and is rebuilt by subsequent flawless automation. Dependability is the most dominant factor influencing trust.","The robot, a simulated driving automation system, controlled the vehicle's longitudinal and lateral movements. The human participant monitored the system and resumed control when the system issued a short notice due to a simulated failure.",Bayesian t-test; Bayesian ANOVA; bayesian regression,"The study used Bayesian t-tests to compare trust levels between groups and across different measurement points. Bayesian ANOVAs were used to analyze the main and interaction effects of knowledge level, automation failure type, and measurement point on trust ratings. Bayesian regressions were employed to identify the most predictive attributions of trust (predictability, dependability, and faith) at each measurement point and across different experimental conditions. The overall purpose was to examine how prior knowledge, automation failure, and experience with the system influence driver trust and its underlying attributions.",TRUE,Robot-verbal-communication-content; Robot-accuracy; Task-environment,Robot-accuracy,Robot-verbal-communication-content; Task-environment,"The study manipulated the level of prior information about the automation system, which falls under 'Robot-verbal-communication-content' because it involves changing what information is communicated to the participants about the system's capabilities and limitations (e.g., 'The information in Fig. 2(d) was presented only to the participants in the detailed group.'). The study also manipulated the type of automation failure (limitation vs. malfunction), which directly impacts the 'Robot-accuracy' as it changes the system's performance and reliability (e.g., 'In the malfunction scenario, the system issued the short notice ('Automation Disengaged') with a beep sound when faced with the curved road. In the limitation scenario, fog suddenly appeared, and the notice was issued.'). The study also manipulated the presence of fog during the failure, which is a change in the 'Task-environment' (e.g., 'In the limitation scenario, fog suddenly appeared, and the notice was issued.'). The results showed that the type of automation failure (limitation vs. malfunction) did not significantly impact trust, indicating that 'Robot-accuracy' did not impact trust. The study found that trust increased with experience of the automation, decreased temporarily after automation failure, and was rebuilt by subsequent flawless automation, indicating that 'Robot-accuracy' impacted trust. The study also found that the level of prior information did not impact trust, indicating that 'Robot-verbal-communication-content' did not impact trust. The presence of fog during the failure did not impact trust, indicating that 'Task-environment' did not impact trust.",10.1016/j.trf.2021.06.013,https://linkinghub.elsevier.com/retrieve/pii/S1369847821001534,"To prompt the use of driving automation in an appropriate and safe manner, system designers require knowledge about the dynamics of driver trust. To enhance this knowledge, this study manipulated prior information of a partial driving automation into two types (detailed and less) and investigated the effects of the information on the development of trust with respect to three trust attributions proposed by Muir (1994): predictability, dependability, and faith. Furthermore, a driving simulator generated two types of automation failures (limitation and malfunction), and at six instances during the study, 56 drivers completed questionnaires about their levels of trust in the automation. Statistical analysis found that trust ratings of automation steadily increased with the experience of simulation regardless of the drivers’ levels of knowledge. Automation failure led to a temporary decrease in trust ratings; however, the trust was rebuilt by a subsequent experi­ ence of flawless automation. Results showed that dependability was the most dominant belief of drivers’ trust throughout the whole experiment, regardless of their knowledge level. Interestingly, detailed analysis indicated that trust can be accounted by different attributions depending on the drivers’ circumstances: the subsequent experience of error-free automation after the exposure to automation failure led predictability to be a secondary predictive attribution of drivers’ trust in the detailed group whilst faith was consistently the secondary contributor to shaping trust in the less group throughout the experiment. These findings have implications for system design regarding transparency and for training methods and instruction aimed at improving driving safety in traffic environments with automated vehicles."
"Lee, Kin Man; Krishna, Arjun; Zaidi, Zulfiqar; Paleja, Rohan; Chen, Letian; Hedlund-Botti, Erin; Schrum, Mariah; Gombolay, Matthew","The Effect of Robot Skill Level and Communication in Rapid, Proximate Human-Robot Collaboration",2023,1,46,42,4,"4 participants were excluded due to issues with the cable tension of the robot arm, resulting in visibly degraded robot performance",Controlled Lab Environment,mixed design,"Participants completed a pre-experiment survey, a skill calibration task, and then four trials of a collaborative table tennis task with a robot, varying robot skill and communication. Participants completed questionnaires after each trial and were debriefed at the end.","Participants collaborated with a robot to return table tennis balls launched from a ball launcher, with the goal of maximizing the number of successful returns.",Barrett WAM arm,Industrial Robot Arms; Collaborative Robots (Cobots),Research,Game,Cooperative Game,direct-contact interaction,Participants physically collaborated with the robot in a shared workspace.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,A physical robot was used in the study.,shared control (fixed rules),"The robot operated autonomously using pre-programmed strokes, but the human could also attempt to hit the ball.",Questionnaires,Godspeed Questionnaire; Trust in Automated Systems Scale,Performance Metrics,Trust was measured using questionnaires and performance metrics.,"parametric models (e.g., regression)",Mixed-effects multiple regression models were used to analyze the data.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's skill level and communication were directly manipulated to influence trust and performance. The robot's skill was varied by using different stroke primitives, and communication was enabled or disabled.","Robot skill was positively correlated with trust, while verbal communication did not improve trust and decreased perceived safety.","Verbal communication, intended to improve collaboration, unexpectedly decreased team performance and perceived safety. Participants also rarely used the option to verbally communicate with the robot.","Robot skill significantly increased perceived trust, while verbal communication decreased team performance and perceived safety.","The robot attempted to return table tennis balls using pre-programmed strokes, and the human participant could also attempt to return the balls, with the goal of maximizing the team's score. The robot could also communicate its intention to hit the ball.",Mixed-effects model; ANOVA; Shapiro-Wilk; Levene's test; Friedman test; Nemenyi test,"The study used mixed-effects multiple regression models to analyze the data, with ANOVA applied to these models to assess the significance of the independent variables (skill, communication, and robot-voice) on dependent variables like trust, safety, and performance. Shapiro-Wilk and Levene's tests were used to check for normality and homoscedasticity assumptions. When these assumptions were violated, non-parametric Friedman's tests were used, followed by Nemenyi post-hoc tests for pairwise comparisons.",TRUE,Robot-verbal-communication-content; Robot-accuracy; Robot-verbal-communication-style,Robot-accuracy,Robot-verbal-communication-content,"The study manipulated several factors. 'Robot-accuracy' was manipulated by varying the robot's skill level, using a well-tuned stroke primitive for high skill and a suboptimal one for low skill, directly impacting the success rate of the robot's actions. This is explicitly stated in the paper: 'The high performance model is comprised of a well-tuned stroke primitive... The low performance model replaces the well-tuned primitive with a suboptimal primitive (∼ 50% of the time), which either misses the ball or does not successfully return the ball over the net.' 'Robot-verbal-communication-content' was manipulated by enabling or disabling the robot's vocal communication of intent using short phrases like 'Mine' or 'Got it' (VAIN signals). This is described as 'Drawing inspiration from how volleyball players communicate deliberative intent, we enable the robot to vocally communicate assertive audio signals such as ""Mine"" or ""Got it""'. 'Robot-verbal-communication-style' was manipulated by using a male or female voice for the robot's communication. This is described as 'We use a voice generation tool to create audio fles for the phrases used in the communication factor with both a male and female tone'. The results showed that 'Robot-accuracy' significantly impacted trust, with higher skill leading to higher trust. The paper states: 'We fnd that when the robot performed poorly, the trust in the robot reduced signifcantly'. However, 'Robot-verbal-communication-content' did not impact trust, as stated: 'we fnd that VAIN communication did not impact the level of trust on the robot'. The study did not find that 'Robot-verbal-communication-style' impacted trust.",10.1145/3568162.3577002,https://dl.acm.org/doi/10.1145/3568162.3577002,"As high-speed, agile robots become more commonplace, these robots will have the potential to better aid and collaborate with humans. However, due to the increased agility and functionality of these robots, close collaboration with humans can create safety concerns that alter team dynamics and degrade task performance. In this work, we aim to enable the deployment of safe and trustworthy agile robots that operate in proximity with humans. We do so by 1) Proposing a novel human-robot doubles table tennis scenario to serve as a testbed for studying agile, proximate human-robot collaboration and 2) Conducting a user-study to understand how attributes of the robot (e.g., robot competency or capacity to communicate) impact team dynamics, perceived safety, and perceived trust, and how these latent factors afect human-robot collaboration (HRC) performance. We fnd that robot competency signifcantly increases perceived trust (��� < .001), extending skill-to-trust assessments in prior studies to agile, proximate HRC. Furthermore, interestingly, we fnd that when the robot vocalizes its intention to perform a task, it results in a signifcant decrease in team performance (��� = .037) and perceived safety of the system (��� = .009)."
"Legler, Franziska; Langer, Dorothea; Dittrich, Frank; Bullinger, Angelika C",I don? care what the robot does! Trust in automation when working with a heavy-load robot,2020,1,25,25,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were informed about the procedure, signed consent, and completed a pre-survey. They watched videos about HRC, learned the assembly task and gesture control, and completed seven test blocks with post-scenario surveys.","Participants assembled hook-and-pile tapes on a front axle carrier, with the robot simulating placement of the component.",KUKA Quantec prime KR 180,Industrial Robot Arms,Industrial; Research,Manipulation,Object Assembly,minimal interaction,"Participants interacted with the robot through gesture control and observed its movements, but did not have physical contact.",real-world,The experiment was conducted in a pseudo real-world test environment with a physical robot.,physical,A physical industrial robot was used in the experiment.,shared control (fixed rules),"The robot's movements were pre-programmed, but it reacted to the participant's gesture control for height adjustment.",Questionnaires,Jian et al. Trust Scale,,Trust was measured using a translated version of the Jian scale.,no modeling,No computational model of trust was used; statistical analysis was performed on the collected data.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the level of human-robot interaction (HRC-level), robot trajectory, and the timing of a simulated system failure to observe their effects on trust and emotional experience.","Trust was lower in HRC-level 3 (direct interaction) compared to HRC-level 1 (no direct interaction). Robot trajectory did not significantly affect trust. The timing of the first failure had a small effect on trust, with early failures slightly reducing trust and late failures causing a stronger decrease.","The study found that state-anxiety decreased and trust increased over time of interaction, which is consistent with overtrust effects in automation. The effect of HRC-level on trust was significant, but the practical implication of the absolute differences in means is questionable. The study also noted that technical problems with gesture control may have influenced the results.","The study found that direct interaction with a heavy-load robot (HRC-level 3) resulted in lower trust compared to no direct interaction (HRC-level 1), although this effect may have been confounded by technical issues.","The robot moved a front axle carrier, simulating placement on a table or presenting it for assembly. The human assembled hook-and-pile tapes on the carrier, using gesture control to adjust the robot's height in one condition.",Wilcoxon signed-rank test; Friedman test; ANOVA,"The study used non-parametric statistical tests due to small sample sizes and non-symmetric data distribution. The Wilcoxon Signed-Rank Test was used for paired comparisons of conditions. Friedman's Rank Sum Test was used to assess the effect of multiple experimental conditions on state-anxiety, trust, and mistrust. A general linear model was used to test for interaction effects between HRC-level and robot trajectory on state-anxiety.",TRUE,Robot-autonomy; Robot-task-strategy; Task-constraints,Robot-autonomy,Robot-task-strategy,"The study manipulated the level of human-robot interaction (HRC-level), which directly relates to the level of robot autonomy. HRC-level 1 involved no direct interaction, while HRC-level 3 allowed for gesture-based height adjustment of the robot, thus changing the level of shared control and therefore 'Robot-autonomy'. The robot trajectory was also manipulated, with a 'from side' (legible) and 'from above' (predictable) trajectory, which is a change in the robot's task completion strategy, thus 'Robot-task-strategy'. The timing of a simulated system failure was also manipulated, with 'early' and 'late' failures, which can be considered a change in 'Task-constraints' as it introduces an unexpected event that impacts the task flow. The study found that the HRC-level (Robot-autonomy) significantly impacted trust, with lower trust in HRC-level 3 compared to HRC-level 1. The robot trajectory (Robot-task-strategy) did not significantly affect trust. The timing of the first failure (Task-constraints) had a small effect on trust, with early failures slightly reducing trust and late failures causing a stronger decrease.",,,"There are many reasons for the implementation of human-robot collaboration (HRC). HRC enables flexibility of increasingly complex production sites. In contrast to this, the economic aim of process efficiency is threatened by workers’ fear and mistrust in collaborative robots. Fenceless heavy-load collaborative robots have associated risks and so under- or overtrust in automation may result in injuries. An experiment with 25 participants and a heavy-load industrial robot was conducted in a pseudo real-world test environment. Interaction level and robot trajectory were used as within-subject independent variables. Additionally, temporal position of first-failure was varied between participants. Emotional experience and trust were dependent variables. Interaction level, robot trajectory and position of the first-failure did not reveal practical relevant effects on fear or trust. While participants showed short-term responses to first-failure events, following scenarios were not influenced by firstfailure regarding emotional experience or trust. Overall, negative emotions were poorly detected and trust in automation was high. These results are in line with findings in the literature regarding overtrust in automation."
"Leichtmann, Benedikt; Meneweger, Thomas; Busch, Christine; Reiterer, Bernhard; Meyer, Kathrin; Rammer, Daniel; Haring, Roland; Mara, Martina","Teaming with a Robot in Mixed Reality: Dynamics of Trust, Self-Efficacy, and Mental Models Affected by Information Richness",2024,1,68,61,7,"simulation sickness, technical errors, inattentional questionnaire responding",Controlled Lab Environment,mixed design,"Participants completed a demographic questionnaire, then a baseline trust questionnaire (T1). They watched one of two video tutorials (high or low information richness), then completed a second questionnaire (T2). They played three mini-games with a robot, and completed a third questionnaire (T3). Finally, they completed a final questionnaire (T4) and some participants were interviewed.","Participants played three cooperative mini-games with a mobile robot in a mixed-reality environment, including collecting algae, analyzing algae, and refueling.",CHIMERA,Mobile Manipulators; Collaborative Robots,Research; Industrial,Game,Cooperative Game,direct-contact interaction,Participants physically interacted with a robot in a mixed-reality environment.,real-world,Participants interacted with a robot in an immersive mixed-reality environment.,physical,"Participants interacted with a real, physical robot.",shared control (fixed rules),The robot operated independently but followed fixed rules and responded to human input.,Questionnaires; Custom Scales,Trust in Automation Scale (TAS),,Trust was measured using a two-item subscale of the Trust in Automation Questionnaire and a custom one-item scale.,no modeling,"Trust was not modeled computationally, and only descriptive statistics were used.",Empirical HRI Studies,Mixed Reality Studies,Direct Manipulation,"The study manipulated information richness in tutorials, providing either basic or detailed information about robot communication, which was intended to influence user expectations and mental models.","The tutorials increased trust and self-efficacy, but there was no significant difference between the two tutorial groups. The more detailed tutorial was rated as more helpful.","The study found no significant difference in trust between the two tutorial groups, despite the more detailed tutorial being rated as more helpful. There was an initial imbalance in self-efficacy between the groups, which converged over time.","Tutorials increased trust and self-efficacy, but the level of information richness did not significantly impact these variables. Participants used trial-and-error strategies to interact with the robot, indicating that the tutorials did not fully shape their mental models.","The robot moved autonomously to collect virtual objects, and the human participant assisted by classifying objects and guiding the robot. The human also had to communicate with the robot using gestures.",brunner-munzel tests; Bayesian t-test; mixed anovas,"The study used Brunner-Munzel tests for group comparisons, adjusting p-values with the Bonferroni-Holm step-down procedure. Bayesian t-tests were also conducted to assess group differences, using a Cauchy prior distribution. Mixed ANOVAs were employed to examine changes in variables over time, with tutorial group as a between-subject factor and time as a within-subject factor. Post-hoc analyses were conducted to further explore significant effects.",TRUE,Robot-verbal-communication-content,,Robot-verbal-communication-content,"The study manipulated the information richness of the tutorial, which is directly related to the content of the verbal communication provided to the participants about the robot. One group received basic information, while the other received additional information about robot signals and human gestures for communication. This manipulation directly alters the content of the information being communicated to the participants, thus fitting the 'Robot-verbal-communication-content' category. The study found that while the more detailed tutorial was rated as more helpful, there was no significant difference in trust between the two tutorial groups. Therefore, the manipulated factor 'Robot-verbal-communication-content' did not impact trust levels. The study did find that the tutorials in general increased trust and self-efficacy, but this was not specific to the manipulation of information richness.",10.1080/10447318.2024.2331878,https://www.tandfonline.com/doi/full/10.1080/10447318.2024.2331878,"This study employs an innovative mixed-reality game to investigate trust, self-efficacy, and collabor­ ation willingness with a mobile robot in relation to information and mental models. Previous research has demonstrated the significant impact of introductory information on technology use and trust, which further evolves over the course of interactions. Expanding on this, we explore new facets of information content through comprehensive measurements in two empirical studies: In study 1, 68 participants watched one of two video tutorials (high or low information richness) before collaborat­ ing with a physical robot in a shared virtual 3D environment. Repeated measures served to investi­ gate temporal dynamics. In study 2, 37 participants additionally engaged in qualitative interviews to better understand how their mental models changed with increasing robot experience. Both tutorials were found to influence user trust and self-efficacy over time, with no significant differences between the two variants. Users valued information on how to communicate with the robot, but still relied on trial-and-error approaches. We argue for a moderating effect of information content and richness and highlight the need for carefully designed tutorials to enhance human-robot interaction."
"Li, Dingjun; Rau, P. L. Patrick; Li, Ye",A Cross-cultural Study: Effect of Robot Appearance and Task,2010,1,108,108,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants interacted with a robot in four scenarios (teaching, tour guide, entertainment, security guard) with different robot appearances (anthropomorphic, zoomorphic, machinelike) and cultural backgrounds (Chinese, Korean, German). A Wizard of Oz approach was used, with an experimenter controlling the robot behind a screen. Participants completed questionnaires after each scenario.","Participants were asked to answer questions, speak certain words, or perform certain actions according to the robot's instructions in four different scenarios: teaching, tour guide, entertainment, and security guard.",Lego Mindstorm NXT,Mobile Robots,Social; Educational,Social,Tutoring,minimal interaction,Participants interacted with the robot through verbal instructions and some physical actions.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical robot with different appearances.,wizard of oz (directly controlled),The robot was controlled by an experimenter behind a screen.,Questionnaires,SHAPE Automation Trust Index (SATI),,Trust was measured using the SHAPE Automation Trust Index (SATI) questionnaire.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers manipulated the robot's appearance (anthropomorphic, zoomorphic, machinelike) and the task (teaching, tour guide, entertainment, security guard) to influence trust and other dependent variables. The robot's behavior was also manipulated through different motions for each appearance.","Cultural background significantly influenced trust, with German participants showing lower trust than Chinese and Korean participants. Robot appearance had a significant effect on likeability, with the zoomorphic robot being more likeable than the machinelike robot. Task sociability influenced active response and engagement, with higher sociability tasks leading to higher active response and engagement. There was no significant interaction between robot appearance and task on trust.","The study found that cultural background had a significant impact on trust, with German participants showing lower trust than Chinese and Korean participants. The interaction between culture and task showed that participants from low-context cultures may have significantly decreased engagement when the sociability of a task is lowered. The study also found that the robot's task has a closer correlation with the participants' active response and engagement than the robot's appearance does, while the appearance has a closer correlation with the likeability of the robot than the task does.","Cultural differences significantly impact trust and engagement in human-robot interaction, with German participants showing lower trust and engagement compared to Chinese and Korean participants.","The robot provided instructions and asked questions related to teaching, tour guiding, entertainment, and security guard tasks. Participants responded to the robot's instructions by answering questions, speaking words, or performing actions.",repeated anova; Kruskal-Wallis; one-sample kolmogorov-smirnov test; Levene's test; t-test; partial correlation analysis,"The study used several statistical tests to analyze the data. Repeated Measures ANOVA was used to analyze the effects of culture, robot appearance, and task on likeability, engagement, trust, and satisfaction. The Kruskal-Wallis H test, a non-parametric test, was used to analyze the active response data, which did not meet the normality assumption. The One-Sample Kolmogorov-Smirnov Test was used to test the normality of the dependent variables. Levene's Test was used to check the equality of error variances. Paired t-tests were used for post-hoc analysis to compare the means of different tasks. Partial correlation analysis was used to examine the relationship between appearance/task and the dependent variables, controlling for gender.",TRUE,Robot-aesthetics; Task-complexity,,Robot-aesthetics; Task-complexity,"The study manipulated the robot's appearance (anthropomorphic, zoomorphic, machinelike), which is categorized as 'Robot-aesthetics' because it involves changes to the visual appeal and design of the robot. The study also manipulated the task (teaching, tour guide, entertainment, security guard), which is categorized as 'Task-complexity' because the different tasks have varying levels of cognitive demands and interaction requirements. The paper states that there was no significant interaction between robot appearance and task on trust, meaning that neither of these factors directly impacted trust. However, the study did find that cultural background significantly influenced trust, but this is not a manipulated factor in the study design. The study also found that robot appearance had a significant effect on likeability, and task sociability influenced active response and engagement, but these are not direct measures of trust. Therefore, neither 'Robot-aesthetics' nor 'Task-complexity' directly impacted trust in this study.",10.1007/s12369-010-0056-9,http://link.springer.com/10.1007/s12369-010-0056-9,"This study investigates the effects of culture, robot appearance and task on human-robot interaction. We propose a model with culture (Chinese, Korean and German), robot appearance (anthropomorphic, zoomorphic and machinelike) and task (teaching, guide, entertainment and security guard) as factors, and analyze these factors’ effects on the robot’s likeability, and people’s active response to, engagement with, trust in and satisfaction with the robot. We conducted a laboratory experiment with 108 participants to test the model and performed Repeated ANOVA and Kruskal Wallis Test on the data. The results show that cultural differences exist in participants’ perception of likeability, engagement, trust and satisfaction; a robot’s appearance affects its likeability, while the task affects participants’ active response and engagement. We found the participants expected the robot appearance to match its task only in the interview but not in the subjective ratings. Interaction between culture and task indicates that participants from low-context cultures may have signiﬁcantly decreased engagement when the sociability of a task is lowered. We found strong and positive correlations between interaction performance (active response and engagement) and preference (likeability, trust and satisfaction) in the human-robot interaction."
"Li, Mingming; Guo, Fu; Li, Zhixing; Ma, Haiyang; Duffy, Vincent G.","Interactive effects of users’ openness and robot reliability on trust: evidence from psychological intentions, task performance, visual behaviours, and cerebral activations",2024,1,87,40,47,47 students who have openness scores less than one standard deviation above or below the average were excluded,Controlled Lab Environment,between-subjects,"Participants completed a personality questionnaire, then performed a voice-based walking task with a robot, while eye-tracking and fNIRS data were collected. Subjective trust was assessed after the task.",Participants used voice commands to instruct a robot to walk to a destination without stepping on lines.,Alpha,Humanoid Robots,Research,Navigation,Path Following,minimal interaction,Participants gave verbal instructions to the robot.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical robot.,wizard of oz (directly controlled),The robot's movements were directly controlled by an experimenter based on the participant's voice commands.,Behavioral Measures; Physiological Measures; Questionnaires,Jian et al. Trust Scale,Eye-tracking Data; Performance Metrics; Physiological Signals,"Trust was measured using questionnaires, task performance, eye-tracking, and fNIRS.",no modeling,Trust was not modeled computationally; statistical analysis was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"Robot reliability was manipulated by introducing errors in the robot's walking steps, and participants were informed of the approximate reliability before the experiment.","Higher robot reliability led to increased subjective trust, particularly for users with high openness. Users with low openness showed similar trust levels regardless of reliability. The number of failures was higher in the low reliability condition.","Users with low openness showed similar trust levels in both high and low reliability conditions, which was unexpected. Also, users with low openness showed more visual attention and neural processing in the highly reliable condition, which was also unexpected.",Users with low openness were more cautious about the highly reliable robot and allocated more visual attention and neural processing to monitor and infer robot status than users with high openness.,The robot walked forward based on voice commands from the participant. The participant observed the robot and gave voice commands to guide the robot to a destination without stepping on lines.,ANOVA,"Two-way analyses of variance (ANOVA) were used to test the effects of users' openness (high vs. low) and robot reliability (high vs. low) on various dependent variables, including subjective trust, task performance metrics (completion time and number of failures), eye-tracking metrics (fixation count, revisit count, scan speed, pupil diameter), and fNIRS indicators (HbO beta values in left mPFC, right mPFC, and rtPJ). The purpose was to determine the main effects of each independent variable and their interaction effects on the dependent variables. Simple effects were also examined to further explore significant interaction effects.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated robot reliability by introducing errors in the robot's walking steps. The paper states, 'accuracy ratios of 95% and 75% were set up to simulate robot systems with high or low reliabilities, respectively.' This directly affects the robot's success rate in completing the task, which is why 'Robot-accuracy' is the appropriate category. The results showed that the manipulated robot reliability impacted subjective trust, task performance (number of failures), visual behaviors, and cerebral activations, thus 'Robot-accuracy' is also listed as a factor that impacted trust. There were no other factors manipulated, so there are no factors that did not impact trust.",10.1080/00140139.2024.2343954,https://www.tandfonline.com/doi/full/10.1080/00140139.2024.2343954,"Although trust plays a vital role in human-robot interaction, there is currently a dearth of literature examining the effect of users’ openness personality on trust in actual interaction. This study aims to investigate the interaction effects of users’ openness and robot reliability on trust. We designed a voice-based walking task and collected subjective trust ratings, task metrics, eye-tracking data, and fNIRS signals from users with different openness to unravel the psychological intentions, task performance, visual behaviours, and cerebral activations underlying trust. The results showed significant interaction effects. Users with low openness exhibited lower subjective trust, more fixations, and higher activation of rTPJ in the highly reliable condition than those with high openness. The results suggested that users with low openness might be more cautious and suspicious about the highly reliable robot and allocate more visual attention and neural processing to monitor and infer robot status than users with high openness."
"Li, Yinglin; Cui, Rongxin; Yan, Weisheng; Zhang, Shi; Yang, Chenguang",Reconciling Conflicting Intents: Bidirectional Trust-Based Variable Autonomy for Mobile Robots,2024,1,20,20,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants received a tutorial, practiced with the robot, and then completed one trial under each of three control schemes (BTVA, HISC, BTSC) in a counterbalanced order. They completed questionnaires after each trial.","Participants remotely controlled a mobile robot to navigate a maze-like environment and observe markers, identifying their orientation before releasing control.",Unspecified,Mobile Robots,Research,Navigation,Remote Navigation,minimal interaction,Participants interacted with the robot through a remote control interface.,real-world,Participants viewed the robot's camera feed on a screen.,physical,Participants interacted with a physical robot in a lab setting.,shared control (adaptive),The robot adapted its behavior based on the bidirectional trust model and human input.,Questionnaires; Real-time Trust Measures,NASA Task Load Index (NASA-TLX); System Usability Scale (SUS),"Performance Metrics; robot data (sensor data, etc.)",Trust was measured using questionnaires and real-time data from the robot's sensors.,"parametric models (e.g., regression)",A Kalman filter was used to model bidirectional trust based on sensor data and human input.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the control scheme (BTVA, HISC, BTSC), which affected the robot's autonomy, performance, and the alignment of goals between the human and robot.","The BTVA method resulted in higher system usability and lower operator workload compared to the other methods, suggesting a positive impact on trust.","The study found that the bidirectional trust model aligned with human intuition, and the BTVA method improved usability and reduced workload. There were no significant differences in safety between the three control conditions.","The bidirectional trust-based variable autonomy (BTVA) method effectively modulates the degree of automation, leading to improved system usability and reduced operator workload compared to other control methods.","The robot autonomously navigates a predefined path, but the human can take control to observe markers. The human identifies the orientation of the markers and then releases control, allowing the robot to continue autonomously.",ANOVA,"A one-way repeated measures ANOVA with Greenhouse-Geisser correction was used to compare the three control schemes (BTVA, HISC, BTSC) on the measures of operational safety, operational smoothness, and time cost. Bonferroni correction was used for pairwise comparisons. The purpose was to determine if there were statistically significant differences between the control schemes on these measures.",TRUE,Robot-autonomy,Robot-autonomy,,"The study manipulated the control scheme (BTVA, HISC, BTSC), which directly affected the robot's autonomy. BTVA used a bidirectional trust model to modulate the degree of automation, HISC used a two-stage method with human intervention and shared control, and BTSC used a bidirectional trust-based shared control method. The paper states, 'Real-time modulation of the degree of automation is achieved through variable weight receding horizon optimization.' and 'The relative magnitudes of human and robot trust indicate the contributions of autonomous and manual control inputs to the optimal output, facilitating control switchovers during goal transfer.' This clearly indicates that the level of autonomy was manipulated. The results showed that the BTVA method resulted in higher system usability and lower operator workload compared to the other methods, suggesting a positive impact on trust. The paper states, 'Compared with BTSC, BTVA reduced time cost by 5.8%, increased SUS by 16.75%, and reduced TLX by 25.7%. Compared with HISC, BTVA increased operational smoothness by 1.5%, improved SUS by 9.9%, and reduced TLX by 14.3%.' These results indicate that the manipulation of robot autonomy impacted trust.",10.1109/LRA.2024.3396100,https://ieeexplore.ieee.org/document/10517369/,"In the realm of semi-autonomous mobile robots designed for remote operation with humans, current variable autonomy approaches struggle to reconcile conﬂicting intents while ensuring compliance, autonomy, and safety. To address this challenge, we propose a bidirectional trust-based variable autonomy (BTVA) control approach. By incorporating diverse trust factors and leveraging Kalman ﬁltering techniques, we establish a core abstraction layer to construct the state-space model of bidirectional computational trust. This bidirectional trust is integrated into the variable autonomy control loop. Real-time modulation of the degree of automation is achieved through variable weight receding horizon optimization. Through a within-group experimental study with twenty participants in a semi-autonomous navigation task, we validate the effectiveness of our method in goal transfer and assisted teleoperation. Statistical analysis reveals that our method achieves a balance between rapid response and trajectory smoothness. Compared with binary control switching, this method reduces operator workload by 14.3% and enhances system usability by 9.9%."
"Li, Yang; Xu, Jiaxin; Guo, Di; Liu, Huaping",Trust-Aware Human–Robot Fusion Decision-Making for Emergency Indoor Patrolling,2024,1,21,21,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a VR-based indoor patrolling task where they were informed that an anomaly would occur in a random area within the first 90 seconds. The robot provided decision support, and the participants' responses were recorded. The robot's lighting capabilities were reduced at irregular intervals during the task. Data was collected on human-robot interactions, including trust levels, robot actions, and human responses to decision support.","Participants were tasked with patrolling a virtual indoor environment to detect anomalies, with the robot providing decision support.",Unspecified,Mobile Robots,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the robot through a VR interface, receiving decision support and monitoring information.",simulation,"The interaction took place in a virtual reality environment, providing an immersive experience.",simulated,The robot was represented virtually within the VR environment.,shared control (adaptive),The robot provided decision support and adapted its behavior based on human responses and trust levels.,Questionnaires; Behavioral Measures,,Performance Metrics; robot data,Trust was assessed using a binary questionnaire and behavioral measures based on human responses to robot decision support.,"parametric models (e.g., regression)",A sparse Gaussian process model was used to model human-robot trust based on interaction history and performance.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's lighting capabilities were manipulated to simulate a power outage, affecting its performance and the task difficulty. The robot also provided decision support, which served as feedback to the human.",The study found that trust levels were influenced by the robot's performance and the interaction history. The robot prioritized enhancing human-robot trust in emergency scenarios to mitigate the long-term costs of human-robot collaboration.,"The study found that humans do not always make optimal decisions, even when they trust the robot, due to computational limitations. The trust-based human monitoring model successfully emulated human irrationality in continuous monitoring tasks.",The MCTS-based human-robot collaborative decision-making method significantly boosts the efficiency of the human-robot team in emergency monitoring tasks by maintaining trust and providing decision support.,"The robot's task was to patrol a virtual indoor environment, detect anomalies, and provide decision support to the human. The human's task was to monitor the environment, respond to the robot's decision support, and detect anomalies.",gaussian process (gp); sparse gaussian process (sparse gp); Hamiltonian Monte Carlo,"The study used Gaussian process (GP) and sparse Gaussian process (sparse GP) models to model human-robot trust based on interaction history and performance. The parameters of these models were optimized using a full Bayesian inference process employing Hamiltonian Monte Carlo (HMC) sampling. These methods were used to capture the relationship between trust-related factors and trust levels, and to estimate the dynamic changes in trust based on human-robot interactions.",TRUE,Robot-interface-design; Robot-autonomy; Task-constraints,Robot-interface-design; Robot-autonomy,Task-constraints,"The study manipulated the robot's lighting capabilities, which is a change to the interface design, to simulate a power outage, directly impacting the robot's ability to perform its task and the information available to the human. This is described in the abstract as 'indoor human-robot collaborative patrolling in the event of sudden power outages' and in the experimental procedure as 'The robot's lighting capabilities were reduced at irregular intervals during the task.' This manipulation directly affected the robot's performance and the task difficulty, which is a change to the task constraints. The robot also provided decision support, which is a form of autonomy, and the level of trust was influenced by the robot's performance and the interaction history. The paper states 'The robot employs a R to facilitate the patrolling process, utilizes a S to assist the human in making patrolling decisions, and deploys com R to enhance the trust between the human and the robot.' and 'The extent of this autonomy relies on the degree of trust within the human-robot relationship'. The study found that trust levels were influenced by the robot's performance and the interaction history, indicating that the changes to the interface design and robot autonomy impacted trust. The task constraints, while manipulated, did not directly impact trust, but rather influenced the context in which trust was evaluated.",10.1109/TASE.2024.3350639,https://ieeexplore.ieee.org/document/10397549/,"Trust plays a crucial role in decision-making during human-robot collaboration, particularly in emergency scenarios where it becomes more susceptible due to dynamic factors. Misalignment of human-robot trust significantly hampers the efficiency of the collaboration. Therefore, it is imperative to establish effective human-robot interaction and decision support mechanisms that mitigate biases in human confidence levels regarding the robot’s capabilities in dynamic environments. Additionally, online correction of human-robot trust based on human behavioral feedback is vital. This paper focuses on a specific type of emergency task scenario, specifically, indoor human-robot collaborative patrolling in the event of sudden power outages. We propose a trust model based on linear Gaussian and sparse Gaussian processes (sparse GP). We also employ Monte Carlo Tree Search (MCTS) method to determine the robot’s optimal fusion decision-making. Through VR-based human-robot collaborative experiments, we ascertain that the robot prioritizes enhancing human-robot trust in emergency scenarios to mitigate the long-term costs of human-robot collaboration."
"Li, Yingke; Zhang, Fumin",Trust-Preserved Human-Robot Shared Autonomy enabled by Bayesian Relational Event Modeling,2024,1,16,16,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants first completed a practice phase with fixed autonomy levels, then completed tasks with two conditions (Baseline SA and Trust-preserved SA) with counterbalanced order, and finally completed a questionnaire.",Participants played a simulated human-robot collaborative search and rescue game.,Unspecified,Mobile Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the robots through a simulation interface.,simulation,The interaction was through a simulation interface.,simulated,The robots were represented as circles in the simulation.,shared control (adaptive),The robot's autonomy level adapted based on the inferred human trust.,Questionnaires; Behavioral Measures,,Performance Metrics; robot data,Trust was measured using a Likert scale questionnaire and behavioral data such as the number of commands sent.,"parametric models (e.g., regression)",A Bayesian Relational Event Model was used to infer trust based on interaction data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's autonomy level was manipulated to either passively match the estimated trust level or actively maintain and repair trust, which influenced the robot's performance and feedback to the user.","The trust-preserved shared autonomy approach led to higher task success rates and reduced human workload, suggesting a positive impact on trust.","Participants initially had low trust in robots, and the trust-preserved strategy outperformed the baseline by actively deploying trust repair approaches. There was no significant difference in task duration between the two conditions, possibly due to the time limit for task completion.",The trust-preserved shared autonomy strategy improved human-robot team performance and user acceptability by actively maintaining and repairing human trust.,"The robot autonomously performs search and rescue tasks, while the human monitors the robot's actions and sends commands. The human can inquire about the robot's status or instruct it to go to a specific building.",t-test,The study used t-tests to compare the means of objective measures (success rate and number of commands sent) between the two conditions (Baseline SA and Trust-preserved SA). The t-tests were also used to assess the subjective measures of user experience across five dimensions of collaboration fluency.,TRUE,Robot-autonomy,Robot-autonomy,,"The study explicitly manipulated the robot's autonomy level. In the baseline condition, the robot's autonomy was fixed based on initial trust reports, while in the trust-preserved condition, the robot's autonomy was dynamically adjusted based on inferred trust and trust repair strategies. This manipulation of autonomy is described in the paper: 'In the baseline condition (Baseline SA), each robot follows a fixed autonomy level according to the reported trust at the first stage. On the other hand, in the trust-preserved condition (Trust-preserved SA), the robot's autonomy level is dynamically adjusted based on the inferred trust level and the trust repair strategies.' The results showed that the trust-preserved approach, which manipulated autonomy, led to higher task success rates and reduced human workload, indicating that the manipulation of robot autonomy impacted trust. The paper states: 'the proposed Trustpreserved SA approach significantly increased the success rate compared to the Baseline SA approach... In addition, it can also reduce the workload of humans by effectively decreasing the number of commands sent from the human teammate... From Fig. 9, it is obvious to interpret significant improvements in the users' experience over all five dimensions of collaboration fluency.' There were no other factors manipulated that were found to impact trust.",10.1109/LRA.2024.3438040,https://ieeexplore.ieee.org/document/10621608/,"Shared autonomy functions as a flexible framework that empowers robots to operate across a spectrum of autonomy levels, allowing for efficient task execution with minimal human oversight. However, humans might be intimidated by the autonomous decision-making capabilities of robots due to perceived risks and a lack of trust. This paper proposed a trust-preserved shared autonomy strategy that allows robots to seamlessly adjust their autonomy level, striving to optimize team performance and enhance their acceptance among human collaborators. By enhancing the relational event modeling framework with Bayesian learning techniques, this paper enables dynamic inference of human trust based solely on time-stamped relational events communicated within human-robot teams. Adopting a longitudinal perspective on trust development and calibration in human-robot teams, the proposed trust-preserved shared autonomy strategy warrants robots to actively establish, maintain, and repair human trust, rather than merely passively adapting to it. We validate the effectiveness of the proposed approach through a user study on a human-robot collaborative search and rescue scenario. The objective and subjective evaluations demonstrate its merits on both task execution and user acceptability over the baseline approach that does not consider the preservation of trust."
"Liang, Yuhua (Jake); Lee, Seungcheol Austin",Advancing the Strategic Messages Affecting Robot Trust Effect: The Dynamic of User- and Robot-Generated Content on Human–Robot Trust and Interaction Outcomes,2016,2,52,52,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants read game instructions and an online blog about the robot (UGC or control). They then played a game with the robot, followed by interaction outcome measures and debriefing.","Participants played a collaborative game with a robot, deciding whether they or the robot would roll a dice to score points.",Unspecified,Expressive Robots,Social; Research,Game,Cooperative Game,minimal interaction,Participants interacted with the robot in a game setting with verbal instructions.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical robot.,wizard of oz (directly controlled),The robot was controlled by a human operator.,Behavioral Measures; Questionnaires,,,Trust was measured by the number of turns allocated to the robot and questionnaires.,"parametric models (e.g., regression)",The study used regression analysis to examine the relationship between trust and interaction outcomes.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"Participants were exposed to either user-generated content (UGC) or descriptive information about the robot, which was intended to influence their expectations and framing of the task.","UGC did not directly affect trust, but it moderated the relationship between trust and interaction outcomes.","The study found that UGC moderated the relationship between trust and interaction outcomes, shifting correlations in a positive direction. The study also found that the conditions failed to elicit variations in HRT.",User-generated content (UGC) positively moderated the relationship between human-robot trust (HRT) and interaction outcomes.,The robot greeted the participant and played a collaborative game. The human decided whether they or the robot would roll a dice to score points.,t-test; correlational analysis; Linear regression,"The study used an independent-samples t-test to check the effectiveness of the experimental manipulation by comparing the user-generated content (UGC) condition with the control condition. A correlational analysis was used to examine the relationship between human-robot trust (HRT) and interaction outcome variables. Finally, regression analysis was used to test the moderation hypothesis, examining how UGC affects the relationship between HRT and interaction outcomes.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the content of the information participants received about the robot before the interaction. Participants were exposed to either user-generated content (UGC) or descriptive information (control). This manipulation directly altered the pre-interaction messages about the robot, which falls under 'Robot-verbal-communication-content' because it is the content of the message that is being manipulated. The study found that the UGC condition moderated the relationship between HRT and interaction outcomes, indicating that the manipulated content impacted how trust influenced the interaction outcomes. The study did not find that the manipulation directly impacted trust levels, but it did impact the relationship between trust and outcomes. Therefore, the factor is listed as impacting trust.",10.1089/cyber.2016.0199,http://www.liebertpub.com/doi/10.1089/cyber.2016.0199,"Human–robot interaction (HRI) will soon transform and shift the communication landscape such that people exchange messages with robots. However, successful HRI requires people to trust robots, and, in turn, the trust affects the interaction. Although prior research has examined the determinants of human–robot trust (HRT) during HRI, no research has examined the messages that people received before interacting with robots and their effect on HRT. We conceptualize these messages as SMART (Strategic Messages Affecting Robot Trust). Moreover, we posit that SMART can ultimately affect actual HRI outcomes (i.e., robot evaluations, robot credibility, participant mood) by affording the persuasive inﬂuences from user-generated content (UGC) on participatory Web sites. In Study 1, participants were assigned to one of two conditions (UGC/control) in an original experiment of HRT. Compared with the control (descriptive information only), results showed that UGC moderated the correlation between HRT and interaction outcomes in a positive direction (average Dr = +0.39) for robots as media and robots as tools. In Study 2, we explored the effect of robot-generated content but did not ﬁnd similar moderation effects. These ﬁndings point to an important empirical potential to employ SMART in future robot deployment."
"Liang, Yuhua (Jake); Lee, Seungcheol Austin",Advancing the Strategic Messages Affecting Robot Trust Effect: The Dynamic of User- and Robot-Generated Content on Human–Robot Trust and Interaction Outcomes,2016,2,16,16,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants read game instructions and an online blog about the robot (robot-generated content). They then played a game with the robot, followed by interaction outcome measures and debriefing.","Participants played a collaborative game with a robot, deciding whether they or the robot would roll a dice to score points.",Unspecified,Expressive Robots,Social; Research,Game,Cooperative Game,minimal interaction,Participants interacted with the robot in a game setting with verbal instructions.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical robot.,wizard of oz (directly controlled),The robot was controlled by a human operator.,Behavioral Measures; Questionnaires,,,Trust was measured by the number of turns allocated to the robot and questionnaires.,"parametric models (e.g., regression)",The study used regression analysis to examine the relationship between trust and interaction outcomes.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"Participants were exposed to robot-generated content, which was intended to influence their expectations and framing of the task.",Robot-generated content did not significantly affect trust or the relationship between trust and interaction outcomes.,"Robot-generated content did not show the same moderation effect as UGC, and some interaction outcomes shifted negatively. The study also found that the robot-generated content failed to elicit differences in HRT.",Robot-generated content did not significantly moderate the relationship between human-robot trust (HRT) and interaction outcomes.,The robot greeted the participant and played a collaborative game. The human decided whether they or the robot would roll a dice to score points.,t-test; correlational analysis; Linear regression,The study used an independent-samples t-test to compare the robot-generated content condition with the control condition from Study 1 in terms of HRT. A correlational analysis was used to examine the relationship between HRT and interaction outcomes. Regression analysis was used to test for moderation effects of robot-generated content on the relationship between HRT and interaction outcomes.,TRUE,Robot-verbal-communication-content,,Robot-verbal-communication-content,"In this study, the researchers manipulated the source of the pre-interaction message about the robot. Instead of UGC, participants received robot-generated content. The content itself was similar to the UGC in Study 1, but the source was attributed to the robot. This manipulation falls under 'Robot-verbal-communication-content' because it is the content of the message that is being manipulated, even though the source is different. The study found that this robot-generated content did not significantly affect trust or the relationship between trust and interaction outcomes. Therefore, the factor is listed as not impacting trust.",10.1089/cyber.2016.0199,http://www.liebertpub.com/doi/10.1089/cyber.2016.0199,"Human–robot interaction (HRI) will soon transform and shift the communication landscape such that people exchange messages with robots. However, successful HRI requires people to trust robots, and, in turn, the trust affects the interaction. Although prior research has examined the determinants of human–robot trust (HRT) during HRI, no research has examined the messages that people received before interacting with robots and their effect on HRT. We conceptualize these messages as SMART (Strategic Messages Affecting Robot Trust). Moreover, we posit that SMART can ultimately affect actual HRI outcomes (i.e., robot evaluations, robot credibility, participant mood) by affording the persuasive inﬂuences from user-generated content (UGC) on participatory Web sites. In Study 1, participants were assigned to one of two conditions (UGC/control) in an original experiment of HRT. Compared with the control (descriptive information only), results showed that UGC moderated the correlation between HRT and interaction outcomes in a positive direction (average Dr = +0.39) for robots as media and robots as tools. In Study 2, we explored the effect of robot-generated content but did not ﬁnd similar moderation effects. These ﬁndings point to an important empirical potential to employ SMART in future robot deployment."
"Liang, Yuhua; Lee, Seungcheol Austin",Employing user-generated content to enhance human-robot interaction in a human-robot trust game,2016,1,31,31,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were randomly assigned to either a positive review condition or a control condition. They then played a trust game with a robot, and completed measures including mood, usefulness of the robot, and trust toward the robot.","Participants played a collaborative game with a robot to score points, where they could choose to let the robot roll a dice.",Unspecified,Telepresence Robots,Research,Game,Economic Game,minimal interaction,"Participants interacted with a robot in a lab setting, with verbal instructions and a game.",real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical telepresence robot.,wizard of oz (directly controlled),"The robot was teleoperated, meaning its actions were directly controlled by a human.",Behavioral Measures; Questionnaires,,,Trust was measured using a behavioral measure (number of times participants allowed the robot to roll a dice) and questionnaires.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"Participants were exposed to either positive user-generated reviews or no reviews about the robot, which was intended to influence their expectations of the robot.","The manipulation did not directly affect trust, but it altered the correlation between trust and interactional outcomes. In the control condition, higher trust was associated with lower perceived competence and trustworthiness, while in the positive review condition, higher trust was associated with better mood, perceived usefulness, ease of use, competence, caring, and trustworthiness.","The study found that user-generated content did not directly affect trust or interactional outcomes, but it moderated the relationship between trust and outcomes. Specifically, in the control condition, higher trust was associated with lower perceived competence and trustworthiness, which is an unexpected result.","User-generated content did not directly affect human-robot trust or interactional outcomes, but it altered the correlation between trust and interactional outcomes, such that higher trust led to more positive outcomes in the positive review condition.","The robot was teleoperated and displayed a smiley face on the screen. The human participant played a collaborative game with the robot, where they could choose to let the robot roll a dice to score points.",fisher r-to-z transformation,The study used a Fisher r-to-z transformation to test the significance of the difference in correlation coefficients between the positive review and control conditions. This test was used to determine if the relationship between human-robot trust and interactional outcomes differed significantly between the two conditions.,TRUE,Other,,Other,"The study manipulated the presence of user-generated content (positive reviews) about the robot. This does not fit into any of the existing categories. It is not a direct manipulation of the robot's behavior, appearance, or task, but rather a manipulation of information provided to the participant about the robot before the interaction. Therefore, it is classified as 'Other'. The manipulation did not directly impact trust levels, but it did alter the correlation between trust and interactional outcomes. Therefore, the 'Other' factor is listed under 'factors_that_did_not_impact_trust' because the manipulation did not directly affect trust levels. The specific factor being manipulated is the presence of positive user-generated reviews. This doesn't fit into any existing category because it's not a manipulation of the robot itself, but rather of the information environment surrounding the robot. A suggested category for future classification would be 'Pre-interactional-information'.",10.1109/HRI.2016.7451810,http://ieeexplore.ieee.org/document/7451810/,"This research explores how user-generated content can strategically affect the relationship between human-robot trust and interactional outcomes. Results showed that usergenerated content did not directly affect human-robot trust or interactional outcomes. Interestingly, the data were consistent with a moderation model, such that exposure to user-generated content altered the correlation between human-robot trust and interactional outcome from an unfavorable to a favorable way. These results prompt the need for a new research program that could be developed to focus on deploying strategic messages via user-generated content to create better HRI."
"Liao, Ting; MacDonald, Erin",Manipulating Trust of Autonomous Products With Affective Priming,2019,1,48,44,4,4 respondents reported a malfunction of the focal product by noticing things like the Echo not glowing and interrupted the experiment,Controlled Lab Environment,between-subjects,"Participants completed a pre-test survey, were primed with visual stimuli, interacted with a mock-up Amazon Echo, and completed a post-test survey.",Participants asked a mock-up of the Amazon Echo ten pre-determined questions and rated the perceived competence and expectation of each response.,Unspecified,Expressive Robots,Social,Social,Conversation,minimal interaction,Participants interacted with a mock-up of a smart speaker by asking questions and receiving pre-recorded responses.,real-world,Participants interacted with a physical mock-up of the Amazon Echo in a lab setting.,physical,Participants interacted with a physical mock-up of the Amazon Echo.,wizard of oz (directly controlled),The responses of the mock-up Amazon Echo were pre-recorded and played by a proctor.,Questionnaires,Jian et al. Trust Scale,,Trust was measured using self-reported questionnaires before and after the interaction.,"parametric models (e.g., regression)",Linear mixed models were used to analyze the effects of priming and perceived competence on trust.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"Participants were primed with visual stimuli to evoke positive, neutral, or negative emotions, and the responses of the mock-up Echo were pre-determined to vary in competence.","Priming did not significantly affect overall trust, but positive priming increased suspicion about the system's outcome while also increasing the perception of security. Perceived competence of the product significantly influenced trust when expectations were met.","The study found that positive priming increased suspicion about the system's outcome while also increasing the perception of security, which is an unexpected result. The impact of uncertain-to-justify responses on trust was also contradictory.","The study's key finding is that product performance, particularly when user expectations are met, plays a critical role in trust formation, and that affective priming alone is not sufficient to influence trust.","The human participant asked pre-determined questions to a mock-up of the Amazon Echo, and the robot (via a hidden speaker) provided pre-recorded responses. The human then rated the competence and expectation of each response.",ANOVA; Mixed-effects model,"The study used ANOVA to test for significant differences between the group means of the three priming conditions on various measures, including initial attitudes, perceived competence, expectation, and post-interaction trust. Linear mixed models (LMM) were then employed to control for individual differences in initial attitudes and to examine the effects of priming conditions and perceived product competence on trust-related metrics. The LMM was also used to analyze the interaction between priming conditions and perceived competence, particularly for responses that were difficult to justify.",TRUE,Robot-accuracy; Task-complexity; Robot-emotional-display,Robot-accuracy,Robot-emotional-display,"The study manipulated the perceived competence of the robot's responses by providing pre-recorded answers that varied in accuracy and usefulness, which directly impacts the 'Robot-accuracy'. The study also manipulated the emotional state of the participants through visual priming, which is categorized as 'Robot-emotional-display' because it is an attempt to influence the user's emotional state before interacting with the robot. The task itself involved asking pre-determined questions, which can be considered a manipulation of 'Task-complexity' as the questions were designed to elicit different levels of perceived competence and expectation. The study found that the perceived competence of the robot's responses ('Robot-accuracy') significantly influenced trust, particularly when user expectations were met. However, the affective priming ('Robot-emotional-display') did not significantly affect overall trust, indicating that this manipulation did not have a direct impact on trust levels. The task complexity was not explicitly tested for its impact on trust, but it was a factor that was manipulated in the study design.",10.1115/DETC2019-98395,"https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings/IDETC-CIE2019/59278/Anaheim,%20California,%20USA/1070141","User-to-product trust has two notable aspects: (1) the user's propensity to trust, and (2) the product's trustworthiness as assessed by the user. Autonomous products, which perform many functions on their own with limited user input, require the user to exhibit trust at an appropriate level before use. Research in product trust thus far has focused on the product trustworthiness: manipulating the product's design, for example, anthropomorphizing an autonomous vehicle and measuring changes in trust. This study flips the usual approach, manipulating a person's propensity to trust and measuring response to an existing autonomous product, the Amazon Echo. We build on our past successes with priming exercises to reveal insights into the user-related factors of product trust. In this study, we used visual stimuli that evoked either positive, neutral or negative emotions as affective primes to influence users’ trust propensity before the interaction. The participants interacted with a mock-up of the Amazon Echo via ten pre-determined question-and-answer (Q&A) sets. During the interaction, the participants evaluated the Echo's competence and if it met participants’ expectations. They also reported trust towards the Echo after the Q&A sets. Holistically, the affective primes show no significant effect on the trust propensity. For the subgroup of participants whose expectations of the product’s performance were met, both the perceived product competence and the affective primes have significant effects on trust propensity. These results demonstrate the complex nature of trust as a multidimensional construct and the critical role of product performance in trust formation. They also suggest that it will be difficult for a product to build trust with users who expect the product to perform in a different way than its intent—if one wants to design a product that builds trust, they should understand user expectations and design to meet them. This learning can facilitate the intentional design of the affective process in trust formation that helps build a healthy level of trust with autonomous products."
"Ligthart, Mike",Is Motion Fluency an Effective Behavioral Style for Regulating Robot Trustworthiness?,2013,1,103,100,3,3 participants were excluded a priori because they were unable to complete the experiment because of disorientation caused by the IVE,Controlled Lab Environment,between-subjects,"Participants were introduced to a virtual environment and a Van Halen task, practiced the task, and then completed the task with a virtual robot that had either smooth or trembling motions for a short or long duration. After the task, participants completed a questionnaire.","Participants performed a Van Halen task, removing brown balls from a conveyor belt, and corrected a virtual robot when it made a mistake in the same task.",TWENDY-ONE,Humanoid Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a virtual robot in a simulated environment.,simulation,Participants used a head-mounted display to experience the virtual environment.,simulated,The robot was a virtual representation in the immersive environment.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the participant.,Questionnaires,,,Trust was measured using a post-experiment questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's motion fluency (smooth vs. trembling) and the task duration (short vs. long) were manipulated to see their effect on trust.,No significant effect of motion fluency or the interaction between motion fluency and task duration on trust was found.,"The study found no significant effect of motion fluency on trust, which contradicts previous findings using video stimuli. A small duration effect was found in the fluency manipulation check, where participants perceived the robot as more trembling in the short condition than in the long condition.","The study found no significant effect of motion fluency on robot trustworthiness, and no interaction effect between motion fluency and task duration.","The robot performed a Van Halen task, picking up brown balls from a conveyor belt, with either smooth or trembling motions. The human participant also performed the same task and corrected the robot when it made a mistake.",ANOVA,"The study used Analysis of Variance (ANOVA) to analyze the data. A 2x2 between-subjects ANOVA was used to examine the effects of duration (long vs. short) and motion fluency (smooth vs. trembling) on the perceived trembling of the robot and on the trustworthiness measure. Additionally, three-way ANOVAs were used to check for experimenter effects, and one-way ANOVAs were used to check for acquaintance effects and performance differences. The primary goal was to determine if motion fluency and task duration, or their interaction, had a significant impact on the perceived trustworthiness of the robot.",TRUE,Robot-nonverbal-communication; Task-constraints,,Robot-nonverbal-communication; Task-constraints,"The study manipulated the robot's motion fluency (smooth vs. trembling), which is a form of nonverbal communication, thus 'Robot-nonverbal-communication' was chosen. The study also manipulated the duration of the task (short vs. long), which is a constraint on the task, thus 'Task-constraints' was chosen. The results showed that neither motion fluency nor task duration had a significant impact on trust, therefore both 'Robot-nonverbal-communication' and 'Task-constraints' are listed under 'factors_that_did_not_impact_trust'. The paper explicitly states 'The results from the experiment reveal no significant main effect of motion fluency or motion fluency x duration interaction effect on the robot's trustworthiness.' and 'The two-way ANOVA of the trustworthiness measure showed that there was no difference between trembling and fluent behavior in both duration conditions: all F's < 1, p's > .608 (see Figure 4). Meaning that no motion fluency x interaction effect was observed.'.",,,"Finding good behavioral styles to express robot trustworthiness will optimize the usage of robots. In previous research, motion fluency as behavioral style was studied. Smooth robot motions were compared with trembling robot motions. In a video experiment an effect of motion fluency on trust was found, while in an Immersive Virtual Environment (IVE) experiment, no effect was observed [1]. In this research, we explored the question whether the motion fluency effect is present in a short version of an IVE task and disappears when the task is longer. Results indicate this is not the case. Several explanations for this null-effect are discussed and several recommendations for further human-robot trust studies are provided."
"Lin, Chin-Teng; Fan, Hsiu-Yu; Chang, Yu-Cheng; Ou, Liang; Liu, Jia; Wang, Yu-Kai; Jung, Tzyy-Ping",Modelling the Trust Value for Human Agents Based on Real-Time Human States in Human-Autonomous Teaming Systems,2022,1,6,6,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a calibration for the eye tracker, were introduced to the experiment, and then controlled a robot in a ball-collection task using keyboard keys, while their physiological data was recorded.",Participants controlled a robot to collect balls in a simulated environment.,Unspecified,Mobile Robots,Research,Navigation,Path Following,minimal interaction,Participants interacted with the robot through a simulation using keyboard controls.,simulation,The interaction took place in a simulated environment.,simulated,The robot was a virtual representation in a simulation.,shared control (adaptive),The robot's behavior was influenced by human input and adapted based on the human's trust level.,Behavioral Measures; Physiological Measures; Real-time Trust Measures,,Eye-tracking Data; Performance Metrics; Physiological Signals,"Trust was assessed using real-time physiological data, behavioral measures, and performance metrics.","deep learning (e.g., neural networks, reinforcement learning)",A Q-learning algorithm with a fuzzy reward system was used to model trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's behavior was influenced by the human's trust level, which was calculated based on physiological data and perception, and the robot could choose to follow human instructions or explore autonomously, which affected the task completion time.","The study showed that the proposed trust model improved coordination in the HAT teams, suggesting that the model can adapt to various levels of human performance and generate reliable trust values.","The study found that the collaboration mode, where the robot's actions were influenced by the human's trust level, resulted in the shortest task completion times compared to human instruction only or robot random search.","The proposed trust model, which combines human attention level, stress index, and perception, can improve the efficiency of human-robot collaboration in a ball-collection task.","The robot moved in the simulated environment to collect balls, and the human controlled the robot's direction using keyboard keys, with the robot's autonomy level influenced by the human's trust level.",,"No specific statistical tests are mentioned in the paper. The analysis focuses on comparing task completion times and decision times across different modes (human instruction, collaboration, and robot random search) and visualizing robot paths. The paper does not explicitly state the use of any statistical tests to compare these values.",TRUE,Robot-autonomy,Robot-autonomy,,"The study manipulated the robot's autonomy level. The robot could either follow human instructions, explore autonomously, or operate in a collaboration mode where its actions were influenced by the human's trust level. This is explicitly stated in the abstract: 'The robot agents can determine whether they follow human commands or not, based on the human trust values evaluated by the proposed trust model.' and in the experimental manipulation description: 'The robot's behavior was influenced by the human's trust level, which was calculated based on physiological data and perception, and the robot could choose to follow human instructions or explore autonomously, which affected the task completion time.' The results showed that the collaboration mode, where the robot's autonomy was influenced by the human's trust level, resulted in the shortest task completion times, indicating that the manipulation of robot autonomy impacted trust and performance. The paper states: 'The comparison results demonstrate that the proposed trust model can improve coordination in the HAT teams with different human participants in all test scenarios, which also suggests that the proposed model can adapt to various levels of human performance and generate reliable trust values via the Q-learning algorithm.' This shows that the robot's autonomy was directly influenced by the human's trust level, and this manipulation had an impact on the task performance and the overall system efficiency. There were no other factors manipulated in the study.",10.3390/technologies10060115,https://www.mdpi.com/2227-7080/10/6/115,"The modelling of trust values on agents is broadly considered fundamental for decisionmaking in human-autonomous teaming (HAT) systems. Compared to the evaluation of trust values for robotic agents, estimating human trust is more challenging due to trust miscalibration issues, including undertrust and overtrust problems. From a subjective perception, human trust could be altered along with dynamic human cognitive states, which makes trust values hard to calibrate properly. Thus, in an attempt to capture the dynamics of human trust, the present study evaluated the dynamic nature of trust for human agents through real-time multievidence measures, including human states of attention, stress and perception abilities. The proposed multievidence human trust model applied an adaptive fusion method based on fuzzy reinforcement learning to fuse multievidence from eye trackers, heart rate monitors and human awareness. In addition, fuzzy reinforcement learning was applied to generate rewards via a fuzzy logic inference process that has tolerance for uncertainty in human physiological signals. The results of robot simulation suggest that the proposed trust model can generate reliable human trust values based on real-time cognitive states in the process of ongoing tasks. Moreover, the human-autonomous team with the proposed trust model improved the system efﬁciency by over 50% compared to the team with only autonomous agents. These results may demonstrate that the proposed model could provide insight into the real-time adaptation of HAT systems based on human states and, thus, might help develop new ways to enhance future HAT systems better."
"Lin, Jinchao; Panganiban, April Rose; Matthews, Gerald; Gibbins, Katey; Ankeney, Emily; See, Carlie; Bailey, Rachel; Long, Michael",Trust in the Danger Zone: Individual Differences in Confidence in Robot Threat Assessments,2022,1,118,118,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were randomly assigned to either a physics-based or psychological analysis condition. They completed questionnaires, a training session, and then a simulation task where they evaluated threat levels in urban scenes with a robot partner. They then completed a post-task trust questionnaire.","Participants were tasked with evaluating the level of threat in a series of urban scenes, aided by a robot partner that provided threat assessments based on either physics-based or psychological analysis.",Unspecified,Unmanned Ground Vehicles,Research,Evaluation,Rating,minimal interaction,"Participants interacted with the robot through a simulation, receiving text-based threat assessments.",simulation,The study used a 3D simulation of an urban environment to create an immersive experience.,simulated,The robot was represented through text-based messages within the simulation.,pre-programmed (non-adaptive),"The robot provided pre-programmed threat assessments based on either physics or psychological analysis, without adapting to the user.",Questionnaires; Custom Scales,Trust Perception Scale - HRI; Negative Attitude towards Robots Scale (NARS); Human-Robot Trust Scale/Questionnaire (HRTS/HRITS),,"Trust was measured using questionnaires and custom scales, assessing confidence in the robot's analysis and willingness to act on its recommendations.","parametric models (e.g., regression)","The study used regression analysis to model the relationship between dispositional factors, situational ratings, and trust.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the type of analysis the robot used (physics-based or psychological) and the level of danger cues in the environment, influencing how participants perceived the robot and the situation.","Trust was higher when the robot's decision matched the visible context, and the robot's mode of analysis moderated the influence of individual differences on trust.","The study found that trust was not directly influenced by the robot's mode of analysis (physics-based vs. psychological), but rather that the mode of analysis moderated the influence of individual differences on trust. The RoTA-Psych scale was more predictive of trust in the psychological condition, suggesting that the mental model of the robot as a teammate was activated in this condition.","Individual differences in trust varied according to the robot's mode of threat analysis, with dispositional factors associated with attitudes to robot teammates predicting trust when the robot performed human-like analyses.",The robot provided text-based threat assessments based on either physics-based or psychological analysis of the simulated environment. The human participant viewed the simulated urban scenes and rated their confidence in the robot's threat assessment and their willingness to act on its recommendations.,t-test; ANOVA; Linear regression,"The study used t-tests to compare means of psychological judgment ratings between conditions. ANOVAs were used to analyze the effects of danger cues on threat and anxiety ratings, and to examine the effects of experimental manipulations (analysis mode, danger cues, robot decision) on trust ratings. Regression analyses were employed to investigate the relationships between dispositional factors, situational ratings, and trust, including testing for moderator effects of the robot's analysis mode.",TRUE,Robot-verbal-communication-content; Task-environment,Robot-verbal-communication-content; Task-environment,,"The study manipulated the content of the robot's verbal communication by providing either physics-based or psychological explanations for its threat assessments. This is a manipulation of 'Robot-verbal-communication-content' because the content of the messages varied, influencing how participants perceived the robot's analysis. The study also manipulated the 'Task-environment' by varying the level of danger cues present in the urban scenes (low, medium, high). This manipulation directly altered the context in which the robot's assessments were made, influencing the participants' perceptions of threat and their trust in the robot. The study found that both the robot's mode of analysis (physics-based or psychological) and the level of danger cues in the environment impacted trust. Specifically, trust was higher when the robot's decision matched the visible context, and the robot's mode of analysis moderated the influence of individual differences on trust. Therefore, both 'Robot-verbal-communication-content' and 'Task-environment' are listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",10.3389/fpsyg.2022.601523,https://www.frontiersin.org/articles/10.3389/fpsyg.2022.601523/full,"Effective human–robot teaming (HRT) increasingly requires humans to work with intelligent, autonomous machines. However, novel features of intelligent autonomous systems such as social agency and incomprehensibility may influence the human’s trust in the machine. The human operator’s mental model for machine functioning is critical for trust. People may consider an intelligent machine partner as either an advanced tool or as a human-like teammate. This article reports a study that explored the role of individual differences in the mental model in a simulated environment. Multiple dispositional factors that may influence the dominant mental model were assessed. These included the Robot Threat Assessment (RoTA), which measures the person’s propensity to apply tool and teammate models in security contexts. Participants (               N                = 118) were paired with an intelligent robot tasked with making threat assessments in an urban setting. A transparency manipulation was used to influence the dominant mental model. For half of the participants, threat assessment was described as physics-based (e.g., weapons sensed by sensors); the remainder received transparency information that described psychological cues (e.g., facial expression). We expected that the physics-based transparency messages would guide the participant toward treating the robot as an advanced machine (advanced tool mental model activation), while psychological messaging would encourage perceptions of the robot as acting like a human partner (teammate mental model). We also manipulated situational danger cues present in the simulated environment. Participants rated their trust in the robot’s decision as well as threat and anxiety, for each of 24 urban scenes. They also completed the RoTA and additional individual-difference measures. Findings showed that trust assessments reflected the degree of congruence between the robot’s decision and situational danger cues, consistent with participants acting as Bayesian decision makers. Several scales, including the RoTA, were more predictive of trust when the robot was making psychology-based decisions, implying that trust reflected individual differences in the mental model of the robot as a teammate. These findings suggest scope for designing training that uncovers and mitigates the individual’s biases toward intelligent machines."
"Lin, Tsung-Chi; Krishnan, Achyuthan Unni; Li, Zhi",The Impacts of Unreliable Autonomy in Human-Robot Collaboration on Shared and Supervisory Control for Remote Manipulation,2023,2,13,13,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants practiced single object pick-and-place tasks using manual, shared, and supervisory control interfaces. They then performed a multi-object sorting task with reliable autonomy, with the order of interfaces randomized. Participants completed questionnaires after the study.","Participants performed a multi-object sorting task, grasping and placing objects in colored boxes following a specific sequence.",Kinova Gen 3,Industrial Robot Arms; Mobile Manipulators,Research,Manipulation,Sorting/Arranging,minimal interaction,Participants interacted with the robot through a motion tracking interface and a button to trigger autonomous actions.,simulation,Participants viewed the robot and the task through a 3D simulation on a desktop monitor.,physical,Participants controlled a physical robot arm remotely.,shared control (fixed rules),"The robot performed precise manipulation actions based on fixed rules, while humans controlled gross manipulation.",Questionnaires; Behavioral Measures; Performance-Based Measures,NASA Task Load Index (NASA-TLX); System Usability Scale (SUS),"Eye-tracking Data; Performance Metrics; robot data (sensor data, etc.)","Trust was assessed using questionnaires, behavioral measures such as task completion time, and eye-tracking data.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The level of robot autonomy was manipulated by using manual, shared, and supervisory control interfaces, which influenced the task difficulty.","Supervisory control was preferred due to better performance and lower workload, despite lower engagement.","Supervisory control was preferred despite lower engagement, which is a notable trend.","Supervisory control with reliable autonomy resulted in the best task performance, lowest workload, and highest user preference, despite lower engagement.","The robot autonomously grasped and placed objects, while the human either controlled the robot's gross movements (shared control) or supervised the robot's actions (supervisory control). The human also corrected errors.",ANOVA,"A one-way repeated-measures analysis of variance (ANOVA) was used to compare the manual, shared, and supervisory control conditions. Post hoc comparisons with Holm-Bonferroni correction were used to control for Type I error in multiple comparisons. The ANOVA was used to analyze the differences in task completion time, traveled trajectory lengths, physical workload, cognitive workload, NASA-TLX scores, SUS scores, visual engagement, and action engagement across the different control conditions.",TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,,"The study manipulated 'Robot-autonomy' by using manual, shared, and supervisory control interfaces, which directly changed the level of decision authority given to the robot. The task complexity was also manipulated by changing the control interface, which influenced the task difficulty. The results showed that the level of autonomy impacted user preference, with supervisory control being preferred despite lower engagement, indicating an impact on trust. The study did not find any factors that did not impact trust.",10.1109/LRA.2023.3287039,https://ieeexplore.ieee.org/document/10154152/,"This work compared human-robot shared and supervisory control of remote robots for dexterous manipulation, and examined how the reliability of robot autonomy affects human operator performance, workload, and preference for robot assistance. Speciﬁcally, we implemented two human-robot collaboration (HRC) paradigms for remote manipulation: (1) shared control, where humans controlled gross manipulation and the robot autonomy controlled precise manipulation actions, and (2) supervisory control, where the robot autonomy controlled both gross and precise manipulation actions but relied on humans to detect and correct errors. We conducted two user studies: one to compare the effectiveness of the two HRC paradigms when assistive autonomy is reliable, and the other to examine the impact of error type and frequency on tasks and human operators in the two HRC paradigms when assistive autonomy is unreliable. Our results show that: (1) the interface with a higher level of reliable autonomy yields signiﬁcantly better performance, lower workload, and higher user preference but lower engagement, and (2) the frequency and type of the error have signiﬁcant impacts on the task performance and human workload but only partially affects the operator’s preference and usage of autonomy."
"Lin, Tsung-Chi; Krishnan, Achyuthan Unni; Li, Zhi",The Impacts of Unreliable Autonomy in Human-Robot Collaboration on Shared and Supervisory Control for Remote Manipulation,2023,2,13,13,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a multi-object sorting task with unreliable autonomy, using shared and supervisory control interfaces. The type and frequency of errors were manipulated. Participants completed questionnaires after the study.","Participants performed a multi-object sorting task, grasping and placing objects in colored boxes following a specific sequence, with errors introduced by the robot.",Kinova Gen 3,Industrial Robot Arms; Mobile Manipulators,Research,Manipulation,Sorting/Arranging,minimal interaction,Participants interacted with the robot through a motion tracking interface and a button to trigger autonomous actions.,simulation,Participants viewed the robot and the task through a 3D simulation on a desktop monitor.,physical,Participants controlled a physical robot arm remotely.,shared control (fixed rules),"The robot performed precise manipulation actions based on fixed rules, while humans controlled gross manipulation.",Questionnaires; Behavioral Measures; Performance-Based Measures,NASA Task Load Index (NASA-TLX); System Usability Scale (SUS),"Eye-tracking Data; Performance Metrics; robot data (sensor data, etc.)","Trust was assessed using questionnaires, behavioral measures such as task completion time, and eye-tracking data.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The reliability of robot autonomy was manipulated by introducing errors of different types and frequencies, which influenced the task difficulty and robot performance.","Higher error frequency led to worse performance and workload, but did not necessarily decrease preference for higher autonomy levels. Users preferred autonomy if errors were easy to correct.","Users still preferred higher autonomy levels even with frequent errors, if the errors were easy to correct. Some participants preferred manual control when errors were frequent, indicating a mixed response to unreliable autonomy.","Users may still prefer to use unreliable autonomy if the errors are easy to correct, and higher error frequency leads to worse performance and workload.","The robot autonomously grasped and placed objects, while the human either controlled the robot's gross movements (shared control) or supervised the robot's actions (supervisory control). The human also corrected errors. The robot introduced errors in the object selection or placement.",ANOVA,"A one-way repeated-measures analysis of variance (ANOVA) was used to analyze the effects of error frequency and error type on task performance, workload, and user preference. Post hoc comparisons with Holm-Bonferroni correction were used to control for Type I error in multiple comparisons. The ANOVA was used to compare the different error frequencies and types within both shared and supervisory control conditions.",TRUE,Robot-autonomy; Robot-accuracy; Task-complexity,Robot-accuracy; Robot-autonomy,,"The study manipulated 'Robot-autonomy' by using shared and supervisory control interfaces. 'Robot-accuracy' was manipulated by introducing errors of different types and frequencies, which directly impacted the robot's performance. The task complexity was also manipulated by changing the error frequency and type, which influenced the task difficulty. The results showed that the frequency and type of errors (accuracy) impacted user preference for autonomy, with users preferring autonomy if errors were easy to correct, indicating an impact on trust. The level of autonomy also impacted user preference. The study did not find any factors that did not impact trust.",10.1109/LRA.2023.3287039,https://ieeexplore.ieee.org/document/10154152/,"This work compared human-robot shared and supervisory control of remote robots for dexterous manipulation, and examined how the reliability of robot autonomy affects human operator performance, workload, and preference for robot assistance. Speciﬁcally, we implemented two human-robot collaboration (HRC) paradigms for remote manipulation: (1) shared control, where humans controlled gross manipulation and the robot autonomy controlled precise manipulation actions, and (2) supervisory control, where the robot autonomy controlled both gross and precise manipulation actions but relied on humans to detect and correct errors. We conducted two user studies: one to compare the effectiveness of the two HRC paradigms when assistive autonomy is reliable, and the other to examine the impact of error type and frequency on tasks and human operators in the two HRC paradigms when assistive autonomy is unreliable. Our results show that: (1) the interface with a higher level of reliable autonomy yields signiﬁcantly better performance, lower workload, and higher user preference but lower engagement, and (2) the frequency and type of the error have signiﬁcant impacts on the task performance and human workload but only partially affects the operator’s preference and usage of autonomy."
"Lingg, Nico; Demiris, Yiannis",Building Trust in Assistive Robotics: Insights From a Real-World Mobile Navigation Experiment,2023,1,27,27,0,No participants were excluded,Real-World Environment,between-subjects,"Participants were introduced to the autonomous wheelchair and the navigation task, completed initial trust questionnaires, were fitted with sensors, interacted with the robot in either good or bad performance conditions, and then completed a second set of trust questionnaires.",Participants collaborated with an autonomous wheelchair to deliver packages to predefined checkpoints in an unstructured office environment.,Unspecified,Mobile Robots; Service and Assistive Robots,Care; Research,Navigation,Path Following,direct-contact interaction,Participants physically interacted with the robot during the navigation task.,real-world,The study was conducted in a real-world office environment with a physical robot.,physical,The robot was a physical autonomous wheelchair.,fully autonomous (limited adaptation),The robot navigated autonomously but with limited adaptation to unexpected scenarios.,Questionnaires; Real-time Trust Measures; Physiological Measures,Trust in Automated Systems Scale; Trust in Automation Scale (TAS); Trust Perception Scale - HRI,Physiological Signals; Eye-tracking Data,"Trust was assessed using questionnaires, real-time verbal reports, and physiological measures.",no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's performance was manipulated to be either good or bad, affecting its trajectory and smoothness of movement, which was intended to influence trust.","Good robot performance increased trust, while bad performance decreased trust.","The study found that exposure to the robot improved participants' understanding of the robot regardless of performance, but only good performance improved attitudes towards the robot.","Exposure to the robot and its performance significantly impact participants' self-reported levels of trust, with good performance increasing trust and bad performance decreasing it.","The robot autonomously navigated to checkpoints, while the human participant accompanied the robot and could take manual control if needed.",paired samples t-test; t-test,The study used a paired samples t-test to compare trust sub-scale scores before and after the interaction with the robot. An independent samples t-test was then used to examine differences in the mean change of scores between participants who interacted with a good-performing robot and those who interacted with a bad-performing robot. These tests were used to assess the impact of robot performance on trust.,TRUE,Robot-accuracy,Robot-accuracy,,"The study explicitly manipulated the robot's performance, creating 'good' and 'bad' performance conditions. This manipulation directly affected the robot's trajectory and smoothness of movement, which falls under the 'Robot-accuracy' category because it directly influences the task performance metrics (e.g., smoothness of movement, which is a component of task success). The paper states, 'The performance of the wheelchair is manipulated between a good-performing condition, in which it follows the trajectory efficiently and smoothly, and a bad-performing condition, in which it swerves and stops abruptly while still avoiding obstacles for the participant's safety.' The results showed that the manipulated robot performance (Robot-accuracy) impacted trust, as good performance increased trust and bad performance decreased it. There were no other factors manipulated in the study.",10.1145/3597512.3597519,https://dl.acm.org/doi/10.1145/3597512.3597519,"Assistive robotics can improve the lives of people with mobility impairments, but widespread acceptance depends on understanding how trust forms during human-robot interaction (HRI). This study reports on a real-world mobile navigation experiment involving 27 participants who used an autonomous wheelchair to deliver packages to predetermined locations. The performance of the wheelchair was manipulated to create good and bad performance conditions. The participants’ trust in the robot was measured using established trust questionnaires before and after the interaction. The results indicate that exposure to the robot and its performance significantly impact participants’ self-reported levels of trust in the robot. Specifically, scores on trust factors relating to the robot taken after the experiment were significantly higher than those taken before, suggesting that exposure to the robot is an essential element of the trust-building process. The study also found that poor robot performance during the interaction negatively affected participants’ perception of the robot’s behaviour and their attitudes towards it. The study concludes with implications for the design and development of autonomous assistive devices, highlighting the importance of potential users’ exposure to assistive robots for building trust in the system and the negative impact of bad robot performance on people’s attitudes towards the technology."
"Lingg, Nico; Demiris, Yiannis",Trust Prediction in Assistive Robotics using Multi-Modal Video Transformers,2024,1,34,34,0,No participants were excluded,Controlled Lab Environment,within-subjects,Participants navigated an autonomous wheelchair to five positions in an office environment. The wheelchair's performance was varied between good and bad conditions. Participants completed initial and final trust questionnaires and provided continuous trust ratings using Trusty. Physiological and eye-tracking data were also collected.,Participants were tasked with navigating to five predetermined positions using an autonomous wheelchair.,Unspecified,Mobile Robots; Service and Assistive Robots,Care; Research,Navigation,Path Following,direct-contact interaction,Participants directly interacted with the autonomous wheelchair during navigation.,real-world,The interaction took place in a real-world office environment with a physical robot.,physical,The study used a physical autonomous wheelchair.,fully autonomous (limited adaptation),"The wheelchair navigated autonomously, but with limited adaptation to unexpected scenarios.",Questionnaires; Real-time Trust Measures; Physiological Measures,,"Video Data; Physiological Signals; Eye-tracking Data; robot data (sensor data, etc.)","Trust was assessed using questionnaires, real-time trust measures, and physiological data.","deep learning (e.g., neural networks, reinforcement learning)",A multi-modal video transformer was used to model trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The wheelchair's performance was manipulated to be either good (smooth and efficient) or bad (sudden stops and swerves) to influence trust.,The study found that the robot's performance significantly impacted trust levels.,"The study found that eye gaze data significantly enhanced the performance of the trust prediction model. Physiological data had a complex contribution, not linearly enhancing performance in all aspects.","A multi-modal video transformer can predict user's trust levels with about 64% accuracy, with eye gaze data significantly enhancing performance.","The robot (autonomous wheelchair) navigated to five predetermined positions, while the human participant sat in the wheelchair and provided continuous trust ratings.",,"The paper focuses on developing and evaluating a multi-modal video transformer for trust prediction. While the paper mentions statistical analysis of previous datasets, it does not explicitly mention any specific statistical tests being used in the current study. The evaluation of the model's performance is based on accuracy metrics (e.g., 64% accuracy in predicting trust levels) and mean-absolute-error, rather than traditional statistical tests. The study uses a multi-task loss function to train the model, but this is not a statistical test in itself.",TRUE,Robot-accuracy,Robot-accuracy,,"The study explicitly manipulated the wheelchair's performance, varying it between 'good-performing (smooth and efficient)' and 'bad-performing (sudden stops and swerves, but safe)'. This manipulation directly affects the robot's accuracy in completing the navigation task, making 'Robot-accuracy' the most appropriate category. The paper states that the robot's performance significantly impacted trust levels, thus 'Robot-accuracy' is also listed as a factor that impacted trust. There were no factors that were manipulated that did not impact trust.",,,"Trust is crucial for effective human-robot interaction in assistive robotics. This paper presents a proof-of-concept study for transformerbased trust prediction, building on our previous work in understanding and measuring trust. We introduce a novel multi-modal video transformer to predict user trust in human-robot interactions with an autonomous wheelchair. We collected a comprehensive dataset comprising over 850,000 tokens, equivalent to approximately 4 hours of egocentric video streams, along with synchronized continuous trust recordings, eye gaze patterns, physiological data, and trust measurements from 34 diverse participants. Our research utilizes Trusty, a tool for continuous trust measurement, which has been previously validated against traditional methods. Our model utilizes a pre-trained Vision Transformer (ViT) with frozen weights to extract spatial features from video frames. Visual representations are enriched with the user’s eye gaze, heart rate, and electrodermal activity in a fusion module. Fusion outputs are temporally processed by a transformer, and the final token in the sequence is used to predict trust. This architectural design leverages transformers’ proven capabilities in processing high-dimensional visual data, effectively fusing signals across modalities, and scaling to larger datasets. The best-performing model achieves approximately 64% accuracy in predicting trust levels (high, medium, low), with eye gaze data significantly enhancing performance. Future work will focus on personalisation and real-time adaptive robot behaviour based on trust predictions."
"Lingg, Nico; Demiris, Yiannis",Beyond Self-Report: A Continuous Trust Measurement Device for HRI,2023,1,29,29,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were introduced to the autonomous wheelchair system and navigation task, completed pre-experiment questionnaires, were fitted with sensors, performed the task, and completed post-experiment questionnaires.",Participants collaborated with an autonomous wheelchair to deliver packages to predefined checkpoints in an unstructured environment.,ARTA smart wheelchair platform,Mobile Robots; Autonomous Vehicles,Research; Assistive,Navigation,Path Following,direct-contact interaction,Participants physically collaborated with the robot in a real-world setting.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The robot was a physical autonomous wheelchair.,fully autonomous (limited adaptation),The robot navigated autonomously but with limited adaptation to unexpected scenarios.,Questionnaires; Real-time Trust Measures; Custom Scales; Behavioral Measures,Trust in Automated Systems Scale; Trust in Automation Scale (TAS); Trust Perception Scale - HRI,"Physiological Signals; Eye-tracking Data; Performance Metrics; robot data (sensor data, etc.)","Trust was measured using questionnaires, a custom real-time device, verbal self-reports, and physiological data.",no modeling,The study did not include computational modeling of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The performance of the autonomous wheelchair was manipulated to be either good or bad, influencing the user's trust.",Participants interacting with a good-performing wheelchair exhibited increased trust levels compared to those interacting with a bad-performing wheelchair.,"The study found that trust scores were significantly higher for participants who interacted with a good-performing wheelchair, and that Trusty scores correlated with verbal self-report and post-experiment questionnaire scores, but not pre-experiment scores.","The study demonstrated that Trusty, a continuous trust measurement device, can effectively measure trust in HRI and correlates with established trust questionnaires and self-reported trust scores.","The robot autonomously navigated to checkpoints, and the human participant was tasked with collaborating with the robot to deliver packages, while also providing trust ratings using the Trusty device and verbal reports.",Mann-Whitney U; spearman's rank correlations,"The study used Mann-Whitney U tests to compare post-experiment questionnaire scores between participants who interacted with a good-performing wheelchair and those who interacted with a bad-performing wheelchair. Spearman's rank correlations were used to examine the relationships between different trust measures, including changes in verbal self-report scores and Trusty scores, post-experiment verbal and Trusty scores with post-experiment questionnaire scores, and verbal self-report and Trusty measures during the experiment. The purpose was to validate the experimental design and to assess the correlation between the newly introduced Trusty device and other trust measures.",TRUE,Robot-accuracy,Robot-accuracy,,"The study explicitly manipulated the performance of the autonomous wheelchair, creating 'good-performing' and 'bad-performing' conditions. This manipulation directly affected the robot's success in the navigation task, influencing its accuracy in reaching checkpoints and its overall smoothness of movement. Therefore, 'Robot-accuracy' is the most appropriate category. The paper states, 'The wheelchairs' performance varied between two conditions: good-performing (smooth and efficient) and bad-performing (sudden stops and swerves, but safe).' This directly relates to the robot's ability to perform the task accurately. The results section also confirms that the performance manipulation impacted trust, stating, 'participants who interacted with the good-performing wheelchair had higher scores on post-experiment questionnaires and selfreported measures of trust.' This indicates that the manipulation of 'Robot-accuracy' directly impacted trust levels. There were no other factors manipulated in the study.",10.1109/RO-MAN57019.2023.10309660,https://ieeexplore.ieee.org/document/10309660/,"Trust is a crucial part of human-robot interactions, and its accurate measurement is a challenging task. We introduce Trusty, a handheld continuous trust level measurement device and investigate its validity by analysing the correlation between its measurements and self-reported trust scores. In a study with 29 participants, we evaluated the effectiveness of the device with an autonomous wheelchair in a mobile navigation task. The participants collaborated with an autonomous wheelchair to deliver packages to predefined checkpoints in an unstructured environment, and the performance of the wheelchair was manipulated to be either under a goodperforming condition or a bad-performing condition. Our first finding reveals a notable influence of wheelchair performance on self-reported trust. Participants interacting with a goodperforming wheelchair exhibited increased trust levels, as evidenced by higher scores on post-experiment trust questionnaires and verbal self-reported trust measures. Additionally, our study proposes Trusty as a continuous measurement tool for assessing trust during HRI, demonstrating its equivalence to self-report measures and traditional questionnaire scores."
"Liu, Rui; Cai, Zekun; Lewis, Michael; Lyons, Joseph; Sycara, Katia",Trust Repair in Human-Swarm Teams+,2019,1,123,123,24,"Data for one of the fourteen conditions, large swarm single motor failure, was lost for 24 participants due to logging difficulties",Online Crowdsourcing,within-subjects,"Participants viewed a tutorial, then watched 14 videos of swarm behavior (normal, faulty, and repaired). After each video, they reported if they detected a fault, rated their trust in the swarm, and, if a fault was detected, identified the faulty robots and rated their trust in them. Finally, they rated the swarm's performance.",Participants watched videos of a simulated robot swarm and rated their trust in the swarm and its performance.,Unspecified,Swarm Robots; Unmanned Aerial Vehicles (UAVs),Research,Evaluation,Rating,passive observation,Participants passively observed videos of the robot swarm.,media,Participants watched videos of the simulated robot swarm.,simulated,The robots were simulated in the videos.,pre-programmed (non-adaptive),The robot swarm's behavior was pre-programmed and did not adapt to user input.,Questionnaires; Custom Scales,,,Trust was measured using a 5-point custom scale.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers directly manipulated the swarm's behavior by introducing faulty robots and then applying a repair algorithm, which influenced the swarm's performance and behavior.","Trust was significantly lower in faulty conditions compared to normal conditions, and the repair algorithm increased trust compared to faulty conditions, but not to the level of normal conditions.","Participants were able to detect faults in the swarm, and the trust repair algorithm did not mask the occurrence of failures. Trust was more affected by motor failures than wind disturbances. The effect of repair was more evident for smaller swarms.","The Trust-repair algorithm effectively corrected faulty swarm behavior and increased human trust compared to faulty conditions, but did not fully restore trust to the level of normal conditions.","The robot swarm performed distributed biased flocking, and the human participant observed videos of the swarm and rated their trust in the swarm's performance.",Mann-Whitney U,"The Mann-Whitney U test was used to compare ordinal categorical variables, specifically to analyze differences in fault detection, trust levels, and performance ratings between different experimental conditions (normal, faulty, and repaired). It was also used to compare trust levels between different numbers of faulty robots and different swarm sizes.",TRUE,Robot-accuracy; Robot-autonomy,Robot-accuracy,Robot-autonomy,"The researchers manipulated the robot's accuracy by introducing faulty robots with motor wear or wind disturbances, which directly impacted the swarm's performance (e.g., speed, heading direction). This is classified as 'Robot-accuracy' because it directly affects the task performance metrics. The study also manipulated the robot's autonomy by implementing a trust-repair algorithm that altered how the swarm responded to faulty robots. The algorithm constrained information sharing with faulty robots, effectively changing the swarm's decision-making process. This is classified as 'Robot-autonomy' because it changes the level of control and decision-making within the swarm. The results showed that the manipulation of 'Robot-accuracy' (introducing faults and then repairing them) significantly impacted trust levels, with lower trust in faulty conditions and higher trust in repaired conditions. However, the manipulation of 'Robot-autonomy' through the trust-repair algorithm, while changing the swarm's behavior, did not directly impact trust levels as much as the performance changes themselves. The trust-repair algorithm did not fully restore trust to the level of normal conditions, indicating that the performance of the swarm was the primary driver of trust, not the change in autonomy itself. The study explicitly states that 'Participants were more likely to report faults in faulty conditions... Participants also expressed higher levels of trust... under normal conditions than when faults occurred... Participants... expressed significantly higher trust... in repair conditions than in Faulty ones.' This shows that the performance of the swarm, which is directly related to the accuracy of the robots, was the main factor influencing trust. The trust-repair algorithm, while changing the autonomy of the swarm, did not fully restore trust, indicating that the performance was the primary driver of trust.",,,"Swarm robots are coordinated via simple control laws to generate emergent behaviors such as ﬂocking, rendezvous, and deployment. Human-swarm teaming has been widely proposed for scenarios, such as human-supervised teams of unmanned aerial vehicles (UAV) for disaster rescue, UAV and ground vehicle cooperation for building security, and soldierUAV teaming in combat. Effective cooperation requires an appropriate level of trust, between a human and a swarm. When an UAV swarm is deployed in a real-world environment, its performance is subject to real-world factors, such as system reliability and wind disturbances. Degraded performance of a robot can cause undesired swarm behaviors, decreasing human trust. This loss of trust, in turn, can trigger human intervention in UAVs’ task executions, decreasing cooperation effectiveness if inappropriate. Therefore, to promote effective cooperation we propose and test a trust-repairing method (Trust-repair) restoring performance and human trust in the swarm to an appropriate level by correcting undesired swarm behaviors. Faulty swarms caused by both external and internal factors were simulated to evaluate the performance of the Trustrepair algorithm in repairing swarm performance and restoring human trust. Results show that Trust-repair is effective in restoring trust to a level intermediate between normal and faulty conditions."
"Lochner, Martin; Duenser, Andreas; Sarker, Shouvojit",Trust and Cognitive Load in semi-automated UAV operation,2019,1,43,43,0,No participants were excluded,Controlled Lab Environment,mixed design,Participants were given verbal and written cues to induce high or low trust in a UAV system. They then completed a point-to-point navigation task with the UAV in both high and low automation conditions. Participants completed the NASA TLX and STS questionnaires after each flight.,Participants operated a UAV to complete a point-to-point navigation task.,Spidex,Unmanned Aerial Vehicles (UAVs),Research,Navigation,Path Following,minimal interaction,Participants interacted with the UAV by controlling it to complete a navigation task.,real-world,Participants interacted with a physical UAV in a real-world setting.,physical,Participants interacted with a physical UAV.,shared control (fixed rules),"The UAV had two automation levels: low (0% scaling) and high (on-board automated scaling), which were fixed.",Questionnaires; Physiological Measures,NASA Task Load Index (NASA-TLX); System Usability Scale (SUS),Physiological Signals,Trust was measured using questionnaires and physiological data (GSR).,"parametric models (e.g., regression)",Decision trees were used to classify trust states based on GSR data.,Empirical HRI Studies,Physical Robot Studies,Indirect Manipulation,Trust was indirectly manipulated by providing verbal and written cues about the system's reliability. The automation level of the UAV was also manipulated.,"The manipulation of trust was not detected by the questionnaires, but the GSR data was able to classify trust states. The automation level had a marginal effect on physical workload.","The pen-and-paper assessment of trust was not useful, but physiological differences were detectable using GSR. Participants crashed slightly more often in the high automation condition, which was unexpected.","GSR data, analyzed with machine learning, can classify trust states with 80% accuracy, suggesting a physiological indicator for trust.","The human participant controlled a UAV to navigate to a series of points. The robot (UAV) moved according to the human's input, with either high or low automation assistance.",Mixed-effects model; decision trees,"Linear mixed-effects ANOVAs were used to analyze the NASA TLX subscales and the System Trust Scale (STS) scores, with subject number as a random factor. The ANOVAs aimed to identify main effects of automation and trust, as well as interaction effects. Decision trees were used in a machine learning analysis to classify trust states based on GSR data. The analysis aimed to determine if GSR features could differentiate between high and low trust states.",TRUE,Robot-autonomy; Robot-verbal-communication-content,Robot-autonomy,Robot-verbal-communication-content,"The study manipulated the level of automation of the UAV, which falls under 'Robot-autonomy'. The paper states, 'The low automation condition used 0% scaling on the UAV controller, while the high automation condition used the on-board automated scaling algorithm which compensated for abrupt changes in UAV throttle acceleration, thus providing for easier flight characteristics.' This is a direct manipulation of the robot's decision authority and control. The study also used verbal and written cues to induce high or low trust, described as 'multiple written and verbal cues at the start of the testing session, indicating that the system was either 'very trustworthy / reliable', or 'not very trustworthy / reliable''. This is a manipulation of the content of the verbal communication, thus 'Robot-verbal-communication-content'. The results section indicates that the automation level had a marginal effect on physical workload, and the GSR data was able to classify trust states, indicating that 'Robot-autonomy' impacted trust. However, the questionnaires did not detect a difference in trust based on the verbal cues, indicating that 'Robot-verbal-communication-content' did not impact trust.",10.1145/3369457.3369509,https://dl.acm.org/doi/10.1145/3369457.3369509,"Trust in automation is an essential precursor to system adoption and use. Given the emerging wave of autonomous systems available for public consumption and the resources devoted to this trend, it’s important to understand trust, and how to measure it. Further, the level of performance demonstrated by a system can affect trust in that system. As such, proper design of an autonomous system can be facilitated by measuring trust in such systems. Rather than relying only on traditional methods of measuring trust, such as pen and paper, or behavioural markers, this work extends previous research by investigating psychophysiological markers for trust, using Galvanic Skin Response (GSR) and machine learning. We induced high vs. low trust states in amateur unmanned aerial vehicle (UAV) operators, and manipulated the automation level of the UAV. We collected workload and trust ratings during and after flying a UAV. Despite moderate results with traditional metrics (NASA TLX, and the System Trust Scale), we were able to classify trust states based on the GSR data with 80% accuracy. This research forms part of our ongoing work on developing a model for the relation between automation, and user trust and cognitive load."
"Loghmani, Mohammad Reza; Haider, Clara; Chebotarev, Yegor; Tsiourti, Christiana; Vincze, Markus",Effects of Task-Dependent Robot Errors on Trust in Human-Robot Interaction: A Pilot Study,2019,1,10,9,1,1 participant was excluded due to technical issues,Controlled Lab Environment,between-subjects,"Participants completed three tasks: 'Name the Object', 'Scavenger Hunt', and 'Escape the Room'. The robot failed in either the first or second task for some participants. Trust was measured in the third task.","Participants interacted with a robot in three tasks, including naming objects, a scavenger hunt, and escaping a simulated burning room.",Pepper,Humanoid Robots; Expressive Robots,Research; Social,Navigation,Guiding,minimal interaction,Participants followed verbal instructions from the robot.,real-world,The study used a real robot in a lab setting with some elements to simulate a real-world scenario.,physical,The study used a physical robot for the interaction.,wizard of oz (directly controlled),The robot's actions were directly controlled by a human operator.,Behavioral Measures; Questionnaires; Custom Scales,Negative Attitude towards Robots Scale (NARS),,"Trust was measured using a questionnaire, a custom open-ended questionnaire, and observation of whether participants followed the robot's instructions.",no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's performance was manipulated by having it fail in either a related or unrelated task, and the final task was framed as a high-risk scenario.","Participants were more likely to trust the robot when it had previously failed in an unrelated task, although this was not statistically significant.","One participant did not follow the robot's instructions because the robot did not give instructions immediately. One participant reported not trusting the robot but still followed its instructions. The NARS scores did not show a clear pattern, but one participant had a large change in score due to emotional engagement.","The study found preliminary evidence that users tend to trust a robot more when it fails in an unrelated task compared to a related task, although this was not statistically significant.",The robot provided verbal instructions to guide the participant in a scavenger hunt and escape task. The human followed the robot's instructions and interacted with objects in the environment.,,"No statistical tests were explicitly mentioned in the paper. The analysis was primarily descriptive, focusing on observed behaviors and changes in questionnaire scores. The authors noted that the results were not statistically significant due to the small sample size.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the robot's accuracy by having it fail in either the 'Name the Object' task (tA) or the 'Scavenger Hunt' task (tB1) for different groups of participants. The robot's performance in these tasks directly influenced its success rate, which is why 'Robot-accuracy' is the appropriate category. The paper states, 'The experiments were conducted by organizing the participants in three groups: one in which the robot fails task tA, on in which the robot fails task tB1 and a control group in which the robot never fails.' This manipulation of robot performance was intended to see how it impacted trust in the final 'Escape the Room' task. The results showed that participants were more likely to trust the robot when it had previously failed in an unrelated task, indicating that 'Robot-accuracy' impacted trust. There were no other factors that were manipulated in the study.",10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00072,https://ieeexplore.ieee.org/document/9060178/,"The growing diffusion of robotics in our daily life demands a deeper understanding of the mechanisms of trust in human-robot interaction. The performance of a robot is one of the most important factors inﬂuencing the trust of a human user. However, it is still unclear whether the circumstances in which a robot fails to affect the users trust. We investigate how the perception of robot failures may inﬂuence the willingness of people to cooperate with the robot by following its instructions in a time-critical task. We conducted an experiment in which participants interacted with a robot that had previously failed in a related or an unrelated task. We hypothesized that users’ observed and self-reported trust ratings would be higher in the condition where the robot has previously failed in an unrelated task. A proof-of-concept study with nine participants timidly conﬁrms our hypothesis. At the same time, our results reveal some ﬂaws in the design experimental, and encourage a future large scale study."
"Lohani, Monika; Stokes, Charlene; McCoy, Marissa; Bailey, Christopher A.; Joshi, Aditi; Rivers, Susan E.",Perceived role of physiological sensors impacts trust and reliance on robots,2016,1,44,44,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were introduced to a virtual environment and a robot avatar. They completed two tasks with the robot, one involving ranking items for a moon landing scenario and another involving analyzing ISR data. Participants were randomly assigned to either a socioemotional interaction or a control interaction with the robot before the tasks. Trust and reliance were measured after each task, and mentalizing propensity was measured using a custom scale.",Participants completed two collaborative ranking tasks with a virtual robot: a moon landing scenario and an ISR data analysis task.,Unspecified,Humanoid Robots,Research; Social,Evaluation,Ranking,minimal interaction,Participants interacted with a virtual robot in a simulated environment.,simulation,The interaction took place in a virtual environment with avatars.,simulated,The robot was represented as a virtual avatar.,pre-programmed (non-adaptive),The robot's behavior was pre-programmed and did not adapt to the user.,Behavioral Measures; Questionnaires; Custom Scales,Interpersonal Trust Scale/Questionnaire,,"Trust was measured using a questionnaire, a custom scale for mentalizing propensity, and behavioral reliance based on task performance.","parametric models (e.g., regression)","Regression models were used to examine the relationship between mentalizing propensity, trust, and reliance.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the social behavior of the virtual robot by having it engage in socioemotional interactions with participants in the experimental group, while the control group had a non-emotional interaction. This was intended to influence participants' mentalizing propensity and subsequent trust and reliance.",Mentalizing propensity positively predicted trust and reliance on the robot. Socioemotional interactions marginally increased mentalizing propensity.,The mediation effect of trust between mentalizing propensity and reliance was significant for task 2 but not for task 1. The experimental group reported a marginally higher mentalizing propensity than the control group.,"Mentalizing propensity, or the belief in the ability of physiological sensors to facilitate mentalization in a robot, positively predicts trust and reliance on the robot.","The robot and human avatars worked together to rank items in two different scenarios. The robot provided its ranking, and the human could adjust their ranking after a discussion with the robot. The human also completed a questionnaire to measure trust and mentalizing propensity.",Linear regression; t-test; Sobel test,"The study used regression analysis to examine the relationship between mentalizing propensity and both trust and reliance on the robot for two different tasks. A t-test was used to compare the mentalizing propensity scores between the experimental and control groups. Finally, Sobel's test was used to assess the mediation effect of trust between mentalizing propensity and reliance.",TRUE,Robot-social-attitude,Robot-social-attitude,,"The study manipulated the social behavior of the virtual robot by having it engage in socioemotional interactions with participants in the experimental group, while the control group had a non-emotional interaction. This manipulation is categorized as 'Robot-social-attitude' because it directly altered the robot's approach to the interaction, making it more or less friendly and engaging. The paper states, 'Participants in the experimental group engaged in socioemotional interactions derived from emotional intelligence literature [33], while those in the control condition completed an alternative task that was matched on duration and engagement.' The results showed that the experimental group, which experienced the socioemotional interaction, reported a marginally higher mentalizing propensity, which in turn influenced trust and reliance. Therefore, 'Robot-social-attitude' is also listed as a factor that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1109/ROMAN.2016.7745166,http://ieeexplore.ieee.org/document/7745166/,"Physiological sensors can be instrumental in facilitating complex behavioral and social interactions between humans and robots. However, there is a limited understanding of the psychological implications of incorporating such sensors in the context of human-robot interaction. We show that perception of sensors’ role can act as an individual difference factor that can implicitly inﬂuence trust and reliance on a robot. We argue that some people may believe in the ability of physiological sensors to facilitate mentalization (or understanding) of humans. We refer to this tendency as mentalizing propensity. Such implicit differences in mentalizing propensity may inﬂuence participants’ trust and reliance on a robot, as found in the current study. We also found that trust can mediate the inﬂuence of mentalizing propensity on reliance. Furthermore, we show that social interactions with a robot inﬂuenced participants’ mentalizing propensity. The current work shows that implicit understanding of sensors’ role in human-robot interaction context is an important attribute to assess in future studies on trust and reliance."
"Lohani, Monika; Stokes, Charlene; McCoy, Marissa; Bailey, Christopher A.; Rivers, Susan E.",Social interaction moderates human-robot trust-reliance relationship and improves stress coping,2016,1,44,44,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were introduced to a virtual environment, then engaged in either a social or information-focused interaction with a virtual robot, and finally completed a collaborative task.",Participants and a virtual robot collaborated on a task to rank items for an emergency moon landing.,Unspecified,Humanoid Robots,Research; Social,Social,Conversation,minimal interaction,Participants interacted with a virtual robot in a simulated environment.,simulation,The interaction took place in a virtual simulation.,simulated,The robot was a virtual avatar in a simulation.,pre-programmed (non-adaptive),The robot followed a pre-set dialogue in both conditions.,Questionnaires; Behavioral Measures,,Performance Metrics,Trust was measured using a questionnaire and behavioral reliance on the robot.,"parametric models (e.g., regression)","A regression model was used to analyze the relationship between trust, condition, and reliance.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's dialogue was manipulated to be either social and emotional or information-focused, influencing the social context of the interaction.",Social interaction increased the positive relationship between trust and reliance on the robot.,"The study found that social interaction moderated the trust-reliance relationship, which was not observed in previous studies with non-social interactions.",Social and emotional interactions moderated the relationship between trust and reliance on a robot teammate and positively impacted perceived ability to cope with stress.,"The robot provided probability calculations for items, and the human ranked the items based on their importance for a moon landing, incorporating the robot's input.",Linear regression; t-test,"A regression model was used to test the moderation hypothesis, examining the interaction between condition (social vs. information-focused), trust, and behavioral reliance. Specifically, it tested if the relationship between trust and reliance was different based on the condition. Additionally, a t-test was used to compare the perceived stress coping abilities between the experimental and control conditions.",TRUE,Robot-verbal-communication-content; Robot-social-attitude,Robot-verbal-communication-content; Robot-social-attitude,,"The study manipulated the robot's dialogue to be either social and emotional (experimental condition) or information-focused (control condition). This directly changes the content of the robot's verbal communication, thus 'Robot-verbal-communication-content' is appropriate. The social and emotional dialogue also aimed to create a positive relationship, which is a change in the robot's social approach, thus 'Robot-social-attitude' is also appropriate. The results showed that the social interaction (both content and attitude) moderated the trust-reliance relationship, indicating that both 'Robot-verbal-communication-content' and 'Robot-social-attitude' impacted trust. There were no factors that were manipulated that did not impact trust.",10.1109/HRI.2016.7451811,http://ieeexplore.ieee.org/document/7451811/,"Previous work with non-social human-robot interaction has found no links between trust and reliance [1]. The current study tested the question: Can social interactions moderate trust-reliance relationship? Human-robot interactions may share similar characteristics to social and emotional interactions between humans. We investigated how social and emotional human-robot interactions moderate the trust-reliance relationship and impacts perceived stress coping abilities. In the experimental condition, social and emotional interactions were used to guide the dialogue between a participant and a virtual robot in order to promote team building. In the matched control condition, the interactions were information-focused, without social or emotional interaction. We show that social interaction moderated the effect of trust on reliance such that higher trust led to greater reliance on the robot. The experimental condition also had higher perceived stress coping abilities. These ﬁndings contribute to the existing literature and suggest that creating deeper social and emotional interactions with a robot teammate can facilitate human-robot partnership."
"Lohani, Monika; Stokes, Charlene K.; Oden, Kevin B.; Frazier, Spencer J.; Landers, Kevin J.; Craven, Patrick L.; Lawton, Durrell V.; McCoy, Marissa; Macannuco, David J.",The Impact of Non-Technical skills on Trust and Stress,2017,1,4,4,0,No participants were excluded,Real-World Environment,between-subjects,"Participants were randomly assigned to either the experiment or control group. They were introduced to a virtual robot teammate and completed a mission over two days. The experiment group's robot used non-technical skills, while the control group's robot did not. Participants completed surveys and focus groups after the mission.","Participants worked as a multi-intelligence processing, exploitation, and dissemination (PED) team to analyze enemy activities and convey findings to friendly forces.",Unspecified,Humanoid Robots,Research; Social,Social,Social Guidance/Coaching,minimal interaction,Participants interacted with a virtual robot through a screen.,simulation,The interaction took place in a simulated environment with a virtual robot.,simulated,The robot was a virtual representation on a screen.,wizard of oz (directly controlled),The virtual robot was controlled by a human confederate.,Questionnaires,,,Trust was measured using questionnaires.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's behavior was manipulated by including non-technical skills (rapport, communication, collaboration) in the experimental group, while the control group only had technical skills.",The experiment group reported higher trust in the robot compared to the control group.,The experiment group showed a larger drop in stress appraisal from the first to the second mission day compared to the control group.,"The integration of non-technical skills in a virtual robot led to higher trustworthiness and trust, and reduced stress in a high-stakes scenario.","The virtual robot provided technical support to the human analysts, while the human analysts analyzed enemy activities and conveyed findings. The robot in the experimental group also used non-technical skills to build rapport and facilitate communication.",Sign test; Sign test; Sign test,"The study used sign tests to compare the experiment and control groups on several variables. The first sign test compared the groups on an index of non-technical skills (rapport, communication, and collaboration). The second sign test compared the groups on trust-related variables. The third sign test compared the groups on stress appraisal items. All tests were two-tailed.",TRUE,Robot-social-attitude; Robot-verbal-communication-content; Robot-verbal-communication-style,Robot-social-attitude; Robot-verbal-communication-content; Robot-verbal-communication-style,,"The study manipulated the robot's behavior by including non-technical skills (rapport, communication, and collaboration) in the experimental group, while the control group only had technical skills. This manipulation is reflected in the following categories: 'Robot-social-attitude' because the robot in the experimental group was designed to build rapport and be more friendly, while the control group robot was neutral. 'Robot-verbal-communication-content' is used because the experimental group robot used scripted prompts to discuss social content (previous experiences, day-to-day activities, and hobbies) and team roles, while the control group robot only provided technical information. 'Robot-verbal-communication-style' is used because the experimental group robot used a more collaborative and encouraging tone, while the control group robot used a neutral tone. The results showed that the experimental group reported higher trust, indicating that these manipulated factors impacted trust. There were no factors that were manipulated that did not impact trust.",10.1145/3029798.3038321,https://dl.acm.org/doi/10.1145/3029798.3038321,"We present a case study that examined the impact of infusing nontechnical skills into the interactions between an expert analyst population and humanoid virtual robot teammates while executing a stressful real-world mission. In the experiment group, a virtual robot employed non-technical skills in addition to the technical skills only present in the control condition. We show that integration of these non-technical skills (rapport, cooperation, and collaboration) led to higher trustworthiness and trust in the virtual robot in the experiment group than the control group. Furthermore, the experiment group perceived lower threat-related appraisals than the control group. These findings contribute to the existing literature by suggesting that fostering non-technical skills with a virtual robot can serve as a bonding mechanism for human-robot teams to promote trustworthiness and trust, and reduce stress in high-stakes scenarios."
"Loizaga, Erlantz; Bastida, Leire; Sillaurren, Sara; Moya, Ana; Toledo, Nerea",Modelling and Measuring Trust in Human–Robot Collaboration,2024,1,55,51,4,4 participants were excluded due to purely technical reasons such as poor data recording,Controlled Lab Environment,between-subjects,"Participants were briefed about the study, provided demographic data, and completed a technology trust survey. They were then familiarized with the experimental setup and equipment. A biocalibration phase was performed, followed by a familiarization stage. During the experimental process, participants interacted with a virtual sensor and made trust decisions. Finally, an informal interview was conducted.","Participants played a variant of the Prisoner's Dilemma, known as the Inspection Game, where they had to detect if a virtual sensor was working correctly.",Unspecified,Industrial Robot Arms,Research,Game,Economic Game,minimal interaction,Participants interacted with a virtual sensor on a screen.,simulation,Participants interacted with a simulated sensor on a screen.,simulated,The robot was represented by a virtual sensor on a screen.,pre-programmed (non-adaptive),The virtual sensor followed a pre-programmed sequence of actions.,Questionnaires; Physiological Measures; Behavioral Measures,,Physiological Signals; Eye-tracking Data; Performance Metrics,"Trust was assessed using questionnaires, physiological signals, and behavioral measures.","deep learning (e.g., neural networks, reinforcement learning)",Deep learning techniques were used to model trust based on collected data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the virtual sensor was manipulated by changing the sequence of correct and incorrect readings, and participants were exposed to different initial conditions.",Participants who started with a series of random sensor readings showed higher trust variation compared to those who started with reliable readings.,"The study found that initial interactions with the system significantly influence the trajectory of future interactions, and that demographic factors did not significantly correlate with dispositional trust.",Initial interactions with a system significantly influence the trajectory of future trust dynamics.,"The robot, represented by a virtual sensor, provided readings about its state, and the human participant decided whether to trust the sensor's reading or not.",Mann-Whitney U,"The Mann-Whitney U test was used to check for disparities in the data distribution of different demographic groups (gender, age, and work role) in relation to both dispositional and perceived trust. It was also used to check the significance of trust rate differences among experimental models (Model-0 and Model-1) for both perfectly working and randomly working sensor scenarios. The test was chosen because the data distribution did not follow normality.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the reliability of the virtual sensor by changing the sequence of correct and incorrect readings. Participants were exposed to two different models: one where the sensor was initially reliable and another where it was initially unreliable. This manipulation directly affects the robot's accuracy in providing correct readings, which is a key factor in the task performance. The paper states, 'Both had the same trustworthiness (the virtual sensor provided a correct answer 75 out of the 100 iterations), but in one of the cases the machine worked perfectly during the first 20 iterations, whereas the second model presented 50% success rate during the same first 20 iterations.' This clearly indicates a manipulation of the robot's accuracy. The results showed that the initial interactions with the system significantly influenced the trajectory of future interactions, indicating that the manipulation of robot accuracy impacted trust. The paper states, 'Specifically, participants who start with a series of iterations exhibiting random behavior (participants interacting with Model-1) experience a higher dynamic variation compared to their counterparts (participants interacting with Model-0), thus exhibiting higher trust levels when the system functions properly and lower trust levels when the system's performance is erratic.' This shows that the manipulation of the robot's accuracy impacted trust. There were no other factors manipulated, and no factors were explicitly stated to have no impact on trust.",10.3390/app14051919,https://www.mdpi.com/2076-3417/14/5/1919,"Recognizing trust as a pivotal element for success within Human–Robot Collaboration (HRC) environments, this article examines its nature, exploring the different dimensions of trust, analysing the factors affecting each of them, and proposing alternatives for trust measurement. To do so, we designed an experimental procedure involving 50 participants interacting with a modified ‘Inspector game’ while we monitored their brain, electrodermal, respiratory, and ocular activities. This procedure allowed us to map dispositional (static individual baseline) and learned (dynamic, based on prior interactions) dimensions of trust, considering both demographic and psychophysiological aspects. Our findings challenge traditional assumptions regarding the dispositional dimension of trust and establish clear evidence that the first interactions are critical for the trust-building process and the temporal evolution of trust. By identifying more significant psychophysiological features for trust detection and underscoring the importance of individualized trust assessment, this research contributes to understanding the nature of trust in HRC. Such insights are crucial for enabling more seamless human–robot interaction in collaborative environments."
"Lokshina, Izabella; Kniezova, Jaroslava; Lanting, Cees",On Building Users’ Initial Trust in Autonomous Vehicles,2022,1,281,208,73,"the respondent did not finish the survey, the respondent had never used a taxi service or Uber, the respondent finished the survey within 4 minutes, the respondent provided conflicting answers",Online Crowdsourcing,,Participants completed an online survey with questions about their trust in autonomous taxis.,"Participants answered questions about their beliefs, attitudes, and intentions regarding using autonomous taxis.",Unspecified,Autonomous Vehicles,Other: Transportation,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the autonomous vehicles and did not interact with them.,media,The interaction was based on written descriptions of autonomous vehicles.,hypothetical,The robot was only described in text and not physically present.,not autonomous,The autonomous vehicle was described but did not perform any actions in the study.,Questionnaires; Custom Scales,,,Trust was measured using questionnaires and custom scales.,"parametric models (e.g., regression)",The study used PLS-SEM to model the relationships between trust factors.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study did not manipulate any factors; it assessed pre-existing beliefs and attitudes.,,"The study found that faith in intelligent machines, calculative cost/benefit perception, perceived societal benefits, organizational situation normality, and technology structural assurance significantly influence trusting beliefs. Subjective norm had a more substantial impact on trusting attitude than trusting intention and no significant impact on trusting beliefs.","Faith in intelligent machines and robots, calculative cost/benefit perception, perceived societal benefits, organizational situation normality, and technology structural assurance have substantial positive impacts on trusting beliefs in the context of autonomous taxis.","The robot is a hypothetical autonomous taxi, and the human participant answers survey questions about their trust in it.",t-test; pls-structural equation modelling (pls-sem),"The study used a t-test to check for non-response bias by comparing early and late responses. PLS-SEM was used to analyze the relationships between various constructs related to trust in autonomous taxis. This involved assessing the measurement model (reliability and validity) and the structural model (path coefficients and R-squared values) to test the proposed hypotheses about the influence of different factors on trusting beliefs, attitudes, and intentions.",FALSE,,,,"The study did not manipulate any factors. It was a survey-based study that assessed pre-existing beliefs and attitudes towards autonomous taxis. The study examined the relationships between various constructs related to trust, but there was no intentional manipulation of any independent variable to observe its effect on trust. The participants were asked to answer questions about their beliefs, attitudes, and intentions regarding using autonomous taxis. The study did not involve any interaction with autonomous vehicles, and no factors were manipulated by the researchers.",10.1016/j.procs.2021.12.205,https://linkinghub.elsevier.com/retrieve/pii/S1877050921024443,"Abstract With a high degree of uncertainty, taxi service companies begin incorporating autonomous vehicles in their transportation sWerivthiceas.hTighhe dauegthroeers oefxaumncineertathinetyfa, cttaoxris tshearvt iicnefluceonmcpeaunsierss’biengiitnialintrcuosrtpionraatuintognaoumtonuosmvoehuisclveesh. iTchleesy ienxttehnedirthtreaninsiptioarltattriuosnt mseorvdiecleas.ndThvealaiduathteortshexmamodinele itnhethfeacctoonrstetxhtaot finafnlueemnceerguisnegrst’ecinhintoiallogtryu.sTt ihne aiuntiotinaol mtruosutsmvoehdiecllesx.aTmhienyesexthtenidnftlhueeninceitioafl trust bmaosdeelfaacntodrsva(lridepatrestehnetimngodceolginitihvee,copnetresxotnoalfitayn, ecmalecruglaintigvete, cahnndoloingsyti.tuTthioenianliticaaltetgruosrtiems)odaenldexsuambjiencetisvtehenoinrmflusenocne toruf sttriunsgt bealsieeffsa, ctrourstin(rgepartetsiteundtien,gancdogtnriutsivtien,gpienrtseonntiaolinty. ,Fcinadlciunlgastivined, icaantde itnhsattitufatiothnailnciantteeglolirgiesn)t amnadchsiunbejse,ctciavlecunlaotrimves coonstt/rbuesntienfigt pberliceefpst,iotrnu,stpinergceaivtteitdudseo,ciaentdal tbruesnteinfigts,inotergnatinoinz.atFioindalinsgistuaintidoincanteortmhaatlitfya,ithanidn tiencthenlloigloegnyt mstaruchctiunreasl, acsaslucuralantcievearceospt/obtenteifaitl ipnedricceapttoiorsn,ofpuesrceersiv' etrdusstoincgiebtaellibefesneinfitsh,isocrgoannteizxat.tiTonhaelpsaiptuearticoonntrniobrumteaslittoy,thaendtheteocrhynaonlodgpyrascttriuccetubryalexatsesnudrainngcethaereiniptoiatlentrtuiastl minodidcealtoarnsdofexuasmerisn'itnrgustrinusgtibnegliefafsctionrsthiins cthoentceoxnt.teTxhteopfaapnerecmoenrtgriibnugtetsectohntohleogthyeothryatarnedqupirraecstiucendbeyrsetaxntednindgingustehres’inmiteianltatrluitsyt bmeofodreel laanudncehxianmg ianuitnognotrmusotuinsgvefhaicctloerssoin athweidceonscteaxlet.of an emerging technology that requires understanding users’ mentality before launching autonomous vehicles on a wide scale."
"Long, Shelby K.; Karpinsky, Nicole D.; Bliss, James P.",Trust of Simulated Robotic Peacekeepers among Resident and Expatriate Americans,2017,1,48,48,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants completed questionnaires, then performed a shopping task in a virtual environment where a robotic peacekeeper (RPK) interrupted them six times, asking them to relinquish an item. Participants then completed a trust questionnaire about their interaction with the RPK.",Participants completed a shopping task in a virtual environment and were interrupted by a robotic peacekeeper (RPK) who asked them to relinquish an item.,Unspecified,Unmanned Ground Vehicles,Research,Social,Persuasion,minimal interaction,Participants interacted with a virtual robot in a simulated environment.,simulation,The interaction took place in a virtual environment.,simulated,The robot was a virtual representation in a simulation.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires; Custom Scales,Interpersonal Trust Scale/Questionnaire; Jian et al. Trust Scale,,Trust was measured using questionnaires and a custom scale adapted from a previous study.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's directive style (analytic or comparative) and the trust element (emotion, behavior, cognition) were manipulated to influence trust.",Americans living in the US trusted the RPK more than Americans in China. Participants reported higher trust for the analytic directive style and emotionally based appeals.,"Americans residing in Japan were more trusting initially than Americans in the US or China. The purpose subscale was low for all participants, suggesting that the concept of robotic peacekeepers is not yet well-established.","Americans residing in Japan were more trusting initially than Americans in the US or China, and Americans living in the US trusted the RPK more than Americans in China.","The robot interrupted participants during a shopping task, asking them to relinquish an item. The human participant decided whether to comply with the robot's request.",ANOVA; ANOVA; scheffé post-hoc tests; Bonferroni correction; mauchly's test of sphericity; greenhouse-geisser correction; levene's test of equality of error variances,"The study used a one-way ANOVA to test for differences in interpersonal trust across the three country groups. A mixed ANOVA was used to test for differences in the overall specific trust questionnaire across groups for trust element and directive style, and also for each of the subscales (performance, process, purpose). Mauchly's Test of Sphericity was used to check the assumption of sphericity, and Greenhouse-Geisser correction was applied when this assumption was violated. Levene's Test of Equality of Error Variances was used to check for homogeneity of variance. Scheffé post-hoc tests were used to identify specific group differences in interpersonal trust. Bonferroni tests were used as follow-up tests for the mixed ANOVAs, adjusting for multiple comparisons.",TRUE,Robot-verbal-communication-style; Robot-emotional-display,Robot-verbal-communication-style; Robot-emotional-display,,"The study manipulated the 'directive style' of the robot's communication, which is categorized as 'Robot-verbal-communication-style'. The paper states, 'Each interaction included a different combination of directive style and trust element.' The directive style was either 'analytic' or 'comparative'. The study also manipulated the 'trust element' of the robot's communication, which is categorized as 'Robot-emotional-display'. The trust element was either 'emotion', 'behavior', or 'cognition'. The results section indicates that both directive style and trust element impacted trust levels, as there was a significant two-way interaction between them. Specifically, participants reported higher trust for the analytic directive style and emotionally based appeals. There were no factors that were manipulated that did not impact trust.",10.1177/1541931213602005,http://journals.sagepub.com/doi/10.1177/1541931213602005,"Researchers have heavily debated the definition and role of trust in human behavior over the past few decades. As robots begin to be used more often, particularly in international military applications, understanding human-robot trust becomes increasingly important. The current study aims to investigate trust differences in robotic peacekeepers between Americans living in the United States, China, and Japan using a simulated environment. We predicted that trust in robots would differ as a function of culture. Results showed that Americans residing in Japan were significantly more trusting than Americans in the United States or China overall. Further, Americans living in America trusted robotic peacekeepers significantly more than Americans residing in China. This suggests that people who adopt a certain trust framework are those who have chosen to live abroad, but more research is needed to understand the differences between resident and expatriate Americans."
"Losey, Dylan P.; Sadigh, Dorsa",Robots that Take Advantage of Human Trust,2019,1,10,10,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants first completed a familiarization phase without robot intervention, then completed five trials with a Nash robot strategy and five trials with a Trust robot strategy, with the order counterbalanced.",Participants used a keyboard to control a 1-DoF cart in the CartPole environment to balance an inverted pendulum.,Unspecified,Other,Research,Game,Cooperative Game,minimal interaction,Participants interacted with a simulated robot through a keyboard interface.,simulation,The interaction was conducted in a simulated environment.,simulated,The robot was a simulated entity in the CartPole environment.,shared control (fixed rules),The robot used fixed rules to respond to the human user.,Questionnaires; Behavioral Measures,,Performance Metrics,Trust was assessed using a survey and behavioral measures of time upright and human effort.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's behavior was manipulated to either assist the user (Nash) or intentionally unbalance the pendulum (Trust), influencing the user's perception of the robot's goal.","The Trust robot led to increased user involvement, suggesting that the robot's actions influenced the user's perception of the robot's objective.","Participants incorrectly believed that the Trust robot did not want to keep the pendulum upright, and they adapted their approach more when interacting with the Trust robot.",A robot that reasons over human trust can increase user involvement during collaborative tasks by intentionally unbalancing the pendulum.,"The robot either assisted the user to balance the pendulum (Nash) or intentionally unbalanced the pendulum (Trust), while the human used a keyboard to control the cart and keep the pendulum upright.",t-test,"Paired t-tests were used to compare the means of different conditions within the same group of participants. Specifically, they were used to compare the Time Upright and Human Effort between the Nash and Trust robot strategies, as well as to compare the subjective responses to survey questions related to understanding, objectives, and adaptation between the two robot strategies.",TRUE,Robot-task-strategy,Robot-task-strategy,,"The study manipulated the robot's task strategy by having it either assist the user to balance the pendulum (Nash strategy) or intentionally unbalance the pendulum (Trust strategy). This manipulation directly influenced the user's perception of the robot's objective and their subsequent behavior. The paper states, 'Under the Trust strategy, u r = sign(φ) at the start of the task to intentionally unbalance the pendulum.' This clearly indicates a change in the robot's approach to the task, not its accuracy or communication style, but rather its strategy. The results showed that the Trust strategy led to increased user involvement and a belief that the robot had a different objective, indicating that this manipulation impacted trust. There were no other factors manipulated in the study.",,http://arxiv.org/abs/1909.05777,"Humans often assume that robots are rational. We believe robots take optimal actions given their objective; hence, when we are uncertain about what the robot's objective is, we interpret the robot's actions as optimal with respect to our estimate of its objective. This approach makes sense when robots straightforwardly optimize their objective, and enables humans to learn what the robot is trying to achieve. However, our insight is that---when robots are aware that humans learn by trusting that the robot actions are rational---intelligent robots do not act as the human expects; instead, they take advantage of the human's trust, and exploit this trust to more efficiently optimize their own objective. In this paper, we formally model instances of human-robot interaction (HRI) where the human does not know the robot's objective using a two-player game. We formulate different ways in which the robot can model the uncertain human, and compare solutions of this game when the robot has conservative, optimistic, rational, and trusting human models. In an offline linear-quadratic case study and a real-time user study, we show that trusting human models can naturally lead to communicative robot behavior, which influences end-users and increases their involvement."
"Lu, Y.; Sarter, N.",Eye Tracking: A Process-Oriented Method for Inferring Trust in Automation as a Function of Priming and System Reliability,2019,1,35,32,3,3 participants were excluded due to malfunctions of the eye tracker or incomplete data,Controlled Lab Environment,mixed design,"Participants were trained on a target identification task, then completed a 30-minute scenario where they monitored UAV video feeds and rated their trust every 2 minutes. Half of the participants were informed about the reliability of the UAVs before the experiment.","Participants monitored six UAV video feeds to detect military targets, confirming or rejecting targets identified by the automation and manually identifying missed targets.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulation of UAVs through a computer interface.,simulation,Participants interacted with a simulated environment of UAV video feeds.,simulated,The robots were represented as simulated video feeds on a computer screen.,shared control (fixed rules),"The UAVs autonomously scanned for targets, but their reliability was fixed.",Questionnaires; Real-time Trust Measures; Behavioral Measures; Performance-Based Measures; Physiological Measures,,Eye-tracking Data; Performance Metrics,"Trust was assessed using subjective ratings, eye-tracking data, and performance metrics.","parametric models (e.g., regression)","Linear mixed models were used to analyze the relationship between trust, eye-tracking metrics, and performance.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"Automation reliability was manipulated by varying the accuracy of the UAVs, and human expectations were manipulated by providing or withholding reliability information.",Lower reliability led to lower trust ratings and increased monitoring behavior. Priming did not affect subjective trust ratings but did influence monitoring behavior.,"Priming did not affect subjective trust ratings, but it did influence eye-tracking metrics. Total fixation duration differed between reliability levels only in the priming condition. There was a dissociation between subjective trust ratings and eye movements.","Eye tracking is an effective technique for inferring changes in operator trust levels in real time, complementing other trust measures.","The UAVs automatically scanned for targets and highlighted potential targets on the screen. The human participant monitored the UAV feeds, confirmed or rejected the highlighted targets, and manually identified missed targets.",Mixed-effects model; Simple slopes analysis; Pearson correlation,"The study used linear mixed models to analyze the effects of automation reliability and priming on subjective trust ratings, eye-tracking metrics, and performance data. Participant number was included as a random effect. Simple effect analysis was used to examine the interaction between automation reliability and priming on total fixation duration. Correlation analysis was used to validate the eye tracking metrics by examining their relationship with subjective trust ratings. Specifically, Pearson correlation coefficients were calculated.",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-accuracy,Robot-verbal-communication-content,"The study manipulated 'Robot-accuracy' by varying the reliability of the UAVs (95% vs. 50% correct target identification). This is explicitly stated in the 'Experiment Design' section: 'Half of the UAVs were highly reliable (95% correct) while the other three UAVs were only 50% reliable.' This manipulation directly affected the performance of the automation, making 'Robot-accuracy' the most appropriate category. The study also manipulated 'Robot-verbal-communication-content' by providing or withholding information about the reliability of the UAVs before the experiment. This is described in the 'Experiment Design' section: 'Half of the participants were informed about the overall reliability of the two groups of UAVs in advance of the experiment while the other half did not receive any reliability information.' This manipulation involves the content of the information provided to the participants, making 'Robot-verbal-communication-content' the most appropriate category. The results section shows that 'Robot-accuracy' impacted trust, as participants rated the highly reliable UAVs as significantly more trustworthy than the low-reliability UAVs. However, 'Robot-verbal-communication-content' did not significantly affect subjective trust ratings, as stated in the 'Results' section: 'There was no significant effect of priming, nor was there an interaction between reliability and priming'.",10.1109/THMS.2019.2930980,,"Trust miscalibration, a mismatch between a person's trust in automation and the system's actual reliability, can lead to either misuse or disuse of automation. Existing techniques to measure trust (e.g., subjective ratings) tend to be discrete and/or disruptive. To better understand and support the process of trust calibration, a nonintrusive continuous measure of trust is needed. The present study investigated whether eye tracking can serve this purpose. In the context of an unmanned aerial vehicle simulation, participants monitored six video feeds to detect predefined targets with the assistance of onboard automation. Automation reliability (95% versus 50% reliable) and priming (reliability information provided or not) were manipulated. Eye movement data, subjective trust ratings, and performance data were collected. The eye tracking data show that people visit more frequently and spend more time on low reliable automation. Priming information could also affect the participants' trust level and trigger different types of searching behavior, as reflected in eye tracking data such as mean saccade amplitude. In summary, these findings confirm that eye tracking is a very promising tool for inferring trust and supporting future research into trust calibration."
"Lu, Yidu; Sarter, Nadine",Modeling and inferring human trust in automation based on real- time eye tracking data,2020,1,20,16,4,4 participants were excluded due to incomplete data,Controlled Lab Environment,within-subjects,Participants completed a UAV simulation task with varying automation reliability (95% and 50%) while eye-tracking data was collected. Baseline data was collected for both high and low reliability scenarios. Participants performed a visual search task and a tracking task simultaneously. They were informed about the system reliability in advance. The sequence of reliability levels was randomized.,"Participants performed a visual search task to detect military targets in UAV video feeds, and a tracking task using a joystick to keep a target circle within larger circles.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulation of a UAV system.,simulation,Participants interacted with a computer-based simulation of a UAV control system.,simulated,The robot was represented as a simulated UAV in a computer environment.,shared control (fixed rules),"The automation provided target detection assistance, but participants could override the automation's assessment.",Real-time Trust Measures,,Eye-tracking Data,Trust was inferred in real-time using eye-tracking data.,"deep learning (e.g., neural networks, reinforcement learning)",Deep learning methods (MLP and CNN) were used to model trust based on eye-tracking data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the automated target detection system was manipulated to be either high (95%) or low (50%), which directly influenced the robot's performance.","The study aimed to infer trust levels based on eye-tracking data, and the results showed that eye-tracking data can be used to infer trust levels with high accuracy. The study did not explicitly measure the impact of the manipulation on trust, but rather used the manipulation to create high and low trust conditions.","Individual-level modeling achieved significantly higher accuracy than group-level modeling, suggesting substantial individual differences in trust calibration. The use of raw eye movement data with CNN achieved similar accuracy to using extracted eye-tracking metrics, indicating that complex feature extraction may not be necessary.","Eye tracking data can be used to infer trust levels in real time with high accuracy, especially when modeling at an individual level.","The robot (simulated UAV) provided automated target detection assistance, highlighting potential targets in the video feeds. The human participant monitored the video feeds, decided whether to review the highlighted scenes, and either agreed or disagreed with the automation's assessment. They also performed a tracking task simultaneously.",linear regression (lr); k-nearest neighbors (knn); random forest (rf); multi-layer perceptron (mlp); convolutional neural networks (cnn),"The study compared the performance of five different classification methods (Linear Regression, k-Nearest Neighbors, Random Forest, Multi-Layer Perceptron, and Convolutional Neural Networks) for inferring trust levels based on eye-tracking data. Linear Regression was used as a baseline. kNN classifies based on feature similarity. RF classifies based on votes from trained decision trees. MLP and CNN were used for modeling raw eye-tracking data. The accuracy of each method was assessed by comparing the accuracy of inferring trust levels in the test datasets. The study used these methods to compare individual-level modeling with group-level modeling, and to compare the use of extracted eye-tracking metrics with raw eye movement data as input.",TRUE,Robot-accuracy,Robot-accuracy,,"The study explicitly manipulated the reliability of the automated target detection system, which directly impacts the robot's accuracy in identifying targets. The system was set to either 95% or 50% reliability. This manipulation of the robot's performance directly influenced the participants' trust levels, as the study aimed to infer trust based on eye-tracking data under these different reliability conditions. The paper states, 'participants were exposed to varying levels of reliability of an automated target detection system.' This clearly indicates a manipulation of the robot's accuracy. The study did not explicitly measure the impact of the manipulation on trust, but rather used the manipulation to create high and low trust conditions, which were then used to train the models. The study did not manipulate any other factors from the provided list.",10.1177/1071181320641078,http://journals.sagepub.com/doi/10.1177/1071181320641078,"Trust miscalibration remains a major challenge for human-machine interaction. It can lead to misuse or disuse of automated systems. To date, most trust research has relied on subjective ratings and behavioral or physiological data to assess trust. Those trust measurements are discrete, disruptive and quite difficult to implement. To better understand the process of trust calibration, we propose eye tracking as an unobtrusive method for inferring trust levels in real time. Using an Unmanned Aerial Vehicle simulation, participants were exposed to varying levels of reliability of an automated target detection system. Eye movement data were captured and labeled as high or low trust based on subjective trust ratings. Feature extraction and raw eye movement data were compared as input for multiple classification modeling methods. Accuracy rates of 92% and 80%, respectively, were achieved with individual-level and group-level modeling, suggesting that eye tracking is a promising technique for tracing trust levels."
"Lu, Peggy Pei-Ying; Konishi, Makoto; Sano, Shin; Hiruta, Sho; Nakagawa, Francis Ken","What Makes An Apology More Effective? Exploring Anthropomorphism, Individual Differences, And Emotion In Human-Automation Trust Repair",2022,1,1269,1074,195,Participants who did not pass the screening question or did not complete the whole survey were eliminated from the results,Online Crowdsourcing,mixed design,"Participants were presented with three trust violation scenarios, each followed by an apology representation (text, emoji, or cartoon car). They rated their trust before and after the apology, and their emotional response before and after the apology. Personality and trust stance were also measured.",Participants read scenarios about a GPS navigation system making errors and then evaluated the system's apology.,Unspecified,Autonomous Vehicles,Other: Navigation,Evaluation,Text Evaluation,passive observation,Participants only read about the interaction scenario.,media,Participants were presented with static images of the scenarios and apology messages.,simulated,The robot was represented through static images of a car interface.,not autonomous,The robot's actions were described in text and did not involve any real autonomy.,Questionnaires; Custom Scales,Ten Item Personality Inventory (TIPI),,"Trust was measured using a custom scale before and after the apology, and personality was measured using a questionnaire.","parametric models (e.g., regression)",The study used ANOVA and Pearson correlation analyses to examine the relationships between variables.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the criticality of the trust violation and the apology representation (text, emoji, or cartoon car) to see how they affected trust repair.","The study found that the criticality of the trust violation had a significant effect on trust repair, but the apology representation did not. Personality traits and emotional response also impacted trust repair.","The study found a polarizing perception of anthropomorphic cues, with some participants responding positively and others negatively. There was also a negative correlation between openness and conscientiousness and the effectiveness of apologies, which is inconsistent with some prior findings.","The effectiveness of apology messages was not significantly impacted by the anthropomorphic cues, but was negatively correlated with openness and conscientiousness, and positively correlated with post-trust-violation emotional response.","The robot, a simulated GPS navigation system, made errors of varying criticality. The human participant read the scenarios, evaluated the system's apology, and reported their emotional response and trust level.",ANOVA; Pearson correlation; chi-squared analyses; t-test; Tukey HSD,"The study used a mixed repeated-measures ANOVA to examine the main effects and interactions of violation type and apology representation on trust repair effectiveness. Pearson correlation analyses were used to assess the relationships between personality traits, trust stance, post-trust-violation emotional response, and the effectiveness of apologies. Chi-squared analyses were used to check for significant differences in gender and age across groups. T-tests were used to compare the effectiveness of apologies across genders. Finally, post-hoc Tukey HSD tests were used to examine significant differences between groups after significant ANOVA results.",TRUE,Robot-emotional-display; Task-complexity,Task-complexity,Robot-emotional-display,"The study manipulated the apology representation, which included text, emoji, and a cartoon car. These different representations can be categorized as 'Robot-emotional-display' because they varied in the presence and type of anthropomorphic cues designed to convey emotion. The study also manipulated the criticality of the trust violation (low, mid, and high), which can be categorized as 'Task-complexity' because it changed the severity of the error made by the navigation system, thus influencing the cognitive demands on the participant. The study found that the criticality of the trust violation ('Task-complexity') had a significant effect on trust repair, while the apology representation ('Robot-emotional-display') did not have a significant effect on trust repair. This is stated in the results section: 'A 3 (violation type: low, mid, and high criticality) × 3 (apology representation: text, emoji, and cartoon car) mixed repeated-measures analysis of variance (ANOVA) revealed a significant main effect of violation type (F (2, 3219) = 51.15, p < 0.001, η 2 = .03). The main effect of the apology representations and the interaction between violation type and the apology representations were, however, found to be non-significant for this study.'",,http://arxiv.org/abs/2211.10045,"Recent advances in technology have allowed an automation system to recognize its errors and repair trust more actively than ever. While previous research has called for further studies of different human factors and design features, their effect on human-automation trust repair scenarios remains unknown, especially concerning emotions. This paper seeks to ﬁll such gaps by investigating the impact of anthropomorphism, users’ individual differences, and emotional responses on human-automation trust repair. Our experiment manipulated various types of trust violations and apology messages with different emotionally expressive anthropomorphic cues. While no signiﬁcant effect from the different apology representations was found, our participants displayed polarizing attitudes toward the anthropomorphic cues. We also found that (1) some personality traits, such as openness and conscientiousness, negatively correlate with the effectiveness of the apology messages, and (2) a person’s emotional response toward a trust violation positively correlates with the effectiveness of the apology messages."
"Lucas, Gale M.; Boberg, Jill; Traum, David; Artstein, Ron; Gratch, Jonathan; Gainer, Alesia; Johnson, Emmanuel; Leuski, Anton; Nakano, Mikio",Getting to Know Each Other: The Role of Social Dialogue in Recovery from Errors in Social Robots,2018,1,165,154,11,11 users had to be excluded due to technical issues experienced during the session,Controlled Lab Environment,mixed design,"Participants completed two ranking tasks with a robot, with either a social dialogue or a control task in between; errors were introduced in either the first task, the second task, or neither.",Participants ranked items in two tasks (Lunar Survival and Save-the-Art) and engaged in dialogue with a robot.,Nao,Humanoid Robots; Expressive Robots,Research; Social,Social,Persuasion,minimal interaction,Participants interacted verbally with the robot during the ranking tasks.,real-world,Participants interacted with a physical robot in a lab setting.,physical,A physical NAO robot was used in the study.,wizard of oz (directly controlled),The robot was controlled by a human operator.,Questionnaires; Custom Scales,,Speech Data; Video Data,Trust was measured using a custom rapport scale and behavioral measures of influence.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,The timing of conversational errors and the presence of a social dialogue were manipulated to influence trust and influence.,"Errors early in the interaction reduced rapport but not influence, while errors later in the interaction reduced both rapport and influence, especially after a social dialogue.","The social dialogue acted as an intensifier, helping the robot recover from prior errors but backfiring if followed by errors, creating a contrast effect.","Social dialogue can mitigate the loss of trust caused by early errors, but it exacerbates the negative impact of errors that occur after a period of good performance.","The robot provided arguments to persuade participants to change their rankings of items in two tasks, and the human participant engaged in dialogue with the robot and re-ranked the items.",ANCOVA; ANOVA; tukey's lsd; Pearson correlation,"The study used ANCOVA to analyze the influence in both tasks, controlling for initial agreement with the robot. ANOVA was used to analyze rapport ratings after each task. Tukey's LSD post-hoc tests were used to determine significant differences between conditions. Correlations were used to examine the relationship between rapport and influence across the tasks.",TRUE,Robot-accuracy; Robot-social-timing,Robot-accuracy; Robot-social-timing,,"The study manipulated the timing of conversational errors made by the robot, which directly impacts the robot's accuracy in communication. Errors were introduced either in the first task, the second task, or not at all, which is a manipulation of 'Robot-accuracy'. Additionally, the presence or absence of a social dialogue (ice-breaker) between the two tasks was manipulated, which affects the timing of social interaction and thus is categorized as 'Robot-social-timing'. The results showed that both the timing of errors and the presence of the social dialogue impacted trust. Specifically, errors later in the interaction reduced rapport and influence, especially after a social dialogue, while early errors did not impact influence. The social dialogue acted as an intensifier, helping the robot recover from prior errors but backfiring if followed by errors. Therefore, both 'Robot-accuracy' and 'Robot-social-timing' are listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1145/3171221.3171258,https://dl.acm.org/doi/10.1145/3171221.3171258,"This work explores the extent to which social dialogue can mitigate (or exacerbate) the loss of trust caused when robots make conversational errors. Our study uses a NAO robot programmed to persuade users to agree with its rankings on two tasks. We perform two manipulations: (1) The timing of conversational errors – the robot exhibited errors either in the first task, the second task, or neither; (2) The presence of social dialogue – between the two tasks, users either engaged in a social dialogue with the robot or completed a control task. We found that the timing of the errors matters: replicating previous research, conversational errors reduce the robot’s influence in the second task, but not on the first task. Social dialogue interacts with the timing of errors, acting as an intensifier: social dialogue helps the robot recover from prior errors, and actually boosts subsequent influence; but social dialogue backfires if it is followed by errors, because it extends the period of good performance, creating a stronger contrast effect with the subsequent errors. The design of social robots should therefore be more careful to avoid errors after periods of good performance than early on in a dialogue."
"Luo, Ruijiao; Huang, Chao; Peng, Yuntao; Song, Boyi; Liu, Rui",Repairing Human Trust by Promptly Correcting Robot Mistakes with An Attention Transfer Model,2021,1,252,252,0,No participants were excluded,Online Crowdsourcing,within-subjects,Participants watched videos of a robot performing tasks and gave verbal instructions when they noticed an error. They assessed their trust before and after the robot corrected the error.,"Participants monitored a robot performing tasks and provided verbal alerts when errors occurred, then rated their trust in the robot before and after the robot corrected the error.",JACO robot arm model,Industrial Robot Arms,Research,Supervision,Monitoring,passive observation,Participants observed videos of the robot performing tasks.,media,The interaction was presented through videos of the robot performing tasks.,simulated,The robot was a simulated robot in a video.,shared control (adaptive),The robot corrected its behavior based on human verbal input.,Custom Scales; Questionnaires,,Speech Data; Video Data,Trust was measured using a self-reported 5-point scale before and after error correction.,no modeling,The study did not model trust computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's behavior was manipulated to include errors, and then corrected based on human input, influencing trust.","Trust levels increased significantly after the robot corrected its errors, although not fully restored to the initial level.","The study found that trust increased after error correction, but did not fully return to the initial level, suggesting that trust repair is possible but not complete. There was no significant difference in trust repair between the different error types.",Human trust in a robot can be repaired by promptly correcting robot mistakes using an attention transfer model.,"The robot performed pick-and-place tasks, making errors in action, region, pose, or spatial relation. The human monitored the robot's actions and provided verbal alerts when errors occurred, then rated their trust in the robot before and after the robot corrected the error.",Mann-Whitney U,"The Mann-Whitney U test was used to compare the trust levels of participants before and after the robot corrected its errors. The test was applied to each of the four error types to determine if there was a significant increase in trust after the correction. Additionally, the Mann-Whitney U test was used to compare the trust levels after correction across the four different error types to see if there were any significant differences in trust repair between the different error types.",TRUE,Robot-accuracy; Robot-autonomy,Robot-accuracy,,"The study manipulated 'Robot-accuracy' by having the robot make errors in its actions (wrong action, wrong region, wrong pose, and wrong spatial relation) and then correcting them based on human input. This directly impacted the robot's task performance and was a key factor in the study design. The study also manipulated 'Robot-autonomy' by having the robot correct its behavior based on human verbal input, which is a form of shared control. The paper states, 'With H2R-AT, a robot localizes human verbal concerns and makes prompt mistake corrections to avoid task failures...'. The study found that the robot's accuracy (or lack thereof) directly impacted trust levels, as trust increased after the robot corrected its errors. The paper states, 'All participants reported a higher level of trust after the correction, which shows the success of human trust repair.' There was no mention of the level of autonomy impacting trust, only that the robot corrected its behavior based on human input.",10.1109/CASE49439.2021.9551502,https://ieeexplore.ieee.org/document/9551502/,"In human-robot collaboration (HRC), human trust in the robot is the human expectation that a robot executes tasks with desired performance. A higher-level trust increases the willingness of a human operator to assign tasks, share plans, and reduce the interruption during robot executions, thereby facilitating human-robot integration both physically and mentally. However, due to real-world disturbances, robots inevitably make mistakes, decreasing human trust and further inﬂuencing collaboration. Trust is fragile and trust loss is triggered easily when robots show incapability of task executions, making the trust maintenance challenging. To maintain human trust, in this research, a trust repair framework is developed based on a human-to-robot attention transfer (H2R-AT) model and a user trust study. The rationale of this framework is that a prompt mistake correction restores human trust. With H2R-AT, a robot localizes human verbal concerns and makes prompt mistake corrections to avoid task failures in an early stage and to ﬁnally improve human trust. User trust study measures trust status before and after the behavior corrections to quantify the trust loss. Robot experiments were designed to cover four typical mistakes, wrong action, wrong region, wrong pose, and wrong spatial relation, validated the accuracy of H2R-AT in robot behavior corrections; a user trust study with 252 participants was conducted, and the changes in trust levels before and after corrections were evaluated. The effectiveness of the human trust repairing was evaluated by the mistake correction accuracy and the trust improvement."
"Lyons, Joseph B.; Ho, Nhut T.; Fergueson, William E.; Sadler, Garrett G.; Cals, Samantha D.; Richardson, Casey E.; Wilkins, Mark A.",Trust of an Automatic Ground Collision Avoidance Technology: A Fighter Pilot Perspective,2016,1,142,142,0,No participants were excluded,Survey/Interview,,Pilots completed an online survey about their trust in the AGCAS system.,Pilots answered questions about their trust in the AGCAS system.,Unspecified,Autonomous Vehicles,Other,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the system and answered questions.,media,The interaction was based on text descriptions of the system.,hypothetical,"The robot was described in text, without any visual representation.",fully autonomous (limited adaptation),"The system operates autonomously to prevent collisions, but with limited adaptation.",Questionnaires,,,Trust was measured using a questionnaire.,"parametric models (e.g., regression)",Multiple regression analysis was used to identify predictors of trust.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study assessed trust without intentionally altering any specific factors.,,"Transparency and benevolence did not uniquely predict trust, which was unexpected.","Reliability, perceived benefits, and automation schema were significant predictors of trust in the AGCAS system.","The robot (AGCAS) automatically takes control of the aircraft to prevent collisions, and the human pilot completes a survey about their trust in the system.",Linear regression,"Multiple regression analysis was used to identify the unique predictors of trust in the AGCAS system. The analysis examined the relationship between trust and several variables, including reliability, transparency, benevolence, perceived benefits, and automation schema. The goal was to determine which of these variables independently predicted trust after accounting for the correlations among the predictors.",FALSE,,,,"The study did not manipulate any factors. It was an observational study where pilots completed a survey about their trust in the AGCAS system. The study assessed the relationship between trust and several variables (reliability, transparency, benevolence, perceived benefits, and automation schema) but did not manipulate these variables. Therefore, no factors were intentionally manipulated by the researchers.",10.1037/mil0000124,https://www.tandfonline.com/doi/full/10.1037/mil0000124,"The present study examined the antecedents of trust among operational Air Force ﬁghter pilots for an automatic ground collision avoidance technology. This technology offered a platform with high face validity for studying trust in automation because it is an automatic system currently being used in operations by the Air Force. Pilots (N ϭ 142) responded to an online survey which asked about their attitudes toward the technology and assessed a number of psychological factors. Consistent with prior research on trust in automation, a number of trust antecedents were identiﬁed which corresponded to human factors, learned trust factors, and situational factors. Implications for the introduction of novel automatic systems into the military are discussed."
"Lyons, Joseph B.; Ho, Nhut T.; Hoffmann, Lauren C.; Sadler, Garrett G.; Lee Van Abel, Anna; Wilkins, Mark",Trust in Sensing Technologies and Human Wingmen: Analogies for Human-Machine Teams,2018,1,74,74,0,No participants were excluded,Survey/Interview,within-subjects,Pilots completed an online survey that gauged their acceptance of sensing technologies and their trust of a human wingman. They also completed items for measuring the Perfect Automation Schema.,Participants rated their comfort with various sensing technologies and their trust in different types of human wingmen.,Unspecified,Other,Other,Evaluation,Survey/Questionnaire Completion,passive observation,Participants responded to survey questions about hypothetical scenarios.,media,The study used text-based descriptions of hypothetical scenarios.,hypothetical,The study described hypothetical robots without any visual representation.,not autonomous,The study described hypothetical robots without any real autonomy.,Questionnaires; Custom Scales,,,Trust was measured using custom scales and questionnaires.,no modeling,The study did not model trust computationally.,Observational & Survey Studies,Quantitative Surveys,Indirect Manipulation,The study indirectly influenced trust by framing the task as involving different types of sensing technologies and different levels of wingman experience and familiarity.,Trust in a human wingman increased with familiarity and experience. The All-or-None (AoN) component of the Perfect Automation Schema was associated with less acceptance of sensing technologies.,"Pilots reported similar comfort levels for all sensing items, which was unexpected. The High Expectations (HE) component of the Perfect Automation Schema was not strongly associated with acceptance of the technologies, except for a marginal relationship with technologies that augment based on task performance.",Trust of a human wingman increased with greater familiarity and task experience. The All-or-None (AoN) component of the Perfect Automation Schema was associated with less acceptance of sensing technologies.,The robot was described as a hypothetical automated system with various sensing capabilities. The human participant's role was to rate their comfort with these technologies and their trust in different types of human wingmen.,ANOVA,"An ANOVA was used to examine the effect of wingman familiarity and experience on trust. Specifically, it tested whether trust varied based on the familiarity and experience of the wingman.",TRUE,Task-complexity; Teaming,Teaming,Task-complexity,"The study manipulated 'Task-complexity' by presenting different sensing technologies that varied in their complexity and the level of information they provided (e.g., monitoring heart rate vs. monitoring brain activity and changing behavior based on it). This is considered a manipulation of task complexity because the cognitive load associated with understanding and accepting these technologies varies. However, the results showed that pilots reported similar comfort levels for all sensing items, indicating that this manipulation did not impact trust. The study also manipulated 'Teaming' by varying the familiarity and experience of a human wingman. The paper states, 'pilots were asked to respond to three items which gauged their trust of a human wingman' where the wingman was described as 'unfamiliar and inexperienced', 'unfamiliar but experienced', and 'familiar and experienced'. The results showed that trust increased as familiarity and experience increased, indicating that this manipulation of teaming did impact trust. The manipulation of 'Teaming' is appropriate because the study is explicitly comparing trust in a human teammate with varying levels of experience and familiarity, which is a core aspect of team dynamics.",,http://link.springer.com/10.1007/978-3-319-91470-1_13,"The true value of a human-machine team (HMT) consisting of a capable human and an automated or autonomous system will depend, in part, on the richness and dynamic nature of the interactions and degree of shared awareness between the human and the technology. Contemporary views of HMTs emphasize the notion of bidirectional transparency, one type of which is Robot-of-Human (RoH) transparency. Technologies that are capable of RoH transparency may have awareness of human physiological and cognitive states, and adapt their behavior based on these states thus providing augmentation to operators. Yet despite the burgeoning presence of health monitoring devices, little is known about how humans feel about an automated system using sensing capabilities to augment them in a work environment. The current study provides some preliminary data on user acceptance of sensing capabilities on automated systems. The present research examines an emerging predictor of trust in automation, Perfect Automation Schema, as a predictor of trust in the sensing capabilities. Additionally, the current study examines trust of a human wingman as an analogy for looking at trust within the context of a HMT. The ﬁndings suggest that Perfect Automation Schema is related to some facets of sensing technology acceptance. Further, trust of a human wingman is contingent on familiarity and experience."
"Lyons, Joseph B.; Nam, Chang S.; Jessup, Sarah A.; Vo, Thy Q.; Wynne, Kevin T.",The Role of Individual Differences as Predictors of Trust in Autonomous Security Robots,2020,1,320,316,4,4 participants were dropped due to poor responding to attention check items,Online Crowdsourcing,between-subjects,Participants viewed a video of an ASR interacting with three confederates. The ASR denied access to one confederate and used a laser dazzler when the confederate was non-compliant. Participants then completed questionnaires.,Participants watched a video of a security robot and then answered questions about their trust and desired use of the robot.,Unspecified,Mobile Robots; Unmanned Ground Vehicles,Security,Evaluation,Survey/Questionnaire Completion,passive observation,Participants passively observed a video of the robot interacting with confederates.,media,The interaction was presented through a video.,physical,The robot was shown in a video.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions.,Questionnaires; Custom Scales,Reliance Intention Scale,,Trust was measured using a 10-item questionnaire.,"parametric models (e.g., regression)",Multiple regression was used to examine the relationship between individual differences and trust.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,The study assessed trust without intentionally altering any specific factors related to the robot or task.,,"The study found that agreeableness and high expectations were associated with higher trust, while all-or-none beliefs were associated with lower trust. Intellect was associated with lower desire to use the ASR.","Individual differences, specifically agreeableness and high expectations, are associated with higher trust and desired use of an autonomous security robot.","The robot acted as a security guard, scanning badges and using a laser dazzler on a non-compliant individual. The human participant watched the video and then completed questionnaires.",Pearson correlation; Linear regression,"The study used correlations to examine the relationships between individual difference variables (personality traits and Perfect Automation Schema) and outcome variables (trust and desired use of the robot). Significant correlations were used to select predictors for multiple regression analyses. Multiple regression was then used to examine the unique relationships between the individual difference variables and trust, desired public use, and desired military use of the robot.",FALSE,,,,"The study did not manipulate any factors related to the robot or task. The study assessed trust based on individual differences (personality and Perfect Automation Schema) without intentionally altering any specific factors related to the robot or task. The robot's actions were pre-programmed and consistent across all participants. Therefore, no factors were manipulated.",10.1109/ICHMS49158.2020.9209544,https://ieeexplore.ieee.org/document/9209544/,"This research used an Autonomous Security Robot (ASR) scenario to examine public reactions to a robot that possesses the authority and capability to inflict harm on a human. Individual differences in terms of personality and Perfect Automation Schema (PAS) were examined as predictors of trust in the ASR. Participants (N = 316) from Amazon Mechanical Turk (MTurk) rated their trust of the ASR and desire to use ASRs in public and military contexts following a 2-minute video depicting the robot interacting with three research confederates. The video showed the robot using force against one of the three confederates with a non-lethal device. Results demonstrated that individual differences factors were related to trust and desired use of the ASR. Agreeableness and both facets of the PAS (high expectations and all-or-none beliefs) demonstrated unique associations with trust using multiple regression techniques. Agreeableness, intellect, and high expectations were uniquely related to desired use for both public and military domains. This study showed that individual differences influence trust and one’s desired use of ASRs, demonstrating that societal reactions to ASRs may be subject to variation among individuals."
"Lyons, Joseph B.; Vo, Thy; Wynne, Kevin T.; Mahoney, Sean; Nam, Chang S.; Gallimore, Darci",Trusting Autonomous Security Robots: The Role of Reliability and Stated Social Intent,2021,1,320,316,4,4 participants were dropped for failure to respond adequately to attention check items or for insufficient effort responding,Online Crowdsourcing,between-subjects,"Participants viewed a video of a robot interacting with humans, then completed a survey.","Participants watched a video of a security robot and rated their trust, trustworthiness, and desire to use the robot in different contexts.",Unspecified,Unmanned Ground Vehicles,Security,Evaluation,Survey/Questionnaire Completion,passive observation,Participants watched a video of a robot interaction.,media,Participants viewed a video of a robot interacting with humans.,physical,The robot was a physical robot shown in the video.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions.,Questionnaires; Custom Scales,Mayer and Davis' Trust/Trustworthiness Scales (1999); Reliance Intention Scale,,Trust was measured using questionnaires and custom scales.,"parametric models (e.g., regression)",The study used ANOVAs to analyze the effects of reliability and social intent on trust and trustworthiness.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated robot reliability (correct rejection vs. false alarm) and stated social intent (benevolence toward visitor, occupants, robot, or self-sacrifice) to influence trust.","Reliability influenced trust and trustworthiness; stated social intent influenced trustworthiness, particularly benevolence and integrity. Self-sacrificial intent led to higher trustworthiness.","Social intent did not influence trust directly, but it did influence trustworthiness. The self-sacrificial intent was associated with the highest perceived benevolence and integrity. There was a higher acceptance of ASRs in military contexts than in public contexts.","Reliability of the robot and stated social intent, particularly self-sacrificial intent, are important factors influencing trust and trustworthiness of autonomous security robots.",The robot's task was to verify access credentials and use a nonlethal device on a noncompliant visitor. The human's task was to watch the video and complete a survey about their perceptions of the robot.,ANOVA; t-test,"The study used ANOVAs to test the main effects of reliability and social intent on trust and trustworthiness, as well as their interaction. Significant main effects for social intent were followed up with post hoc comparisons using a Bonferroni correction. Paired samples t-tests were used to compare the means between public and military use.",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-accuracy,Robot-verbal-communication-content,"The study manipulated two factors: Robot-accuracy and Robot-verbal-communication-content. Robot-accuracy was manipulated by presenting participants with one of two scenarios: a correct rejection (the robot correctly prevented an unauthorized person from accessing the secure area) or a false alarm (the robot incorrectly prevented an authorized person from accessing the secure area). This directly impacts the robot's task performance. Robot-verbal-communication-content was manipulated by providing participants with one of four different scripts describing the robot's stated social intent: benevolence toward the visitor, benevolence toward the building occupants, benevolence toward the robot, or benevolence toward the visitor with self-sacrifice. These scripts altered the content of the robot's stated purpose and priorities. The results showed that Robot-accuracy (correct rejection vs. false alarm) significantly impacted trust, with higher trust in the correct rejection condition. However, the manipulation of Robot-verbal-communication-content (stated social intent) did not directly influence trust, although it did influence trustworthiness (benevolence and integrity).",10.1177/0018720820901629,http://journals.sagepub.com/doi/10.1177/0018720820901629,"Objective:  This research examined the effects of reliability and stated social intent on trust, trustworthiness, and one’s willingness to endorse use of an autonomous security robot (ASR). Background:  Human–robot interactions in the domain of security is plausible, yet we know very little about what drives acceptance of ASRs. Past research has used static images and game-b­ ased simulations to depict the robots versus actual humans interacting with actual robots. Method:  A video depicted an ASR interacting with a human. The ASR reviewed access credentials and allowed entrance once verified. If the ASR could not verify one’s credentials it instructed the visitor to return to the security checkpoint. The ASR was equipped with a nonlethal device and the robot used this device on one of the three visitors (a research confederate). Manipulations of reliability and stated social intent of the ASR were used in a 2 × 4 between subjects design (N = 320). Results:  Reliability influenced trust and trustworthiness. Stated social intent influenced trustworthiness. Participants reported being more favorable toward use of the ASR in military contexts versus public contexts. Conclusion:  The study demonstrated that reliability of the ASR and statements regarding the ASR’s stated social intent are important considerations influencing the trust process (inclusive of intentions to be vulnerable and trustworthiness perceptions)."
"Lyons, Joseph B.; Jessup, Sarah A.; Vo, Thy Q.",The Role of Decision Authority and Stated Social Intent as Predictors of Trust in Autonomous Robots,2022,1,340,309,31,31 participants were dropped for failure to respond correctly to attention check items and or for failure to show any variation in their item responding,Online Crowdsourcing,between-subjects,"Participants viewed a video of an ASR, read a description of the robot's intent, and then a description of the robot's decision authority, and then completed surveys.",Participants watched a video of an ASR interacting with people and then answered survey questions about the robot.,Unspecified,Unmanned Ground Vehicles,Research; Social,Evaluation,Survey/Questionnaire Completion,passive observation,Participants passively observed a video of the robot.,media,Participants watched a video of the robot interacting with people.,physical,The robot was a physical robot shown in a video.,shared control (fixed rules),The robot's actions were either fully autonomous or controlled by a human operator based on the experimental condition.,Questionnaires; Custom Scales,Mayer and Davis' Trust/Trustworthiness Scales (1999); Reliance Intention Scale,,Trust was measured using questionnaires and custom scales.,"parametric models (e.g., regression)",The study used ANOVAs to analyze the effects of the independent variables on trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The study manipulated the robot's stated social intent and decision authority to see how these factors influence trust.,Trust was higher when the robot was described as having self-sacrificial intent compared to self-protective intent. The effect of intent on trust was moderated by the level of decision authority.,"The interaction between decision authority and stated social intent was significant for ability and integrity perceptions, but not for reliance intentions or benevolence. Ability and integrity perceptions were higher in the low decision authority condition when benevolence was focused on the building occupants, which was unexpected.","Stated social intent, particularly self-sacrificial intent, increases trust in a robot, and this effect is moderated by the robot's decision authority.","The robot acted as a security guard, checking badges and using a non-lethal weapon when needed. The human participant watched a video of the robot and then answered survey questions.",ANOVA; t-test,"The study used ANOVAs to test the main effects of decision authority and stated social intent on reliance intentions, trustworthiness (ability, benevolence, and integrity), and desired use, as well as their interactions. Post-hoc comparisons using a Bonferroni correction were used for significant main effects. A paired samples t-test was used to compare desired military use with desired public use.",TRUE,Robot-autonomy; Robot-verbal-communication-content,Robot-verbal-communication-content,Robot-autonomy,"The study manipulated two main factors: the robot's decision authority (Robot-autonomy) and the stated social intent of the robot (Robot-verbal-communication-content). Decision authority was manipulated by describing the robot as either fully autonomous or controlled by a human operator. Stated social intent was manipulated through narrative descriptions of the robot's programming, focusing on benevolence toward different entities (visitor, building occupants, or self) or self-sacrifice. The results showed that stated social intent (specifically self-sacrificial intent) impacted trust, as participants reported higher reliance intentions and perceived benevolence for the robot with self-sacrificial intent. However, the main effect of decision authority on reliance intentions was not significant, indicating that it did not directly impact trust levels. The interaction between decision authority and stated social intent was significant for ability and integrity perceptions, but not for reliance intentions or benevolence. Therefore, while decision authority was manipulated, it did not directly impact trust, but it did moderate the effect of intent on ability and integrity perceptions. The choice of 'Robot-autonomy' is based on the explicit manipulation of the robot's decision-making capabilities, and 'Robot-verbal-communication-content' is based on the manipulation of the robot's stated intent through narrative descriptions.",10.1111/tops.12601,https://onlinelibrary.wiley.com/doi/10.1111/tops.12601,"Prior research has demonstrated that trust in robots and performance of robots are two important factors that inﬂuence human–autonomy teaming. However, other factors may inﬂuence users’ perceptions and use of autonomous systems, such as perceived intent of robots and decision authority of the robots. The current study experimentally examined participants’ trust in an autonomous security robot (ASR), perceived trustworthiness of the ASR, and desire to use an ASR that varied in levels of decision authority and benevolence. Participants (N = 340) were recruited from Amazon Mechanical Turk. Results revealed the participants had increased trust in the ASR when the robot was described as having benevolent intent compared to self-protective intent. There were several interactions between decision authority and intent when predicting the trust process, showing that intent may matter the most when the robot has discretion on executing that intent. Participants stated a desire to use the ASR in a military context compared to a public context. Implications for this research demonstrate that as robots become more prevalent in jobs paired with humans, factors such as transparency provided for the robot’s intent and its decision authority will inﬂuence users’ trust and trustworthiness."
"Lyons, Joseph B.; Hamdan, Izz Aldin; Vo, Thy Q.",Explanations and trust: What happens to trust when a robot partner does something unexpected?,2023,1,160,148,12,12 participants were dropped for failure to respond correctly to attention check items and or for failure to show any variation in their item responding,Online Crowdsourcing,mixed design,"Participants viewed PowerPoint slides describing an autonomous mobile robot, a video showing the features of the robot, and two videos of the robot operating in an office environment with a human teammate. Following each video, participants responded to survey items related to the robot. The second video included one of four explanation strategies during the robot debrief.",Participants watched videos of a robot performing a search and rescue task and rated their trust in the robot.,Unspecified,Mobile Robots; Telepresence Robots,Research,Supervision,Monitoring,passive observation,Participants passively observed videos of the robot.,media,Participants watched videos of the robot performing a task.,physical,The robot was a physical robot shown in the videos.,fully autonomous (limited adaptation),The robot operated autonomously but with limited adaptation to the environment.,Questionnaires; Custom Scales,Mayer and Davis' Trust/Trustworthiness Scales (1999),,Trust was measured using questionnaires and custom scales.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's behavior was manipulated by having it deviate from a planned path, and different explanation types were provided after the deviation to influence trust.","Trust and trustworthiness decreased after the unexpected behavior, but an explanation with a detailed observational rationale buffered this decline.",The acknowledgement + observational rationale condition was the only condition that maintained ability and benevolence perceptions after the violation. The study also found that responsibility attribution shifted from the human to the robot after the unexpected behavior.,Explanations that provide a specific external rationale for why a robot deviated from a planned path are most effective at maintaining trust and trustworthiness.,"The robot was tasked with searching rooms in a USAR context, and the human participant observed the robot's actions and rated their trust in the robot. The robot followed a planned path in the first video and deviated from the planned path in the second video.",ANOVA; t-test,"The study used mixed-effects ANOVAs to examine the effects of explanation type (between-subjects) and behavior (within-subjects: planned path vs. unexpected path) on trust and trustworthiness (ability, benevolence, and integrity). Paired samples t-tests were used to compare responsibility attribution items before and after the unexpected behavior. The ANOVAs tested for main effects of explanation type and behavior, as well as interaction effects between these two factors. Post-hoc tests were used to interpret significant interactions. The t-tests examined whether there was a significant change in responsibility attribution from the human to the robot after the unexpected behavior.",TRUE,Robot-verbal-communication-content; Robot-autonomy,Robot-verbal-communication-content,Robot-autonomy,"The study manipulated the content of the robot's verbal communication during the debriefing phase after the robot deviated from its planned path. Four conditions were used: no explanation, acknowledgement, acknowledgement + observational rationale, and acknowledgement + best for mission goals. This manipulation directly altered the information provided by the robot to the human teammate, thus it is classified as 'Robot-verbal-communication-content'. The robot's behavior was also manipulated by having it deviate from a planned path, which is an example of the robot acting autonomously, thus 'Robot-autonomy' is also a manipulated factor. The study found that the 'acknowledgement + observational rationale' condition had the most positive impact on trust and trustworthiness, indicating that the content of the robot's verbal communication influenced trust. The study did not find that the robot's autonomy level (deviation from the planned path) had a differential impact on trust across the explanation conditions, but rather that the explanation type was the key factor influencing trust after the deviation. Therefore, 'Robot-autonomy' is listed as a factor that was manipulated but did not have a differential impact on trust.",10.1016/j.chb.2022.107473,https://linkinghub.elsevier.com/retrieve/pii/S074756322200293X,"Performance within Human-Autonomy Teams (HATs) is influenced by the effectiveness of communication be­ tween humans and robots. Communication is particularly important when robot teammates engage in behaviors that were not anticipated by the human teammate which could degrade trust. However, the literature on trust repair focuses on the role of apologies which may not be appropriate for an unexpected behavior since this behavior may not be an error. Explanations are one method that can be used by robot teammates to avoid costly trust degradation when human expectations are violated. The current study used an Urban Search and Rescue (USAR) scenario to examine the role of explanation in a context wherein a robot teammate deviated from an expected behavior. The current study examined how trust, trustworthiness, and responsibility attribution were influenced by observing the robot teammate deviate from an expected behavior. Participants (N = 148) used an online platform to view videos of the robot: 1) following a planned search path, and 2) deviating from a planned search path. A debriefing event between the human and the robot followed each search activity. Four expla­ nation conditions were tested in the debriefing phase following the behavioral violation. Results showed that trust and trustworthiness (ability, benevolence, and integrity) declined following the unexpected behavior. Accordingly, responsibility attribution shifted from the human to the robot following the unexpected behavior. Explanation strategies that focused on communicating why the event occurred by highlighting the robot’s environmental awareness were most effective at buffering the decline in trust and trustworthiness."
"MacArthur, Keith R.; Stowers, Kimberly; Hancock, P. A.",Human-Robot Interaction: Proximity and Speed—Slowly Back Away from the Robot!,2017,1,148,148,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were informed that the robot was autonomous, but it was controlled by a hidden operator. Participants observed the robot approach at different speeds and proximities, and then completed trust questionnaires.",Participants observed a robot approach them at varying speeds and distances.,Unspecified,Mobile Robots,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants passively observed the robot's movements.,real-world,The robot was physically present in the real-world environment.,physical,A physical robot was used in the study.,wizard of oz (directly controlled),The robot was controlled by a hidden human operator.,Questionnaires; Custom Scales,Human-Robot Trust Scale/Questionnaire (HRTS/HRITS); Trust in Automation Scale (TAS); Interpersonal Trust Scale/Questionnaire; Negative Attitude towards Robots Scale (NARS),,Trust was measured using questionnaires and custom scales.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,The robot's speed and proximity were directly manipulated to observe their effect on trust.,Trust decreased when the robot moved quickly and was closer to the participant.,"The effect of speed on trust was only significant on the Trust in Automation Scale, not the Human Robot Trust Scale.","Proximity and speed of approach significantly affect trust levels in human-robot interaction, with closer proximity and faster speeds leading to lower trust.","The robot moved towards the participant at different speeds and distances, while the participant observed and then completed questionnaires.",ANOVA; Bonferroni correction,"A mixed-design ANOVA was used to analyze the effects of proximity and speed on trust scores. Bonferroni corrected pairwise comparisons were then used to examine the mean differences between conditions (near vs. far proximity, and fast vs. slow speed) for both the Human Robot Trust Scale and the Trust in Automation Scale. The ANOVA tested the main effects of proximity and speed, and their interaction, on trust. The pairwise comparisons further explored the direction of these effects.",TRUE,Robot-nonverbal-communication; Robot-task-strategy,Robot-nonverbal-communication; Robot-task-strategy,,"The study manipulated the robot's speed and proximity to the participant. 'Robot-nonverbal-communication' was chosen because the robot's movement and distance are nonverbal cues that influence proxemics, as described in the paper: 'One important social norm is the use of interpersonal space...'. The paper also states 'Specifically, we aimed to address this gap by examining the influence of two factors deemed important in social norms: proximity and speed of movement.' The robot's speed and proximity are also part of the robot's task strategy, as the robot was moving towards the participant at different speeds and distances. The results section states 'Analyses indicate proximity as a significant factor contributing to changes in trust scores' and 'The speed of approach was also a significant factor contributing to changes in trust scores', indicating that both factors impacted trust. The paper also states 'Support for the effects of speed on trust were found only in the Trust in Automation Scale... and not the Human Robot Trust Scale', but this does not mean that speed did not impact trust, only that the effect was not significant on one of the scales. Therefore, no factors were found to not impact trust.",,http://link.springer.com/10.1007/978-3-319-41959-6_30,"This experiment was designed to evaluate the effects of proximity and speed of approach on trust in human-robot interaction (HRI). The experimental design used a 2 (Speed) × 2 (Proximity) mixed factorial design and trust levels were measured by self-report on the Human Robot Trust Scale and the Trust in Automation Scale. Data analyses indicate proximity [F(2, 146) = 6.842, p < 0.01, partial ŋ2 = 0.086] and speed of approach [F(2, 146) = 2.885, p = 0.059, partial ŋ2 = 0.038] are signiﬁcant factors contributing to changes in trust levels."
"Mackay, Ana; Fortes, Inês; Santos, Catarina; Machado, Dário; Barbosa, Patrícia; Boas, Vera Vilas; Ferreira, João Pedro; Costa, Nélson; Silva, Carlos; Sousa, Emanuel",The Impact of Autonomous Vehicles’ Active Feedback on Trust,2020,1,40,40,6,"6 participants were excluded because they either failed to respond to all three trials in the first task (N = 2, all from Sensors group), to all three trials in the second task (N = 1, from Decision group), or to all trials, in both tasks (N = 3, all from Decision group)",Controlled Lab Environment,between-subjects,"Participants were randomly assigned to one of three groups: No Feedback, Sensors, and Decision. They drove in a simulator with different levels of feedback on the assistive cluster. A visual search task was presented twice during the drive. Participants completed a trust questionnaire at the end.",Participants drove in a simulator while performing a visual search task.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated autonomous vehicle through a driving simulator.,simulation,The study used a driving simulator to create an immersive experience.,simulated,The robot was a simulated autonomous vehicle within the driving simulator.,fully autonomous (limited adaptation),"The autonomous vehicle operated without direct human control, but with limited adaptation.",Questionnaires; Behavioral Measures; Physiological Measures,,Physiological Signals; Performance Metrics,"Trust was assessed using a custom questionnaire, a visual search task, and heart rate measurements.",no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the amount of feedback provided to the driver through the assistive cluster, which influenced the information available about the vehicle's state and decisions.","The study found an inverse relation between available feedback and performance on a visual search task, and no significant differences in trust based on the questionnaire. More information did not necessarily lead to more trust.","The study found an inverse relation between available feedback and performance on a visual search task, which was unexpected. The Decision group, which received the most information, had the worst performance on the visual search task.",More information does not necessarily lead to more trust and may negatively affect cognitive load.,"The robot (simulated autonomous vehicle) drove along a highway, while the human participant monitored the driving and performed a visual search task when prompted.",ANOVA; ANOVA; ANOVA,"The study used mixed Analysis of Variance (ANOVA) to analyze the effect of feedback information (no feedback, sensors, decision) and time (before and after visual search task) on heart rate. Another mixed ANOVA was used to analyze the effect of feedback information and time (t1 and t2) on the number of missing answers in the visual search task. A third mixed ANOVA was used to analyze the effect of feedback information and time (t1 and t2) on the percentage of correct answers in the visual search task. One-way ANOVAs were also conducted to analyze differences between the three information levels for each one of the questionnaire items.",TRUE,Robot-interface-design; Task-complexity,Task-complexity,Robot-interface-design,"The study manipulated the information provided through the assistive cluster (AC), which is a part of the robot's interface. This is categorized as 'Robot-interface-design' because it involves changes to interactive elements of the system. The three groups received different levels of feedback: no feedback, feedback about surrounding vehicles, and feedback about surrounding vehicles and the vehicle's decisions. The visual search task was also presented twice during the drive, which can be considered a manipulation of 'Task-complexity' as it introduced a secondary task that increased the cognitive load. The study found that the amount of feedback provided through the interface did not significantly impact trust as measured by the questionnaire, thus 'Robot-interface-design' did not impact trust. However, the study found an inverse relation between available feedback and performance on the visual search task, with the Decision group having the worst performance. This indicates that the increased cognitive load due to the more complex interface and the visual search task impacted performance, which is related to trust, thus 'Task-complexity' impacted trust.",,http://link.springer.com/10.1007/978-3-030-20497-6_32,"The successful introduction of self-driving technology may depend on the ability of the vehicles’ human-machine interface to convey trust to the vehicle occupants. Using a driving simulator, in this experiment we aimed to evaluate drivers’ trust on an autonomous system, depending on the feedback the vehicle provided by an assistive cluster’s interface. Forty participants were divided into three groups regarding levels of feedback: (a) cluster without feedback (N = 13); (b) cluster with feedback regarding the surrounding vehicles (N = 14); (c) cluster with feedback regarding the surrounding vehicles and the vehicle’s own decisions (N = 13). For all groups, a visual search task was introduced as an indirect indicator of trust in the autonomous system. Results showed an inverse relation between available feedback and correct answers. The system was evaluated as trustable and safe by all groups. Overall, the results may contribute to design requirements for future vehicle HMIs, as they indicate that more information does not necessarily convey more trust."
"Maehigashi, Akihiro; Tsumura, Takahiro; Yamada, Seiji",Effects of Beep-Sound Timings on Trust Dynamics in Human-Robot Interaction,2022,1,176,159,17,"6 participants were excluded due to irregular data related to the trust rating, participants were excluded due to irregular data related to the accuracy rate",Online Crowdsourcing,between-subjects,"Participants first performed 10 calculation problems by themselves, then 32 calculation problems with the robot. Trust was measured before the task and after every two problems.","Participants performed a calculation task, answering two-digit addition and subtraction problems, with the option to refer to the robot's answers.",Sota,Humanoid Robots,Research,Evaluation,Rating,minimal interaction,Participants interacted with a picture of the robot on a screen.,media,Participants viewed a still picture of the robot during the task.,physical,A picture of a physical robot was shown to participants.,pre-programmed (non-adaptive),The robot provided pre-programmed answers to calculation problems.,Custom Scales; Real-time Trust Measures,,Performance Metrics,Trust was measured using a 7-point scale before the task and after every two problems.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,Beep sounds were presented before either correct or incorrect trials to influence participants' expectations and trust in the robot's performance.,"A beep sound before a correct trial increased trust, while a beep sound before an error trial initially decreased trust but later increased trust after a correct trial.","The beep sound before an error trial led to a decrease in trust in the first error trial, but also an increase in trust in the second correct trial, suggesting the sound made participants more sensitive to the robot's performance.","A beep sound presented before a correct trial increased trust in the robot, while a beep sound before an error trial initially decreased trust but later increased trust after a correct trial.","The robot provided answers to calculation problems, and the human participant could choose to use the robot's answer or provide their own.",ANOVA; t-test; correlational analyses,"The study used a 3x2 ANOVA to analyze the accuracy rate, examining the effects of beep timing and task situation (with or without robot). One-way ANOVAs were used to analyze trust ratings at different stages of the experiment (before the task, first and second correct/error trials), followed by t-tests (using Ryan's method) for post-hoc comparisons. Correlational analyses were also conducted to examine the relationship between trust and accuracy ratings, and between trust and impression ratings.",TRUE,Robot-accuracy; Robot-interface-design,Robot-accuracy,Robot-interface-design,"The study manipulated the robot's accuracy by having it provide either all correct or all incorrect answers in blocks of trials. This is a direct manipulation of the robot's performance on the task, which is why 'Robot-accuracy' is chosen. The study also manipulated the timing of a beep sound presented alongside a picture of the robot on a screen, which is a change to the interface design. The beep sound was presented either before correct trials or before error trials, or not at all. The beep sound itself is part of the interface, as it is an auditory cue presented to the user in conjunction with the visual display of the robot. The study found that the robot's accuracy (correct vs. incorrect answers) significantly impacted trust levels, with trust increasing after correct trials and decreasing after error trials. The timing of the beep sound also influenced trust, but this was mediated by the robot's accuracy. The interface design (beep sound timing) did not directly impact trust, but rather influenced how participants interpreted the robot's accuracy. The beep sound was presented on the screen with the robot, and the timing of the beep sound was manipulated, but the visual interface itself was not manipulated. Therefore, the interface design was not found to directly impact trust, but rather the robot's accuracy was the factor that impacted trust.",,https://link.springer.com/10.1007/978-3-031-24670-8_57,"This study investigated the effects of a combination of anthropomorphic and mechanical features in a social robot on human trust, especially focusing on how beep sounds emitted by a social robot with anthropomorphic physicality influence human trust in the robot. Beep sounds were experimentally manipulated to be presented right before a robot showed high or low task performance. As a result, (1) a sound right before high performance increased trust in the robot when it performed accurately, and (2) a sound right before low performance caused a greater decrease in trust in the robot when it performed inaccurately, and also a greater increase in the trust when it performed accurately. Even though identical beep sounds were presented, their effects on human trust differed depending on the timing. On the basis of the results, possible methods for designing trust in human-robot interactions are discussed."
"Maehigashi, Akihiro",The Nature of Trust in Communication Robots: Through Comparison with Trusts in Other People and AI systems,2022,1,34,34,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants performed a calculation and an emotion recognition task with one of three partners (human, AI, or robot). In each task, participants saw a question, then the partner's answer, and then entered their own answer. Trust was rated before the task and after each trial.",Participants completed a calculation task (two-digit addition/subtraction) and an emotion recognition task (identifying emotions from facial expressions).,PALRO,Humanoid Robots; Expressive Robots,Research; Social,Social,Emotion Recognition,minimal interaction,Participants interacted with the robot by observing its answers on a screen and hearing its voice.,real-world,Participants interacted with a physical robot in a lab setting.,physical,A physical robot was present during the experiment.,pre-programmed (non-adaptive),The robot provided pre-determined answers without adapting to the participant's input.,Questionnaires,,,Trust was measured using a 7-point Likert scale questionnaire.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers manipulated the correctness of the partner's answers (correct or incorrect) and the type of partner (human, AI, or robot), influencing the participant's trust.",Trust in the robot was similar to trust in AI for the calculation task and similar to trust in humans for the emotion recognition task. Errors caused a decline in trust.,"The study found that trust in robots is similar to trust in AI systems in calculation tasks, but similar to trust in humans in emotion recognition tasks. This suggests that the type of task influences how people perceive and trust robots.","Trust in robots is similar to trust in AI systems for tasks with single solutions, but similar to trust in humans for tasks with multiple interpretations.",The robot displayed answers on a screen and read them aloud. The human participant observed the answers and then provided their own answer to the calculation or emotion recognition task.,ANOVA,"A one-way ANOVA was performed to compare the mean trust ratings across the three conditions (human, AI, and robot) for each of the four types of trust (initial trust, trust formation, trust decline, and trust recovery) in both the calculation and emotion recognition tasks. The purpose was to determine if there were significant differences in trust ratings between the conditions and to test the hypotheses about the nature of trust in robots compared to trust in humans and AI systems.",TRUE,Robot-accuracy; Teaming,Robot-accuracy,Teaming,"The study manipulated the correctness of the partner's answers (correct or incorrect), which directly impacts the 'Robot-accuracy'. The study also manipulated the type of partner (human, AI, or robot), which can be classified as 'Teaming' because it changes the nature of the interaction and the expectations of the participant. The results showed that 'Robot-accuracy' impacted trust, as errors caused a decline in trust. The type of partner ('Teaming') did not directly impact trust decline, but it did influence the overall level of trust, with trust in the robot being similar to AI for calculation and similar to humans for emotion recognition. However, the manipulation of the partner type did not directly impact the trust decline, which was primarily influenced by the accuracy of the answers. Therefore, 'Teaming' is listed as a factor that did not impact trust.",10.1109/HRI53351.2022.9889521,https://ieeexplore.ieee.org/document/9889521/,"In this study, the nature of human trust in communication robots was experimentally investigated comparing with trusts in other people and artificial intelligence (AI) systems. The results of the experiment showed that trust in robots is basically similar to that in AI systems in a calculation task where a single solution can be obtained and is partly similar to that in other people in an emotion recognition task where multiple interpretations can be acceptable. This study will contribute to designing a smooth interaction between people and communication robots."
"Maehigashi, Akihiro; Yamada, Seiji",Modeling Trust and Reliance with Wait Time in a Human-Robot Interaction,2023,2,100,89,11,"3 participants in the AI condition, 3 in the robot condition, and 5 in the human condition were excluded because they did not select a partner at all during the task",Online Crowdsourcing,mixed design,"Participants performed an emotion recognition task with a partner (AI, robot, or human). They were shown a facial expression and the partner indicated an estimated answer time. Participants then rated their trust in the partner after each question.","Participants completed an emotion recognition task, identifying which of six emotions was expressed in images of human facial expressions.",Sota,Expressive Robots,Research,Evaluation,Image Analysis,minimal interaction,"Participants interacted with a virtual partner through a screen, with no physical interaction.",media,The interaction was presented through static images of the partner.,simulated,The robot was represented by a static image on a screen.,pre-programmed (non-adaptive),The partner's wait time was pre-programmed and did not adapt to the participant's actions.,Custom Scales; Questionnaires,Godspeed Questionnaire; Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using a custom 7-point scale and the MDMT questionnaire.,"parametric models (e.g., regression)",Multiple regression analysis was used to investigate the effects of anthropomorphism and wait time on trust and reliance.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the anthropomorphism of the partner (AI, robot, or human) and the wait time for the partner's action to influence trust.","Anthropomorphism had a positive effect on trust, while wait time had a negative effect on trust and reliance. The effect of wait time on reliance was mediated by trust.","The correlation between trust ratings during the task and MDMT ratings after the task was only significant in the robot condition, suggesting that the manipulation of wait time may have had different effects on trust depending on the partner type.","Wait time negatively influenced trust in and reliance on the partner, and this effect was mediated by trust.",The robot (or AI/human partner) was presented with a facial expression and estimated an answer time. The human participant then rated their trust in the partner.,ANOVA; t-test; Linear regression; Mediation analysis,"One-way ANOVAs were used to confirm the manipulation of anthropomorphism, followed by t-tests for multiple comparisons. Multiple regression analysis was used to investigate the effects of anthropomorphism and wait time on trust and reliance. A mediation analysis with a Sobel test was performed to test the mediation effect of trust from wait time on reliance. Correlational analyses were also conducted between trust ratings during the task and MDMT ratings after the task.",TRUE,Robot-social-attitude; Robot-social-timing,Robot-social-attitude; Robot-social-timing,,"The study manipulated the anthropomorphism of the partner (AI, robot, or human), which is categorized as 'Robot-social-attitude' because it changes the perceived social characteristics of the partner. The study also manipulated the wait time for the partner's action, which is categorized as 'Robot-social-timing' because it changes the timing of the partner's response. The results showed that both anthropomorphism and wait time influenced trust, so both are listed in 'factors_that_impacted_trust'. There were no factors that were manipulated that did not impact trust.",,http://arxiv.org/abs/2302.08064,"This study investigated how wait time inﬂuences trust in and reliance on a robot. Experiment 1 was conducted as an online experiment manipulating the wait time for the task partner’s action from 1 to 20 seconds and the anthropomorphism of the partner. As a result, the anthropomorphism inﬂuenced trust in the partner and did not inﬂuence reliance on the partner. However, the wait time negatively inﬂuenced trust in and reliance on the partner. Moreover, a mediation effect of trust from the wait time on reliance on the partner was conﬁrmed. Experiment 2 was conducted to conﬁrm the effects of wait time on trust and reliance in a human-robot face-to-face situation. As a result, the same effects of wait time found in Experiment 1 were conﬁrmed. This study revealed that wait time is a strong and controllable factor that inﬂuences trust in and reliance on a robot."
"Maehigashi, Akihiro; Yamada, Seiji",Modeling Trust and Reliance with Wait Time in a Human-Robot Interaction,2023,2,19,17,2,"One participant did not wait for the robot's auditory voice indicating the answer time, and one participant kept tapping the tablet PC while the robot was answering the questions.",Controlled Lab Environment,within-subjects,"Participants performed an emotion recognition task with a physical robot (Sota). The robot indicated the estimated answer time, and participants rated their trust in the robot after each question.","Participants completed an emotion recognition task, identifying which of six emotions was expressed in images of human facial expressions.",Sota,Expressive Robots,Research,Evaluation,Image Analysis,minimal interaction,"Participants interacted with a physical robot, but the interaction was limited to observing the robot's actions and responses.",real-world,The interaction took place in a real-world setting with a physical robot present.,physical,The robot was physically present in the interaction.,pre-programmed (non-adaptive),The robot's wait time was pre-programmed and did not adapt to the participant's actions.,Custom Scales; Questionnaires,Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using a custom 7-point scale and the MDMT questionnaire.,"parametric models (e.g., regression)",Simple regression analysis was used to investigate the effects of wait time on trust and reliance.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The study manipulated the wait time for the robot's action to influence trust.,Wait time had a negative effect on trust and reliance. The effect of wait time on reliance was mediated by trust.,"The R-squared values of the regression models were small, suggesting that individual differences may have played a larger role in this experiment due to the robot's physical presence.","Wait time negatively influenced trust in and reliance on the robot, and this effect was mediated by trust, even in a face-to-face interaction.",The robot was presented with a facial expression and estimated an answer time. The human participant then rated their trust in the robot.,simple regression analysis; Mediation analysis,Simple regression analysis was used to investigate the effects of wait time on trust and reliance. A mediation analysis with a Sobel test was performed to test the mediation effect of trust from wait time on reliance. Correlational analyses were also conducted between trust ratings during the task and MDMT ratings after the task.,TRUE,Robot-social-timing,Robot-social-timing,,"The study manipulated the wait time for the robot's action, which is categorized as 'Robot-social-timing' because it changes the timing of the robot's response. The results showed that wait time influenced trust, so it is listed in 'factors_that_impacted_trust'. There were no factors that were manipulated that did not impact trust.",,http://arxiv.org/abs/2302.08064,"This study investigated how wait time inﬂuences trust in and reliance on a robot. Experiment 1 was conducted as an online experiment manipulating the wait time for the task partner’s action from 1 to 20 seconds and the anthropomorphism of the partner. As a result, the anthropomorphism inﬂuenced trust in the partner and did not inﬂuence reliance on the partner. However, the wait time negatively inﬂuenced trust in and reliance on the partner. Moreover, a mediation effect of trust from the wait time on reliance on the partner was conﬁrmed. Experiment 2 was conducted to conﬁrm the effects of wait time on trust and reliance in a human-robot face-to-face situation. As a result, the same effects of wait time found in Experiment 1 were conﬁrmed. This study revealed that wait time is a strong and controllable factor that inﬂuences trust in and reliance on a robot."
"Maggi, Gianpaolo; Dell’Aquila, Elena; Cucciniello, Ilenia; Rossi, Silvia","“Don’t Get Distracted” The Role of Social Robots’ Interaction Style on Users’ Cognitive Performance, Acceptance, and Non-Compliant Behavior",2020,1,60,60,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants completed personality and technology acceptance questionnaires, then interacted with a Pepper robot administering cognitive tasks, and finally completed the technology acceptance questionnaire again.",Participants completed the Trail Making Test and Attentive Matrices cognitive tasks following instructions from a Pepper robot.,Pepper,Humanoid Robots; Expressive Robots,Educational; Research; Social,Evaluation,Survey/Questionnaire Completion,minimal interaction,Participants interacted with the robot through verbal instructions and task administration.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical Pepper robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions without adapting to the user.,Questionnaires,Unified Theory of Acceptance and Use of Technology (UTAUT),,Trust was measured using the UTAUT questionnaire.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's interaction style (friendly, authoritarian, neutral) was manipulated through verbal and non-verbal cues, including tone of voice, gaze, gestures, and motivational phrases, to influence user performance and trust.","Trust was found to be related to cognitive performance in the authoritarian condition, suggesting that an assertive interaction style may enhance trust.","The authoritarian interaction style improved performance on cognitively demanding tasks, and trust was related to performance in this condition. The neutral robot condition led to more non-compliant behavior.","An authoritarian interaction style of a robot can improve users' performance on cognitive tasks that require high cognitive demands, and this performance is related to users' trust in the robot.","The robot provided instructions for cognitive tasks, and the human participant completed the tasks following the robot's instructions. The robot also recorded the session for later analysis.",χ 2; Shapiro-Wilk; Kruskal-Wallis; bonferroni's correction; Wilcoxon signed-rank test; Spearman correlation; Mann-Whitney U,"The study used a variety of statistical tests. A chi-squared test (χ 2) was used to compare the distribution of male and female participants across the three groups. The Shapiro-Wilk test was used to assess the normality of data distribution, leading to the use of non-parametric methods. The Kruskal-Wallis H-test was used to compare performance on cognitive tasks and personality traits between groups with different robot interaction styles, with Bonferroni's correction applied for post-hoc pairwise comparisons. The Wilcoxon signed-rank test was used to evaluate changes in technology acceptance levels after interaction with the robot within each group, also with Bonferroni's correction. Spearman's rank correlation analysis was used to investigate associations between cognitive performance, personality traits, technology acceptance, and compliance. The Mann-Whitney U test was used to compare cognitive performance between male and female participants.",TRUE,Robot-nonverbal-communication; Robot-verbal-communication-style; Robot-social-attitude,Robot-verbal-communication-style; Robot-social-attitude,Robot-nonverbal-communication,"The study manipulated the robot's interaction style through several factors. 'Robot-nonverbal-communication' was manipulated through changes in gaze behavior (static, socially responsive, sustained/direct), gestures (frequency and speed), and proxemics (robot positioned in the user's personal space). 'Robot-verbal-communication-style' was manipulated through changes in tone of voice, speed of speech, and language type (formal vs. informal), and motivational phrases. 'Robot-social-attitude' was manipulated by creating three distinct interaction styles: friendly, authoritarian, and neutral, each with different social cues and behaviors. The paper states that in the authoritarian condition, participants' cognitive performance was related to their trust and acceptance of the technology, indicating that the 'Robot-verbal-communication-style' and 'Robot-social-attitude' impacted trust. The study found no direct relationship between non-verbal cues and trust, but rather the overall interaction style, which is a combination of verbal and social cues, impacted trust. Therefore, 'Robot-nonverbal-communication' is listed as a factor that was manipulated but did not directly impact trust.",10.1007/s12369-020-00702-4,http://link.springer.com/10.1007/s12369-020-00702-4,"Social robots are developed to provide companionship and assistance in the daily life of the children, older, and disable people but also have great potential as educational technology by facilitating learning. In these application areas, a social robot can take the role of a coach by training and assisting individuals also in cognitive tasks. Since a robot’s interaction style affects users’ trust and acceptance, customizing its behavior to the proposed tasks could, potentially, have an impact on the users’ performance. To investigate these phenomena, we enrolled sixty volunteers and endowed a social robot with a friendly and an authoritarian interaction style. The aim was to explore whether and how the robot’s interaction style could enhance users’ cognitive performance during a psychometric evaluation. The results showed that the authoritarian interaction style seems to be more appropriate to improve the performance when the tasks require high cognitive demands. These differences in cognitive performance between the groups did not depend on users’ intrinsic characteristics, such as gender and personality traits. Nevertheless, in the authoritarian condition, participants’ cognitive performance was related to their trust and the acceptance of the technology. Finally, we found that users’ non-compliant behavior was not related to their personality traits. This ﬁnding indirectly supports the role of the robot’s interaction style in inﬂuencing the compliance behavior of the users."
"Maithani, Harsh; Antonio Corrales-Ramon, Juan; Mezouar, Youcef",Trust-Based Variable Impedance Control for Cooperative Physical Human-Robot Interaction,2019,1,5,5,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a trajectory tracking task with a robot, with the robot's stiffness varying based on a trust model. Pre-experiment trials were conducted to determine performance metrics. The robot's stiffness was adjusted based on the user's performance and the calculated trust value. Three different strategies were tested, each with different weights for force and time performance.",Participants were asked to track a rectangular trajectory using a laser pointer attached to the end-effector of a robot.,KUKA LWR 4+,Industrial Robot Arms; Collaborative Robots (Cobots),Research,Manipulation,Path Following,direct-contact interaction,Participants physically interacted with the robot by moving a tool attached to its end-effector.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,A physical robot was used in the experiment.,shared control (adaptive),The robot adapted its stiffness based on the user's performance and a trust model.,Performance-Based Measures,,"Performance Metrics; robot data (sensor data, etc.)","Trust was assessed using performance metrics such as time, force, and accuracy.","parametric models (e.g., regression)","A linear trust model was used, with trust updated based on performance and fault metrics.",Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's stiffness was directly manipulated based on the user's performance, which influenced the task difficulty and provided feedback to the user.","The robot's stiffness was adjusted based on the trust model, which was influenced by the user's performance. The study showed that different weighting strategies for force and time performance resulted in different user behaviors and trust levels.","The study found that different weighting strategies for force and time performance resulted in different user behaviors. When time was not a factor, users performed the task with lower forces and higher accuracy. When time was a factor, users applied more force and committed more faults.","A trust-based variable impedance control scheme was developed, where the robot acts as a supervisor, adjusting its stiffness based on the user's performance and a trust model.",The robot maintains a constant orientation of the tool and adjusts its stiffness based on the user's performance. The human moves a laser pointer attached to the robot's end-effector to track a rectangular trajectory.,,"No statistical tests were explicitly mentioned in the paper. The analysis focused on calculating and comparing performance metrics (time, force, and accuracy) and using these metrics to update a trust model, which in turn influenced the robot's stiffness. The study used pre-experiment trials to establish baseline performance values and thresholds for fault detection. The results were presented as graphs and tables showing the variation of trust and performance metrics under different experimental conditions.",TRUE,Robot-autonomy; Task-constraints,Robot-autonomy,,"The study manipulated 'Robot-autonomy' by varying the robot's stiffness based on a trust model that was influenced by the user's performance. The robot acted as a supervisor, adjusting its stiffness, which is a form of adaptive shared control. This is described in the paper as 'a new trust based variable impedance control scheme for a cooperative human-robot task' and 'a scheme to demonstrate how a robot can supervise the performance of a human partner in a cooperative task and reward or punish the user depending on the performance'. The paper also manipulated 'Task-constraints' by introducing a time limit for the task, which is described as 'To simulate a time-pressure environment, the user was given a time of 2 minutes with the instruction -""Complete as many trials with the least amount of error""'. The study found that the robot's stiffness adjustments, which are a direct result of the trust model and thus the robot's autonomy, impacted the user's behavior and trust levels. The different weighting strategies for force and time performance (which are part of the trust model) resulted in different user behaviors and trust levels, indicating that the robot's adaptive behavior influenced trust. The paper states 'The study showed that different weighting strategies for force and time performance resulted in different user behaviors and trust levels.' The time constraint also influenced the user's behavior, but the paper does not explicitly state that it impacted trust, only that it influenced performance. Therefore, only 'Robot-autonomy' is listed as a factor that impacted trust.",10.1109/ICMECH.2019.8722839,https://ieeexplore.ieee.org/document/8722839/,"This paper proposes a novel trust-based impedance control scheme based on task performance metrics and faults that allow a robot to act as a supervisor for its human partner in a cooperative human-robot task. A dynamic Trust model is used to modulate the robot stiffness as a function of the user performance. The task metrics are accuracy, forces applied by the user and the time taken for completion of the task. Results show that the proposed control scheme results in lower forces applied by the user while simultaneously ensuring accuracy of the task. The proposed methodology can be expanded to train a novice user to match the performance of a professional user."
"Maj, Konrad; Grzybowicz, Paulina; Kopeć, Julia","“No, I Won't Do That.” Assertive Behavior of Robots and its Perception by Children",2024,1,191,185,6,6 participants were excluded after cleaning the data,Educational Setting,between-subjects,"Children observed a robot demonstration of either assertive or submissive behavior, then chose a response to the robot and completed the AMS questionnaire.",Participants chose a message to direct to the robot and then completed a questionnaire.,Pepper,Humanoid Robots; Expressive Robots,Research; Social,Social,Social Perception,minimal interaction,Participants observed a robot demonstration and then interacted with the robot through a tablet interface.,real-world,The study involved a physical robot in a real-world classroom setting.,physical,The robot was physically present during the study.,wizard of oz (directly controlled),The robot's actions were remotely controlled by a researcher.,Questionnaires,Attribution of Mental States (AMS) Scale,,Trust was assessed using the AMS questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's behavior was manipulated to be either assertive or submissive, and the robot was introduced with either a male or female name.",Assertive robot behavior increased the perception of the robot's mental state and led to more commanding responses from children. The gender of the robot also influenced the type of response.,Children used more commanding phrases with assertively behaving robots and with robots framed as female. Younger children anthropomorphized the robot more than older children.,Assertive behavior from a robot increases children's perception of the robot's mental state and leads to more commanding responses.,The robot demonstrated either assertive or submissive behavior by mimicking an animal. The human participant chose a message to direct to the robot and then completed a questionnaire.,Shapiro-Wilk; Mann-Whitney U; Chi-squared; student's t test for independent samples,"The study used several statistical tests. The Shapiro-Wilk test was used to check the normality of the distribution of quantitative variables. The Mann-Whitney test was used to compare groups based on the degree of anthropomorphism. The chi-square test of independence was used to compare the relationship between robot behavior and children's message choices, as well as the relationship between the degree of anthropomorphism and message choices, and the relationship between robot gender and message choices. The Student's t-test for independent samples was used to compare robot behaviors based on the degree of anthropomorphism.",TRUE,Robot-verbal-communication-style; Robot-aesthetics,Robot-verbal-communication-style,,"The study manipulated the robot's behavior to be either assertive or submissive, which is a change in the style of verbal communication, specifically how the robot expressed its willingness to perform a task. This is why 'Robot-verbal-communication-style' was chosen. The robot was also introduced with either a male or female name, which is a manipulation of the robot's aesthetics, specifically its perceived gender, which is why 'Robot-aesthetics' was chosen. The study found that the robot's assertive behavior (a style of verbal communication) impacted the children's perception of the robot's mental state and led to more commanding responses, indicating that 'Robot-verbal-communication-style' impacted trust. There was no mention of the robot's perceived gender impacting trust, so 'Robot-aesthetics' was not included in 'factors_that_impacted_trust'.",10.1007/s12369-024-01139-9,https://link.springer.com/10.1007/s12369-024-01139-9,"This paper contributes to the understanding of child-robot interaction through the investigation of child interactions with and anthropomorphization of humanoid robots when manipulating robot-related variables such as behavior and gender. In this study, children observe a robot demonstration in a classroom setting, during which the robot showcases either assertive or submissive behavior and is attributed a gender, either robot-female or robot-male. Afterwards, participant anthropomorphization is measured using the Attributed Mental States Questionnaire (AMS-Q). Results suggest that when prompted to select a response directed at the robot, children used signiﬁcantly more commanding phrases when addressing the assertively behaving robot when compared to the submissively behaving robot. Further, younger children ages 7–9 anthropomorphize robots at a higher degree than older children 10–12 and assertive behavior from the robot lead to higher rates of anthropomorphization. Results also suggest that children are more likely to respond to female robots in an imperative way than male robots. This widened understanding of child perception of and interaction with humanoid robots can contribute to the design of acceptable robot interaction patterns in various settings."
"Malle, Bertram F.; Ullman, Daniel",A multidimensional conception and measure of human-robot trust,2021,3,62,62,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants rated 62 words on a slider scale from 'more similar to capacity trust' to 'more similar to personal trust'. Principal Components Analysis (PCA) and item analysis were used to arrive at 32 items distributed over four components, which were then shortened to five items each.",Participants rated the similarity of trust-related words to 'capacity trust' and 'personal trust'.,Unspecified,,Research,Evaluation,Rating,passive observation,Participants only read and rated words related to trust.,media,The interaction was based on text descriptions of trust-related words.,hypothetical,The study involved no physical or simulated robot.,not autonomous,No robot was involved in the study.,Custom Scales,,,Trust was assessed using a custom scale based on word ratings.,no modeling,No computational model of trust was used.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,No factors were manipulated; participants rated words based on their understanding of trust.,,"The study found that trust-related words clustered into four components: Reliable, Capable, Sincere, and Ethical.","Trust can be conceptualized as having four dimensions: Reliable, Capable, Sincere, and Ethical.",Participants rated the similarity of trust-related words to 'capacity trust' and 'personal trust'.,Principal component analysis; item analysis,"Principal Components Analysis (PCA) was used to identify underlying components in the word ratings. Item analysis was used to refine the selection of items for each component, resulting in 32 items distributed over four components, which were then shortened to five items each.",FALSE,,,,"This study did not involve any manipulation of factors. Participants were asked to rate words on a scale, but no experimental conditions were manipulated. Therefore, no factors were manipulated, and no factors impacted or did not impact trust.",,http://www.sciencedirect.com/science/article/pii/B9780128194720000010,"Robots are increasingly used in social applications, which raise challenges regarding people's trust in robots. A modern conception of human-robot trust must go beyond the conventional notions of human-automation relations and better connect to the current understanding of human-human trust, without assuming that human-robot trust is identical. A review of the literature together with our recent empirical work suggests that trust is multidimensional, incorporating both performance aspects (central in the human-automation literature) and moral aspects (central in the human-human trust literature). A multidimensional conception can be applied to human-robot trust, even if only some of the dimensions will be relevant for any given interaction with a robot. In addition to proposing an integrative conception of trust, we offer a measurement instrument for public use: the Multidimensional Measure of Trust (MDMT). This measure captures two superordinate factors of trust (Performance trust, Moral trust) that each break into two subfacets (Reliable and Capable within Performance, and Sincere and Ethical within Moral). We are continuing to test this measure in follow-up research and encourage other researchers to join us in collectively validating it."
"Malle, Bertram F.; Ullman, Daniel",A multidimensional conception and measure of human-robot trust,2021,3,60,60,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants sorted 32 words or short phrases into four categories representing different character traits: Reliable, Capable, Sincere, and Ethical. Sorting consensus scores were computed to group items.",Participants sorted trust-related words into categories representing different dimensions of trust.,Unspecified,,Research,Evaluation,Rating,passive observation,Participants only read and sorted words related to trust.,media,The interaction was based on text descriptions of trust-related words.,hypothetical,The study involved no physical or simulated robot.,not autonomous,No robot was involved in the study.,Custom Scales,,,Trust was assessed using a custom scale based on word sorting.,no modeling,No computational model of trust was used.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,No factors were manipulated; participants sorted words based on their understanding of trust.,,The study replicated the four dimensions of trust expectations found in the previous study.,"The four dimensions of trust (Reliable, Capable, Sincere, and Ethical) were replicated using a sorting task.",Participants sorted trust-related words into categories representing different dimensions of trust.,,"Sorting consensus scores were computed to group items based on how participants classified them into the four categories (Reliable, Capable, Sincere, and Ethical). No specific statistical tests were mentioned.",FALSE,,,,"This study did not involve any manipulation of factors. Participants were asked to sort words into categories, but no experimental conditions were manipulated. Therefore, no factors were manipulated, and no factors impacted or did not impact trust.",,http://www.sciencedirect.com/science/article/pii/B9780128194720000010,"Robots are increasingly used in social applications, which raise challenges regarding people's trust in robots. A modern conception of human-robot trust must go beyond the conventional notions of human-automation relations and better connect to the current understanding of human-human trust, without assuming that human-robot trust is identical. A review of the literature together with our recent empirical work suggests that trust is multidimensional, incorporating both performance aspects (central in the human-automation literature) and moral aspects (central in the human-human trust literature). A multidimensional conception can be applied to human-robot trust, even if only some of the dimensions will be relevant for any given interaction with a robot. In addition to proposing an integrative conception of trust, we offer a measurement instrument for public use: the Multidimensional Measure of Trust (MDMT). This measure captures two superordinate factors of trust (Performance trust, Moral trust) that each break into two subfacets (Reliable and Capable within Performance, and Sincere and Ethical within Moral). We are continuing to test this measure in follow-up research and encourage other researchers to join us in collectively validating it."
"Malle, Bertram F.; Ullman, Daniel",A multidimensional conception and measure of human-robot trust,2021,3,798,798,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants read a baseline sentence about a robot's behavior and provided initial trust ratings on the MDMT. Then they received new information designed to prompt either an increase or decrease in dimension-specific trust in the robot, followed by final ratings on the MDMT. A Principal Component Analysis (PCA) was performed on the change scores.",Participants rated their trust in a robot before and after receiving new information about the robot's behavior.,Unspecified,,Research,Evaluation,Rating,passive observation,Participants read text descriptions of robot behavior and rated their trust.,media,The interaction was based on text descriptions of robot behavior.,hypothetical,The study involved no physical or simulated robot.,not autonomous,No robot was involved in the study.,Questionnaires; Multidimensional Measures,Multi-Dimensional Measure of Trust (MDMT),,Trust was assessed using the Multi-Dimensional Measure of Trust (MDMT).,no modeling,No computational model of trust was used.,Observational & Survey Studies,Quantitative Surveys,Direct Manipulation,"Participants received new information designed to increase or decrease dimension-specific trust in the robot, specifically manipulating the robot's reliability and ethical behavior.","The subscales were sensitive to evidence change overall, but not systematically sensitive to only dimension-specific information. The measures of Reliable and Capable moved in tandem, as did the measures of Sincere and Ethical.","The study found that the measures of Reliable and Capable moved together, as did the measures of Sincere and Ethical, suggesting a two-factor structure of trust (Performance and Moral).",The study provided evidence for a two-factor model of human-robot trust: Performance trust and Moral trust.,Participants rated their trust in a robot before and after receiving new information about the robot's behavior.,Principal component analysis; ANOVA,A Principal Component Analysis (PCA) was performed on the change scores of the MDMT to examine the underlying structure of the trust dimensions. ANOVA was used to assess the sensitivity of the four subscales to the experimental manipulation of trust-relevant evidence.,TRUE,Robot-morality; Robot-accuracy,Robot-morality; Robot-accuracy,,"The study manipulated the information participants received about the robot's behavior to prompt either an increase or decrease in dimension-specific trust. Specifically, the manipulations targeted the robot's reliability (related to accuracy) and ethical behavior (related to morality). The paper states, 'Then they received new information designed to prompt either an increase or decrease in dimension-specific trust in the robot'. The results showed that the subscales were sensitive to evidence change overall, indicating that both the manipulations of reliability and ethical behavior impacted trust. The paper states, 'each of the four subscales was highly sensitive to evidence change overall'. The paper also states, 'the measures of Reliable and Capable moved in tandem across manipulated evidence, and so did the measures of Sincere and Ethical', which indicates that both accuracy and morality manipulations impacted trust.",,http://www.sciencedirect.com/science/article/pii/B9780128194720000010,"Robots are increasingly used in social applications, which raise challenges regarding people's trust in robots. A modern conception of human-robot trust must go beyond the conventional notions of human-automation relations and better connect to the current understanding of human-human trust, without assuming that human-robot trust is identical. A review of the literature together with our recent empirical work suggests that trust is multidimensional, incorporating both performance aspects (central in the human-automation literature) and moral aspects (central in the human-human trust literature). A multidimensional conception can be applied to human-robot trust, even if only some of the dimensions will be relevant for any given interaction with a robot. In addition to proposing an integrative conception of trust, we offer a measurement instrument for public use: the Multidimensional Measure of Trust (MDMT). This measure captures two superordinate factors of trust (Performance trust, Moral trust) that each break into two subfacets (Reliable and Capable within Performance, and Sincere and Ethical within Moral). We are continuing to test this measure in follow-up research and encourage other researchers to join us in collectively validating it."
"Manchon, J. B.; Bueno, Mercedes; Navarro, Jordan",Calibration of Trust in Automated Driving: A Matter of Initial Level of Trust and Automated Driving Style?,2021,1,165,61,104,"104 participants were excluded based on their initial trust score, to create two homogenous groups of Trustful and Distrustful people",Controlled Lab Environment,mixed design,"Participants completed an initial trust questionnaire, then participated in two simulated driving sessions separated by one week. During the sessions, they experienced either a positive or negative driving style and were exposed to several driving scenarios, including an unplanned take-over request. Trust was measured at multiple points during the sessions and again four months later.","Participants were asked to engage in simulated automated driving, monitoring the environment and taking over control when prompted.",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a simulated automated driving system, with limited direct interaction.",simulation,The study used a driving simulator to create an interactive virtual environment.,simulated,The robot was a simulated autonomous vehicle within the driving simulator.,fully autonomous (limited adaptation),"The simulated vehicle operated autonomously, but with limited adaptation to unexpected scenarios.",Questionnaires; Real-time Trust Measures; Behavioral Measures,,Eye-tracking Data; Performance Metrics,"Trust was assessed using questionnaires, real-time trust ratings, and behavioral measures such as eye-tracking and take-over performance.",no modeling,"The study did not use computational models of trust, focusing on statistical analysis of the collected data.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The study manipulated the automated driving style (positive or negative) and the initial level of trust of the participants (trustful or distrustful) to influence trust calibration.,"Trust increased over time, with different calibration patterns for trustful and distrustful participants. Driving style also influenced trust, with negative driving styles leading to more erratic trust calibration. The initial level of trust had a persistent effect on trust.","Distrustful participants showed a stronger variation in trust during the drives, particularly in the negative driving style condition. The initial level of trust had a persistent effect on trust, even after the first interaction. The study also found that the same scenario inspired more trust on the second occasion, suggesting that drivers' expectations and mental models are updated during and after an experience with HAD. There was a conflict between the study's findings and previous studies regarding the effect of initial trust on take-over performance.","The initial level of trust significantly influences trust calibration during and after interaction with an automated driving system, and this initial level has a persistent effect on trust.","The robot (simulated autonomous vehicle) drove along a highway, managing lateral and longitudinal control. The human participant monitored the driving environment and took over control when prompted by the system, including during an unplanned take-over request.",ANOVA; Levene's test; ANOVA; t-test,"The study used a mixed-design ANOVA to investigate the effects of initial trust level, driving style, and experience with HAD on subjective trust, both from questionnaires and single trust items. A repeated measures ANOVA was used to analyze the 11 single trust item measures. Levene's test was used to check for equal variances between the trustful and distrustful groups. A t-test was used to compare drivers who were monitoring the driving environment when the TOR was triggered and drivers who were not. Additionally, ANOVAs were used to analyze visual strategies during monotonous periods and specific scenarios, and to analyze time-to-collision profiles during the unplanned take-over request.",TRUE,Robot-accuracy; Robot-autonomy,Robot-accuracy,Robot-autonomy,"The study manipulated the automated driving style, which directly impacts the robot's (simulated vehicle) performance and thus its accuracy. This is categorized as 'Robot-accuracy' because the driving style (positive/smooth vs. negative/abrupt) directly influenced the smoothness and safety of the vehicle's actions, which are performance metrics. The study also implicitly manipulated 'Robot-autonomy' by having the system initiate take-over requests (TORs), both planned and unplanned, which changes the level of decision authority and control between the human and the automated system. The 'Robot-accuracy' was found to impact trust, as the negative driving style led to more erratic trust calibration. The 'Robot-autonomy' was not found to impact trust, as the take-over requests did not have a differential impact on trust based on the initial level of trust or driving style.",10.1177/00187208211052804,http://journals.sagepub.com/doi/10.1177/00187208211052804,"Objective: Automated driving is becoming a reality, and such technology raises new concerns about human–machine interaction on road. This paper aims to investigate factors inﬂuencing trust calibration and evolution over time. Background: Numerous studies showed trust was a determinant in automation use and misuse, particularly in the automated driving context. Method: Sixty-one drivers participated in an experiment aiming to better understand the inﬂuence of initial level of trust (Trustful vs. Distrustful) on drivers’ behaviors and trust calibration during two sessions of simulated automated driving. The automated driving style was manipulated as positive (smooth) or negative (abrupt) to investigate human–machine early interactions. Trust was assessed over time through questionnaires. Drivers’ visual behaviors and take-over performances during an unplanned take-over request were also investigated. Results: Results showed an increase of trust over time, for both Trustful and Distrustful drivers regardless the automated driving style. Trust was also found to ﬂuctuate over time depending on the speciﬁc events handled by the automated vehicle. Take-over performances were not inﬂuenced by the initial level of trust nor automated driving style. Conclusion: Trust in automated driving increases rapidly when drivers’ experience such a system. Initial level of trust seems to be crucial in further trust calibration and modulate the effect of automation performance. Long-term trust evolutions suggest that experience modify drivers’ mental model about automated driving systems."
"Mangalindan, Dong Hae; Rovira, Ericka; Srivastava, Vaibhav",On Trust-aware Assistance-seeking in Human-Supervised Autonomy,2023,2,9,9,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants observed a robot performing object collection tasks, sometimes autonomously and sometimes with human assistance. The robot's assistance-seeking policy was randomized. Data was collected on human actions, robot actions, task complexity, and task outcome.","Participants supervised a robot performing object collection tasks, intervening when they perceived the robot might fail or assisting when asked.",Unspecified,Mobile Manipulators,Research,Supervision,Monitoring,minimal interaction,"Participants observed the robot and could intervene or assist, but there was no direct physical contact.",media,Participants viewed the robot's actions through live video feeds.,physical,The robot was a physical mobile manipulator.,shared control (fixed rules),The robot operated autonomously but could request human assistance based on pre-set rules.,Behavioral Measures,,Performance Metrics; robot data,Trust was inferred from human intervention and reliance on the robot.,hidden markov model,A Hidden Markov Model was used to model human trust as a hidden state.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's assistance-seeking behavior was randomized, and the task difficulty was varied by the presence of obstacles, influencing human trust through robot performance and behavior.","The study found that human trust was influenced by the robot's success or failure, and the complexity of the task. Success in complex tasks increased trust, while failure in easy tasks decreased trust.","The study found that success in easy tasks did not repair trust, while failure in easy tasks had a negative effect on trust. Also, seeking assistance in high complexity may increase trust.","Human trust is influenced by robot performance and task complexity, with success in complex tasks increasing trust and failure in easy tasks decreasing it.","The robot autonomously attempts to collect objects, sometimes asking for human assistance. The human monitors the robot and intervenes if they perceive a failure or assists when asked.",,"No specific statistical tests were explicitly mentioned in the description of the first experiment. The study focused on estimating parameters of a Hidden Markov Model (HMM) using an extended Baum-Welch algorithm, which is a parameter estimation technique rather than a statistical test for hypothesis testing. The data collected was used to estimate the probabilities of the human behavioral model.",TRUE,Robot-autonomy; Task-complexity,Robot-autonomy; Task-complexity,,"The study manipulated 'Robot-autonomy' by randomizing the robot's assistance-seeking policy, sometimes operating autonomously and sometimes requesting human assistance. This is described in the paper as 'In the first set, we adopt a randomized assistance-seeking policy'. The 'Task-complexity' was manipulated by varying the presence of obstacles in the robot's path, as stated in the paper: 'The complexity C of a trial is determined by the presence of an obstacle in the direct path between the manipulator and the object to be collected, and it can either be low C L or high C H'. Both of these factors were found to impact trust. The paper states 'For most participants, the estimated POMDP reveals that humans are more likely to intervene when their trust is low and the robot is performing a highcomplexity task; and that robot asking for assistance in highcomplexity tasks can increase human trust in the robot.' This indicates that both the robot's autonomy (when it seeks assistance) and the task complexity influenced human trust.",10.23919/ACC55779.2023.10156103,https://ieeexplore.ieee.org/document/10156103/,"Using the context of human-supervised object collection tasks, we explore policies for a robot to seek assistance from a human supervisor and avoid loss of human trust in the robot. We consider a human-robot interaction scenario in which a mobile manipulator chooses to collect objects either autonomously or through human assistance; while the human supervisor monitors the robot’s operation, assists when asked, or intervenes if the human perceives that the robot may not accomplish its goal. We design an optimal assistance-seeking policy for the robot using a Partially Observable Markov Decision Process (POMDP) setting in which human trust is a hidden state and the objective is to maximize collaborative performance. We conduct two sets of human-robot interaction experiments. The data from the first set of experiments is used to estimate POMDP parameters, which are used to compute an optimal assistance-seeking policy that is used in the second experiment. For most participants, the estimated POMDP reveals that humans are more likely to intervene when their trust is low and the robot is performing a highcomplexity task; and that robot asking for assistance in highcomplexity tasks can increase human trust in the robot. Our experimental results show that the proposed trust-aware policy yields superior performance compared with an optimal trustagnostic policy."
"Mangalindan, Dong Hae; Rovira, Ericka; Srivastava, Vaibhav",On Trust-aware Assistance-seeking in Human-Supervised Autonomy,2023,2,5,5,0,No participants were excluded,Controlled Lab Environment,within-subjects,Participants interacted with a robot using both a trust-aware and a trust-agnostic assistance-seeking policy. The order of the policies was randomized. The cumulative reward was measured for each policy.,"Participants supervised a robot performing object collection tasks, intervening when they perceived the robot might fail or assisting when asked.",Unspecified,Mobile Manipulators,Research,Supervision,Monitoring,minimal interaction,"Participants observed the robot and could intervene or assist, but there was no direct physical contact.",media,Participants viewed the robot's actions through live video feeds.,physical,The robot was a physical mobile manipulator.,shared control (adaptive),The robot operated autonomously but adapted its assistance-seeking behavior based on the estimated human trust.,Behavioral Measures,,Performance Metrics; robot data,Trust was inferred from human intervention and reliance on the robot.,POMDP,A POMDP was used to model human trust as a hidden state and to design an optimal assistance-seeking policy.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's assistance-seeking policy was manipulated to be either trust-aware or trust-agnostic, influencing human trust through the robot's behavior and autonomy.","The trust-aware policy resulted in higher cumulative rewards compared to the trust-agnostic policy, indicating that the trust-aware policy was more effective at maintaining human trust and reducing unnecessary interventions.","The study noted that some participants were more prone to low trust and intervened more often, highlighting individual differences in trust dynamics.","A trust-aware assistance-seeking policy, designed using a POMDP framework, outperforms a trust-agnostic policy in a human-supervised object collection task.","The robot autonomously attempts to collect objects, sometimes asking for human assistance based on its trust model. The human monitors the robot and intervenes if they perceive a failure or assists when asked.",,"No specific statistical tests were explicitly mentioned in the description of the second experiment. The study compared the cumulative reward obtained under two different robot policies (trust-aware and trust-agnostic) and noted that the trust-aware policy outperformed the trust-agnostic policy for most participants. The comparison was based on median scores, but no specific statistical test was used to determine statistical significance.",TRUE,Robot-autonomy,Robot-autonomy,,"The study manipulated 'Robot-autonomy' by comparing a trust-aware assistance-seeking policy with a trust-agnostic policy. The paper states 'Every participant performed two blocks of experiments in which the robot adopted trust-aware and trustagnostic policies, respectively.' The trust-aware policy adapts its assistance-seeking behavior based on estimated human trust, while the trust-agnostic policy does not. The paper also states 'The trust-aware policy outperformed the trust-agnostic policy for most participants. With the trust-aware policy, the manipulator collected more items autonomously as compared to the trust-agnostic policy.' This indicates that the manipulation of the robot's autonomy through different assistance-seeking policies impacted human trust and performance.",10.23919/ACC55779.2023.10156103,https://ieeexplore.ieee.org/document/10156103/,"Using the context of human-supervised object collection tasks, we explore policies for a robot to seek assistance from a human supervisor and avoid loss of human trust in the robot. We consider a human-robot interaction scenario in which a mobile manipulator chooses to collect objects either autonomously or through human assistance; while the human supervisor monitors the robot’s operation, assists when asked, or intervenes if the human perceives that the robot may not accomplish its goal. We design an optimal assistance-seeking policy for the robot using a Partially Observable Markov Decision Process (POMDP) setting in which human trust is a hidden state and the objective is to maximize collaborative performance. We conduct two sets of human-robot interaction experiments. The data from the first set of experiments is used to estimate POMDP parameters, which are used to compute an optimal assistance-seeking policy that is used in the second experiment. For most participants, the estimated POMDP reveals that humans are more likely to intervene when their trust is low and the robot is performing a highcomplexity task; and that robot asking for assistance in highcomplexity tasks can increase human trust in the robot. Our experimental results show that the proposed trust-aware policy yields superior performance compared with an optimal trustagnostic policy."
"Mangalindan, Dong Hae; Srivastava, Vaibhav",Assistance-Seeking in Human-Supervised Autonomy: Role of Trust and Secondary Task Engagement,2024,2,11,11,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a dual-task paradigm, simultaneously supervising a robot collecting objects and performing a target-tracking task. The robot either autonomously collected objects or asked for assistance. Participants reported their trust after each trial.",Participants supervised a robot collecting objects while also performing a target-tracking task.,Unspecified,Mobile Manipulators,Research,Supervision,Monitoring,minimal interaction,Participants observed the robot's actions on a screen and could intervene via teleoperation.,simulation,The interaction was conducted in a simulated environment displayed on a screen.,simulated,The robot was represented virtually on a screen.,shared control (fixed rules),"The robot operated autonomously but could request assistance, and the human could intervene.",Custom Scales,,Performance Metrics,Trust was measured using an 11-point scale after each trial and performance metrics were collected.,"parametric models (e.g., regression)",Trust dynamics were modeled using a linear dynamical system with parameters estimated using the EM algorithm.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's success rate and the complexity of the object collection task were manipulated to influence trust. The robot's autonomy level was also manipulated by allowing it to ask for assistance.,"Trust decreased after robot failures and human interventions, while successful collections and robot requests for assistance increased trust.","The study found that asking for assistance can help increase human trust, which is consistent with previous findings.",The study found that successful collection increases and maintains trust; failed collections decrease it; and asking for assistance can help repair and increase trust.,"The robot autonomously attempts to collect objects or asks for human assistance. The human supervises the robot and can intervene via teleoperation, while also performing a target-tracking task.",expectation-maximization (em) algorithm,"The Expectation-Maximization (EM) algorithm was used to estimate the parameters of the linear dynamical system (LDS) model for human trust dynamics. This involved estimating parameters such as A_T, B_T, C_T, σ²_T, and σ²_y based on the reported trust values after each trial. The EM algorithm was used to find the maximum likelihood estimates of the model parameters given the observed data.",TRUE,Robot-accuracy; Robot-autonomy; Task-complexity,Robot-accuracy; Robot-autonomy; Task-complexity,,"The study manipulated 'Robot-accuracy' by varying the success rate of the robot's object collection attempts, with different success probabilities for low and high complexity tasks (p suc H = 0.75 in C 1,H and p suc L = 0.96 in C 1,L). This is explicitly stated in the paper: 'When the manipulator operates autonomously, it has a success probability of p suc H = 0.75 in C 1,H and p suc L = 0.96 in C 1,L'. 'Robot-autonomy' was manipulated by allowing the robot to either autonomously collect objects or request human assistance, as described in the paper: 'In each trial, the mobile manipulator has the choice of attempting autonomous collection (a R+ ) or requesting human assistance (a R− )'. 'Task-complexity' was manipulated by varying the difficulty of the object collection task based on whether the robot's direct path to the object was obstructed (high complexity) or not (low complexity), as stated in the paper: 'Specifically, the complexity is high (C 1 = C 1,H ) if the manipulator's direct path to the object is obstructed by an obstacle; otherwise, the complexity is low (C 1 = C 1,L )'. The paper also states that 'successful collection increases and maintains trust; failed collections decrease it; and asking for assistance can help repair and increase trust', indicating that all three manipulated factors impacted trust.",10.1109/RO-MAN60168.2024.10731300,https://ieeexplore.ieee.org/document/10731300/,"Using a dual-task paradigm, we explore how robot actions, performance, and the introduction of a secondary task influence human trust and engagement. In our study, a human supervisor simultaneously engages in a target-tracking task while supervising a mobile manipulator performing an object collection task. The robot can either autonomously collect the object or ask for human assistance. The human supervisor also has the choice to rely on or interrupt the robot. Using data from initial experiments, we model the dynamics of human trust and engagement using a linear dynamical system (LDS). Furthermore, we develop a human action model to define the probability of human reliance on the robot. Our model suggests that participants are more likely to interrupt the robot when their trust and engagement are low during high-complexity collection tasks. Using Model Predictive Control (MPC), we design an optimal assistance-seeking policy. Evaluation experiments demonstrate the superior performance of the MPC policy over the baseline policy for most participants."
"Mangalindan, Dong Hae; Srivastava, Vaibhav",Assistance-Seeking in Human-Supervised Autonomy: Role of Trust and Secondary Task Engagement,2024,2,5,5,0,No participants were excluded,Controlled Lab Environment,within-subjects,Participants completed two experiment blocks where the robot followed either an MPC or a greedy policy in a randomized order. Trust and task engagement were estimated based on participant actions and tracking performance.,Participants supervised a robot collecting objects while also performing a target-tracking task.,Unspecified,Mobile Manipulators,Research,Supervision,Monitoring,minimal interaction,Participants observed the robot's actions on a screen and could intervene via teleoperation.,simulation,The interaction was conducted in a simulated environment displayed on a screen.,simulated,The robot was represented virtually on a screen.,shared control (adaptive),The robot's assistance-seeking policy adapted based on estimated human trust and engagement.,Behavioral Measures; Performance-Based Measures; Real-time Trust Measures,,Performance Metrics,Trust was estimated based on participant actions and tracking performance using a particle filter.,"parametric models (e.g., regression)",Trust and engagement were modeled using linear dynamical systems and a particle filter was used for real-time estimation.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's assistance-seeking policy was manipulated using MPC, which adapted based on estimated human trust and engagement. The task difficulty was also manipulated by varying the complexity of the object collection task.","The MPC policy, which adapted based on trust and engagement, resulted in higher cumulative rewards and fewer interruptions compared to the greedy policy.","The MPC policy outperformed the greedy policy, indicating that adapting robot behavior based on human trust and engagement can improve team performance.",The optimal assistance-seeking policy is to never seek assistance in low-complexity object-collection trials and to seek assistance in high-complexity object-collection trials only when the trust is below a secondary-task-engagement-dependent threshold.,"The robot autonomously attempts to collect objects or asks for human assistance based on an MPC policy. The human supervises the robot and can intervene via teleoperation, while also performing a target-tracking task.",expectation-maximization (em) algorithm; maximum likelihood estimation; monte carlo simulation; particle filter,"The Expectation-Maximization (EM) algorithm was used to estimate the parameters of the linear dynamical system (LDS) model for human target-tracking engagement dynamics. This involved estimating parameters such as A_G, B_G, C_G, σ²_G, and σ²_p. Maximum Likelihood Estimation using the Monte Carlo simulation method was used to learn the parameters of the human action model, which predicts the probability of human reliance on the robot. A particle filter was used to estimate trust and task engagement based on participant actions and tracking performance in real-time, instead of using reported trust values.",TRUE,Robot-autonomy; Task-complexity,Robot-autonomy; Task-complexity,,"In this study, 'Robot-autonomy' was manipulated by using two different assistance-seeking policies: an MPC policy that adapted based on estimated human trust and engagement, and a greedy policy that always attempted autonomous collection. This is described in the paper: 'We recruited 5 participants (3 females and 2 males) who completed two experiment blocks where the robot followed the MPC and greedy policies, in a randomized order.' and 'The optimal baseline policy, referred to as the ""greedy policy"", always attempts autonomous collection.' The paper also states that 'The MPC policy outperforms the greedy policy for most participants' and 'The optimal assistance-seeking policy is to never seek assistance in low-complexity object-collection trials and to seek assistance in high-complexity object-collection trials only when the trust is below a secondary-task-engagement-dependent threshold', indicating that the robot's autonomy level impacted trust. 'Task-complexity' was manipulated by varying the difficulty of the object collection task, as in the first study, with low and high complexity trials, as stated in the paper: 'Each block included 15 trials each of low-and high-complexity object collection tasks'. The paper also states that the optimal policy is different for low and high complexity tasks, indicating that task complexity impacted trust.",10.1109/RO-MAN60168.2024.10731300,https://ieeexplore.ieee.org/document/10731300/,"Using a dual-task paradigm, we explore how robot actions, performance, and the introduction of a secondary task influence human trust and engagement. In our study, a human supervisor simultaneously engages in a target-tracking task while supervising a mobile manipulator performing an object collection task. The robot can either autonomously collect the object or ask for human assistance. The human supervisor also has the choice to rely on or interrupt the robot. Using data from initial experiments, we model the dynamics of human trust and engagement using a linear dynamical system (LDS). Furthermore, we develop a human action model to define the probability of human reliance on the robot. Our model suggests that participants are more likely to interrupt the robot when their trust and engagement are low during high-complexity collection tasks. Using Model Predictive Control (MPC), we design an optimal assistance-seeking policy. Evaluation experiments demonstrate the superior performance of the MPC policy over the baseline policy for most participants."
"Manor, Adi; Parush, Avi; Erel, Hadas",Attentiveness: A Key Factor in Fostering Affective and Cognitive Trust with Non-Humanoid Robots,2024,1,51,51,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were asked to share their future plans with a robot in one of three conditions: Attentive, Inattentive, or Baseline. After the interaction, they completed questionnaires and a semi-structured interview.",Participants shared their future plans with a robot.,Unspecified,Other,Research; Social,Social,Self-Disclosure,minimal interaction,Participants interacted with the robot by sharing their future plans.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot performed pre-programmed gestures without adapting to the user.,Behavioral Measures; Questionnaires; Custom Scales,Robotic Social Attributes Scale (RoSAS); N/A,,"Trust was measured using questionnaires, a custom scale for interpersonal distance, and direct questions.",no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's attentiveness was manipulated through different gestures, including leaning, gazing, and nodding, to influence trust.","Attentive robot behavior increased both affective and cognitive trust, while inattentive behavior decreased trust.","Participants associated their cognitive trust with their affective trust, suggesting that the robot's perceived attentiveness influenced both dimensions of trust. Some participants in the inattentive condition still trusted the robot due to its perceived technological capabilities.","Attentive robotic behavior, even with minimal gestures, can enhance both affective and cognitive trust in human-robot interactions.",The robot performed pre-programmed attentive or inattentive gestures while the human participant shared their future plans. The robot was a simple desk lamp-like object.,ANOVA; Scheffe post-hoc; Chi-squared,"The study used one-way ANOVA to analyze the ratings of the affective and cognitive trust scales, as well as the interpersonal distance scores. Scheffe post-hoc comparisons were used to determine where significant differences existed between the three conditions (Attentive Robot, Inattentive Robot, and Baseline). A chi-square test of independence was used to analyze the frequency of yes/no answers to direct questions about trust in the different conditions.",TRUE,Robot-nonverbal-communication,Robot-nonverbal-communication,,"The study manipulated the robot's attentiveness through different non-verbal gestures, including leaning, gazing, and nodding. These gestures were designed to create either an attentive or inattentive behavior. The 'Attentive Robot' condition involved the robot turning towards the participant, leaning and gazing upwards and towards the participant, and subtly nodding. The 'Inattentive Robot' condition involved the robot turning towards the participant, leaning and gazing upwards and towards the participant, then leaning back, turning away from the participant, and directing its gaze towards the wall, and then turning back to face the participant. The 'Baseline' condition involved the robot remaining stationary. These manipulations directly altered the robot's physical movements and posture, which falls under the category of 'Robot-nonverbal-communication'. The results showed that the manipulated non-verbal communication significantly impacted both affective and cognitive trust, as participants in the 'Attentive Robot' condition reported higher trust scores compared to the other conditions. Therefore, 'Robot-nonverbal-communication' is the factor that impacted trust. There were no other factors manipulated in the study.",10.1109/RO-MAN60168.2024.10731320,https://ieeexplore.ieee.org/document/10731320/,"Affective trust and cognitive trust are fundamental elements in human-robot interactions. They impact robots’ acceptance, the level of engagement, and the tendency to rely on robots in various contexts. Opposite effects are observed when the interaction with a robot is unreliable and untrustworthy. In this study, we examined the possibility of enhancing both aspects of trust by manipulating the robot’s level of attentiveness to the participant. We focused on robotic attentiveness since it can be easily applied even to highly simple non-humanoid robots, positioning it as a method for enhancing trust for robots with different morphologies. Specifically, we evaluated whether minimal attentive robotic gestures can enhance affective and cognitive aspects of trust and whether inattentive robotic behavior can decrease them. Quantitative and qualitative results indicated that the robot’s attentiveness impacted both aspects of trust. Participants in the Attentive Robot condition reported higher affective and cognitive trust scores, smaller interpersonal distance, and a higher number of participants reported that the robot would ”be there for them”, in comparison to the Inattentive Robot and Baseline conditions. Our findings suggest that an attentive robotic behavior, can support human affective and cognitive trust and enhance human-robot interaction."
"Marble, J.L.; Bruemmer, D.J.; Few, D.A.; Dudenhoeffer, D.D.",Evaluation of supervisory vs. peer-peer interaction with human-robot teams,2004,1,11,11,0,No participants were excluded,Real-World Environment,within-subjects,"Participants searched a building for targets using a robot with varying levels of autonomy. They completed four searches, three with a fixed autonomy level (teleoperation, safe mode, or shared control) and one with dynamic autonomy. During each search, they also solved math problems.","Participants were tasked with locating three targets in a building using a robot, while also identifying secondary targets and solving math problems.",ATRV Jr,Mobile Robots; Unmanned Ground Vehicles,Research,Navigation,Path Following,minimal interaction,"Participants interacted with the robot through a screen and joystick, giving commands and receiving feedback.",real-world,"The study was conducted in a real building, with the robot navigating a physical environment.",physical,"A physical robot was used in the study, interacting with the real-world environment.",shared control (adaptive),"The robot had varying levels of autonomy, including shared control where it adapted its path based on user input and its own sensors.",Questionnaires; Behavioral Measures; Performance-Based Measures,,"Video Data; Performance Metrics; robot data (sensor data, etc.)","Trust was assessed using subjective questionnaires, behavioral observations, and performance metrics.",no modeling,Trust was not modeled computationally in this study.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The level of robot autonomy was directly manipulated, and the task difficulty was indirectly influenced by the environment. Feedback was provided through the interface.","The feeling of control was lowest in shared mode, where the robot had more autonomy, and highest in dynamic mode, where users could switch between autonomy levels. Participants' willingness to release control to the robot varied.","Participants varied in their willingness to release control to the robot in shared autonomy mode. Some participants relied heavily on video, while others used the obstacle indicator. Experienced users often tried to force the robot to follow their commands, even when the robot was blocked.","Participants' feeling of control was highest when they could dynamically switch between different levels of robot autonomy, suggesting that user agency is important for trust.","The robot navigated a building, avoiding obstacles and providing sensor data to the user. The human operator directed the robot's movement, identified targets, and solved math problems.",ANOVA; ANOVA; ANOVA; ANOVA,"The study used ANOVA to analyze the feeling of control across sessions and modes of control, the relationship between certainty of identifying secondary targets and actual performance, and the effect of secondary targets identified on the number of math problems solved. Additionally, ANOVA was used to analyze the usefulness of the video feed based on skill level, mode of autonomy, and the interaction between session and mode.",TRUE,Robot-autonomy; Task-complexity; Task-environment,Robot-autonomy,Task-complexity; Task-environment,"The study manipulated 'Robot-autonomy' by varying the level of control the robot had over navigation, from full teleoperation to shared control where the robot could make its own path decisions. This is explicitly stated in the 'System Design' section where four levels of autonomy are described: Teleoperation, Safe Mode, Shared Control, and Dynamic Control. The 'Method' section also describes how participants used different autonomy levels in different sessions. The study also manipulated 'Task-complexity' by including a secondary task of solving math problems while searching for targets, as described in the 'Method' section. The 'Task-environment' was manipulated by placing obstacles in the environment, as described in the 'Test Location' section, which influenced the difficulty of the navigation task. The results showed that 'Robot-autonomy' impacted trust, as participants felt the least control in shared mode where the robot had more autonomy, and the most control in dynamic mode where they could switch between autonomy levels, as described in the 'Quantitative results' section. The study found that the number of secondary targets identified was not affected by previous robotic experience, and the number of math problems solved was only slightly affected by the number of secondary targets identified, indicating that 'Task-complexity' did not have a significant impact on trust. The study also found that the map was not used much, indicating that the 'Task-environment' did not have a significant impact on trust.",10.1109/HICSS.2004.1265326,http://ieeexplore.ieee.org/document/1265326/,"We submit that the most interesting and fruitful humanrobot interaction (HRI) may be possible when the robot is able to interact with the human as a true team member, rather than a tool. However, the benefits of shared control can all too easily be overshadowed by challenges inherent to blending human and robot initiative. The most important requirements for peer-peer interaction are system trust and ability to predict system behavior. The human must be able to understand the reason for and effects of robot initiative. These requirements can only be met through careful application of human factors principles and usability testing to determine how users will interact with the system. This paper discusses the recent human participant usability testing, which took our current implementation to task using a search and rescue scenario within a complex, real-world environment. The purpose of testing was to examine how human operators work with the robotic system at each level of autonomy, and how interaction with the robot should be structured to enable situation awareness and task completion. Analyses revealed that our architecture equally supported situation awareness and target detection by novices and experts, although experienced users were more likely to have more performance expectations of the interface. Results also had implications regarding the ability of participants to effectively utilize the collaborative workspace and, most importantly, their ability to understand and willingness to accept robot initiative."
"Martinez, Jennifer E.; VanLeeuwen, Dawn; Stringam, Betsy Bender; Fraune, Marlena R.",Hey?: ! What did you think about that Robot? Groups Polarize Users' Acceptance and Trust of Food Delivery Robots,2023,1,60,60,0,No participants were excluded,Real-World Environment,between-subjects,"Participants were assigned to either an individual or group condition. They completed surveys before ordering food, after ordering food via an app, and after receiving food from a delivery robot. The study took place both in a lab and in the field.",Participants ordered food through an app and received it from a delivery robot.,Unspecified,Mobile Robots; Service and Assistive Robots,Service,Social,Other: Participants ordered food through an app and received it from a delivery robot.,minimal interaction,"Participants interacted with the robot by watching it approach, opening its hatch, and retrieving their food.",real-world,Participants interacted with a real robot in a real-world setting.,physical,Participants interacted with a physical food delivery robot.,fully autonomous (limited adaptation),The robot autonomously delivered food to designated pick-up points.,Multidimensional Measures; Questionnaires,Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using the MDMT questionnaire at three different time points.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The study manipulated the social context by having participants interact with the robot either individually or in groups.,"Individual participants showed increased trust with exposure, while group participants showed more variation and were less likely to increase in trust.","Group participants showed more variation in their responses, indicating group polarization. Individual participants showed a more consistent increase in trust over time.","Individual users increased in acceptance and trust of the robots over exposures, while groups had wider variance in their responses over time and were less likely to increase in acceptance and trust, consistent with patterns of group polarization.","The robot autonomously delivered food to a designated pick-up point. Participants ordered food through an app, watched the robot arrive, opened the robot's hatch using the app, and retrieved their food.",hierarchical linear mixed model; REML; kenward-roger method; cronbach's alpha,"The study used a hierarchical linear mixed model fitted using REML and computing denominator degrees of freedom using the Kenward-Roger method to compare differences between the Group condition (Gc) and Individual condition (Ic) across three exposures (Pre-Exposure, Exposure 1, and Exposure 2) on acceptance and trust measures. The model included fixed effects for Number (Gc/Ic), Exposure, and their interaction. For the repeated factor, Exposure fitted separate unstructured residual covariance matrices to Gc and Ic. Gc participants had an additional level corresponding to the groups of two to three people, which was accounted for with Group-level random effects having an unstructured covariance. Cronbach's alpha was used to assess the reliability of the subscales of the TAM and MDMT questionnaires.",TRUE,Task-environment,Task-environment,,"The study manipulated the social context by having participants interact with the robot either individually or in groups. This manipulation of the social context is best captured by the 'Task-environment' category, as it changes the conditions under which the task is performed. The paper states, 'We manipulated the Number of participants (Group, Individual) between subjects.' and 'The Group condition (Gc) included groups of two or three individuals, and the Individual condition (Ic) included individuals.' The results showed that the group condition had a different impact on trust than the individual condition, indicating that the manipulated factor, 'Task-environment', impacted trust. The paper states, 'Individual participants became more positive toward the robots with increasing exposure on trust (but not acceptance), but Group participants did not (H1 partially supported). Group participants also showed more variation in responses than Individual participants (H2 supported), indicative of Group polarization.' There were no other factors manipulated in the study.",10.1145/3568162.3576984,https://dl.acm.org/doi/10.1145/3568162.3576984,"As food delivery robots are spreading onto streets and college campuses worldwide, users’ views of these robots will depend on their direct and indirect interactions with the robots and their conversations with other people, such as those with whom they are ordering food via a robot. We examined if being in a group of 2 to 3 people affects the acceptance and trust of the robot compared to being an individual user. First-time users of the food delivery robot service (N = 60) ordered food either as an Individual or in a Group. We measured the acceptance and trust of the robots after three Exposures (pre-exposure, after ordering food on the app, and after the robots delivered the food). Results indicated that Individual users had more acceptance and trust compared to Group users. Further, as hypothesized, groups had more variation in acceptance and trust compared to individual users, consistent with patterns of group polarization – i.e., group members influencing each other’s perceptions to become more positive or negative. Further analysis demonstrated that group members were highly influenced by their groupmates. Designers and restaurant operators should consider how to enhance group members’ experience of delivery robots."
"Mason, Erika; Nagabandi, Anusha; Steinfeld, Aaron; Bruggeman, Christian",Trust During Robot-Assisted Navigation,2013,1,12,12,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants navigated an arrow maze using a robot for guidance, with varying levels of reliability in the robot's directions. Participants completed a pre-experiment questionnaire, four maze runs with post-run questionnaires, and an exit questionnaire.","Participants were tasked with navigating an arrow maze as quickly as possible, using a robot's directional advice displayed on a tablet.",TurtleBot,Mobile Robots,Research,Navigation,Guiding,minimal interaction,"Participants walked alongside the robot and received directional advice, but did not physically interact with it.",real-world,Participants interacted with a physical robot in a real-world maze environment.,physical,Participants interacted with a physical robot in the study.,wizard of oz (directly controlled),"The robot's directions were controlled by an experimenter, but participants were told the robot was autonomous.",Behavioral Measures; Questionnaires,,Performance Metrics,"Trust was assessed using questionnaires and behavioral measures, such as the number of times participants did not follow the robot's directions.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's reliability was manipulated by providing correct, long, or incorrect directions, which influenced participants' expectations and trust.",Trust decreased during low reliability runs but recovered after the robot provided correct directions. Initial trust in robots correlated with trust in the robot during the experiment.,"Participants generally followed the robot's directions, even when incorrect, indicating a strong initial trust. The order of reliability conditions influenced how participants perceived the robot's intentions, with the ABCA sequence leading to more suspicion of intentional errors.","Participants generally trusted the robot's directions more than their own judgment, and trust was re-established after the robot functioned correctly following a period of low reliability.","The robot provided directional advice on a tablet, and the human participant drove the robot through a maze while walking alongside it, deciding whether to follow the robot's guidance.",Pearson correlation; chronbach's alpha,"The study used correlation analysis to examine the relationship between participant's age and their general trust in robots, as well as the relationship between general trust in robots and trust in the robot at the end of the run. A Chronbach's alpha analysis was also used to assess the internal consistency of a set of post-experiment questions related to trust and robot evaluation.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the robot's accuracy by providing correct, long, or incorrect directions. This is explicitly stated in the 'Methods' section: 'Participants were exposed to 3 reliability levels. The most reliable level (A), guided the participant through the shortest and most efficient path of the maze. Reliability B took the participant through a long route. Reliability C guided the participant the wrong way, to a dead end or a never-ending loop, before recalculating and advising a correct and short path.' The 'Results' section also confirms that the robot's accuracy impacted trust: 'Whether the participant recognized a shorter path as the robot was taking them on a long path, or whether the robot led them to a dead end, the low reliability led to a decrease in the participants' trust.' Therefore, 'Robot-accuracy' is the most appropriate category for the manipulated factor and the factor that impacted trust. There were no other factors that were manipulated or that impacted trust.",,,"Robotics is becoming more integrated into society and small user-friendly robots are becoming more common in office spaces and homes. This increases the importance of trust in human-robot interaction, which is essential to understand in order to design systems that foster appropriate levels of trust. Too much or not enough trust in a robotic system can lead to inefficiencies, risks, and other damages. The robot in this experiment was used as a navigational system to guide a participant through an arrow maze. This experiment examined human trust in robots, the decision between doing a task or relying on a robot, and inconsistencies between human awareness and robot guidance."
"Matthews, Gerald; Lin, Jinchao; Panganiban, April Rose; Long, Michael D.",Individual Differences in Trust in Autonomous Robots: Implications for Transparency,2020,1,82,82,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants completed demographics, NARS, PAS, and RoTA scales online.",Participants rated their confidence in a robot's threat analysis and action recommendations across 20 security scenarios.,Unspecified,Other,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the robot and interaction scenarios.,media,The interaction was based on text descriptions of scenarios.,hypothetical,"The robot was only described in text, with no visual representation.",not autonomous,"The robot's actions were described in text, without any real autonomy.",Custom Scales; Questionnaires,Negative Attitude towards Robots Scale (NARS); N/A,,Trust was measured using custom scales and questionnaires.,"parametric models (e.g., regression)",Regression analysis was used to predict trust based on individual differences.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The type of analysis (physics-based or psychological) was manipulated to influence the mental model activated, and thus, trust.","Trust was higher for physics-based scenarios, and individual differences in attitudes towards robots predicted trust in psychological scenarios.","Women had more negative attitudes towards robots, and people with higher education had higher scores on the all-or-none thinking subscale of PAS. Participants were more confident in the robot's action recommendation than its analysis of threat in psychological scenarios.","Individual differences in mental models of robots, specifically the 'advanced tool' and 'teammate' models, influence trust in different types of robot analyses.","The robot was described as performing threat detection in security scenarios, using either physics-based or psychological analysis, and the human participant rated their confidence in the robot's analysis and action recommendations.",Factor analysis; principal axis extraction; direct oblimin rotation; cronbach alphas; paired-samples t-tests; Multilevel Model; maximum likelihood method; Linear regression,"The study used exploratory factor analysis (EFA) with principal axis extraction and direct oblimin rotation to identify underlying factors in the trust ratings. Cronbach's alpha was used to assess the internal consistency of the scales. Paired-samples t-tests were conducted to compare ratings between physics-based and psychological scenarios, and to compare confidence in analysis vs action recommendation. Confirmatory factor analysis (CFA) using maximum likelihood method was used to confirm the factor structure. Finally, regression analyses were performed to predict trust ratings based on demographic factors, PAS, and NARS scores.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the type of analysis the robot performed (physics-based or psychological) which was communicated to the participants through text descriptions of the scenarios. This manipulation directly influenced the content of the robot's communication, as the scenarios described different types of analysis and recommendations. The paper states, 'In the present study, we presented participants with a series of security-related scenarios in which an advanced robot made analyses of threat based on either a physics-based or psychologybased analysis.' This manipulation of the analysis type is a manipulation of the robot's verbal communication content, as it changes the information being conveyed to the participant about the robot's actions and reasoning. The results showed that trust was higher for physics-based scenarios, indicating that the manipulation of the robot's communication content impacted trust levels. The paper states, 'People were more confident with the robot's analysis and were more likely to adopt the robot's action recommendations in the physics-based scenarios.' This shows that the manipulation of the robot's communication content impacted trust. There were no other factors manipulated in the study.",10.1109/THMS.2019.2947592,https://ieeexplore.ieee.org/document/8908731/,"The introduction of increasingly intelligent and autonomous systems raises novel human factors challenges for human–machine teaming. People utilize differing mental models in understanding the functioning of complex systems that may be capable of social agency. Operators may perceive the machine as either a complex tool or a humanlike teammate. When the “advanced tool” mental model is adopted, operator trust may reﬂect individual differences in expectations of automation. By contrast, when the “teammate” mental model is activated, trust may depend on evaluative attitudes to robots. This article investigates predictors of trust in an autonomous robot detecting threat on either a physics-based or psychological basis. Distinct dimensions of physics-based and psychological trust are identiﬁed, corresponding to advanced tool and team mental models, respectively. Dispositional perceptions of automation, measured with the perfect automation schema scale, are associated with both aspects of trust. By contrast, the negative attitudes toward robots scale is speciﬁcally associated with lower psychological trust. The ﬁndings suggest that transparency information should be designed for compatibility with the operator’s mental model in order to support accurate trust calibration and situation awareness. Transparency may be personalized to emphasize either the machine’s data-analytic capabilities (advanced tool) or its humanlike social functioning (teammate)."
"Maurtua, Iñaki; Ibarguren, Aitor; Kildal, Johan; Susperregi, Loreto; Sierra, Basilio","Human–robot collaboration in industrial applications: Safety, interaction and trust",2017,2,38,38,0,No participants were excluded,Real-World Environment,within-subjects,"Participants performed three tasks: pointing gesture, safety monitoring, and manual guidance, followed by a collision demonstration and a questionnaire.","Participants interacted with a robot to perform tasks involving object manipulation, safety monitoring, and manual guidance.",UR5,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Passing,direct-contact interaction,Participants physically interacted with the robot during the tasks.,real-world,The study was conducted in a real-world trade fair setting with a physical robot.,physical,A physical robot was used in the study.,shared control (fixed rules),The robot followed pre-programmed actions but responded to human input for specific tasks.,Questionnaires,,,Trust was assessed using questionnaires.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's behavior was manipulated by demonstrating safety features and different interaction mechanisms, and the task difficulty was varied by changing the interaction type.","The study found a positive acceptance of the safety measures and interaction mechanisms, suggesting that these manipulations increased trust.","The study showed a very high acceptance of the safety measures, with 97% of participants agreeing that collaboration between robots and workers will be possible in the future.","Participants showed a positive perception of the safety strategy and interaction mechanisms, indicating a high level of trust in the tested system.","The robot moved objects between trays based on pointing gestures, stopped when a human approached, and was manually guided by the participant. The human pointed at objects, approached the robot to test safety, and manually guided the robot.",,"No statistical tests were explicitly mentioned in the description of Study 1. The results are presented using descriptive statistics, such as percentages and Likert scale responses, but no inferential statistical tests were reported.",TRUE,Robot-autonomy; Robot-interface-design; Task-complexity,Robot-interface-design,,"The study manipulated 'Robot-autonomy' by having the robot perform pre-programmed actions but also respond to human input for specific tasks like pointing gestures and manual guidance, which is described in the paper as the robot moving objects based on pointing gestures, stopping when a human approached, and being manually guided by the participant. The 'Robot-interface-design' was manipulated by demonstrating different interaction mechanisms (pointing, manual guidance, collision) and safety features, which is described in the paper as the participants performing three tasks: pointing gesture, safety monitoring, and manual guidance, followed by a collision demonstration. The 'Task-complexity' was manipulated by varying the interaction type, as described in the paper as the participants performing three different tasks. The study found that the different interaction mechanisms and safety features (Robot-interface-design) impacted trust, as participants showed a positive perception of the safety strategy and interaction mechanisms. The study did not explicitly state that the manipulation of 'Robot-autonomy' or 'Task-complexity' did not impact trust, but the results focused on the positive acceptance of the safety measures and interaction mechanisms.",10.1177/1729881417716010,http://journals.sagepub.com/doi/10.1177/1729881417716010,"Human–robot collaboration is a key factor for the development of factories of the future, a space in which humans and robots can work and carry out tasks together. Safety is one of the most critical aspects in this collaborative human–robot paradigm. This article describes the experiments done and results achieved by the authors in the context of the FourByThree project, aiming to measure the trust of workers on fenceless human–robot collaboration in industrial robotic applications as well as to gauge the acceptance of different interaction mechanisms between robots and human beings."
"Maurtua, Iñaki; Ibarguren, Aitor; Kildal, Johan; Susperregi, Loreto; Sierra, Basilio","Human–robot collaboration in industrial applications: Safety, interaction and trust",2017,2,77,77,0,No participants were excluded,Real-World Environment,within-subjects,"Participants interacted with the robot by testing safety monitoring, tapping, and pointing gestures, followed by a questionnaire.","Participants interacted with a robot to perform tasks involving safety monitoring, tapping, and pointing gestures.",KUKA IIWA,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Passing,direct-contact interaction,Participants physically interacted with the robot during the tasks.,real-world,The study was conducted in a real-world trade fair setting with a physical robot.,physical,A physical robot was used in the study.,shared control (fixed rules),The robot followed pre-programmed actions but responded to human input for specific tasks.,Questionnaires,,,Trust was assessed using questionnaires.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's behavior was manipulated by demonstrating safety features and different interaction mechanisms, and the task difficulty was varied by changing the interaction type. The addition of an LED band provided feedback to the user.","The study found a positive acceptance of the safety measures and interaction mechanisms, with the LED feedback improving the perceived response time of the pointing gesture, suggesting that these manipulations increased trust.",The study showed an improvement in the perceived response time of the pointing gesture due to the introduction of the LED feedback mechanism.,"Participants showed a positive perception of the safety strategy and interaction mechanisms, with the LED feedback improving the perceived response time of the pointing gesture, indicating a high level of trust in the tested system.","The robot moved a bin from side to side, stopped when a human approached, and moved the bin based on tapping and pointing gestures. The human approached the robot to test safety, tapped the robot, and pointed at bins.",,"No statistical tests were explicitly mentioned in the description of Study 2. The results are presented using descriptive statistics, such as percentages and Likert scale responses, but no inferential statistical tests were reported.",TRUE,Robot-autonomy; Robot-interface-design; Task-complexity,Robot-interface-design,,"The study manipulated 'Robot-autonomy' by having the robot perform pre-programmed actions but also respond to human input for specific tasks like tapping and pointing gestures, which is described in the paper as the robot moving a bin from side to side, stopping when a human approached, and moving the bin based on tapping and pointing gestures. The 'Robot-interface-design' was manipulated by demonstrating different interaction mechanisms (tapping, pointing) and safety features, and by adding an LED band to provide feedback, which is described in the paper as the participants interacting with the robot by testing safety monitoring, tapping, and pointing gestures, followed by a questionnaire. The 'Task-complexity' was manipulated by varying the interaction type, as described in the paper as the participants performing different tasks. The study found that the different interaction mechanisms and safety features, particularly the LED feedback (Robot-interface-design), impacted trust, as the LED feedback improved the perceived response time of the pointing gesture. The study did not explicitly state that the manipulation of 'Robot-autonomy' or 'Task-complexity' did not impact trust, but the results focused on the positive acceptance of the safety measures and interaction mechanisms, with the LED feedback improving the perceived response time.",10.1177/1729881417716010,http://journals.sagepub.com/doi/10.1177/1729881417716010,"Human–robot collaboration is a key factor for the development of factories of the future, a space in which humans and robots can work and carry out tasks together. Safety is one of the most critical aspects in this collaborative human–robot paradigm. This article describes the experiments done and results achieved by the authors in the context of the FourByThree project, aiming to measure the trust of workers on fenceless human–robot collaboration in industrial robotic applications as well as to gauge the acceptance of different interaction mechanisms between robots and human beings."
"McBride, Sara E; Rogers, Wendy A; Fisk, Arthur D",Do Younger and Older Adults Differentially Depend on an Automated System?,2010,1,84,84,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants completed a dual-task scenario with an automated aid, followed by workload and trust assessments. They completed practice trials, then 8 blocks of experimental trials over two days.","Participants managed a simulated warehouse, receiving packages and dispatching trucks, with an automated aid for the dispatching task.",Unspecified,Other,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated automated system.,simulation,The interaction was conducted in a simulated dual-task environment.,simulated,The automated aid was a simulated system.,pre-programmed (non-adaptive),The automated aid provided notifications based on pre-set rules.,Questionnaires,,Performance Metrics,Trust was measured using a 5-point Likert scale questionnaire and performance metrics.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the automated aid was manipulated to be correct 70% of the time, influencing user's expectations and trust.","Older adults reported higher trust in the automation than younger adults, despite the automation's errors.","Older adults exhibited greater dependence on the automation when it was incorrect, and they also took longer to verify the automation's suggestions. Older adults also reported higher trust and workload.","Younger adults showed less dependence on the automation when it was incorrect, while older adults exhibited greater dependence and higher trust in the automation.","The automated aid notified participants when a truck was full, and participants could choose to verify the truck's status. Participants also had to manage incoming packages.",ANOVA,"The study used a two-way analysis of variance (ANOVA) to examine the effects of age on performance in the receiving packages and dispatching trucks tasks, dependence on the automated aid (both overall and when the automation was correct or incorrect), time spent viewing the truck, trust in the automated aid, and workload ratings. The ANOVA was used to compare the means of different groups (younger vs. older adults) on these various measures.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy,,"The study manipulated the reliability of the automated aid, which was correct only 70% of the time, with 30% of the time it either committing a false alarm or a miss. This directly impacts the 'Robot-accuracy' as it changes the success rate of the automated aid. The study also involved a dual-task scenario where participants had to manage both receiving packages and dispatching trucks, which increases the 'Task-complexity' compared to a single-task scenario. The paper explicitly states that the reliability of the automation was manipulated to be correct 70% of the time, and this manipulation of the automation's accuracy directly impacted trust, as older adults reported higher trust despite the errors. The study did not find any factors that did not impact trust.",,,"Various factors, including trust, system reliability, and error type have been found to affect how people interact with automated systems. Another variable that is becoming increasingly important is the role of age in human-automation interaction. As automation continues to emerge in numerous domains, including the home, older adults will likely interact with these types of systems to a greater extent than ever before. Therefore, understanding if age-related changes in cognition, such as diminished working memory capacity or processing speed, affect how older adults use automated systems is critical to ensure these systems are designed and implemented effectively. This study examined the role of age in a simulated dual task environment using an automated aid. Younger adults outperformed older adults in both tasks. When the automation was incorrect, younger adults exhibited less dependence than older adults. Further, when older adults verified the automation’s suggestion, they took significantly more time to do so than younger adults. Additionally, older adults reported greater trust in the automation and higher workload compared to younger adults."
"Mendel, Jerry M.",Uncertain rule-based fuzzy logic systems: introduction and new directions,2001,1,20,20,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed five ten-minute driving sessions in a simulator, with different types of assistance (visual, auditory) and self-confidence expression (fixed, variable). Questionnaires were administered after each session, and braking behavior was recorded.","Participants were asked to safely and closely follow a lead car in a driving simulator, while receiving assistance from a machine driver to prevent lane departures and tailgating collisions.",Unspecified,Autonomous Vehicles,Research,Navigation,Path Following,minimal interaction,"Participants interacted with a simulated machine driver through visual and auditory cues, but there was no physical interaction.",simulation,"The interaction took place in a driving simulator, providing a virtual environment for the driving task.",simulated,"The machine driver was a simulated entity, represented through visual and auditory cues in the driving simulator.",shared control (fixed rules),"The machine driver provided assistance based on pre-defined rules, with the human driver maintaining control of the vehicle.",Behavioral Measures; Questionnaires,,Performance Metrics,Trust was assessed using questionnaires and behavioral measures based on braking behavior.,no modeling,The study did not include any computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The machine driver's self-confidence was manipulated through variable multi-modal interfaces (visual and auditory), which influenced the perceived reliability and trustworthiness of the assistance.","Variable self-confidence significantly increased trust in the machine driver, especially for auditory assistance, without significantly decreasing self-reliance.","The improvement in trust was more significant for auditory assistance than for visual assistance, suggesting that self-confidence cues are more critical for decision-making-oriented assistance.",Communicating variable self-confidence through multi-modal interfaces significantly improved human drivers' trust in the machine driver and did not significantly decrease self-reliance.,The machine driver provided visual and auditory cues to assist the human driver in avoiding collisions and lane departures. The human driver was responsible for controlling the vehicle while following a lead car.,ANOVA,"ANOVA was used to analyze the subjective attitude evaluation data (correctness, disturbance, satisfaction, trust, annoyance, and usefulness) and the percentage of trust and self-reliance based on braking behavior. The analysis aimed to determine if there were significant differences between the conditions with fixed and variable self-confidence for both visual and auditory assistance.",TRUE,Robot-interface-design; Robot-verbal-communication-content,Robot-interface-design; Robot-verbal-communication-content,,"The study manipulated the robot's interface design by varying the size of visual signs and the volume of auditory cues to represent the machine driver's self-confidence. This is described in the paper as 'Some basic elements of the interface, such as sign size, sound volume, and vibration intensity varied with the machine driver's self-confidence level.' This directly relates to the 'Robot-interface-design' category. The study also manipulated the content of the robot's verbal communication by providing different voice instructions such as 'Keep left,' 'Keep right,' and 'Brake' which is a manipulation of 'Robot-verbal-communication-content'. The results showed that both the variable interface design and the variable verbal communication content significantly impacted trust, as stated in the paper: 'For all dimensions of positive evaluation, the human drivers rated the machine driver much higher when it expressed variable self-confidence. Improvement of the grand average score is much higher for auditory assistance than for visual assistance (2.23 vs. 0.78).' and 'For auditory assistance, there was significant improvement on all positive evaluations (p<<0.05), including usefulness.' and 'For visual assistance, the percentage of trust induced by fixed self-confidence and variable self-confidence showed significant difference at a 95percent confidence level, F(1,19)=11.34, p=0.0032. The average percentage of trust increased from 57-percent to 66-percent when the machine driver showed variable selfconfidence (Figure 5). For auditory assistance, the improvement of the percentage of trust was even more significant, F(1,19)=21.1, p=0.0002. The corresponding average percent trust jumped from 47-percent to 69-percent.' There were no factors that were manipulated that did not impact trust.",,,"In human-machine collaborative systems, human operators usually disuse, misuse, or abuse the assistance from machines. They distrust the systems with poor reliability and may be over-dependent on the highly reliable systems. Human’s trust of the assistance of machines currently seems trapped in a dilemma. It prevents achieving harmonious human-machine collaboration. Considering the characteristics of human cognitive process, this paper suggests providing extra cognitive cues to help operators to tune their trust and decision making. The cues are expressed through variable multi-modal human-machine interfaces and serve as intelligent machines’ human-like self-confidence, or confidence, in short. To test this approach, a car-following driving experiment was conducted to investigate drivers’ perception of unreliable visual/auditory assistance. Experimental results showed that communication of confidence significantly ( 05 . 0 = α ) affected human drivers’ cognitive processing, especially for decision-making–oriented auditory assistance. Human drivers felt the machines were more trustworthy and useful when they showed variable self-confidence. Meanwhile, a rise in trust did not significantly induce a fall in self-reliance and over-dependence on the machines. Variable self-confidence continuously reminded human drivers that the assistance was not always reliable and they should maintain sufficient awareness of their situation. Using these extra cognitive cues, human operators are more comfortable to tune their trust in real-time situations and to build better human-machine collaboration relationships."
"Menolotto, Matteo; Komaris, Dimitrios-Sokratis; O'Sullivan, Patricia; O'Flynn, Brendan",Assessing Trust in Collaborative Robotics with Different Human-Robot Interfaces,2024,1,11,11,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed two tasks (object handover and collaborative manufacturing) using three different HRIs (button keypad, touchscreen, and data glove). The order of tasks and HRIs was pseudo-randomized. After each task completion, participants filled out questionnaires.",Participants completed two tasks: object handover (assembling a jigsaw puzzle) and collaborative manufacturing (connecting components to a core held by the robot).,UR16e,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically interacted with the robot to complete the tasks.,real-world,The study involved real-world interaction with a physical robot.,physical,Participants interacted with a physical robot.,shared control (fixed rules),"The robot's actions were triggered by the human via the HRI, but the robot's movements were pre-programmed.",Questionnaires; Behavioral Measures; Physiological Measures,NASA Task Load Index (NASA-TLX); System Usability Scale (SUS); Schaefer's Trust Questionnaire/Scale,"Physiological Signals; Performance Metrics; robot data (sensor data, etc.)","Trust was assessed using questionnaires, behavioral measures (jerk), and physiological data (GSR).",no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers manipulated the type of HRI used (button keypad, touchscreen, data glove), which influenced the task difficulty and user expectations.","The data glove resulted in lower trust scores compared to the button keypad and touchscreen, particularly for the 3D core task. The button and touchscreen interfaces were found to be more intuitive and reliable.","The data glove, despite being a wearable technology, performed worse than the simpler button and touchscreen interfaces, indicating that advanced technology does not always lead to better usability or trust. There was a statistically significant difference in the perceived effort as measured with the NASA Task Load Index on which type of technology (button, touchpad, glove) was used to manipulate the robotic arm for the puzzle task.","The study found that the type of human-robot interface significantly impacts trust in collaborative robotics, with simpler interfaces like button keypads and touchscreens leading to higher trust levels compared to a data glove.","The robot picked and placed puzzle pieces for the object handover task and rotated a core for the collaborative manufacturing task. The human assembled the puzzle and connected components to the core, using different HRIs to control the robot.",Friedman test; Wilcoxon signed-rank test,"Friedman tests were used to compare the total scores from the questionnaires (NASA-TLX, SUS, and Trust) across the different HRIs for both the puzzle and 3D core tasks. Post-hoc pairwise comparisons were then conducted using Wilcoxon signed-rank tests with a Bonferroni correction to determine which specific HRI pairs showed significant differences. The tests aimed to identify statistically significant differences in perceived effort, trust, and usability based on the type of HRI used.",TRUE,Robot-interface-design; Task-complexity,Robot-interface-design,Task-complexity,"The study manipulated the 'Robot-interface-design' by using three different HRIs: a button keypad, a touchscreen, and a data glove. This is explicitly stated in the 'Human-Robot Interfaces and Collaborative Robots' section, where the different interfaces are described. The study also implicitly manipulated 'Task-complexity' by having two different tasks: object handover (puzzle) and collaborative manufacturing (3D core). The 3D core task is more complex than the puzzle task, as it involves more steps and requires more precise manipulation. The results section indicates that the 'Robot-interface-design' significantly impacted trust, with the data glove resulting in lower trust scores compared to the button keypad and touchscreen, particularly for the 3D core task. The paper states, 'Post-hoc pairwise comparisons demonstrated that the glove was inferior to both the touchpad and button for the completion of the 3D core task (p < 0.008)'. While the study used two different tasks, the results indicate that the task itself did not have a significant impact on trust, but rather the interface used to control the robot. The paper states, 'There were no significant differences between the operation of the robotic arm with the touchpad or the button for both tasks (p > 0.058)'. Therefore, 'Task-complexity' is included in the manipulated factors, but not in the factors that impacted trust.",10.1109/I2MTC60896.2024.10561211,https://ieeexplore.ieee.org/document/10561211/,"Human-robot interfaces (HRIs) serve as the main communication tools for controlling and programming robots in industry 4.0 applications. To be effective, the design of these interfaces should consider not only functional and morphological characteristics, but also factors influencing human interactions, such as trust. A lack of trust is linked to the underutilization or misuse of collaborative robots, leading to ineffective automation implementation and compromised safety. The assessment of human factors is therefore gaining traction in robotics, with the emergence of both objective and subjective methodologies. Nevertheless, the absence of a holistic approach hinders the development of a unified assessment framework. This study introduces a novel assessment methodology that integrates self-reporting questionnaires with human-centric data collected through wearable sensing technologies. The approach aims to offer a comprehensive evaluation of HRIs, considering both perceptual and behavioral dimensions. Empirical testing on three different HRIs substantiates the effectiveness of this methodology. Preliminary results reveal variations in trust levels based on the combination of tasks performed and the specific HRI used for communication with a collaborative robot. These findings not only contribute to advancing our understanding of trust dynamics in human-robot interactions but also lay the groundwork for a more inclusive evaluation framework, enhancing our comprehension of the intricate interplay between humans and robots in the context of smart manufacturing."
"Merritt, Stephanie M.",Affective Processes in Human–Automation Interactions,2011,1,130,130,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants watched video clips to induce positive or negative moods, then interacted with a fictitious automated system on an X-ray screening task. Variables including trust, liking, perceived machine accuracy, user self-perceived accuracy, and reliance were assessed at five time points.","Participants identified guns and knives in X-ray images of luggage with the assistance of a fictitious automated system, the 'Automatic Weapons Detector' (AWD). They recorded their initial opinion, received advice from the AWD, and then recorded their final decision.",Unspecified,Other,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a fictitious automated system through a computer interface.,simulation,The interaction was conducted through a simulated X-ray screening task on a computer.,simulated,The robot was a fictitious automated system presented through a computer interface.,pre-programmed (non-adaptive),The automated system provided recommendations based on a pre-set accuracy level.,Custom Scales; Questionnaires,,,Trust was measured using a custom self-report scale created by the author.,"parametric models (e.g., regression)",Structural equation modeling (SEM) was used to analyze the relationships between variables.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"Mood was manipulated using video clips, and the reliability of the automated system was set to either 80% or 90%. The task also included a cognitive load manipulation.",Happiness significantly increased trust and liking for the system. Perceived machine reliability was significantly associated with trust and liking. Self-perceived accuracy was negatively associated with trust.,"The study found that liking was a stronger predictor of reliance early in the task, while trust became more important later. Perceived machine reliability had no direct effect on reliance, but was mediated by trust and liking.","Affective influences, particularly happiness, significantly impact trust and liking for automated systems, and liking is a key predictor of reliance early in the task.","The robot, a fictitious automated system, provided recommendations on whether an X-ray image contained a weapon. The human participant made an initial decision, received the robot's advice, and then made a final decision.",ANOVA; confirmatory factor analyses (cfas); Structural equation modeling; Sobel test,"The study used one-way ANOVAs to confirm the effectiveness of the mood manipulations. Confirmatory factor analyses (CFAs) were performed to verify the factor structure of the trust, liking, and propensity to trust scales. Structural equation modeling (SEM) was used to test the hypothesized relationships between mood, trust, liking, perceived reliability, self-perceived accuracy, and reliance across five time points. The Sobel technique was used to assess the mediation effects of trust and liking on the relationship between perceived machine reliability and reliance.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy,,"The study manipulated the reliability of the automated system (AWD) by setting it to either 80% or 90% accuracy. This directly impacts the robot's performance on the task, making 'Robot-accuracy' the appropriate category. The study also included a cognitive load manipulation by requiring participants to remember an eight-digit number during the task, which increases the cognitive demands of the task, thus 'Task-complexity' is the appropriate category. The results showed that perceived machine reliability (which is directly related to the manipulated robot accuracy) was significantly associated with trust, thus 'Robot-accuracy' is included in 'factors_that_impacted_trust'. There were no factors that were manipulated that did not impact trust.",10.1177/0018720811411912,https://doi.org/10.1177/0018720811411912,"Objective: This study contributes to the literature on automation reliance by illuminating the influences of user moods and emotions on reliance on automated systems.Background: Past work has focused predominantly on cognitive and attitudinal variables, such as perceived machine reliability and trust. However, recent work on human decision making suggests that affective variables (i.e., moods and emotions) are also important. Drawing from the affect infusion model, significant effects of affect are hypothesized. Furthermore, a new affectively laden attitude termed liking is introduced.Method: Participants watched video clips selected to induce positive or negative moods, then interacted with a fictitious automated system on an X-ray screening task. At five time points, important variables were assessed including trust, liking, perceived machine accuracy, user self-perceived accuracy, and reliance. These variables, along with propensity to trust machines and state affect, were integrated in a structural equation model.Results: Happiness significantly increased trust and liking for the system throughout the task. Liking was the only variable that significantly predicted reliance early in the task. Trust predicted reliance later in the task, whereas perceived machine accuracy and user self-perceived accuracy had no significant direct effects on reliance at any time.Conclusion: Affective influences on automation reliance are demonstrated, suggesting that this decision-making process may be less rational and more emotional than previously acknowledged.Application: Liking for a new system may be key to appropriate reliance, particularly early in the task. Positive affect can be easily induced and may be a lever for increasing liking."
"Merritt, Stephanie M.; Ako-Brew, Alicia; Bryant, William J.; Staley, Amy; McKenna, Michael; Leone, Austin; Shirase, Lei",Automation-Induced Complacency Potential: Development and Validation of a New Scale,2019,1,500,475,25,"16 individuals were flagged by at least two of the three indicators and their data were removed, 9 additional participants were flagged as multivariate outliers and, after visual inspection of their data, they were subsequently removed",Online Crowdsourcing,,"Participants completed a web-based survey including demographic items, study measures, and ratings on a hypothetical complacent behavior scale.",Participants completed several questionnaires and rated their anticipated level of monitoring in hypothetical situations involving automation.,Unspecified,,,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about hypothetical scenarios.,media,The interaction was based on text descriptions of hypothetical scenarios.,hypothetical,The robots were only described in text.,not autonomous,"The robot's actions were only described in text, without any real autonomy.",Questionnaires; Custom Scales,Propensity to Trust Scales,,Trust was measured using questionnaires and a custom scale.,"parametric models (e.g., regression)",Hierarchical linear regression analyses were conducted to examine the relationship between the scales and hypothetical complacency.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study assessed trust without intentionally altering any specific factors.,,"The monitoring scale was negatively associated with hypothetical complacency, while the alleviating workload scale was not significantly related to complacency.",The monitoring scale of the new AICP-R measure has incremental validity over the CPRS in predicting hypothetical complacency.,Participants completed questionnaires and rated their anticipated monitoring behavior in hypothetical scenarios involving automation.,Multilevel Model; Factor analysis; principal axis factoring; oblimin rotation; hierarchical linear regression,"The study used confirmatory factor analysis (CFA) to verify the factor structures of existing scales and to assess the fit of the proposed two-factor model for the new AICP-R scale. Exploratory factor analysis (EFA) with principal axis factoring and oblimin rotation was used to determine the factor structure of the AICP-R scale. Hierarchical linear regression analyses were conducted to examine the relationship between the scales (CPRS and AICP-R) and hypothetical complacency, assessing the incremental validity of the new AICP-R factors over the CPRS.",FALSE,,,,"The study did not manipulate any factors. Participants completed questionnaires and rated their anticipated monitoring behavior in hypothetical scenarios. There was no intentional manipulation of any variables related to the robot or the task. The study focused on assessing individual differences in complacency potential and its relationship with trust and hypothetical monitoring behavior, rather than manipulating any specific factors to observe their impact on trust. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust due to manipulation.",10.3389/fpsyg.2019.00225,https://www.frontiersin.org/article/10.3389/fpsyg.2019.00225/full,"Complacency, or sub-optimal monitoring of automation performance, has been cited as a contributing factor in numerous major transportation and medical incidents. Researchers are working to identify individual differences that correlate with complacency as one strategy for preventing complacency-related accidents. Automation-induced complacency potential is an individual difference reﬂecting a general tendency to be complacent across a wide variety of situations which is similar to, but distinct from trust. Accurately assessing complacency potential may improve our ability to predict and prevent complacency in safety-critical occupations. Much past research has employed an existing measure of complacency potential. However, in the 25 years since that scale was published, our conceptual understanding of complacency itself has evolved, and we propose that an updated scale of complacency potential is needed. The goal of the present study was to develop, and provide initial validation evidence for, a new measure of automation-induced complacency potential that parallels the current conceptualization of complacency. In a sample of 475 online respondents, we tested 10 new items and found that they clustered into two separate scales: Alleviating Workload (which focuses on attitudes about the use of automation to ease workloads) and Monitoring (which focuses on attitudes toward monitoring of automation). Alleviating workload correlated moderately with the existing complacency potential rating scale, while monitoring did not. Further, both the alleviating workload and monitoring scales showed discriminant validity from the previous complacency potential scale and from similar constructs, such as propensity to trust. In an initial examination of criterion-related validity, only the monitoring-focused scale had a signiﬁcant relationship with hypothetical complacency (r = −0.42, p < 0.01), and it had signiﬁcant incremental validity over and above all other individual difference measures in the study. These results suggest that our new monitoring-related items have potential for use as a measure of automation-induced complacency potential and, compared with similar scales, this new measure may have unique value."
"Merwin, Elizabeth R.; Hammack, Jacqueline; Wilcox, Teresa",Robotic Arm Perception: An Eyetracking Study Exploring Causal Relations and Perceived Trust,2024,1,75,66,9,"7 participants were excluded due to prior history of TBIs, 2 excluded due to technical errors during data collection",Controlled Lab Environment,mixed design,Participants were randomly assigned to view either a human or robot agent performing gestures. They completed questionnaires before and after viewing videos of the agent. Eye-tracking data was collected during the video viewing.,"Participants watched videos of a human or robot arm pointing to boxes, and their gaze was tracked. They also completed questionnaires about trust and perception of the agent.",Unspecified,Industrial Robot Arms,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants passively observed videos of a robot or human arm.,media,Participants watched pre-recorded videos of the robot or human arm.,physical,Participants viewed videos of a physical robot arm.,pre-programmed (non-adaptive),The robot arm followed a pre-programmed sequence of actions.,Questionnaires,Checklist for Trust between People and Automation; Interpersonal Trust Scale; Godspeed Questionnaire,Eye-tracking Data,Trust was assessed using questionnaires and eye-tracking data.,no modeling,Trust data was analyzed statistically but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The agent was either a human or a robot arm, and the pointing behavior was either congruent, incongruent, or control, which influenced the perceived reliability of the agent.","There was no significant difference in trust ratings between the human and robot agents, but the trend was towards the robot being rated as more trustworthy.","The study found that participants were more receptive to information provided by the human agent compared to the robot agent, as indicated by gaze patterns during the prediction phase. There was a trend towards participants rating the robot as more trustworthy than the human, although this was not statistically significant.","Participants showed differential gaze patterns during the prediction phase, suggesting they were more receptive to information provided by the human agent compared to the robot agent, although there was no significant difference in trust ratings.","The robot arm pointed to one of two boxes, and the human participant watched the video and completed questionnaires. The human's task was to observe the pointing behavior and then report their trust and perception of the agent.",ANOVA; t-test,"The study used mixed-model ANOVAs to analyze eye-tracking data (average dwell time) across different phases (Action, Event), blocks, trial types (experimental, control, congruent, incongruent), and areas of interest (AOIs), with agent type (human, robot) as a between-subjects factor. Greenhouse-Geisser corrections were applied when sphericity was violated, and Bonferroni adjustments were used for post hoc tests. An independent samples t-test was used to compare trust scores from the Checklist between People and Automation between the human and robot agent conditions.",TRUE,Robot-nonverbal-communication; Robot-accuracy,,Robot-nonverbal-communication; Robot-accuracy,"The study manipulated the agent type (human vs. robot) which directly impacts the nonverbal communication, as the robot arm's gestures are inherently different from a human arm's. This is classified as 'Robot-nonverbal-communication'. The study also manipulated the pointing behavior of the agent (congruent, incongruent, control), which influenced the perceived reliability of the agent's actions, thus impacting the accuracy of the agent's nonverbal communication. This is classified as 'Robot-accuracy'. The results showed no significant difference in trust ratings between the human and robot agents, indicating that neither the difference in nonverbal communication nor the accuracy manipulation significantly impacted trust. Therefore, both 'Robot-nonverbal-communication' and 'Robot-accuracy' are listed as factors that did not impact trust.",10.1177/10711813241275496,https://journals.sagepub.com/doi/10.1177/10711813241275496,"With the increased integration of robots into industrial, service, and educational spaces, it is imperative to understand factors that influence human-robot interactions. While the existing literature has explored human perception of robot-produced verbal communication, there is a notable gap in understanding how adults process robot-produced nonverbal communication. Current research focuses on subjective measures of the individual’s perception of robots during interactions, but often fails to have objective measures or control groups in their designs which limits the conclusions that can be made. In this study, we aim to implement a design that allows for the comparison of robot and human nonverbal communication understanding and perceiving through both objective eye-tracking measures and subjective survey measures. This design should prove useful to gain insights into how people understand information provided by robots and humans when watching their nonverbal gestures, and their perceptions of the agents after their interactions."
"Miller, Dave; Sun, Annabel; Ju, Wendy",Situation awareness with different levels of automation,2014,1,48,48,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants drove a simulated car in one of four automation conditions (fully autonomous, autonomous steering, autonomous speed control, or no automation). They experienced critical traffic incidents after regaining control of the vehicle or under partial control conditions. Post-drive questionnaires were administered.",Participants drove a simulated car and reacted to potential accident scenarios after transitions between manual and autonomous driving.,Unspecified,Autonomous Vehicles,Research,Navigation,Path Following,minimal interaction,"Participants interacted with a driving simulator, with limited physical interaction.",simulation,Participants experienced a simulated driving environment.,simulated,The robot was a simulated car within the driving simulator.,shared control (fixed rules),"The car had fixed rules for autonomous driving, with transitions to manual control.",Questionnaires,,Performance Metrics,Trust was measured using a post-drive questionnaire and performance metrics.,no modeling,"Trust was not modeled computationally, only descriptive statistics were used.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The level of automation was directly manipulated, influencing the car's control over steering and speed, which affected the driver's performance and perceived trust.","Participants in the fully autonomous condition showed greater trust and comfort than those in the autonomous speed control condition. The autonomous steering condition resulted in slower reaction times, potentially due to over-trust.","Participants in the autonomous steering condition reacted slower to critical events, potentially due to over-trust in the system. The autonomous speed control condition was the least confidence-inspiring.","The type of automation significantly influences driver trust and comfort, with full autonomy leading to higher trust and comfort compared to autonomous speed control.","The robot (simulated car) autonomously controlled steering, speed, or both, depending on the condition. The human participant drove the car and reacted to potential accident scenarios, taking over control when necessary.",ANOVA; Tukey HSD,"The study used a one-way ANOVA to compare the effect of different automation levels on post-transition reaction time, trust in the car, and comfort with the car. Tukey post-hoc analyses were then used to determine which specific automation conditions differed significantly from each other for each of these measures.",TRUE,Robot-autonomy,Robot-autonomy,,"The study explicitly manipulated the level of automation of the vehicle, which is a direct manipulation of the robot's decision authority. The car was either fully autonomous, had autonomous steering, autonomous speed control, or no automation. This directly impacts the level of control the robot has over the vehicle, which is why 'Robot-autonomy' is the most appropriate category. The results showed that the level of automation significantly impacted trust and comfort, with full autonomy leading to higher trust and comfort compared to autonomous speed control. Therefore, 'Robot-autonomy' is also listed as a factor that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1109/SMC.2014.6973989,https://ieeexplore.ieee.org/document/6973989,"What effect will periods of automated driving will have on driver performance after transfer of control? In our driving simulator experiment (N = 48) participants in four different automation conditions (fully autonomous vehicle, autonomous steering, autonomous speed control, no automation) were evaluated based on their post-transition accident avoidance, situational awareness, and feelings of trust in and comfort with autonomous or partially autonomous driving."
"Miller, David; Johns, Mishel; Mok, Brian; Gowda, Nikhil; Sirkin, David; Lee, Key; Ju, Wendy",Behavioral Measurement of Trust in Automation: The Trust Fall,2016,1,42,42,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants experienced a series of driving scenarios in a simulator, with varying levels of automation capability and interface design. They were asked to predict the system's behavior and then complete a trust questionnaire.",Participants drove a simulated vehicle and were required to take control when the automated system shut down or if they did not trust the system.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the automated driving system in a simulator.,simulation,Participants experienced the interaction in a highly immersive virtual reality driving simulator.,simulated,The robot was a simulated automated driving system within the virtual reality environment.,shared control (fixed rules),"The automated driving system operated with fixed rules, requiring human intervention in certain situations.",Behavioral Measures; Questionnaires,Jian et al. Trust Scale,Eye-tracking Data; Performance Metrics,Trust was assessed using a combination of behavioral measures and a post-drive questionnaire.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the automation capability (high/low) and interface design (navigation, perception, planning) to influence trust. The system's performance and the information provided to the user were varied.","Behavioral measures of trust differed from self-reported trust, with participants often intervening even when they expected the system to handle the situation. The study found that drivers often did not trust the automated driving system even when it would perform flawlessly.","Participants often intervened even when they expected the computer to handle the situation, indicating a bias towards conservatism. There was a split between self-reported trust and trusting behavior, validating the trust fall method.","The study demonstrated the value of using behavioral measures, such as the trust fall, to assess trust in automation, as these measures can reveal discrepancies between self-reported trust and actual behavior.","The robot, a simulated automated driving system, was designed to drive the vehicle autonomously, with varying levels of capability. The human participant was tasked with monitoring the system and taking control when necessary, or when they felt the system was not trustworthy.",Chi-squared; Multilevel Model; cronbach's alpha,"The study used a Chi-squared test to compare participants' expectations of the automated driving system's behavior to their actual inputs. Factor analysis was used to assess the reliability of the self-reported trust questionnaire, and Cronbach's alpha was used to measure the internal consistency of the indices developed from the factor analysis.",TRUE,Robot-accuracy; Robot-interface-design,Robot-accuracy,Robot-interface-design,"The study manipulated two key factors: 'Robot-accuracy' by varying the automation capability (high/low), which directly impacted the system's ability to handle driving situations without human intervention. This is described in the 'Automation Capability levels' section, where it states that the high capability system could handle more situations without driver intervention, while the low capability system required more driver intervention. This directly relates to the robot's performance and success rate on the task. The study also manipulated 'Robot-interface-design' by varying the information provided to the driver through three interface levels: navigation, perception, and planning. This is described in the 'Interface Communication Levels' section, where it explains that the navigation interface displayed only navigation and system status, the perception interface added warnings of hazards, and the planning interface illustrated future actions. The results section indicates that the automation capability (Robot-accuracy) impacted trust, as participants' behavior differed based on whether the system was high or low capability. However, the interface design (Robot-interface-design) did not significantly impact self-reported trust, as stated in the 'Self-Reported Trust in Automation' section: 'The three indices are highly correlated, but do not show statistically significant differences between interface displays or automation capability levels'.",10.1177/1541931213601422,http://journals.sagepub.com/doi/10.1177/1541931213601422,"Stating that one trusts a system is markedly different from demonstrating that trust. To investigate trust in automation, we introduce the trust fall: a two-stage behavioral test of trust. In the trust fall paradigm, first the one learns the capabilities of the system, and in the second phase, the ‘fall,’ one’s choices demonstrate trust or distrust. Our first studies using this method suggest the value of measuring behaviors that demonstrate trust, compared with self-reports of one’s trust. Designing interfaces that encourage appropriate trust in automation will be critical for the safe and successful deployment of partially automated vehicles, and this will rely on a solid understanding of whether these interfaces actually inspire trust and encourage supervision."
"Miller, Linda; Kraus, Johannes; Babel, Franziska; Baumann, Martin",More Than a Feeling—Interrelation of Trust Layers in Human-Robot Interaction and the Role of User Dispositions and State Anxiety,2021,1,34,28,6,"technical issues with the robot, non-compliance with instruction, univariate statistical outlier regarding distance",Controlled Lab Environment,mixed design,"Participants were introduced to a domestic robot, completed questionnaires, and then the robot approached them twice. Participants indicated their comfort distance and trust after each approach.",Participants were asked to say 'stop' when they felt uncomfortable as the robot approached them.,TIAGo,Humanoid Robots; Mobile Manipulators; Service and Assistive Robots,Research; Social; Care,Navigation,Path Following,minimal interaction,Participants verbally instructed the robot to stop when they felt uncomfortable.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical robot.,wizard of oz (directly controlled),The robot was remotely controlled by an operator.,Questionnaires; Custom Scales,Negative Attitude towards Robots Scale (NARS); Propensity to Trust Scales,,Trust was measured using questionnaires and a custom scale.,"parametric models (e.g., regression)",Regression and mediation analyses were used to model trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's height and manipulator outreach were manipulated to influence the perceived distance and comfort level of the participants.,"The robot's size had a significant effect on the comfort distance in the first trial, with the distance being larger in the tall robot condition.","The study found that state anxiety was negatively correlated with initial learned trust, and this relationship diminished over time. The propensity to trust in automation was positively related to initial and dynamic learned trust. Negative attitudes toward robots were negatively related to initial and dynamic learned trust. Comfort distance decreased with repeated interaction, but was not correlated with trust.","The propensity to trust in automation and negative attitudes toward robots influence initial learned trust, and this relationship is mediated by state anxiety. Learned trust in a robot increases with repeated error-free interaction.","The robot approached the participant, and the participant said 'stop' when they felt uncomfortable. The robot's movement was controlled by a human operator.",Linear regression; Mediation analysis; Pearson correlation; t-test; ANOVA,"The study used regression and mediation analyses to examine the relationships between user dispositions (propensity to trust in automation, negative attitudes toward robots), state anxiety, learned trust (initial and dynamic), and comfort distance. Pearson correlations were used to assess bivariate relationships between variables. Paired t-tests and ANOVAs were used to assess changes in learned trust and comfort distance over repeated interactions.",TRUE,Robot-nonverbal-communication; Robot-aesthetics,,,"The study manipulated the robot's height (short vs. tall) and manipulator outreach (retracted vs. extended). The height manipulation is categorized as 'Robot-aesthetics' because it changes the visual appearance of the robot. The manipulator outreach is categorized as 'Robot-nonverbal-communication' because it changes the robot's physical presence and the space it occupies, influencing proxemics. The paper states that the robot's size had a significant effect on the comfort distance in the first trial, but it did not have a significant effect on dynamic learned trust. The manipulator outreach did not have a significant effect on dynamic learned trust or comfort distance. Therefore, neither of these factors impacted trust.",10.3389/fpsyg.2021.592711,https://www.frontiersin.org/articles/10.3389/fpsyg.2021.592711/full,"With service robots becoming more ubiquitous in social life, interaction design needs to adapt to novice users and the associated uncertainty in the ﬁrst encounter with this technology in new emerging environments. Trust in robots is an essential psychological prerequisite to achieve safe and convenient cooperation between users and robots. This research focuses on psychological processes in which user dispositions and states affect trust in robots, which in turn is expected to impact the behavior and reactions in the interaction with robotic systems. In a laboratory experiment, the inﬂuence of propensity to trust in automation and negative attitudes toward robots on state anxiety, trust, and comfort distance toward a robot were explored. Participants were approached by a humanoid domestic robot two times and indicated their comfort distance and trust. The results favor the differentiation and interdependence of dispositional, initial, and dynamic learned trust layers. A mediation from the propensity to trust to initial learned trust by state anxiety provides an insight into the psychological processes through which personality traits might affect interindividual outcomes in human-robot interaction (HRI). The ﬁndings underline the meaningfulness of user characteristics as predictors for the initial approach to robots and the importance of considering users’ individual learning history regarding technology and robots in particular."
"Miramontes, Adriana; Tesoro, Andriana; Trujillo, Yuri; Barraza, Edward; Keeler, Jillian; Boudreau, Alexander; Strybel, Thomas Z.; Vu, Kim-Phuong L.",Training Student Air Traffic Controllers to Trust Automation,2015,1,48,48,0,No participants were excluded,Educational Setting,mixed design,"Participants were trained in a radar simulation course. One group received trust training with more exposure to NextGen tools, while the control group had less exposure. Both groups were tested at midterm and final exams. The number of near misses moved, LOS, and average time through sector were recorded.","Participants managed air traffic using a simulator, including NextGen tools, and were tasked with preventing loss of separation between aircraft.",Unspecified,Other,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a simulation of air traffic control, using a computer interface.",simulation,The study used a medium fidelity simulator to represent the air traffic control environment.,simulated,The robots were simulated as aircraft within the air traffic control simulation.,shared control (fixed rules),"The automation tools provided alerts and suggestions, but the human made the final decisions.",Behavioral Measures; Questionnaires,Potential for Complacency Questionnaire,Performance Metrics,Trust was measured using a questionnaire and by observing the number of near misses moved.,"parametric models (e.g., regression)",A multiple regression was used to assess the relationship between personality traits and trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The trust training group had more exposure to NextGen tools and received feedback when they moved near miss aircraft, while the control group had less exposure and received normal training. This was intended to influence trust by increasing familiarity and providing feedback on automation use.","The trust training group moved fewer near miss aircraft, indicating higher trust in automation. Trust also increased over time for both groups.","The study found that emotional stability was a significant predictor of trust in automation, while conscientiousness was not. There was also an early benefit in efficiency for the trust training group at the midterm, but not at the final exam.","Student air traffic controllers can be trained to trust automation in a NextGen environment, and this training can lead to early improvements in efficiency.","The simulated aircraft moved according to pre-programmed flight plans, and the human participant monitored the air traffic and used NextGen tools to manage the aircraft and prevent loss of separation.",ANOVA; Linear regression,"The study used mixed factorial ANOVAs to assess the effects of trust training and exam period on the number of near miss aircraft moved, the number of loss of separation (LOS) incidents, and the average time through sector. A multiple regression was used to assess the relationship between personality traits (emotional stability and conscientiousness) and trust in automation.",TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,Task-complexity,"The study manipulated the level of exposure to NextGen tools, which directly impacts the level of automation the participants experienced. The trust training group had more exposure to NextGen tools (75% equipped aircraft) in the first 8 weeks, while the control group had less (25% equipped aircraft). This difference in exposure to automation tools is a manipulation of 'Robot-autonomy' because it changes the degree to which the system is making decisions and assisting the human operator. The study also manipulated the task complexity by having the trust training group receive feedback when they moved a near miss aircraft, while the control group did not. This feedback was intended to influence trust by increasing familiarity and providing feedback on automation use. This is a manipulation of 'Task-complexity' because it changes the cognitive demands of the task. The results showed that the trust training group moved fewer near miss aircraft, indicating higher trust in automation, which was influenced by the manipulation of 'Robot-autonomy'. The manipulation of 'Task-complexity' did not have a significant impact on trust, as both groups showed an increase in trust over time.",10.1016/j.promfg.2015.07.844,https://linkinghub.elsevier.com/retrieve/pii/S2351978915008458,"The Next Generation Air Transportation System (NextGen) will implement new automation tools to allow controllers to effectively manage the projected increase in air travel over the next decade. In order for the implementation of NextGen tools to be successful, it is important that air traffic controllers (ATCos) develop appropriate levels of trust in these automated tools. The present study investigated whether students could be trained to trust automation in a NextGen environment, and whether their trust in automation would affect their air traffic management performance. We also examined if personality traits influence the student’s likelihood to trust automation. We found an early benefit in terms of efficiency in air traffic management for students who were trained to trust automation. We also found that people with high emotional stability reported higher levels of trust in automation."
"Mishler, Scott; Chen, Jing",Effect of automation failure type on trust development in driving automation systems,2023,1,133,122,11,11 participants were excluded due to data-collection errors caused by simulator crashes,Controlled Lab Environment,mixed design,"Participants completed a practice session with manual and automated driving, followed by seven experimental drives with varying automation failure types. Trust ratings were collected after each drive.","Participants drove a simulated vehicle, with the automation system performing perfectly except for a critical event in the fourth drive, where the automation either issued a takeover request, malfunctioned, or performed without failure.",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a driving simulator and were required to monitor the automated driving system.,simulation,Participants interacted with a simulated driving environment.,simulated,The robot was a simulated automated driving system.,shared control (fixed rules),"The automation system operated independently but with fixed rules, requiring human intervention at specific points.",Questionnaires,Human-Computer Trust Scale/Questionnaire (HCT/HCTM),Performance Metrics,Trust was measured using a modified Human-Computer Trust Questionnaire and performance metrics.,"parametric models (e.g., regression)",The study used ANOVA and regression to analyze the trust data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the type of automation failure (no failure, takeover request, system malfunction) to influence trust.","Trust increased with errorless drives, decreased after automation failures, and showed some repair over time, with the no-failure condition showing higher trust than the other two conditions.","The study found that trust decreased similarly in response to both takeover requests and system malfunctions, although takeover requests resulted in better takeover performance. Trust did not fully recover to pre-failure levels.","Trust in automated driving systems develops over time, declines due to system failures, and shows some repair after failures, but does not fully recover to pre-failure levels.","The robot (automated driving system) drove the vehicle, and the human monitored the system and took over control when necessary, especially during automation failures.",Logistic regression; ANOVA; t-test; trend analysis,"The study used a binary logistic regression to analyze the effect of automation failure type on takeover accuracy. ANOVA was used to compare trust levels across different conditions and drives, including a mixed-factor ANOVA to examine the interaction between drive number and failure type. T-tests were used for pairwise comparisons of trust levels between different conditions. Trend analysis was used to examine the overall pattern of trust across drives, using polynomial contrasts to identify linear and cubic trends.",TRUE,Robot-accuracy; Robot-autonomy,Robot-accuracy,Robot-autonomy,"The study manipulated the type of automation failure, which directly impacts the robot's accuracy in performing the driving task. The three conditions were no-failure (perfect accuracy), takeover request (TOR, where the robot acknowledges its limitations and requests human intervention), and system malfunction (where the robot fails to detect a hazard). This manipulation of the robot's ability to perform the task correctly is categorized as 'Robot-accuracy'. The study also manipulated whether the robot would request a takeover or not, which is a change in the decision authority of the robot, and thus is categorized as 'Robot-autonomy'. The results showed that the 'Robot-accuracy' manipulation significantly impacted trust, with trust being higher in the no-failure condition and lower in the TOR and system-malfunction conditions. However, the study found no significant difference in trust between the TOR and system-malfunction conditions, despite the difference in the robot's decision authority (takeover request vs. no takeover request), indicating that the 'Robot-autonomy' manipulation did not have a significant impact on trust levels. The key factor influencing trust was the presence or absence of a failure, not the specific type of failure (takeover request vs. system malfunction).",10.1016/j.apergo.2022.103913,https://linkinghub.elsevier.com/retrieve/pii/S0003687022002368,"The performance of a driving automation system (DAS) can influence the human drivers’ trust in the system. This driving-simulator study examined how different types of DAS failures affected drivers’ trust. The automationfailure type (no-failure, takeover-request, system-malfunction) was manipulated among 122 participants, when a critical hazard event occurred. The dependent measures included participants’ trust ratings after each of seven drives and their takeover performance following the hazard. Results showed that trust improved before any automation failure occurred, demonstrating proper trust calibration toward the errorless system. In the takeover-request and system-malfunction conditions, trust decreased similarly in response to the automation failures, although the takeover-request condition had better takeover performance. For the drives after the automation failure, trust was gradually repaired but did not recover to the original level. This study demon­ strated how trust develops and responds to DAS failures, informing future research for trust repair interventions in designing DASs."
"Mizanoor Rahman, S. M.; Wang, Yue; Walker, Ian D.; Mears, Laine; Pak, Richard; Remy, Sekou",Trust-based compliant robot-human handovers of payloads in collaborative assembly in flexible manufacturing,2016,1,20,20,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were divided into two groups. Group I performed assembly with trust-based handover, where the robot adjusted its handover based on its trust in the human. Group II performed assembly with a fixed handover. Both groups performed a mock experiment to familiarize themselves with the procedure. The robot's speed was adjusted to match the human's standard speed. Data was collected during the formal experiment.","Participants performed a collaborative assembly task with a robot, including a handover of a screwdriver.",Kinova MICO,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically collaborated with the robot during the assembly and handover tasks.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The study used a physical robot for the interaction.,shared control (adaptive),"The robot adapted its handover motion based on its trust in the human, which was calculated in real-time.",Behavioral Measures; Performance-Based Measures; Real-time Trust Measures; Questionnaires,NASA Task Load Index (NASA-TLX),"Performance Metrics; robot data (sensor data, etc.)","Trust was measured using a combination of real-time performance metrics, behavioral measures, and questionnaires.","parametric models (e.g., regression)",A deterministic time-series model was used to compute the robot's trust in the human.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's handover behavior was manipulated based on its calculated trust in the human, with the robot adjusting its motion and speed. The robot also provided real-time feedback to the human about its trust level.","The trust-based handover strategy increased human trust in the robot, improved safety, and increased assembly efficiency.","The study found that a trust-based handover strategy, where the robot adjusts its behavior based on its trust in the human, improved human-robot interaction and assembly performance. The handover efficiency was slightly reduced due to the cautious handover motion.",Consideration of robot trust in human during assembly and adjustment in handover configuration and motion based on robot's trust levels in human significantly improve human-robot interaction and assembly performance.,The robot manipulated assembly parts and handed over a screwdriver to the human. The human assembled the parts and received the screwdriver from the robot.,ANOVA,"Analyses of Variances were used to assess the statistical significance of variations in HRI and handover efficiency between subjects. The results indicated that these variations were statistically insignificant (p>0.05), suggesting the generality of the findings.",TRUE,Robot-autonomy; Robot-verbal-communication-content,Robot-autonomy; Robot-verbal-communication-content,,"The study manipulated the robot's handover behavior based on its calculated trust in the human, which is a form of adaptive shared control, thus 'Robot-autonomy' is selected. The robot adjusted its motion and speed based on its trust level, and also provided real-time feedback to the human about its trust level, which is a form of 'Robot-verbal-communication-content'. The study found that the trust-based handover strategy and real-time feedback influenced trust outcomes, therefore both 'Robot-autonomy' and 'Robot-verbal-communication-content' impacted trust. There were no factors that were manipulated that did not impact trust.",10.1109/COASE.2016.7743428,https://ieeexplore.ieee.org/document/7743428/,"A human-robot hybrid cell is developed for performing assembly in flexible manufacturing in collaboration between a robot and its human co-worker. Robot trust in human is considered, a computational model for the trust is derived, and a method to measure and display the trust in real-time is developed. The collaborative assembly includes robot-to-human handovers of payloads (assembly tools). A novel trust-based compliant handover motion planning strategy for the robot is derived. The robot varies its handover configuration and motion based on robot trust in human through kinematic redundancy with the aim of reducing potential impulse forces on human body through payload during handover. A comprehensive scheme is developed to evaluate the collaborative assembly including the trust-based handover strategy. The evaluation results show that consideration of robot trust in human during the assembly and adjustment in handover configuration and motion based on robot’s trust levels in human significantly improve human-robot interaction and assembly performance through increasing safety, human trust in robot, handover success rate, and the overall assembly efficiency by 20%, 37.58%, 30% and 6.73% respectively and reducing cognitive workload by 25.63%, with a minor reduction in the handover efficiency by 1.87%."
"Mizanoor Rahman, S. M.; Sadrfaridpour, Behzad; Walker, Ian D.; Wang, Yue",Trust‐Triggered Robot–Human Handovers Using Kinematic Redundancy for Collaborative Assembly in Flexible Manufacturing,2023,2,20,20,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants practiced subtasks, then performed collaborative assembly with and without trust-based motion planning. Human performance and fault were measured to compute robot trust. The robot handed over a screwdriver to the human based on trust levels.","Participants assembled center console parts with the help of a robot, which handed over a screwdriver when needed.",Kinova MICO,Industrial Robot Arms; Collaborative Robots,Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically collaborated with the robot during assembly and handover tasks.,real-world,The study was conducted in a real-world lab setting with a physical robot.,physical,The robot was a physical industrial robot arm.,shared control (adaptive),The robot adapted its handover motion based on real-time trust levels derived from human performance.,Behavioral Measures; Performance-Based Measures; Real-time Trust Measures; Questionnaires,NASA Task Load Index (NASA-TLX),Performance Metrics; Video Data,"Trust was assessed using real-time performance metrics, behavioral observations, and questionnaires.","parametric models (e.g., regression)",A parametric model was used to compute trust based on human performance and fault.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's handover motion was directly manipulated based on computed trust levels, which were influenced by human performance and fault. The robot's speed and configuration changed based on trust.",The study showed that the robot's trust-based motion planning improved human trust in the robot and reduced cognitive workload. The robot's cautious movements in low-trust situations also reduced potential impulse forces.,"The study found that human hand trajectories became less smooth and more erratic as robot trust decreased, validating the hypothesis that humans exhibit unplanned movements when they perceive low trust. The robot's reach along the y-axis was reduced as trust decreased due to the robot adopting braced configurations.","The robot's trust-triggered handover motion planning, which adjusted robot behavior based on human performance, improved human trust in the robot, reduced cognitive workload, and increased safety during human-robot collaborative assembly.","The robot picked and placed parts, and handed over a screwdriver to the human. The human assembled the parts and dispatched the final product.",ANOVA,"Analysis of variances (ANOVAs) were conducted to assess the statistical significance of variations in physical HRI (pHRI) criteria (transparency, naturalness, engagement, cooperation, and team fluency) and cognitive HRI (cHRI) criteria (workload, human trust), as well as safety and efficiency, due to the manipulation of robot trust. The ANOVAs also checked for variations between subjects to ensure the generality of the results.",TRUE,Robot-autonomy; Robot-nonverbal-communication; Robot-accuracy,Robot-autonomy; Robot-nonverbal-communication; Robot-accuracy,,"The study manipulated the robot's handover motion based on real-time trust levels derived from human performance and fault. This is categorized as 'Robot-autonomy' because the robot's decision to adjust its motion was based on a computed trust value, which is a form of adaptive shared control. The robot's speed and configuration changes based on trust are classified as 'Robot-nonverbal-communication' because these are physical movements that communicate the robot's internal state (trust). The robot's changes in speed and configuration also directly influenced the task performance metrics, such as the smoothness of the handover and the potential for impulse forces, which is why 'Robot-accuracy' is also included. The paper explicitly states that these manipulations impacted human trust in the robot, as evidenced by the changes in human hand trajectories and the reported increase in trust scores. There were no factors that were manipulated that did not impact trust.",,https://onlinelibrary.wiley.com/doi/10.1002/9781119857433.ch12,
"Mizanoor Rahman, S. M.; Sadrfaridpour, Behzad; Walker, Ian D.; Wang, Yue",Trust‐Triggered Robot–Human Handovers Using Kinematic Redundancy for Collaborative Assembly in Flexible Manufacturing,2023,2,10,10,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed hose assembly with a robot, which handed over a cutter when needed. The robot's handover motion was planned based on real-time trust values. Human performance and fault were measured to compute robot trust.","Participants assembled hoses with the help of a robot, which handed over a cutter when needed.",Baxter,Humanoid Robots; Collaborative Robots,Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically collaborated with the robot during assembly and handover tasks.,real-world,The study was conducted in a real-world lab setting with a physical robot.,physical,The robot was a physical humanoid robot.,shared control (adaptive),The robot adapted its handover motion based on real-time trust levels derived from human performance.,Behavioral Measures; Performance-Based Measures; Real-time Trust Measures; Questionnaires,NASA Task Load Index (NASA-TLX),Performance Metrics; Video Data,"Trust was assessed using real-time performance metrics, behavioral observations, and questionnaires.","parametric models (e.g., regression)",A parametric model was used to compute trust based on human performance and fault.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's handover motion was directly manipulated based on computed trust levels, which were influenced by human performance and fault. The robot's speed and configuration changed based on trust.",The study showed that the robot's trust-based motion planning improved human trust in the robot and reduced cognitive workload. The robot's cautious movements in low-trust situations also reduced potential impulse forces.,The study found that the robot's angular position changed significantly to produce braced configurations as trust decreased. The handover speeds also reduced with trust levels.,"The robot's trust-triggered handover motion planning, which adjusted robot behavior based on human performance, improved human trust in the robot, reduced cognitive workload, and increased safety during human-robot collaborative assembly.","The robot picked and placed hoses and fitting parts, and handed over a cutter to the human. The human assembled the hoses and adjusted their length.",,"No specific statistical tests were explicitly mentioned for this study. The results were presented with comparisons of means and trends, but no specific statistical tests were named.",TRUE,Robot-autonomy; Robot-nonverbal-communication; Robot-accuracy,Robot-autonomy; Robot-nonverbal-communication; Robot-accuracy,,"Similar to the first study, the robot's handover motion was manipulated based on real-time trust values. This is categorized as 'Robot-autonomy' because the robot's decision to adjust its motion was based on a computed trust value, which is a form of adaptive shared control. The robot's changes in angular position and handover speed are classified as 'Robot-nonverbal-communication' because these are physical movements that communicate the robot's internal state (trust). The robot's changes in speed and configuration also directly influenced the task performance metrics, such as the smoothness of the handover and the potential for impulse forces, which is why 'Robot-accuracy' is also included. The paper explicitly states that these manipulations impacted human trust in the robot, as evidenced by the changes in human hand trajectories and the reported increase in trust scores. There were no factors that were manipulated that did not impact trust.",,https://onlinelibrary.wiley.com/doi/10.1002/9781119857433.ch12,
"Momen, Ali; De Visser, Ewart J.; Fraune, Marlena R.; Madison, Anna; Rueben, Matthew; Cooley, Katrina; Tossell, Chad C.",Group trust dynamics during a risky driving experience in a Tesla Model X,2023,1,65,50,15,"15 participants were excluded because their discussions were not recorded, or audio could not be transcribed",Real-World Environment,,"Participants, in groups of two or three, rode three loops in a Tesla Model X with Autopilot on campus roads while engaging in normal conversations. Conversations were transcribed and analyzed for themes related to trust.",Participants rode in a Tesla Model X with Autopilot on campus roads and engaged in normal conversation.,Tesla Model X,Autonomous Vehicles,Research,Navigation,Path Following,minimal interaction,Participants rode in a car with automation features and were instructed to engage in normal conversation.,real-world,The study was conducted in a real-world setting with participants riding in a car on public roads.,physical,Participants interacted with a physical Tesla Model X.,shared control (fixed rules),"The Tesla's autopilot system operated with fixed rules, and the driver could engage or disengage the system.",Behavioral Measures,,Speech Data; Video Data,Trust was assessed through qualitative analysis of conversations and video recordings.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Physical Robot Studies,Indirect Manipulation,"The study indirectly influenced trust by placing participants in a potentially risky driving scenario with an automated vehicle, and by having them interact in groups.","The study found that the risky driving context led to negative emotions and distrust, and that group dynamics influenced trust perceptions.","The study found that negative emotions were often shared among group members, and that groups often used storytelling and expert referents to make sense of the automation.","Group dynamics significantly influence trust processes in automated vehicles, with collective risk perception, experimentation, and sense-making playing key roles.","The Tesla drove on a predefined route using its autopilot features, while participants engaged in normal conversation.",,The study employed a thematic analysis of qualitative data (conversations) and did not use any statistical tests. The analysis focused on identifying overarching themes related to trust in automation.,TRUE,Task-environment; Teaming,Task-environment; Teaming,,"The study manipulated the task environment by placing participants in a real-world driving scenario with a Tesla Model X on campus roads, which inherently introduces elements of risk and uncertainty not present in a controlled lab setting. This is supported by the paper's emphasis on the 'risky driving context' and the 'naturalistic experiment' design. The study also implicitly manipulated 'Teaming' by having participants ride in groups of two or three and engage in conversation. This group interaction was a core aspect of the study design, as the researchers aimed to understand how group dynamics influence trust in automation. The paper explicitly states that 'group dynamics significantly influence trust processes' and that 'collective risk perception' was a key finding. The study found that the risky driving context and group dynamics impacted trust, as evidenced by the negative emotions and distrust that arose from the driving scenario and the influence of group conversations on trust perceptions. The paper states, 'Our findings highlight the untested and experimental nature of AVs and confirm serious concerns about the safety and readiness of this technology for on-road use.' and 'Group dynamics significantly influence trust processes in automated vehicles, with collective risk perception, experimentation, and sense-making playing key roles.' Therefore, both 'Task-environment' and 'Teaming' are listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",10.3389/fpsyg.2023.1129369,https://www.frontiersin.org/articles/10.3389/fpsyg.2023.1129369/full,"The growing concern about the risk and safety of autonomous vehicles (AVs) has made it vital to understand driver trust and behavior when operating AVs. While research has uncovered human factors and design issues based on individual driver performance, there remains a lack of insight into how trust in automation evolves in groups of people who face risk and uncertainty while traveling in AVs. To this end, we conducted a naturalistic experiment with groups of participants who were encouraged to engage in conversation while riding a Tesla Model X on campus roads. Our methodology was uniquely suited to uncover these issues through naturalistic interaction by groups in the face of a risky driving context. Conversations were analyzed, revealing several themes pertaining to trust in automation: (1) collective risk perception, (2) experimenting with automation, (3) group sense-making, (4) human-automation interaction issues, and (5) benefits of automation. Our findings highlight the untested and experimental nature of AVs and confirm serious concerns about the safety and readiness of this technology for on-road use. The process of determining appropriate trust and reliance in AVs will therefore be essential for drivers and passengers to ensure the safe use of this experimental and continuously changing technology. Revealing insights into social group–vehicle interaction, our results speak to the potential dangers and ethical challenges with AVs as well as provide theoretical insights on group trust processes with advanced technology."
"Moorman, Nina; Gombolay, Matthew",Do People Trust Robots that Learn in the Home?,2022,2,60,60,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were shown a video of a robot being unboxed, completed a pre-study questionnaire, observed videos of the robot learning and performing tasks in three domains (manipulation, navigation, preparation), completed a mid-study questionnaire, and then a post-study questionnaire.","Participants observed videos of a robot learning and performing tasks in three domains: navigation (guiding a user between rooms), manipulation (pouring liquids), and preparation (dispensing medicine).",LoCoBot,Mobile Manipulators,Care; Research,Supervision,Monitoring,passive observation,Participants passively observed videos of the robot performing tasks.,media,Participants watched videos of the robot performing tasks.,simulated,The robot was represented virtually in videos.,wizard of oz (directly controlled),The robot's actions were controlled by a human using a Wizard-of-Oz policy.,Behavioral Measures; Questionnaires,,Performance Metrics; Video Data,"Trust was assessed using questionnaires and behavioral measures, including intervention rates.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the level of robot learning (pre-programmed, low-involvement RL, high-involvement LfD) and the robot's performance by including both successful and failed attempts at the task.","The study aimed to determine if trust differed between pre-programmed and learning robots, and how different learning methods affected trust.",,The study investigated the difference in user trust between fully pre-engineered agents and agents that learn in the home.,"The robot performed navigation, manipulation, and preparation tasks, while participants observed and could intervene if they lost trust in the robot.",,"No specific statistical tests are mentioned in the paper for Study 1. The study focuses on comparing trust levels between different agent conditions (pre-programmed, low-involvement RL, high-involvement LfD) using surveys and behavioral measures (intervention rates), but the specific statistical methods used for this comparison are not detailed.",TRUE,Robot-adaptability; Robot-accuracy,Robot-adaptability,,"The study manipulated the level of robot learning by having three conditions: a pre-programmed agent, a low-involvement learning agent (RL), and a high-involvement learning agent (LfD). This directly manipulates the 'Robot-adaptability' factor, as the learning conditions allow the robot to adapt to the environment, while the pre-programmed agent does not. The study also manipulated the robot's performance by including both successful and failed attempts at the task, which is classified as 'Robot-accuracy'. The paper states that the study aimed to determine if trust differed between pre-programmed and learning robots, indicating that 'Robot-adaptability' was a factor that impacted trust. The paper does not explicitly state that 'Robot-accuracy' impacted trust, but it was a factor that was manipulated.",,http://arxiv.org/abs/2204.04260,"It is not scalable for assistive robotics to have all functionalities pre-programmed prior to user introduction. Instead, it is more realistic for agents to perform supplemental on site learning. This opportunity to learn user and environment particularities is especially helpful for care robots that assist with individualized caregiver activities in residential or nursing home environments. Many assistive robots, ranging in complexity from Roomba to Pepper, already conduct some of their learning in the home, observable to the user. We lack an understanding of how witnessing this learning impacts the user. Thus, we propose to assess end-user attitudes towards the concept of embodied robots that conduct some learning in the home as compared to robots that are delivered fully-capable. In this virtual, betweensubjects study, we recruit end users (care-givers and care-takers) from nursing homes, and investigate user trust in three different domains: navigation, manipulation, and preparation. Informed by the ﬁrst study where we identify agent learning as a key factor in determining trust, we propose a second study to explore how to modulate that trust. This second, in-person study investigates the effectiveness of apologies, explanations of robot failure, and transparency of learning at improving trust in embodied learning robots."
"Moorman, Nina; Gombolay, Matthew",Do People Trust Robots that Learn in the Home?,2022,2,50,50,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were shown a video of a robot being unboxed, completed a pre-study questionnaire, observed videos of the robot learning and performing a task, and then observed the robot performing the test task with different trust repair techniques (apology, explanation, transparency) or no repair.","Participants observed videos of a robot learning and performing a task, and then observed the robot performing a test task with different trust repair techniques.",LoCoBot,Mobile Manipulators,Care; Research,Supervision,Monitoring,passive observation,Participants passively observed videos of the robot performing tasks.,media,Participants watched videos of the robot performing tasks.,physical,Participants interacted with a physical robot in a lab setting.,wizard of oz (directly controlled),The robot's actions were controlled by a human using a Wizard-of-Oz policy.,Behavioral Measures; Questionnaires,,Performance Metrics; Video Data,"Trust was assessed using questionnaires and behavioral measures, including intervention rates.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated trust repair techniques (apology, explanation, transparency) after a trust violation.",The study investigated how different trust repair techniques affected trust in a learning robot.,,The study investigated how to best perform trust repair with respect to embodied agents that learn in the home.,"The robot performed a task, and participants observed and could intervene if they lost trust in the robot, and were given different trust repair techniques after a failure.",multivariate regression analysis,"The study mentions using multivariate regression analysis to account for the inclusion of participants outside of the target population. This suggests that the analysis will examine the relationship between trust repair techniques (apology, explanation, transparency) and trust, while controlling for other variables related to participant demographics.",TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-verbal-communication-content,,"The study manipulated trust repair techniques after a trust violation, specifically by providing an apology, an explanation, or transparency of learning. These manipulations are classified as 'Robot-verbal-communication-content' because they involve changing what the robot communicates to the user. The study also included a control condition with no trust repair technique. The study also manipulated the robot's performance by including both successful and failed attempts at the task, which is classified as 'Robot-accuracy'. The paper states that the study investigated how different trust repair techniques affected trust, indicating that 'Robot-verbal-communication-content' was a factor that impacted trust. The paper does not explicitly state that 'Robot-accuracy' impacted trust, but it was a factor that was manipulated.",,http://arxiv.org/abs/2204.04260,"It is not scalable for assistive robotics to have all functionalities pre-programmed prior to user introduction. Instead, it is more realistic for agents to perform supplemental on site learning. This opportunity to learn user and environment particularities is especially helpful for care robots that assist with individualized caregiver activities in residential or nursing home environments. Many assistive robots, ranging in complexity from Roomba to Pepper, already conduct some of their learning in the home, observable to the user. We lack an understanding of how witnessing this learning impacts the user. Thus, we propose to assess end-user attitudes towards the concept of embodied robots that conduct some learning in the home as compared to robots that are delivered fully-capable. In this virtual, betweensubjects study, we recruit end users (care-givers and care-takers) from nursing homes, and investigate user trust in three different domains: navigation, manipulation, and preparation. Informed by the ﬁrst study where we identify agent learning as a key factor in determining trust, we propose a second study to explore how to modulate that trust. This second, in-person study investigates the effectiveness of apologies, explanations of robot failure, and transparency of learning at improving trust in embodied learning robots."
"Moorman, Nina; Hedlund-Botti, Erin; Schrum, Mariah; Natarajan, Manisha; Gombolay, Matthew C.",Impacts of Robot Learning on User Attitude and Behavior,2023,1,131,131,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants first watched a video of the robot introducing itself, then completed a pre-study questionnaire. Next, they observed the robot learning a task in a training phase, followed by a final performance video. Finally, they observed the robot performing a similar test task and could intervene if they felt unsafe. Post-trial and post-study questionnaires were administered.","Participants observed a robot learning to assemble a breakfast plate, including cutting a banana and dispensing medicine. The test phase involved a similar task with a sharp knife and labeled medications.",Kinova MOVO,Mobile Manipulators,Care; Research,Manipulation,Cooking/Food Preparation,passive observation,"Participants observed videos of the robot performing tasks, with some participants also observing the robot in person during the testing phase.",media,"Participants primarily watched videos of the robot's learning and performance, with some in-person observation during the testing phase.",physical,"The robot was physically present for in-person participants during the testing phase, while remote participants observed videos of the robot.",wizard of oz (directly controlled),"The robot's actions were controlled by pre-recorded trajectories, implemented using Wizard-of-Oz policies.",Behavioral Measures; Questionnaires; Custom Scales,Checklist for Trust between People and Automation; Godspeed Questionnaire; Technology Acceptance Model (TAM),Performance Metrics,"Trust was assessed using questionnaires, a custom scale for perceived safety, and behavioral measures such as intervention counts.","parametric models (e.g., regression)",The study used ANOVA and linear regression models to analyze the impact of different factors on trust.,Empirical HRI Studies,Mixed Reality Studies,Direct Manipulation,"The study manipulated the robot's learning method (Download, RL, LfD, IntRL), the physical presence of the robot (in-person vs. remote), and the population (general vs. caregiver). The learning method was manipulated by showing different videos of the robot learning, and the physical presence was manipulated by having some participants observe the robot in person and others remotely. The population was manipulated by recruiting different groups of participants.","The robot's learning method did not directly impact trust, but physical presence significantly increased trust. Perceived success was found to have a large impact on trust. Caregivers were more critical of the robot's success on the medicine sub-task.","The study found that physical presence significantly increased trust, reliability, and usability, and decreased perceived risk. Caregivers were more critical of the robot's success on the medicine sub-task. The level of human involvement in the learning condition did not strongly impact user trust in the robot, perceived robot usability, or riskiness of the task.","Physical presence favorably impacts perceived safety, trust, and usability for tabletop manipulation tasks, and perceived success has a large impact on trust.","The robot performed a plate-making task, including cutting a banana and dispensing medicine. Human participants observed the robot learning and performing the task, and could intervene during the testing phase if they felt unsafe.",kruskal-wallis (kw) test; Wilcoxon rank sum; ANOVA; Linear regression; Spearman correlation; friedman rank sum test; Nemenyi test,"The study used a variety of statistical tests to analyze the impact of different factors on user perception and trust in robots. Kruskal-Wallis tests were used to compare groups for non-parametric data, such as anthropomorphism across learning conditions and perceived success between caregiver and general populations. Wilcoxon rank sum tests with Bonferroni correction were used for post-hoc analysis of significant Kruskal-Wallis results. ANOVA and linear regression models were employed to analyze the effects of physical presence and other factors on reliability, trust, usability, and risk. Spearman's correlation tests were used to examine relationships between variables like interruption count and perceived success. Friedman rank sum tests were used to compare interruption counts across different trial types (success, failure, ambiguous) and sub-tasks, with Nemenyi post-hoc tests for pairwise comparisons.",TRUE,Robot-adaptability; Task-environment; Teaming,Task-environment,Robot-adaptability,"The study manipulated the robot's learning method, which falls under 'Robot-adaptability' as it involves different ways the robot learns a task (Download, RL, LfD, IntRL). The physical presence of the robot (in-person vs. remote) was also manipulated, which is categorized as 'Task-environment' because it changes the setting in which the task is performed. The study also manipulated the population (general vs. caregiver), which is categorized as 'Teaming' because it changes the type of human interacting with the robot. The paper states, 'While we fnd that the degree of user involvement in the robot’s learning method impacts perceived anthropomorphism (��� = .001), we fnd that it is the participants’ perceived success of the robot that impacts the participants’ trust in (��� < .001) and perceived usability of the robot (��� < .001) rather than the robot’s learning method.' This indicates that 'Robot-adaptability' did not directly impact trust. The paper also states, 'Furthermore, we fnd that the physical presence of the robot impacts perceived safety (��� < .001), trust (��� < .001), and usability (��� < .014).' This indicates that 'Task-environment' impacted trust. The paper does not explicitly state that the population manipulation impacted trust, but it does state that caregivers were more critical of the robot's success on the medicine sub-task, which is related to trust but not a direct impact on trust itself.",10.1145/3568162.3576996,https://dl.acm.org/doi/10.1145/3568162.3576996,"With an aging population and a growing shortage of caregivers, the need for in-home robots is increasing. However, it is intractable for robots to have all functionalities pre-programmed prior to deployment. Instead, it is more realistic for robots to engage in supplemental, on-site learning about the user’s needs and preferences. Such learning may occur in the presence of or involve the user. We investigate the impacts on end-users of in situ robot learning through a series of human-subjects experiments. We examine how diferent learning methods infuence both in-person and remote participants’ perceptions of the robot. While we fnd that the degree of user involvement in the robot’s learning method impacts perceived anthropomorphism (��� = .001), we fnd that it is the participants’ perceived success of the robot that impacts the participants’ trust in (��� < .001) and perceived usability of the robot (��� < .001) rather than the robot’s learning method. Therefore, when presenting robot learning, the performance of the learning method appears more important than the degree of user involvement in the learning. Furthermore, we fnd that the physical presence of the robot impacts perceived safety (��� < .001), trust (��� < .001), and usability (��� < .014). Thus, for tabletop manipulation tasks, researchers should consider the impact of physical presence on experiment participants."
"Morales-Alvarez, Walter; Marouf, Mohamed; Tadjine, Hadj. Hamma; Olaverri-Monreal, Cristina","Real-World Evaluation of the Impact of Automated Driving System Technology on Driver Gaze Behavior, Reaction Time and Trust",2021,1,14,14,0,No participants were excluded,Real-World Environment,within-subjects,"Participants drove a car with an automated driving system (ADS) engaged. They performed non-driving related tasks (NDRT) at two different speeds (30 and 50 km/h). A take over request (TOR) was issued after 30 seconds, and the driver's reaction time, gaze behavior, and self-reported trust were measured.","Participants were asked to perform non-driving related tasks (NDRT) while the car was being driven by an automated system, and then take over control of the vehicle when a take over request (TOR) was issued.",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the automated driving system by performing secondary tasks and taking over control when prompted.,real-world,The study was conducted in a real-world driving scenario with a physical vehicle.,physical,The study involved a physical car with an automated driving system.,shared control (fixed rules),"The automated driving system controlled the vehicle until a take over request was issued, at which point the human took control.",Questionnaires,,Eye-tracking Data; Performance Metrics,Trust was assessed using a post-task questionnaire and behavioral measures such as reaction time and gaze behavior.,no modeling,"The study did not use computational models of trust, focusing on statistical analysis of the collected data.",Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The researchers manipulated the type of secondary task performed by the driver and the vehicle speed to influence driver behavior and trust.,"Self-reported trust in the automation decreased when participants were performing secondary tasks, but vehicle speed did not affect the level of trust. However, drivers reported to be more trustful in the correct functioning of the ADS when they were not performing any other task at a higher vehicle velocity.","The study found that reaction time to a TOR was faster when no secondary tasks were performed, and that the visual nature of the SCWT task resulted in the shortest reaction time for vehicle take over. The increase in vehicle velocity augmented the total time and frequency looking at the road, a lower driving speed resulting in drivers spending more time looking at their phones.","Both vehicle speed and the type of secondary task affect driver behavior, reaction time, and trust in the automation.","The robot (automated driving system) controlled the vehicle's driving, while the human performed secondary tasks and took over control when a take over request was issued.",ANOVA; Bonferroni correction; Wilcoxon signed-rank test; t-test,"The study used a One Way ANOVA with a Bonferroni multiple post-hoc comparison to determine if statistically significant relationships existed between the dependent variables (gaze behavior, reaction time) and the independent variables (secondary tasks and vehicle speed). A non-parametric Wilcoxon signed-rank test was used to investigate the effect of tasks and vehicle speed on self-reported trust in the automation. Finally, a Student T-test was used to analyze the effect of speed on each secondary task.",TRUE,Task-complexity; Task-constraints; Robot-autonomy,Task-complexity,Task-constraints,"The study manipulated the type of secondary task performed by the driver, which directly impacts the cognitive load and thus the 'Task-complexity'. The study also manipulated the vehicle speed, which introduces 'Task-constraints' by altering the time available to react to a TOR. The automated driving system's engagement and disengagement, which dictates when the human driver has control, is a manipulation of 'Robot-autonomy'. The results showed that the type of secondary task (Task-complexity) affected self-reported trust in the automation, with trust decreasing when participants were performing secondary tasks. However, the vehicle speed (Task-constraints) did not affect the level of trust, although drivers reported to be more trustful in the correct functioning of the ADS when they were not performing any other task at a higher vehicle velocity. The study did not explicitly manipulate the level of autonomy, but the experimental design involved a clear shift in control from the robot to the human, which is why 'Robot-autonomy' is included in the manipulated factors. However, the study did not find that the level of autonomy impacted trust, as the level of autonomy was fixed and not varied across conditions.",10.1109/IVWorkshops54471.2021.9669230,https://ieeexplore.ieee.org/document/9669230/,"Recent developments in advanced driving assistance systems (ADAS) that rely on some level of autonomy have led the automobile industry and research community to investigate the impact they might have on driving performance. However, most of the research performed so far is based on simulated environments. In this study we investigated the behavior of drivers in a vehicle with automated driving system (ADS) capabilities in a real life driving scenario. We analyzed their response to a take over request (TOR) at two different driving speeds while being engaged in non- driving-related tasks (NDRT). Results from the performed experiments showed that driver reaction time to a TOR, gaze behavior and self-reported trust in automation were affected by the type of NDRT being concurrently performed and driver reaction time and gaze behavior additionally depended on the driving or vehicle speed at the time of TOR."
"Morita, Junya",Cognitive Modeling of Automation Adaptation in a Time Critical Task,2020,1,63,63,0,No participants were excluded,Controlled Lab Environment,within-subjects,Participants performed a line-following task in 25 conditions where the capability of auto and manual controls were manipulated. Participants could switch between manual and auto control by pressing the space bar. Each condition lasted 40 seconds.,"Participants controlled a vehicle to follow a line using either manual or automated control, switching between modes as needed.",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated vehicle using keyboard inputs and monitoring the vehicle's movement.,simulation,The interaction was conducted in a simulated environment on a computer screen.,simulated,The robot was represented as a simulated vehicle on a computer screen.,shared control (fixed rules),"The robot's automated control followed fixed rules to follow the line, and the human could switch between manual and auto control.",Behavioral Measures; Performance-Based Measures,,Performance Metrics,"Trust was assessed through behavioral measures such as auto use ratio and number of switches, and performance metrics such as the ratio of time the vehicle was on the line.","deep learning (e.g., neural networks, reinforcement learning)",The study used a reinforcement learning model with a gating mechanism to simulate human adaptation to automation.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers directly manipulated the success rates of the auto and manual controls, which influenced the perceived reliability of each mode and the difficulty of the task.","The study found that the auto use ratio increased as the capability of the manual control decreased and the capability of the auto control increased, suggesting that trust in automation is influenced by its perceived reliability.","The models without SMDP rewarding failed to replicate the influence of the manual control capability on the auto use ratio, indicating that the model was not influenced by the capability of the manual mode. The model made more switches compared to the participants.","The study demonstrated that a reinforcement learning model with a gating mechanism can simulate human adaptation to automation in a time-critical task, and that the model's reliance on automation interacts with the performance of the task.","The robot, represented as a vehicle, attempts to follow a line using either automated or manual control. The human participant monitors the vehicle and switches between manual and auto control as needed to keep the vehicle on the line.",Pearson correlation; Root Mean Square Error,"The study used correlation analysis (specifically, R-squared) to assess the fit between the model's performance and the human data, and Root-Mean Square Errors (RMSE) to quantify the difference between the model's predictions and the observed data. These tests were used to evaluate how well the model replicated the trends observed in the human experiment, particularly in terms of performance, auto use ratio, and the number of switches between manual and auto control.",TRUE,Robot-accuracy; Robot-autonomy,Robot-accuracy,Robot-autonomy,"The study manipulated the success rates of both the manual and auto controls, which directly impacts the perceived reliability of each mode. This is categorized as 'Robot-accuracy' because it directly affects the performance of the robot in completing the task. The study also allowed participants to switch between manual and auto control, which is a form of shared control and thus categorized as 'Robot-autonomy'. The study found that the auto use ratio increased as the capability of the manual control decreased and the capability of the auto control increased, indicating that trust in automation is influenced by its perceived reliability, thus 'Robot-accuracy' impacted trust. The study did not find that the ability to switch between modes (autonomy) directly impacted trust, but rather the performance of each mode, thus 'Robot-autonomy' did not impact trust.",,,"This paper presents a cognitive model that simulates an adaptation process to automation in a time-critical task. The paper uses a simple tracking task (which represents vehicle operation) to reveal how the reliance on automation changes as the success probabilities of the automatic and manual mode vary. The model was developed by using a cognitive architecture, ACT-R (Adaptive Control of Thought-Rational). We also introduce two methods of reinforcement learning: the summation of rewards over time and a gating mechanism. The model performs this task through productions that manage perception and motor control. The utility values of these productions are updated based on rewards in every perception-action cycle. A run of this model simulated the overall trends of the behavioral data such as the performance (tracking accuracy), the auto use ratio, and the number of switches between the two modes, suggesting some validity of the assumptions made in our model. This work shows how combining different paradigms of cognitive modeling can lead to practical representations and solutions to automation and trust in automation."
"Morra, Lia; Lamberti, Fabrizio; Prattico, F. Gabriele; Rosa, Salvatore La; Montuschi, Paolo",Building Trust in Autonomous Vehicles: Role of Virtual Reality Driving Simulators in HMI Design,2019,1,39,38,1,1 subject was excluded due to excessive motion sickness,Controlled Lab Environment,mixed design,Participants were randomly assigned to either the OMN or SEL HUD group. They completed a VR driving simulation with either the OMN or SEL HUD. GSR data was collected during the simulation. Participants completed questionnaires before and after the simulation.,"Participants experienced a simulated ride in a virtual urban environment as a passenger in an autonomous vehicle, while viewing a head-up display (HUD) that provided information about the vehicle's sensory and planning systems.",Unspecified,Autonomous Vehicles,Research,Evaluation,Survey/Questionnaire Completion,minimal interaction,Participants passively experienced a simulated ride in a virtual environment.,simulation,Participants were immersed in a virtual reality driving simulation.,simulated,The autonomous vehicle was a virtual representation in the simulation.,fully autonomous (limited adaptation),"The autonomous vehicle operated without direct human control, but with pre-programmed responses to events.",Physiological Measures; Questionnaires,NASA Task Load Index (NASA-TLX); Situational Awareness Rating Technique (SART),Physiological Signals,Trust was assessed using questionnaires and physiological measures (GSR).,"parametric models (e.g., regression)",Linear regression was used to correlate questionnaire ratings with GSR data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the amount of information displayed on the HUD (omni-comprehensive vs. selective), which influenced the perceived performance of the vehicle, the user's cognitive load, and the user's ability to anticipate events.","The omni-comprehensive HUD led to a less stressful experience and increased willingness to test a real AV, suggesting a positive impact on trust. The selective HUD resulted in higher emotional arousal for some events.","The omni-comprehensive HUD, despite increasing cognitive load, resulted in a less stressful experience and increased willingness to test a real AV. The selective HUD showed higher emotional arousal for some events, suggesting that the amount of information displayed can impact the user's stress response.","Providing a complete picture of the vehicle's surroundings, despite higher cognitive load, leads to a less stressful experience and increased willingness to test a real autonomous vehicle.","The robot (autonomous vehicle) drove through a simulated urban environment, reacting to various events. The human participant was a passenger, observing the vehicle's behavior and the information displayed on the HUD.",ANOVA; t-test; Linear regression; Mann-Whitney U,A two-way factorial ANOVA was used to examine the main effect of HUD type and the interaction effect between event and HUD type on GSR features and questionnaire data related to test events. Post-hoc comparisons were performed using Bonferroni correction. Two-tailed t-tests were used to compare pre- and post-event GSR values. Multiple linear regression was used to predict GSR outcomes from questionnaire ratings. The Mann-Whitney U-test was used to compare questionnaire data between the OMN and SEL groups for categorical data.,TRUE,Robot-interface-design,Robot-interface-design,,"The study manipulated the amount of information displayed on the HUD, comparing an omni-comprehensive (OMN) display to a selective (SEL) display. This directly alters the interactive elements of the system, which falls under the 'Robot-interface-design' category. The paper states, 'Two variants of the HUD were designed, which in the following are referred to as omni-comprehensive (OMN) and selective (SEL).' The results showed that the OMN HUD, despite increasing cognitive load, led to a less stressful experience and increased willingness to test a real AV, indicating that the interface design impacted trust. The paper states, 'Moreover, after having been exposed to a more informative interface, users involved in the study were also more willing to test a real AD system.' and 'Finally, users in the OMN group were better disposed towards participating in a real AD experience (OMN M = 4.68, SD = 0.58, SEL M = 4.05, SD = 0.85, p = .012).'",10.1109/TVT.2019.2933601,https://ieeexplore.ieee.org/document/8789466/,"The investigation of factors contributing at making humans trust Autonomous Vehicles (AVs) will play a fundamental role in the adoption of such technology. The user’s ability to form a mental model of the AV, which is crucial to establish trust, depends on effective user-vehicle communication; thus, the importance of Human-Machine Interaction (HMI) is poised to increase. In this work, we propose a methodology to validate the user experience in AVs based on continuous, objective information gathered from physiological signals, while the user is immersed in a Virtual Reality-based driving simulation. We applied this methodology to the design of a head-up display interface delivering visual cues about the vehicle’s sensory and planning systems. Through this approach, we obtained qualitative and quantitative evidence that a complete picture of the vehicle’s surrounding, despite the higher cognitive load, is conducive to a less stressful experience. Moreover, after having been exposed to a more informative interface, users involved in the study were also more willing to test a real AV. The proposed methodology could be extended by adjusting the simulation environment, the HMI and/or the vehicle’s Artiﬁcial Intelligence modules to dig into other aspects of the user experience."
"Morris, Drew M.; Erno, Jason M.; Pilcher, June J.",Electrodermal Response and Automation Trust during Simulated Self-Driving Car Use,2017,1,28,28,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a driving task, followed by two autonomous driving scenarios (safe and risky), and completed the Automation Trust Survey after each autonomous scenario.",Participants drove a simulated vehicle manually and experienced two autonomous driving scenarios.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated autonomous vehicle.,simulation,The study used a high-fidelity driving simulator.,simulated,The robot was a simulated autonomous vehicle.,fully autonomous (limited adaptation),The simulated vehicle drove autonomously with pre-programmed behaviors.,Questionnaires; Physiological Measures,Jian et al. Trust Scale,Physiological Signals,Trust was measured using the Automation Trust Survey and skin conductance.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The autonomous vehicle's behavior was manipulated to be either safe or risky, influencing the perceived trustworthiness of the system.",Participants reported lower trust in the risky driving scenario compared to the safe driving scenario.,"Participants showed increased physiological stress (skin conductance) when the vehicle was in autonomous mode, and even more stress when the vehicle behaved in a risky manner. There was no significant difference in muscle tension.","Users show more signs of physiological stress when the vehicle drives autonomously than when the users is in control, and this stress increases when the user reports low trust in the autonomous vehicle.","The robot (simulated autonomous vehicle) drove either safely or riskily, while the human participant monitored the vehicle's behavior and completed trust questionnaires.",paired samples t-test; paired samples t-test; paired samples t-test,"The study used paired samples t-tests to compare trust ratings between safe and risky autonomous driving scenarios, skin conductance levels between manual driving and safe autonomous driving, and skin conductance levels between safe and risky autonomous driving scenarios. The t-tests were used to determine if there were statistically significant differences in these measures.",TRUE,Robot-accuracy; Robot-autonomy,Robot-accuracy,,"The study manipulated the behavior of the autonomous vehicle, making it either safe (obeying all rules) or risky (speeding and drifting). This directly impacts the 'Robot-accuracy' as the risky behavior represents a decrease in the robot's ability to perform the driving task correctly. The study also manipulated 'Robot-autonomy' by switching between manual driving and autonomous driving scenarios. The results showed that the risky behavior of the autonomous vehicle (Robot-accuracy) significantly impacted trust, with participants reporting lower trust in the risky scenario. The study did not find any factors that did not impact trust.",10.1177/1541931213601921,http://journals.sagepub.com/doi/10.1177/1541931213601921,The integration of self-driving vehicles may expose individuals with health concerns to undue amounts of stress. Psychophysiological indicators of stress were used to determine changes in tonic and phasic stress levels brought about by a high-fidelity autonomous vehicle simulation. Twenty-eight participants completed one manual driving task and two automated driving tasks. Participants reported their subjective level of trust in the automated systems using the Automation Trust Survey. Psychophysiological stress was indexed using skin conductance and trapezius muscle tension. Results indicate that users show more signs of physiological stress when the vehicle drives autonomously than when the users is in control. Results also indicate that users show an additional increase in stress when the user reports low trust in the autonomous vehicle. These findings suggest that health-care professionals and manufactures should be aware of additional stress associated with self-driving technology.
"Mota, Roberta C. Ramos; Rea, Daniel J.; Le Tran, Anna; Young, James E.; Sharlin, Ehud; Sousa, Mario C.",Playing the ‘trust game’ with robots: Social strategies and experiences,2016,1,7,5,2,2 participants were excluded because they figured out the study was Wizard-of-Oz,Controlled Lab Environment,within-subjects,"Participants completed a pre-study questionnaire, played two rounds of a trust game with a robot, and completed a post-study questionnaire and interview after each round.","Participants played a trust game where they invested money with a robot trustee, hoping for a return.",Baxter,Humanoid Robots; Collaborative Robots,Research; Social,Game,Economic Game,minimal interaction,Participants interacted with the robot in a controlled setting with verbal instructions.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,wizard of oz (directly controlled),The robot's actions were directly controlled by a human operator.,Behavioral Measures; Questionnaires,Godspeed Questionnaire,Video Data; Speech Data,"Trust was assessed using questionnaires and behavioral measures, including investment amounts and interview data.",no modeling,Trust was not modeled computationally; the analysis was qualitative.,Empirical HRI Studies,Wizard-of-Oz Studies,No Manipulation,The study did not manipulate any specific factors related to trust; it observed trust in a standard trust game scenario.,"The study observed that participants used human-like trust strategies, but also tried to leverage or build social experience with the robot.","Some participants tried to evaluate the robot's trustworthiness by looking for suspicious traits, similar to how they would evaluate humans. Some participants also reasoned about the robot's programming and the programmers behind it.","People may follow a human-robot trust model similar to the human-human trust model, but they try to leverage or build social experience due to the lack of social cues from the robot.","The robot acted as the trustee in a trust game, returning half of the tripled investment. The human participant acted as the investor, deciding how much money to invest with the robot.",ANOVA,"A one-way ANOVA was used to analyze the change in investment for the second round of the trust game. However, the results were not significant.",FALSE,,,,"The study did not manipulate any specific factors related to trust. The robot's behavior was scripted and consistent across all participants. The study observed trust in a standard trust game scenario without any intentional manipulation of robot behavior, task parameters, or environmental conditions. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust due to manipulation.",10.1109/ROMAN.2016.7745167,,"We present the results of a pilot study that investigates if and how people judge the trustworthiness of a robot during social Human-Robot Interaction (sHRI). Current research in sHRI has observed that people tend to interact with robots socially. However, results from neuroscience suggests people use different cognitive mechanisms interacting with robots than they do with humans, leading to a debate about whether people truly perceive robots as social entities. Our paper focuses on one aspect of this debate, by examining trustworthiness between people and robots using behavioral economics' `Trust Game' scenario. Our pilot study replicates a trust game scenario, where a person invests money with a robot trustee in hopes they will receive a larger sum (trusting the robot to give more back), then gets a chance to invest once more. Our qualitative analysis of investing behavior and interviews with participants suggests that people may follow a human-robot (h-r) trust model that is quite similar to the human-human trust model. Our results also suggest a possible resolution to the sHRI and Neuroscience debate: people try to interact socially with robots, but due to lack of common social cues, they draw from social experience, or create new experiences by actively exploring the robot behavior."
"Mou, Wenxuan; Ruocco, Martina; Zanatto, Debora; Cangelosi, Angelo",When Would You Trust a Robot? A Study on Trust and Theory of Mind in Human-Robot Interactions,2020,1,32,29,3,3 participants did not comply with the experimental procedure,Controlled Lab Environment,between-subjects,"Participants watched a video of a robot demonstrating either high or low ToM, then played a price game with the robot, and finally completed questionnaires.","Participants played a price game where they chose a price for an object, and the robot either agreed or disagreed, giving the participant a chance to change their choice.",Pepper,Humanoid Robots; Expressive Robots,Research; Social,Game,Economic Game,minimal interaction,"Participants interacted with the robot through a price game, with limited verbal interaction.",media,Participants watched a video of the robot before interacting with it in the price game.,physical,Participants interacted with a physical robot during the price game.,pre-programmed (non-adaptive),The robot's responses in the price game were pre-programmed and did not adapt to the participant's choices.,Behavioral Measures; Questionnaires,Godspeed Questionnaire; Trust Perception Scale - HRI; Credibility Scale,,Trust was measured using a behavioral measure (willingness to change price) and questionnaires.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's ToM abilities were manipulated by showing a video of the robot either correctly or incorrectly answering a false-belief task, influencing participants' expectations of the robot's competence.","Participants trusted the robot more when it was presented with high-level ToM abilities, as indicated by a higher willingness to change their price judgements.","The study found that a robot with higher ToM abilities was rated as more credible, anthropomorphic, and safe, although not all questionnaires showed a significant effect of the ToM level.","The study's main finding is that a robot's perceived Theory of Mind (ToM) abilities significantly affect human trust, with higher ToM leading to increased trust.","The robot presented two prices for an object, and the human participant selected one. If the robot disagreed, the participant could change their choice. The robot also provided a brief verbal description of the object.",t-test,"The study used a two-tailed t-test to determine the statistical power of the sample size. The t-test was used to compare the trust rate between the two groups (high-level ToM vs. low-level ToM). Although not explicitly stated, the t-test is implied in the text when discussing the statistical power calculation and the comparison of trust rates between the two groups. The text mentions that a sample size of twenty-seven participants would provide 80% statistical power for detecting a medium-sized effect equivalent to what was observed in a previous study, assuming a two-tailed t-test and an alpha level of 0.05. The study also mentions that the trust rate was calculated and compared between the two groups, which implies the use of a t-test or similar statistical test for comparison.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the robot's perceived Theory of Mind (ToM) abilities by showing participants a video of the robot either correctly or incorrectly answering a false-belief task. This manipulation directly influenced the content of the robot's verbal communication in the video, specifically whether it correctly identified the location of an object based on another person's belief. The robot's response in the video (correct or incorrect) is a form of verbal communication content that was manipulated. The study found that this manipulation of the robot's perceived ToM, through the video, impacted trust levels, as participants were more willing to change their price judgements when the robot was presented with high-level ToM abilities. Therefore, 'Robot-verbal-communication-content' is the most appropriate category for both the manipulated factor and the factor that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1109/RO-MAN47096.2020.9223551,https://ieeexplore.ieee.org/document/9223551/,"Trust is a critical issue in human–robot interactions (HRI) as it is the core of human desire to accept and use a non-human agent. Theory of Mind (ToM) has been deﬁned as the ability to understand the beliefs and intentions of others that may differ from one’s own. Evidences in psychology and HRI suggest that trust and ToM are interconnected and interdependent concepts, as the decision to trust another agent must depend on our own representation of this entity’s actions, beliefs and intentions. However, very few works take ToM of the robot into consideration while studying trust in HRI. In this paper, we investigated whether the exposure to the ToM abilities of a robot could affect humans’ trust towards the robot. To this end, participants played a Price Game with a humanoid robot (Pepper) that was presented having either low-level ToM or high-level ToM. Speciﬁcally, the participants were asked to accept the price evaluations on common objects presented by the robot. The willingness of the participants to change their own price judgement of the objects (i.e., accept the price the robot suggested) was used as the main measurement of the trust towards the robot. Our experimental results showed that robots possessing a high-level of ToM abilities were trusted more than the robots presented with low-level ToM skills."
"Mullins, T’kara; Necaise, Aaron; Fiore, Stephen M.; Amon, Mary Jean",Navigating Trust: The Interplay of Trust in Automation and Team Communication in an Extended Simulated Military Mission,2024,1,7,7,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed 18 missions over 2 weeks in a computer-simulated environment, interacting with autonomous technologies to operate combat vehicles. They completed two 90-min missions daily for 9 days, navigating varied terrains and combatting enemy forces. Trust in automation, team trust, and workload were captured immediately following each mission.","Participants operated combat vehicles in a simulated military mission, coordinating one manned vehicle and two robotic combat vehicles, while also combatting enemy forces.",Unspecified,Unmanned Ground Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the robots through a computer simulation.,simulation,The study used a high-visual fidelity computer simulation of a military environment.,simulated,The robots were represented as virtual entities within the simulation.,shared control (fixed rules),"The robotic combat vehicles navigated and avoided obstacles independently, but their waypoints were selected by the human drivers.",Questionnaires,Trust in Automation Scale (TAS),Speech Data; Performance Metrics,"Trust was measured using the Trust in Automation Scale, and speech data and performance metrics were collected.","parametric models (e.g., regression)",A mixed-effects regression model was used to predict speech dynamics based on trust in automation and other factors.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,"The study did not directly manipulate any factors related to trust, but measured trust in automation, team trust, and workload.","The study found that lower trust in automation was associated with increased speech, but did not directly manipulate trust.","The study found a negative association between trust in automation and team communication, which is contrary to the common intuition that more trust and communication are better for team performance.","Lower trust in automation was significantly associated with increased speech among team members, even when controlling for other factors.","The human participants controlled manned and robotic combat vehicles in a simulated environment, while also monitoring the autonomous target recognition system. The robots autonomously navigated and avoided obstacles, but their waypoints were selected by the human drivers.",repeated measures correlation; Linear regression,"The study used repeated measures correlation to examine the relationships between total words spoken, workload, and trust in automation. A mixed-effects regression model was then used to predict the number of words spoken by each participant based on trust in automation, team trust, team cooperation, perceived workload, and mission performance, while controlling for participant role and mission scenario as random intercepts.",FALSE,,,,"The study did not manipulate any factors related to the robot or the task. The study measured trust in automation, team trust, and workload, but these were not manipulated as independent variables. The study design involved a longitudinal observation of a team performing a simulated military mission, where the team interacted with autonomous technologies. The study did not change any aspect of the robot's behavior, communication, or task strategy. The study also did not change the task complexity, constraints, or environment. Therefore, no factors were manipulated.",10.1177/10711813241262991,https://journals.sagepub.com/doi/10.1177/10711813241262991,"Trust in automation and team communication are crucial factors in human-agent teaming. While research has examined how trust in automation and team dynamics impact performance separately, less is known about how they combine to influence team dynamics. This study investigated how team speech dynamics are related to trust in automation, team perceptions, and workload in the context of a longitudinal multi-participant computer simulated military mission using active-duty military personnel. Results showed that participants with lower trust in automation spoke more than their teammates with higher trust in automation, even after controlling for perceptions of team trust, cooperation, workload, role on the team, and team performance. A common finding in the team literature is that more team communication and more trust in automation are, separately, better for team performance. Thus, this research is an initial step toward demonstrating how automation alters the team dynamics typically considered essential to their success."
"Muthumani, Arun; Diederichs, Frederik; Galle, Melanie; Schmid-Lorch, Sebastian; Forsberg, Christian; Widlroither, Harald; Feierle, Alexander; Bengler, Klaus","How Visual Cues on Steering Wheel Improve Users’ Trust, Experience, and Acceptance in Automated Vehicles",2020,1,38,33,5,5 participants were excluded for technical reasons,Controlled Lab Environment,within-subjects,"Participants completed a driving simulation with three different HMI designs, including a baseline and two designs with visual cues on the steering wheel. They first had a practice session, then completed the experimental drive twice for each HMI design. Subjective questionnaires were used to measure user experience, trust, and acceptance.",Participants drove a simulated vehicle and performed a Surrogate Reference Task (SuRT) while the vehicle was in automated driving modes. They experienced unscheduled transitions to manual mode.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a driving simulator and a tablet, with limited physical interaction with the vehicle.",simulation,The study used a driving simulator with multiple projection screens to create an immersive driving experience.,simulated,The vehicle was a simulated representation within the driving simulator.,shared control (fixed rules),The vehicle operated in automated modes with fixed rules for transitions and responses to the driver.,Questionnaires,,,Trust was measured using a two-item Likert scale questionnaire.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the visual cues on the steering wheel, changing the appearance and feedback provided to the driver about the automation state. This was intended to influence trust by increasing transparency and clarity of the system's status.",The designs with visual cues on the steering wheel significantly increased trust compared to the baseline design.,"Concept B scored higher than Concept A in user acceptance, possibly due to the distinctive colors differentiating AD and ASD modes.",Visual cues on the steering wheel significantly increased drivers' trust in the automated vehicle system.,"The human participant drove a simulated vehicle, monitored the automation system, and performed a secondary task (SuRT) when the vehicle was in automated mode. The robot (simulated vehicle) provided visual and auditory cues about its state and automation level, and initiated unscheduled transitions to manual mode.",ANOVA; t-test; Friedman test; Wilcoxon rank sum,"The study used ANOVA and Bonferroni-adjusted post-hoc t-tests for parametric data, and Friedman T-test and Bonferroni-adjusted post-hoc Wilcoxon tests for non-parametric data. These tests were used to compare the subjective questionnaire results (user experience, trust, and acceptance) across the three HMI designs (Concept A, Concept B, and Baseline).",TRUE,Robot-interface-design,Robot-interface-design,,"The study manipulated the visual cues on the steering wheel by using different LED patterns and colors to indicate the automation state (AD, ASD, and manual mode). This falls under 'Robot-interface-design' because it involves changes to interactive elements of the system, specifically the visual display on the steering wheel. The paper explicitly states that the designs with visual cues on the steering wheel significantly increased trust compared to the baseline design, indicating that the 'Robot-interface-design' was a factor that impacted trust. There were no factors that were manipulated that did not impact trust.",,http://link.springer.com/10.1007/978-3-030-50943-9_24,"With the introduction of ADAS systems and vehicle automation, an interface informing the driver of the automation state is required. This study evaluates the suitability of a visual interface comprising up to 64 LEDs on the steering wheel perimeter; it displays continuous visual feedback about the automation state—including notiﬁcations of an unscheduled hand-over due to sudden system failure. Three HMI (Human Machine Interface) designs were evaluated: two versions with visual cues on the steering wheel and one without (baseline). We implemented the designs in a driving simulator and compared the subjective responses of 38 participants to questionnaires measuring user experience, trust, and acceptance. The designs with visual cues improved the participants’ user experience, as well as their trust in, and acceptance of, automated vehicles. Moreover, both designs were well perceived by participants."
"Na, Gyounghwa; Choi, Junho; Kang, Hyunmin","It’s Not My Fault, But I’m to Blame: The Effect of a Home Robot’s Attribution and Approach Movement on Trust and Emotion of Users",2023,1,62,49,13,"3 participants were excluded based on time, 10 participants were excluded because they were unable to respond to the screening questions",Online Crowdsourcing,within-subjects,"Participants were randomly exposed to four scenario videos, then responded to a survey after each video.","Participants watched videos of a robot in a home setting and responded to a scenario where the robot either attributed an error to the user or itself, while either approaching or staying in place.",Unspecified,Service and Assistive Robots,Social; Care,Social,Conversation,passive observation,Participants watched videos of the robot interaction.,media,The interaction was presented through a 3D video.,simulated,The robot was presented as a 3D model in a video.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to user input.,Questionnaires,,,Trust was measured using a questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's verbal attribution of responsibility (to user or self) and its approach movement (approaching or staying in place) were manipulated to influence user trust and emotions.,"Trust decreased when the robot attributed responsibility to the user, especially when the robot also approached the user.","The study found that self-serving bias occurs even in situational errors, and that users reacted negatively to robots that attributed blame to them, even when the user was at fault. The robot's approach behavior was also found to negatively impact user experience when combined with user-blaming attribution.","Users' trust in a robot decreases when the robot attributes responsibility for an error to the user, especially when the robot also approaches the user.","The robot provided feedback to the user's question about the home temperature, attributing the error to either the user or itself, while either approaching or staying in place. The human participant watched the video and answered a questionnaire.",ANOVA; t-test,The study used repeated measures ANOVA to analyze the effects of responsibility attribution (robot vs. user) and approach behavior (in-place vs. approach) on user emotions (positive and negative) and trust. Independent samples t-tests were used to check for gender differences and ordering effects on the dependent variables (trust and emotions).,TRUE,Robot-verbal-communication-content; Robot-nonverbal-communication,Robot-verbal-communication-content; Robot-nonverbal-communication,,"The study manipulated two factors: the robot's verbal attribution of responsibility (to the user or itself) and the robot's approach movement (approaching or staying in place). The verbal attribution is categorized as 'Robot-verbal-communication-content' because it directly changes the content of what the robot communicates to the user regarding the cause of the error. The approach movement is categorized as 'Robot-nonverbal-communication' because it involves a change in the robot's physical movement and proxemics. The results showed that both the robot's verbal attribution and approach behavior impacted user trust. Specifically, trust decreased when the robot attributed responsibility to the user, especially when the robot also approached the user. Therefore, both 'Robot-verbal-communication-content' and 'Robot-nonverbal-communication' are listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1080/10447318.2023.2209977,https://www.tandfonline.com/doi/full/10.1080/10447318.2023.2209977,"This study investigated the effect of attribution and approach movement of the social robot when the user wrongly perceives an error as the robot’s responsibility. The robot’s responsibility attribution and approach movement strategies for the error recovery were examined in a situation where the robot was functioning normally, but the user misunderstood it as robot’s fault. In the experiment participants were exposed to four different verbal and movement interaction scenarios with a social robot and then responded to a survey concerning emotions and trust. Results showed that people no longer trusted the robot that approached while attributing the responsibility to the user. The implication of this study is that a powerful self-serving bias is aroused when a robot attributes the responsibility to the user, and thus, it negatively impacts user experiences even if the event took place due to the user’s misunderstanding. This study suggests empirical guidance for designing a social robot’s attribution strategies and movement interactions."
"Nam, Changjoo; Walker, Phillip; Lewis, Michael; Sycara, Katia",Predicting trust in human control of swarms via inverse reinforcement learning,2017,1,10,10,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a 5-minute training followed by three 5-minute trials. In the first two trials, participants controlled a swarm and provided trust feedback. The third trial involved classifying command inputs as intervention or non-intervention.",Participants controlled a swarm of robots to find targets in a simulated environment.,Unspecified,Swarm Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the robot swarm through a simulation interface.,simulation,The interaction took place in a simulated environment.,simulated,The robots were represented as dots in a simulation.,shared control (fixed rules),"The swarm followed a flocking algorithm, but the human could influence its heading.",Custom Scales; Behavioral Measures; Real-time Trust Measures,,Performance Metrics; robot data,"Trust was measured using a slider, command inputs, and swarm parameters.","deep learning (e.g., neural networks, reinforcement learning)",Inverse reinforcement learning was used to model trust based on swarm parameters and user actions.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers introduced occasional alignment errors in the second trial to mimic sensing and communication failures, and the task was to find targets.",Trust decreased when alignment errors were introduced and when participants issued interventions.,"The study found that humans are more affected by appearance-related factors of the swarm than its performance. Also, the task performance was better on intervention segments, which was unexpected.",Human trust in swarm control is more influenced by the swarm's appearance (heading variance and convex hull area) than its task performance.,The robot swarm moved using a flocking algorithm to search for targets. The human operator provided heading direction commands to guide the swarm.,t-test; ANOVA,"The study used t-tests to compare the means of different groups, such as the length of command vectors for intervention and non-intervention commands, trust feedback values for intervention and non-intervention/no-command groups, swarm parameters (heading variance and convex hull area) between intervention and non-intervention groups, and task performance between intervention and non-intervention segments. A 3-way ANOVA was used to analyze individual differences in trust feedback based on participant, heading variance, and convex hull area.",TRUE,Robot-accuracy,Robot-accuracy,,"The researchers introduced occasional alignment errors in the second trial to mimic sensing and communication failures. This manipulation directly affected the swarm's ability to accurately follow the user's commands and find targets, thus impacting the robot's accuracy. The paper states, 'In the second trial, occasional alignment errors were injected to mimic sensing and communication failures.' This directly relates to the robot's performance in following commands and achieving the task goal, which is a core aspect of 'Robot-accuracy'. The study found that trust decreased when these alignment errors were introduced, as stated in the paper: 'Trust decreased when alignment errors were introduced and when participants issued interventions.' Therefore, 'Robot-accuracy' is the factor that impacted trust. There were no other factors manipulated in the study.",10.1109/ROMAN.2017.8172353,http://ieeexplore.ieee.org/document/8172353/,"In this paper, we study the model of human trust where an operator controls a robotic swarm remotely for a search mission. Existing trust models in human-inthe-loop systems are based on task performance of robots. However, we ﬁnd that humans tend to make their decisions based on physical characteristics of the swarm rather than its performance since task performance of swarms is not clearly perceivable by humans. We formulate trust as a Markov decision process whose state space includes physical parameters of the swarm. We employ an inverse reinforcement learning algorithm to learn behaviors of the operator from a single demonstration. The learned behaviors are used to predict the trust level of the operator based on the features of the swarm."
"Nam, Changjoo; Walker, Phillip; Li, Huao; Lewis, Michael; Sycara, Katia",Models of Trust in Human Control of Swarms With Varied Levels of Autonomy,2020,2,10,10,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a 5-min training session followed by three 5-min trials. In the third trial, participants labeled their commands as intervention or non-intervention. Data from the first two trials were used to investigate factors affecting trust.",Participants controlled a swarm of robots to discover hidden targets in a simulated environment.,Unspecified,Swarm Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the robot swarm through a simulation interface.,simulation,Participants interacted with a simulated swarm of robots.,simulated,The robots were represented as dots in a simulation.,shared control (fixed rules),The swarm moved autonomously but the human could provide heading direction commands.,Real-time Trust Measures,,Performance Metrics; robot data,Trust was measured using a real-time slider and data was collected on swarm parameters and performance.,"deep learning (e.g., neural networks, reinforcement learning)",Trust was modeled using a Markov decision process and inverse reinforcement learning.,Empirical HRI Studies,Virtual/Simulated Studies,Indirect Manipulation,"The study indirectly manipulated trust by varying the swarm's behavior and task difficulty, which influenced the user's perception of the swarm's performance.","Participants tended to have lower trust when they issued interventions, and trust was influenced by swarm appearance rather than performance.","The study found that participants were more influenced by the appearance of the swarm than its performance, which was unexpected. The performance of the swarm was better when interventions were issued, which contradicted expectations.",Human trust in swarm control is more influenced by the swarm's appearance (heading variance and convex hull area) than its actual performance.,"The robot swarm moved autonomously, and the human provided heading direction commands to guide the swarm to discover targets.",t-test; ANOVA,"The study used t-tests to compare the means of different groups, such as the length of command vectors for intervention and non-intervention commands, trust feedback between intervention and non-intervention groups, swarm parameters (heading variance and convex hull area) between intervention and non-intervention groups, and performance between intervention and non-intervention groups. A three-way ANOVA was used to examine the effects of participant ID, heading variance, and convex hull area on trust feedback, including interaction effects.",TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,Task-complexity,"The study manipulated the level of autonomy by having the swarm move autonomously but allowing the human to provide heading direction commands, which is a form of shared control. This is categorized as 'Robot-autonomy'. The study also indirectly manipulated task difficulty by varying the swarm's behavior and the environment, which influenced the user's perception of the swarm's performance. This is categorized as 'Task-complexity'. The study found that trust was influenced by the swarm's appearance (heading variance and convex hull area) and the level of autonomy, but not by the actual task performance (number of targets found), which is related to task complexity. Therefore, 'Robot-autonomy' is listed as a factor that impacted trust, and 'Task-complexity' is listed as a factor that did not impact trust.",10.1109/THMS.2019.2896845,https://ieeexplore.ieee.org/document/8651317/,"In this paper, we study human trust and its computational models in supervisory control of swarm robots with varied levels of autonomy (LOA) in a target foraging task. We implement three LOAs: manual, mixed-initiative (MI), and fully autonomous LOA. While the swarm in the MI LOA is controlled by a human operator and an autonomous search algorithm collaboratively, the swarms in the manual and autonomous LOAs are fully directed by the human and the search algorithm, respectively. From user studies, we ﬁnd that humans tend to make their decisions based on physical characteristics of the swarm rather than its performance since the task performance of swarms is not clearly perceivable by humans. Based on the analysis, we formulate trust as a Markov decision process whose state space includes the factors affecting trust. We develop variations of the trust model for different LOAs. We employ an inverse reinforcement learning algorithm to learn behaviors of the operator from demonstrations where the learned behaviors are used to predict human trust. Compared to an existing model, our models reduce the prediction error by at most 39.6%, 36.5%, and 28.8% in the manual, MI, and auto-LOA, respectively."
"Nam, Changjoo; Walker, Phillip; Li, Huao; Lewis, Michael; Sycara, Katia",Models of Trust in Human Control of Swarms With Varied Levels of Autonomy,2020,2,20,20,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a 2-min training session followed by three 5-min trials in each of three different levels of autonomy (manual, mixed-initiative, and fully autonomous).",Participants controlled a swarm of robots to discover hidden targets in a simulated environment with varying levels of autonomy.,Unspecified,Swarm Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the robot swarm through a simulation interface.,simulation,Participants interacted with a simulated swarm of robots.,simulated,The robots were represented as dots in a simulation.,shared control (fixed rules),The swarm moved autonomously but the human could provide heading direction commands.,Real-time Trust Measures,,Performance Metrics; robot data,Trust was measured using a real-time slider and data was collected on swarm parameters and performance.,"deep learning (e.g., neural networks, reinforcement learning)",Trust was modeled using a Markov decision process and inverse reinforcement learning.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study directly manipulated the level of autonomy of the swarm, which influenced the user's perception of the swarm's performance and their trust.","Participants tended to have lower trust when they issued interventions, and trust was influenced by swarm appearance rather than performance. The level of autonomy also affected trust.","The study confirmed that participants were more influenced by the appearance of the swarm than its performance, and that this effect was consistent across different levels of autonomy. The connectivity of the swarm was also found to influence trust.","Human trust in swarm control is more influenced by the swarm's appearance (heading variance and convex hull area) than its actual performance, and this is consistent across different levels of autonomy.","The robot swarm moved autonomously, and the human provided heading direction commands to guide the swarm to discover targets. The level of autonomy varied between manual, mixed-initiative, and fully autonomous.",t-test; ANOVA,"The study used t-tests to compare the means of different groups, such as trust feedback between intervention and non-intervention groups in manual and MI LOAs, swarm parameters (heading variance, convex hull area, and connectivity) between intervention and non-intervention groups, and trust feedback values when users or the system initiated the mode switch. A three-way ANOVA was used to examine the effects of participant ID, heading variance, and convex hull area on trust feedback across all LOAs. One-way ANOVAs were used to test for learning effects with respect to task performance.",TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,Task-complexity,"The study directly manipulated the level of autonomy of the swarm by implementing three different levels: manual, mixed-initiative, and fully autonomous. This is categorized as 'Robot-autonomy'. The study also indirectly manipulated task difficulty by varying the swarm's behavior and the environment, which influenced the user's perception of the swarm's performance. This is categorized as 'Task-complexity'. The study found that trust was influenced by the swarm's appearance (heading variance and convex hull area) and the level of autonomy, but not by the actual task performance (number of targets found), which is related to task complexity. Therefore, 'Robot-autonomy' is listed as a factor that impacted trust, and 'Task-complexity' is listed as a factor that did not impact trust.",10.1109/THMS.2019.2896845,https://ieeexplore.ieee.org/document/8651317/,"In this paper, we study human trust and its computational models in supervisory control of swarm robots with varied levels of autonomy (LOA) in a target foraging task. We implement three LOAs: manual, mixed-initiative (MI), and fully autonomous LOA. While the swarm in the MI LOA is controlled by a human operator and an autonomous search algorithm collaboratively, the swarms in the manual and autonomous LOAs are fully directed by the human and the search algorithm, respectively. From user studies, we ﬁnd that humans tend to make their decisions based on physical characteristics of the swarm rather than its performance since the task performance of swarms is not clearly perceivable by humans. Based on the analysis, we formulate trust as a Markov decision process whose state space includes the factors affecting trust. We develop variations of the trust model for different LOAs. We employ an inverse reinforcement learning algorithm to learn behaviors of the operator from demonstrations where the learned behaviors are used to predict human trust. Compared to an existing model, our models reduce the prediction error by at most 39.6%, 36.5%, and 28.8% in the manual, MI, and auto-LOA, respectively."
"Natarajan, Manisha; Gombolay, Matthew",Effects of Anthropomorphism and Accountability on Trust in Human Robot Interaction,2020,1,75,72,3,3 subjects were discarded due to loss of network connectivity and a hardware issue -one of the robots shut down unexpectedly during the study,Controlled Lab Environment,mixed design,"Participants completed a demographics survey, then played an online math quiz with virtual and embodied robots providing hints. After each agent interaction, participants completed a trust and anthropomorphism questionnaire. Participants were given a break between physical and virtual agents.",Participants completed an online math quiz where a robot provided hints to solve complex arithmetic problems under time pressure.,Pepper; Nao; Sawyer; Kuri,Humanoid Robots; Humanoid Robots; Industrial Robot Arms; Mobile Robots,Research; Social,Game,Cooperative Game,minimal interaction,"Participants interacted with the robot through a screen and verbal instructions, with the robot present in the room for the embodied condition.",simulation,The interaction was through an online math quiz with pre-recorded videos for virtual robots and real robots for the embodied condition.,simulated,"The study used both virtual and physical robots, with virtual robots presented through pre-recorded videos.",pre-programmed (non-adaptive),"The robots provided pre-programmed hints and feedback based on the user's actions, without adapting to the user's behavior.",Questionnaires; Behavioral Measures,Godspeed Questionnaire,Performance Metrics,Trust was assessed using a 7-point Likert scale questionnaire and behavioral measures of compliance and reliance.,"parametric models (e.g., regression)",The study used linear mixed-effects models and ANOVA to analyze the relationship between trust and various factors.,Empirical HRI Studies,Mixed Reality Studies,Direct Manipulation,"The researchers manipulated robot behavior (correct, apologetic, accountable, indifferent), robot appearance (different robot models), interaction medium (virtual vs. embodied), and human expectations (coalition-building preface) to see how these factors affect trust.","Perceived anthropomorphism and robot behavior significantly influenced trust, with correct behaviors leading to higher trust. The coalition-building preface increased trust for specific behaviors. Robot presence was not a significant factor.","The study found that perceived anthropomorphism, rather than the robot's form factor, was the key contributor to trust. The coalition-building preface only increased trust for the accountable agent. Robot presence was not a significant factor, which was unexpected.",The study's main finding is that both the behavior and perceived anthropomorphism of a robot are the most significant factors in predicting trust and compliance in human-robot interaction.,"The robot provided hints to math problems, and the human decided whether to accept or reject the hint. The robot then provided feedback on the user's answer.",ANOVA; Repeated measures ANOVA; Mixed-effects model; tukey-hsd; Kruskal-Wallis,"The study used a variety of statistical tests to analyze the relationships between trust, anthropomorphism, robot behavior, robot presence, and other factors. ANOVA was used as an omnibus test to evaluate the main hypotheses (H1-H3). Repeated measures ANOVA (rANOVA) was used to analyze the effects of within-subject factors, such as behavior and robot presence, on trust. Linear mixed-effects models were employed to account for the random effect of subject ID and to analyze the impact of various factors on trust, inappropriate compliance, and inappropriate reliance. Post-hoc analysis using Tukey-HSD was performed for pairwise comparisons of different behaviors. Kruskal-Wallis test was used when the assumptions of ANOVA were not met, specifically for analyzing the effect of robot presence on perceived anthropomorphism and for analyzing inappropriate reliance. The overall purpose of these tests was to determine the statistical significance of the manipulated factors on trust and related measures.",TRUE,Robot-nonverbal-communication; Robot-verbal-communication-content; Robot-accuracy; Teaming,Robot-verbal-communication-content; Robot-accuracy,Robot-nonverbal-communication; Teaming,"The study manipulated several factors related to the robot's behavior and communication. 'Robot-nonverbal-communication' was manipulated through different gestures and body movements of the robots (Pepper, Nao, Sawyer, Kuri) to convey feedback, as described in the 'Design of Gestures and Responses for Robots' section. 'Robot-verbal-communication-content' was manipulated by varying the type of feedback given by the robot (apologetic, accountable, indifferent) after the user's answer, as detailed in the 'Behavior' section. 'Robot-accuracy' was manipulated by having the robot provide correct advice 50% of the time for incorrect agents, and always correct advice for correct agents, directly impacting task performance. 'Teaming' was manipulated by introducing a coalition-building preface, where the agent confesses to the subject whether it is prone to making errors at the start of the study, which changes the nature of the human-robot interaction from a simple task to a collaborative one. The results showed that 'Robot-verbal-communication-content' and 'Robot-accuracy' significantly impacted trust, as different feedback types and the correctness of the robot's advice influenced user trust. However, 'Robot-nonverbal-communication' and 'Teaming' did not significantly impact trust. The study found that the robot's form factor and physical presence did not significantly impact trust, which is related to the nonverbal communication, and the coalition-building preface only increased trust for specific behaviors, indicating that the teaming aspect was not a general factor for trust.",10.1145/3319502.3374839,https://dl.acm.org/doi/10.1145/3319502.3374839,"This paper examines how people’s trust and dependence on robot teammates providing decision support varies as a function of different attributes of the robot, such as perceived anthropomorphism, type of support provided by the robot, and its physical presence. We conduct a mixed-design user study with multiple robots to investigate trust, inappropriate reliance, and compliance measures in the context of a time-constrained game. We also examine how the effect of human accountability addresses errors due to over-compliance in the context of human robot interaction (HRI). This study is novel as it involves examining multiple attributes at once, thus enabling us to perform multi-way comparisons between different attributes on trust and compliance with the agent. Results from the 4x4x2x2 study show that behavior and anthropomorphism of the agent are the most significant factors in predicting the trust and compliance with the robot. Furthermore, adding a coalition-building preface, where the agent provides context to why it might make errors while giving advice, leads to an increase in trust for specific behaviors of the agent."
"Nayyar, Mollik; Wagner, Alan R.",When Should a Robot Apologize? Understanding How Timing Affects Human-Robot Trust Repair,2018,1,558,324,269,"35 submissions were considered invalid due to bad surveys, repeated attempts, etc., 234 participants failed the embedded manipulation check in the experiment",Online Crowdsourcing,between-subjects,"Participants completed an online simulation where they were guided by a robot to a meeting room, the robot made mistakes, and then an emergency occurred. The robot then offered to guide them to an exit, with a trust repair message presented either before or during the emergency. The timing and display duration of the message, and the presence of an emergency were manipulated.","Participants were tasked with following a robot to a meeting room and then, during an emergency, deciding whether to follow the robot to an exit or find their own way.",Unspecified,Mobile Robots,Research; Social,Navigation,Guiding,minimal interaction,Participants interacted with a robot in a simulation environment.,simulation,The interaction took place in an interactive virtual environment.,simulated,The robot was a virtual representation in the simulation.,pre-programmed (non-adaptive),The robot followed a pre-set path and delivered pre-scripted messages.,Behavioral Measures,,Performance Metrics,Trust was measured by whether participants chose to follow the robot to the exit.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The timing of the robot's trust repair message (early vs. late), the display time of the message, and the presence of an emergency were manipulated to influence trust.","A late trust repair message was more effective than an early one, and the message needed to be internalized to be effective. The presence of an emergency also influenced trust.","The study found that the timing of a trust repair message significantly impacts its effectiveness, with late messages being more effective. The study also found that the message needs to be internalized to be effective, and that the presence of an emergency influences trust. There was a small increase in trust when the time between the early trust repair and the decision point was reduced, and a small decrease in trust when the time between the mistake and the late trust repair was reduced.","The timing of a robot's trust repair message significantly impacts its effectiveness, with late messages being more effective than early messages.","The robot guided the participant to a meeting room, making mistakes along the way. The human participant followed the robot and then decided whether to follow the robot again during an emergency to find an exit.",Chi-squared,"The study used chi-squared tests to determine the statistical significance of differences in the proportion of participants who chose to follow the robot in different experimental conditions. Specifically, these tests were used to compare the effectiveness of early versus late trust repair messages, the impact of message display time, the effect of time between the mistake and decision, and the influence of framing the situation as an emergency.",TRUE,Robot-social-timing; Robot-verbal-communication-content; Task-constraints,Robot-social-timing; Robot-verbal-communication-content; Task-constraints,,"The study manipulated the timing of the robot's trust repair message (early vs. late), which falls under 'Robot-social-timing' because it's about the timing of a social behavior. The content of the message itself (apology or promise) is a manipulation of 'Robot-verbal-communication-content'. The presence or absence of an emergency, which introduces time pressure, is a manipulation of 'Task-constraints'. The results showed that the timing of the message, the content of the message, and the presence of an emergency all impacted trust. Specifically, a late trust repair message was more effective than an early one, the message needed to be internalized to be effective, and the presence of an emergency influenced trust. The study also manipulated the display time of the message, but this was found to impact the effectiveness of the message, and thus is included in 'Robot-verbal-communication-content'. The study also manipulated the time between the mistake and the decision point, which is a manipulation of 'Robot-social-timing', and this was found to impact trust. The study also manipulated the presence of an emergency, which is a manipulation of 'Task-constraints', and this was found to impact trust. The study did not find any factors that did not impact trust.",,http://link.springer.com/10.1007/978-3-030-05204-1_26,"If robots are to occupy a space in the human social sphere, then the importance of trust naturally extends to human-robot interactions. Past research has examined human-robot interaction from a number of perspectives, ranging from overtrust in human robot interactions to trust repair. Studies by [15] have suggested a relationship between the success of a trust repair method and the time at which it is employed. Additionally, studies have shown a potentially dangerous tendency in humans to trust robotic systems beyond their operational capacity. It therefore becomes essential to explore the factors that affect trust in greater depth. The study presented in this paper is aimed at building upon previous work to gain insight into the reasons behind the success of trust repair methods and their relation to timing. Our results show that the delayed trust repair is more effective than the early case, which is consistent with the previous results. In the absence of an emergency, the participant’s decision were similar to those of a random selection. Additionally, there seem to be a strong inﬂuence of attention on the participants’ decision to follow the robot."
"Nees, Michael A.",Drivers’ Perceptions of Functionality Implied by Terms Used to Describe Automation in Vehicles,2018,1,301,237,64,"5 participants were excluded for finishing the experiment in less than 125 s, 41 participants were excluded for failing the first attention check, 8 participants were excluded for failing the second attention check, 5 participants were excluded for self-reporting they had no driver's license, 5 participants were excluded for meeting more than one of these criteria",Online Crowdsourcing,within-subjects,"Participants were presented with 13 different terms used to describe vehicle automation and rated the extent to which the human driver or the automated system was responsible for steering, accelerating/braking, and monitoring the driving scenario.","Participants rated the perceived responsibility of the human driver or the automated system for steering, accelerating/braking, and monitoring the driving scenario for 13 different automation terms.",Unspecified,Autonomous Vehicles,Other: Vehicle automation,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read descriptions of automation terms.,media,The interaction was based on text descriptions of automation terms.,hypothetical,The robots were only described hypothetically.,not autonomous,The robot's actions were only described hypothetically.,Questionnaires,,,Trust was assessed using a questionnaire.,no modeling,No computational model of trust was used.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study assessed the perceptions of different automation terms without manipulating any specific factors.,"The study did not directly measure trust, but it explored how different terms used to describe automation influence perceptions of responsibility, which can affect trust.","The study found that the perceived level of automation implied by different terms often did not match their technical definitions. For example, 'Autopilot' was perceived to imply a higher level of automation than its actual capabilities.","The study revealed that the terms used to describe vehicle automation can create misperceptions about the system's capabilities, which may influence trust before drivers even use the system.","The human participant read descriptions of different automation terms and rated the perceived responsibility of the human driver or the automated system for steering, accelerating/braking, and monitoring the driving scenario. The robot did not perform any actions.",ANOVA; post hoc comparisons,"A 3 (DDT) X 13 (automation term) repeated measures ANOVA was used to examine ratings of responsibility. Sphericity was violated, so Greenhouse-Geisser corrections were used. Significant main effects of DDT and automation term, as well as a significant interaction, were found. Bonferroni adjustments were used to control the Type I error rate for all follow-up tests. Post hoc comparisons were conducted to examine the significant effect of DDT and to examine differences in ratings for automation terms holding DDTs constant.",FALSE,,,,"The study did not manipulate any factors. Participants were presented with 13 different terms used to describe vehicle automation and rated the extent to which the human driver or the automated system was responsible for steering, accelerating/braking, and monitoring the driving scenario. The study aimed to understand the perceptions evoked by these terms, not to manipulate any specific variable to observe its impact on trust. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust as a result of manipulation.",10.1177/1541931218621430,http://journals.sagepub.com/doi/10.1177/1541931218621430,"The expectations induced by the labels used to describe vehicle automation are important to understand, because research has shown that expectations can affect trust in automation even before a person uses the system for the first time. An online sample of drivers rated the perceived division of driving responsibilities implied by common terms used to describe automation. Ratings of 13 terms were made on a scale from 1 (“human driver is entirely responsible”) to 7 (“vehicle is entirely responsible”) for three driving tasks (steering, accelerating/braking, and monitoring). In several instances, the functionality implied by automation terms did not match the technical definitions of the terms and/or the actual capabilities of the automated vehicle functions currently described by the terms. These exploratory findings may spur and guide future research on this under-examined topic."
"Nenna, Federica; Zanardi, Davide; Maria Orlando, Egle; Mingardi, Michele; Buodo, Giulia; Gamberini, Luciano",Addressing Trust and Negative Attitudes Toward Robots in Human-Robot Collaborative Scenarios: Insights from the Industrial Work Setting,2024,1,17,17,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed questionnaires about demographics and attitudes, were trained on safe and unsafe interactions with a cobot, performed a collaborative assembly task with varying mental load levels, and then rated their trust towards the robot.","Participants performed a collaborative assembly task with a cobot, assembling a metal plate with various components.",UR10e,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically collaborated with the robot during the assembly task.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,Participants interacted with a physical industrial robot arm.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions during the assembly task.,Questionnaires; Custom Scales,Negative Attitude towards Robots Scale (NARS),,"Trust was assessed using a custom questionnaire that evaluated the robot's movement speed, safe cooperation, and reliability.",no modeling,"The study did not use computational modeling of trust, only statistical analysis.",Empirical HRI Studies,Physical Robot Studies,No Manipulation,"The study did not manipulate any specific factors related to trust, but measured trust after the interaction.","The study found that overall dispositional NARS was not significantly related to trust, but gender-dependent dynamics were observed.","The study found that men with stronger NARS showed enhanced trust, particularly in robot motion speed perceptions, while women exhibited stronger correlations between emotional attitudes and trust. There were also unexpected negative correlations between the NARS sub-scale 'Emotions in Interaction with Cobot' and various functional aspects of the robot.","The main finding was that overall dispositional NARS was not directly related to the level of trust built during close collaboration with an industrial robot, but gender-dependent dynamics were observed.","The robot performed pick-and-place actions, while the human performed manual screwing, manual composition, and used an industrial screwdriver to assemble a metal plate with various components.",Spearman correlation,"The study used Spearman rank correlation tests to explore relationships between: a) the NARS questionnaire and the trust toward industrial HRC questionnaire scores; b) each of the NARS questionnaire dimensions (Situations of interaction, Social influence, and Emotions in interaction) and the overall trust score; c) each NARS dimension and the single dimensions of the trust questionnaire (Robot's motion speed, Safe co-operation, Robot and gripper reliability); d) the same relations independently computed for women and men.",FALSE,Task-complexity,,,"The study manipulated task complexity by having participants perform the assembly task under varying mental load levels (single-task and dual-task with mental arithmetic). This is described in the 'Task, procedure and measures' section: 'All participants repeated the assembly task under varying mental load levels (i.e., single-task, and dual-task, created via concurrent execution of mental arithmetic operations). The latter aimed at simulating the working conditions of factory operators, who often operate under two degrees of mental fatigue [5].' Although the study did not explicitly manipulate factors to directly impact trust, the task complexity was varied, which is a manipulation. The study did not find that this manipulation impacted trust, as the main findings were related to the relationship between NARS and trust, and gender differences. Therefore, no factors impacted or did not impact trust.",10.1145/3652037.3663905,https://dl.acm.org/doi/10.1145/3652037.3663905,"Recent advancements in Human-Robot Collaboration (HRC) have brought to light the significance of ethical, psychological, and attitudinal factors in advanced work and industrial settings, whereby collaborative robots assist humans in work tasks. In these environments, individual factors, attitudes, and trust beliefs of human workers towards robots have a direct impact on the perceived efficiency and safety of HRCs, contributing to worker well-being in the workplace. However, most of the existing research on these topics has been concentrated on social robots and much less on industrial ones. This study aims to fill this gap by exploring the relationships between Negative Attitudes toward robots (NARS) and Trust in industrial HRC. Results demonstrated how, while the overall correlation between NARS and Trust was non-significant, unexpected trends also arose. Gender-dependent dynamics added complexity, with women exhibiting stronger correlations between emotional attitudes and trust. Men, on the other hand, demonstrated a link between stronger NARS and enhanced trust, particularly in robot motion speed perceptions. These intricate findings emphasize the need for tailored design considerations in cobot development, acknowledging the nuanced interplay between dispositional attitudes and trust in shaping human perceptions of robotic technologies in practical scenarios."
"Nesset, Birthe; Robb, David A.; Lopes, José; Hastie, Helen",Transparency in HRI: Trust and Decision Making in the Face of Robot Errors,2021,1,61,58,3,3 sets of responses were rejected as they failed an attention check,Online Crowdsourcing,within-subjects,"Participants watched four video vignettes of a robot performing COVID-19 triage, each with different levels of transparency and error. After each video, they completed a trust survey and a question about their likelihood to seek a second opinion.",Participants watched videos of a robot performing a COVID-19 triage and then answered questions about their trust in the robot and their likelihood to seek a second opinion.,Pepper,Humanoid Robots; Expressive Robots,Care; Social; Research,Social,Social Guidance/Coaching,passive observation,Participants watched videos of the robot interaction.,media,Participants watched video vignettes of the robot interaction.,physical,The robot was physically present in the video.,pre-programmed (non-adaptive),The robot followed a pre-programmed script.,Questionnaires,Jian et al. Trust Scale,,Trust was measured using a questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the robot's transparency level and whether it made an error, which influenced the robot's behavior and performance.","High transparency increased trust when the robot made no error, but transparency had no significant effect on trust when the robot made an error. Participants were more likely to seek a second opinion when the robot made an error.","The study found that high transparency increased trust when the robot made no error, but this effect was not significant when the robot made an error. Participants were more likely to seek a second opinion when the robot made an error, especially with high transparency.",High transparency in a robotic diagnostic system leads to better calibration of trust and more appropriate health decisions when the robot makes no error.,"The robot conducted a COVID-19 triage consultation, asking questions about symptoms and personal details. The human participant watched the video and answered questions about trust and their likelihood to seek a second opinion.",two-way repeated measures anova; friedman's two-way analysis of variance by ranks,"The study used Two-Way Repeated Measures ANOVA to analyze the trust scale data, which was treated as continuous data. Friedman's Two-Way Analysis of Variance by Ranks, a non-parametric test, was used to analyze the Likert scale data on the likelihood to request a second opinion, which was treated as ordinal data. These tests were used to examine the effects of transparency and error on trust and decision-making.",TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-verbal-communication-content; Robot-accuracy,,"The study manipulated two factors: the level of transparency in the robot's communication and whether the robot made an error. The transparency manipulation was achieved by varying the content of the robot's verbal communication, specifically whether it provided explanations for its actions or not. This falls under 'Robot-verbal-communication-content'. The error manipulation was achieved by having the robot make an incorrect diagnosis in some conditions, which directly impacts the robot's task performance and thus falls under 'Robot-accuracy'. Both of these manipulations were found to impact trust. The paper states, 'Our findings consolidate prior work showing a main effect of robot errors on trust, but also showing that this is dependent on the level of transparency.' and 'High transparency (HT) interactions had significantly higher trust when compared with low transparency (LT) when no error was made by the robot.' This indicates that both the transparency (communication content) and the presence of errors (accuracy) influenced trust levels. There were no factors that were manipulated that did not impact trust.",10.1145/3434074.3447183,https://dl.acm.org/doi/10.1145/3434074.3447183,"Robots are rapidly gaining acceptance in recent times, where the general public, industry and researchers are starting to understand the utility of robots, for example for delivery to homes or in hospitals. However, it is key to understand how to instil the appropriate amount of trust in the user. One aspect of a trustworthy system is its ability to explain actions and be transparent, especially in the face of potentially serious errors. Here, we study the various aspects of transparency of interaction and its effect in a scenario where a robot is performing triage when a suspected Covid-19 patient arrives at a hospital. Our findings consolidate prior work showing a main effect of robot errors on trust, but also showing that this is dependent on the level of transparency. Furthermore, our findings indicate that high interaction transparency leads to participants making better informed decisions on their health based on their interaction. Such findings on transparency could inform interaction design and thus lead to greater adoption of robots in key areas, such as health and well-being."
"Nesset, Birthe; Rajendran, Gnanathusharan; Lopes, Jose David Aguas; Hastie, Helen",Sensitivity of Trust Scales in the Face of Errors,2022,1,100,85,15,"6 participants were excluded due to failure of one or multiple attention checks, 9 participants were excluded due to not identifying the error",Online Crowdsourcing,within-subjects,"Participants watched two videos of a robot interaction, one with an error and one without, and completed trust questionnaires after each video.",Participants watched videos of a robot performing a COVID-19 triage task and evaluated their trust in the robot.,Pepper,Humanoid Robots; Expressive Robots,Social; Research,Social,Conversation,passive observation,Participants passively observed the robot interaction through pre-recorded videos.,media,The interaction was presented through pre-recorded videos.,physical,The robot was physically present in the video recordings.,pre-programmed (non-adaptive),The robot's actions were pre-scripted and did not adapt to the user.,Questionnaires; Multidimensional Measures,Trust in Automation Scale (TAS); Multi-Dimensional Measure of Trust (MDMT); Negative Attitude towards Robots Scale (NARS); Propensity to Trust Scales,,Trust was measured using questionnaires and a single item assessment.,no modeling,The study did not use any computational models of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's performance was manipulated by having it make an error in one of the videos.,Trust decreased significantly after the robot made an error.,"The study found that a single item assessment of trust was as sensitive to changes in trust as longer, validated questionnaires. There was an inconsistency in the results when comparing age as a variable, where the TAS scale did not report a significant difference, while the MDMT and single item assessment did.","Trust measures are equivalent in terms of sensitivity to changes in trust, and a single item assessment can be used to measure trust in scenarios with distinct breaks in trust.","The robot asked the actor questions related to COVID-19 symptoms and provided a diagnosis, either correctly or incorrectly. The human participant watched the video and completed questionnaires.",Pearson correlation; Wilcoxon signed-rank test; Mann-Whitney U,"The study used Pearson's correlation tests to determine the correlation between the three trust measures (TAS, MDMT, and single item assessment) before and after the robot error. It also used Pearson's correlation to compare the single item assessment with item 11 of the TAS scale. Wilcoxon signed-rank tests were used to compare trust scores before and after the error for each measure, assessing the sensitivity of each measure to changes in trust. Finally, Mann-Whitney U tests were used to compare participant groups based on age, gender, propensity to trust, negative attitudes towards robots, and personal assessment of COVID impact, to see if the trust measures would report the same findings across these groups.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the robot's accuracy by having it make an error in one of the videos. In the first video, the robot correctly recommended that the actor get tested for COVID-19, while in the second video, the robot made an error and sent the sick actor home. This manipulation of the robot's performance directly impacted the participants' trust, as evidenced by the significant decrease in trust scores after the error. The paper states, 'In the first interaction, the robot gives the correct evaluation and argues that the actor should get tested, while in the second interaction, the robot makes an error and sends the actor home, with no instruction to test or self isolate themselves.' This clearly indicates a manipulation of the robot's accuracy. The results section also confirms that the trust measures captured the drop in trust after the error, indicating that the manipulation of robot accuracy impacted trust.",10.1109/HRI53351.2022.9889427,https://ieeexplore.ieee.org/document/9889427/,"Trust between humans and robots is a complex, multifaceted phenomenon and measuring it subjectively and reliably is challenging. It is also context dependent and so choosing the right tool for a speciﬁc study can prove difﬁcult. This paper aims to evaluate various trust measures and compare them in terms of sensitivity to changes in trust. This is done by comparing two validated trust questionnaires (TAS and MDMT) and one single item assessment in a COVID-19 triage scenario. We found that trust measures are equivalent in terms of sensitivity to changes in trust. Furthermore, the study showed that trust could be measured similarly through a single item assessment in comparison with other lengthier scales, in scenarios with distinct breaks in trust. This ﬁnding would be of use for experiments where lengthy questionnaires are not appropriate, such as those in the wild."
"Neubauer, Catherine",HAT3: The Human Autonomy Team Trust Toolkit,2023,1,7,7,0,No participants were excluded,Controlled Lab Environment,,"Data was collected from a vehicle crew of seven members during a simulation experiment. Each crew station is labeled (cs01-cs07), and each crew member performed a specific role (e.g., Commander, Gunner, or Driver).",Participants performed specific roles within a vehicle crew during a simulation.,Unspecified,Other: Vehicle crew simulation,Research,Supervision,Monitoring,minimal interaction,Participants interacted in a simulation environment.,simulation,The interaction took place in a simulation environment.,simulated,The robot was represented in a simulation.,not autonomous,The robot's actions were part of a simulation.,Questionnaires; Behavioral Measures,Propensity to Trust Scales,Speech Data,Trust was measured using questionnaires and communication analysis.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,The study did not manipulate any specific factors related to trust.,,The study focused on communication patterns as indicators of trust and cohesion.,Communication patterns can be used to infer team dynamics of trust and cohesion.,"The robot was part of a vehicle crew simulation, and the human participants performed specific roles within the crew, monitoring the simulation and communicating with each other.",,"No statistical tests were explicitly mentioned in the paper. The study focused on developing and demonstrating the capabilities of the HAT3 toolkit, which includes subjective measures, communication analysis, and visualizations. The communication analysis involved metrics like message counts, network flow, and semantic content analysis using tools like empath, but no specific statistical tests were described.",FALSE,,,,"The paper describes the development and functionality of the HAT3 toolkit, which measures trust using subjective and communication-based measures. The study did not manipulate any factors related to trust. The communication analysis was based on data collected from a vehicle crew simulation, but this was not a manipulation, rather a demonstration of the toolkit's capabilities. The paper focuses on the toolkit's ability to measure trust through various metrics, not on manipulating factors to observe their impact on trust. Therefore, no factors were manipulated in this study.",10.1145/3610661.3620660,https://dl.acm.org/doi/10.1145/3610661.3620660,"Advances in artifcial intelligence capabilities in autonomy-enabled systems and robotics have pushed research to address the unique nature of human-autonomy team collaboration. The goals of these advanced technologies are to enable rapid decision making, enhance situation awareness, promote shared understanding, and improve team dynamics. Simultaneously, use of these technologies is expected to reduce risk to those who collaborate with these systems. Yet, for appropriate human- autonomy teaming to take place, especially as we move beyond dyadic partnerships, proper calibration of team trust is needed to efectively coordinate interactions during high-risk operations. To meet this end, multimodal measures of team trust for this new dynamic of human-autonomy teams are needed. This paper provides an overview of the purpose, components, and functionality of the Human-Autonomy Teaming Trust Toolkit (HAT3), which is a multimodal measurement tool for real, and near-real time measures of trust. More specifcally, HAT3 is a modular software toolkit capable of measuring and visualizing individual and team-level trust via subjective measures, multiple communication tools, and a forthcoming physiological module."
"Newaz, Fahd; Saplacan, Diana",Exploring the role of feedback on trust for the robots used in homes of the elderly,2018,1,11,11,0,No participants were excluded,Real-World Environment,,"Participants used commercially available robot vacuum cleaners in their homes, and data was collected through observations and interviews.",Participants used a robot vacuum cleaner in their homes.,iRobot Roomba,Mobile Robots,Other,Supervision,Monitoring,minimal interaction,Participants observed and interacted with the robot vacuum cleaner in their homes.,real-world,Participants interacted with the robot in a real-world setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot vacuum cleaner operated autonomously based on pre-programmed settings.,Questionnaires; Behavioral Measures,,Video Data; Speech Data,"Trust was assessed through observations, interviews, and user feedback.",no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Indirect Manipulation,The study indirectly manipulated trust by observing the impact of the robot's performance and feedback on user trust.,"Lack of control and improper feedback from the robot decreased trust, especially among elderly participants.","Elderly participants generally expressed lower levels of trust compared to younger participants, which correlated with their previous exposure to technology.","The study found that the design of feedback mechanisms significantly influences the level of trust users have towards domestic robots, especially for elderly users.","The robot vacuum cleaner autonomously cleaned the floor, while the human participant observed and interacted with the robot, and provided feedback on their experience.",,"The study is qualitative and exploratory, using observation and interviews. No statistical tests were used.",FALSE,Robot-autonomy; Robot-verbal-communication-content,Robot-autonomy; Robot-verbal-communication-content,,"The study did not explicitly manipulate factors, but the design of the study implicitly influenced trust. The robot's autonomy was a key factor, as participants expressed a lack of control due to the robot's autonomous operation ('It behaves as it wants to'). This lack of control was not directly manipulated by the researchers, but was a consequence of the robot's design and operation. The robot's verbal communication content, specifically the error messages ('Error 6'), also impacted trust because the messages were not understandable to the users, leading to insecurity and doubt. The study did not manipulate the content of the error messages, but the lack of understandable feedback was a factor that influenced trust. Therefore, 'Robot-autonomy' and 'Robot-verbal-communication-content' are the most appropriate categories to describe the factors that influenced trust in this study. The study did not explicitly manipulate any other factors from the list.",10.1145/3240167.3240248,https://dl.acm.org/doi/10.1145/3240167.3240248,"This paper explores the role of design and use of digital feedback mechanisms in domestic autonomous devices. We explore this in relation to elderly’s trust towards robots. Specifically, the paper reflects on a case study conducted with various user groups using semiautonomous vacuum cleaning robots. We have included both young and old participants in the study to expand our understanding of the potential challenges encountered. The methods employed were observation and interviews. The data shows that depending on the information users are provided about the actions and functionality of a given device, their degree of trust towards the device can be influenced. Similarly, there is a relation between the degree of trust toward the technology and the users’ willingness to use it. Bridging the gap between the degree of trust towards a technology, through feedback, can play a critical role in encouraging people to use and retain any given technology."
"Nie, Jiaqi; Pak, Michelle; Marin, Angie Lorena; Sundar, S. Shyam",Can you hold my hand?: physical warmth in human-robot interaction,2012,1,39,39,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants completed a pre-questionnaire, were randomly assigned to one of three conditions (warm touch, cold touch, no touch), watched a horror film clip while holding or not holding the robot's hand, and then completed a post-questionnaire.","Participants watched a horror film clip while either holding a warm robot hand, a cold robot hand, or not holding a robot hand.",RoboSapien RS-Media,Humanoid Robots,Research,Social,Social Perception,direct-contact interaction,Participants had direct physical contact with the robot by holding its hand.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,wizard of oz (directly controlled),"The robot was remote-controlled by the researcher, but participants believed it was autonomous.",Questionnaires,,,Trust was measured using a post-questionnaire with multiple items.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The temperature of the robot's hand was manipulated to be warm, cold, or no touch, influencing the perceived warmth and trust towards the robot.",Warm touch significantly increased perceived friendship and trust compared to cold and no touch conditions.,"The study found that warm touch increased both trust and fright towards the robot, suggesting a potential uncanny valley effect.",Physical warmth during handholding with a robot increases feelings of friendship and trust.,The robot greeted participants and then remained still while participants watched a horror film clip. Participants either held the robot's hand (warm or cold) or did not touch the robot.,ANOVA; baron and kenny's method,"An ANOVA was used to analyze the effects of 'human touch' (warm, cold, no touch) on perceived friendship, trust, and human likeness of the robot. Specifically, it compared the means of these variables across the three conditions. Baron and Kenny's method was used to test the mediation effect of 'human likeness' on perceived friendship and trust toward the robot.",TRUE,Robot-social-attitude,Robot-social-attitude,,"The study manipulated the temperature of the robot's hand (warm, cold, or no touch) to influence the perceived warmth and, consequently, the social attitude of the robot. This manipulation directly relates to the robot's perceived friendliness and warmth, which falls under the 'Robot-social-attitude' category. The results showed that the warm touch condition significantly increased perceived friendship and trust, indicating that 'Robot-social-attitude' impacted trust. The study did not find any factors that did not impact trust.",10.1145/2157689.2157755,http://dl.acm.org/citation.cfm?doid=2157689.2157755,"This study investigates whether the temperature of a robot’s hand can affect perceptions of the robot as a companion. Our research empirically analyzes the responses of 39 individuals randomly assigned to one of three conditions: (1) holding a warm robot hand or (2) holding a cold robot hand or (3) not holding a robot hand. The effects of this simulated ‘human touch’ on HRI were examined in the context of viewing a horror film clip. Results suggest that experiences of physical warmth and handholding increase feelings of friendship and trust toward the robot. However, the discrepancy between the expectation of an actual human touch and the mechanical appearance of a robot could result in negative effects."
"Nielsen, Christina; Mathiesen, Mia; Nielsen, Jacob; Jensen, Lars Christian",Changes in Heart Rate and Feeling of Safety When Led by a Rehabilitation Robot,2019,1,30,30,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants' arms were led by a rehabilitation robot in two different exercises at three different velocities. Heart rate was measured during each exercise, and participants rated their feeling of safety after each exercise.",Participants had their arm led by a robot in two different exercises at three different velocities.,UR5,Industrial Robot Arms,Care; Research,Manipulation,Path Following,direct-contact interaction,Participants physically interacted with the robot by holding its handle while their arm was led through exercises.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The study used a physical robot for the interaction.,pre-programmed (non-adaptive),The robot followed pre-programmed paths without adapting to the user.,Physiological Measures; Questionnaires,,Physiological Signals,Trust was assessed using heart rate measurements and a post-exercise safety rating questionnaire.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The researchers manipulated the velocity and length of the robot's movement to influence the perceived safety of the interaction.,Increasing velocity and longer exercises negatively influenced participants' perception of safety.,"The study found that while velocity and type of exercise did not significantly influence heart rate, they did significantly influence the feeling of safety. Participants felt less safe with higher velocities and longer exercises.",Increasing velocity and longer exercises negatively influence participants' perception of safety when interacting with a rehabilitation robot.,The robot led the participant's arm through two different exercise paths at three different velocities. The participant held the robot's handle and rated their feeling of safety after each exercise.,ANOVA; Bonferroni correction,"Two 2-way ANOVAs were used to analyze the data, with velocity and type of exercise as factors, and heart rate and feeling of safety as dependent variables. Post-hoc testing was done with a Bonferroni test to further examine significant differences in feeling of safety based on velocity and type of exercise.",TRUE,Robot-task-strategy; Task-constraints,Robot-task-strategy,Task-constraints,"The study manipulated the robot's movement by varying the length of the exercise path (Exercise 1 ≈0.36 m, Exercise 2 ≈0.66 m) and the velocity (slow ≈ 0.05 m/s, medium ≈ 0.19 m/s, high ≈ 0.37 m/s). The change in the robot's path length and velocity is classified as 'Robot-task-strategy' because it directly alters how the robot completes the task without changing the task's success rate. The study also manipulated the task constraints by changing the velocity of the robot's movement, which can be seen as a form of time pressure, and is therefore classified as 'Task-constraints'. The results showed that the robot's task strategy (specifically the length of the exercise path and the velocity) significantly influenced the participants' feeling of safety, as participants felt less safe with longer exercises and higher velocities. However, the task constraints (velocity) did not significantly influence the participants' heart rate, indicating that while the velocity was manipulated, it did not impact trust as measured by heart rate. Therefore, 'Robot-task-strategy' is listed as a factor that impacted trust, and 'Task-constraints' is listed as a factor that did not impact trust.",10.1109/HRI.2019.8673165,https://ieeexplore.ieee.org/document/8673165/,"Trust is an important topic in medical human-robot interaction, since patients may be more fragile than other groups of people. This paper investigates the issue of users’ trust when interacting with a rehabilitation robot. In the study, we investigate participants’ heart rate and perception of safety in a scenario when their arm is led by the rehabilitation robot in two types of exercises at three different velocities. The participants’ heart rate are measured during each exercise and the participants are asked how safe they feel after each exercise. The results showed that velocity and type of exercise has no signiﬁcant inﬂuence on the participants’ heart rate, but they do have signiﬁcant inﬂuence on how safe they feel. We found that increasing velocity and longer exercises negatively inﬂuence participants’ perception of safety."
"Nikolaidis, Stefanos; Shah, Julie","Human-robot cross-training: Computational formulation, modeling and evaluation of a human team training strategy",2013,1,36,36,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were randomly assigned to either a cross-training group or a reinforcement learning group. Both groups trained with a virtual robot in a place-and-drill task. After training, participants performed the task with a real robot, and completed a post-experiment survey.",Participants trained with a virtual robot and then performed a place-and-drill task with a real robot.,Unspecified,Industrial Robot Arms,Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically interacted with the robot during the task execution phase.,real-world,The study involved real-world interaction with a physical robot.,physical,The study used a physical industrial robot.,shared control (adaptive),The robot adapted its behavior based on the training interaction with the human.,Questionnaires,,Performance Metrics,Trust was assessed using a post-experiment survey and performance metrics.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the training method (cross-training vs. reinforcement learning) and the role of the participant during training, which influenced the robot's behavior and the participant's perception of the task.",Cross-training led to higher perceived robot performance and increased trust compared to reinforcement learning.,The entropy rate was found to be a potential metric for detecting changes in human behavior or mistakes by the operator. The cross-training session lasted slightly longer than the reinforcement learning session.,"Human-robot cross-training improves quantitative measures of human-robot mental model convergence and similarity, and increases trust in the robot.","The robot drills screws, and the human places screws. During training, the human and robot switch roles in the cross-training condition. In the reinforcement learning condition, the human provides rewards after each robot action.",Mann-Whitney U,"The study used a two-tailed Mann-Whitney-Wilcoxon test to compare the agreement levels of participants in the cross-training group versus the reinforcement learning group regarding the robot's performance and their trust in the robot. The test was also used to compare the entropy rate after the last training round between those that kept their preference throughout the training and those that switched preferences. Additionally, the study used p-values to compare the means of different groups for metrics such as mental model convergence, mental model similarity, concurrent motion, human idle time, robot idle time, and human-robot distance.",TRUE,Robot-adaptability; Robot-autonomy,Robot-adaptability,,"The study manipulated the training method, comparing cross-training (where the robot adapts to the human's preferences by switching roles) with reinforcement learning (where the human provides rewards). This directly influences the robot's adaptability, as the robot learns from the human's actions in the cross-training condition, and the robot's autonomy, as the robot's behavior is influenced by the training method. The cross-training method, which allowed the robot to adapt to the human's preferences, was found to significantly impact trust, as participants in the cross-training group reported higher trust in the robot. The study did not find any factors that did not impact trust.",10.1109/HRI.2013.6483499,http://ieeexplore.ieee.org/document/6483499/,"We design and evaluate human-robot cross-training, a strategy widely used and validated for effective human team training. Cross-training is an interactive planning method in which a human and a robot iteratively switch roles to learn a shared plan for a collaborative task."
"Nikolaidis, Stefanos; Lasota, Przemyslaw; Ramakrishnan, Ramya; Shah, Julie","Improved human–robot team performance through cross-training, an approach inspired by human team training practices",2015,1,36,36,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were randomly assigned to either a cross-training group or a reinforcement learning group. Both groups trained in a virtual environment, then performed the task with a real robot. The cross-training group switched roles with the robot during training, while the reinforcement learning group provided rewards after each robot action. After training, participants completed a mental model assessment and then performed the task with a real robot. Finally, participants completed a post-experiment survey.","Participants and a robot performed a place-and-drill task, where they placed screws into holes and the robot drilled them.",YuMi; Abbie,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically interacted with the robot during the task execution phase.,real-world,Participants trained in a virtual environment and then interacted with a real robot.,physical,Participants interacted with a physical robot during the task execution phase.,shared control (adaptive),The robot adapted its behavior based on the human's actions during the cross-training phase.,Questionnaires; Behavioral Measures; Performance-Based Measures,,Performance Metrics,"Trust was assessed using a post-experiment survey, behavioral measures such as concurrent motion and idle time, and performance metrics.",no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Mixed Reality Studies,Direct Manipulation,"The study manipulated the training method (cross-training vs. reinforcement learning), which influenced the robot's behavior, the framing of the task, the participant's role, and the feedback provided to the robot.",Cross-training led to higher trust ratings and better team fluency compared to reinforcement learning.,"The study found that cross-training improved mental model similarity, robot mental model convergence, and team fluency metrics. The entropy rate was sensitive to changes in human strategy, suggesting its potential use for dynamic error detection. Participants in the reinforcement learning group tended to give more neutral rewards, which slowed down the learning process.","Cross-training, inspired by human team training practices, significantly improved human-robot team performance, trust, and fluency compared to standard reinforcement learning techniques.","The robot drilled screws into pre-placed holes, while the human placed the screws. During cross-training, the human also drilled screws in the rotation phase. In the reinforcement learning condition, the human provided rewards after each robot action.",t-test; Mann-Whitney U,"The study used two-tailed, unpaired t-tests with unequal variance to compare quantitative measures of human-robot mental model convergence and similarity, as well as team fluency metrics (concurrent motion and human idle time) between the cross-training and reinforcement learning groups. Two-tailed Mann-Whitney-Wilcoxon tests were used to compare subjective measures of perceived robot performance and trust in the robot between the two groups.",TRUE,Robot-adaptability; Robot-task-strategy,Robot-adaptability,Robot-task-strategy,"The study manipulated the training method, comparing cross-training to reinforcement learning. This directly impacts 'Robot-adaptability' because cross-training allows the robot to learn the human's preferences by switching roles, while reinforcement learning relies on human-provided rewards. The robot's task strategy was also implicitly manipulated, as the robot's actions (drilling sequence) were influenced by the training method, which affected the robot's task completion strategy. The study found that cross-training, which allowed the robot to adapt to the human's preferences, led to higher trust ratings. The second experiment, where the learning component was removed, showed no significant difference in trust, indicating that the manipulation of 'Robot-task-strategy' alone did not impact trust. The key factor impacting trust was the robot's ability to adapt to the human's preferences, which is captured by 'Robot-adaptability'. The robot's task strategy was manipulated by the training method, but this manipulation did not impact trust when the learning component was removed.",10.1177/0278364915609673,http://journals.sagepub.com/doi/10.1177/0278364915609673,"We design and evaluate a method of human–robot cross-training, a validated and widely used strategy for the effective training of human teams. Cross-training is an interactive planning method in which team members iteratively switch roles with one another to learn a shared plan for the performance of a collaborative task."
"Nikolaidis, Stefanos; Kuznetsov, Anton; Hsu, David; Srinivasa, Siddhartha",Formalizing human-robot mutual adaptation: A bounded memory model,2016,1,69,69,0,"Participants with initial preference for the robot goal, participants who failed the control question, and participants who gave inconsistent answers on the Likert scale were excluded.",Online Crowdsourcing,between-subjects,"Participants were instructed on a table-carrying task and chose a preferred goal configuration. They then interacted with a simulated robot in one of three conditions: Fixed, Mutual-adaptation, or Cross-training. Participants completed a post-experimental questionnaire.","Participants collaborated with a simulated robot to carry a table to a goal configuration, choosing rotation actions via a user interface.",Unspecified,Mobile Robots,Research,Manipulation,Object Passing,minimal interaction,"Participants interacted with a simulated robot through a user interface, with limited direct interaction.",simulation,"The interaction was conducted in a simulated environment, without physical embodiment of the robot.",simulated,The robot was represented as a simulated entity within the task environment.,shared control (adaptive),"The robot adapted its behavior based on the human's actions, using a model of human adaptability.",Questionnaires,,Performance Metrics,Trust was assessed using a post-experimental questionnaire and performance metrics.,POMDP,The robot used a Partially Observable Markov Decision Process (POMDP) to model human adaptability and guide its actions.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's behavior was manipulated by using different policies (fixed, mutual-adaptation, cross-training), influencing how the robot responded to the human's actions and the task was framed as a collaborative task.",The robot using the mutual-adaptation policy was found to be more trustworthy than the robot using a fixed policy. The mutual-adaptation policy also led to more participants adapting to the robot's strategy compared to the cross-training condition.,"Participants in the Fixed condition showed a positive correlation between inferred adaptability and robot trustworthiness, while no such correlation was found in the Mutual-adaptation condition. This suggests that the MOMDP formulation helped maintain trust even with non-adaptive participants.","The proposed formalism for human-robot mutual adaptation, using a bounded memory model and a MOMDP, led to more efficient policies and higher trust compared to a fixed robot policy, and more adaptation compared to cross-training.","The robot and human collaborated to rotate a table to a goal configuration. The human chose rotation actions via a user interface, and the robot either followed a fixed policy, adapted to the human, or used a cross-training approach. The robot's actions were displayed via a video.",Mann-Whitney U; Pearson correlation; Chi-squared; t-test,"The study used a two-tailed Mann-Whitney-Wilcoxon test to compare the trustworthiness ratings between the Mutual-adaptation and Fixed conditions. Pearson's r was used to assess the correlation between inferred adaptability and robot trustworthiness within the Fixed and Mutual-adaptation conditions. A Pearson's chi-square test was used to compare the proportion of participants who adapted to the robot's strategy in the Mutual-adaptation and Cross-training conditions. Finally, a one-tailed unpaired t-test was used to assess the non-inferiority of the Mutual-adaptation condition compared to the Cross-training condition on several measures of robot performance and collaboration.",TRUE,Robot-task-strategy; Robot-adaptability,Robot-task-strategy; Robot-adaptability,,"The study manipulated the robot's task strategy by having three conditions: Fixed, Mutual-adaptation, and Cross-training. In the Fixed condition, the robot always moved towards the optimal goal, ignoring human input. In the Mutual-adaptation condition, the robot used a MOMDP to model human adaptability and adjust its strategy accordingly. In the Cross-training condition, the robot learned the human's preference through a training phase. This manipulation of the robot's task strategy directly influenced how the robot behaved during the table-carrying task, which is a manipulation of 'Robot-task-strategy'. The robot's adaptability was also manipulated, as the Mutual-adaptation condition explicitly used a model of human adaptability to guide its actions, while the Fixed condition had no adaptability and the Cross-training condition adapted to the human's preference but did not model human adaptability. The results showed that the Mutual-adaptation condition, which used a model of human adaptability, led to higher trust compared to the Fixed condition, and more adaptation compared to the Cross-training condition. Therefore, both 'Robot-task-strategy' and 'Robot-adaptability' impacted trust. There were no factors that were manipulated that did not impact trust.",10.1109/HRI.2016.7451736,http://ieeexplore.ieee.org/document/7451736/,"Mutual adaptation is critical for effective team collaboration. This paper presents a formalism for humanrobot mutual adaptation in collaborative tasks. We propose the bounded-memory adaptation model (BAM), which captures human adaptive behaviors based on a bounded memory assumption. We integrate BAM into a partially observable stochastic model, which enables robot adaptation to the human. When the human is adaptive, the robot will guide the human towards a new, optimal collaborative strategy unknown to the human in advance. When the human is not willing to change their strategy, the robot adapts to the human in order to retain human trust. Human subject experiments indicate that the proposed formalism can signiﬁcantly improve the effectiveness of human-robot teams, while human subject ratings on the robot performance and trust are comparable to those achieved by cross training, a state-ofthe-art human-robot team training practice."
"Nikolaidis, Stefanos; Zhu, Yu Xiang; Hsu, David; Srinivasa, Siddhartha",Human-Robot Mutual Adaptation in Shared Autonomy,2017,1,51,51,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were trained on using a joystick to control a robot arm, then asked to choose a bottle to grab first, and then completed the task three times in one of three conditions: no-adaptation, mutual-adaptation, or one-way adaptation. Finally, they completed a post-experimental questionnaire and a video-taped interview.","Participants used a joystick to control a robotic arm to clear a table of two bottles, with one bottle being the optimal goal.",Unspecified,Industrial Robot Arms,Research,Manipulation,Cleaning,direct-contact interaction,Participants directly controlled the robot arm using a joystick to complete the task.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot arm.,shared control (adaptive),The robot adapted its behavior based on the user's actions and inferred adaptability.,Questionnaires; Custom Scales,,Video Data,"Trust was measured using questionnaires and custom scales, and video data was collected.",POMDP,A Partially Observable Markov Decision Process (POMDP) was used to model trust as a hidden state influencing both human behavior and robot policy.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's behavior was manipulated by varying its adaptation strategy (no-adaptation, mutual-adaptation, one-way adaptation), which influenced its performance, autonomy level, and goal alignment with the user, and also influenced human expectations.","Trust was higher in the mutual-adaptation condition compared to the no-adaptation condition, but not significantly different from the one-way adaptation condition. The robot's ability to adapt to the user's preference increased trust.","The mutual-adaptation condition balanced performance and trust, achieving better performance than the one-way adaptation condition and higher trust than the no-adaptation condition. There was no significant difference in trust between the mutual-adaptation and one-way adaptation conditions.","The proposed mutual adaptation formalism improved human-robot team performance while retaining a high level of human trust in the robot, compared to the robot following participants' preference or always moving towards the optimal goal.","The robot moved a robotic arm to clear a table of two bottles, and the human used a joystick to control the robot's movement, with the goal of picking up the bottles.",Kruskal-Wallis; Mann-Whitney U; ANOVA; tukey post-hoc tests,"The study used a Kruskal-Wallis H test to determine if there was a statistically significant difference in performance among the three experimental conditions (no-adaptation, mutual-adaptation, and one-way adaptation). Pairwise Mann-Whitney-Wilcoxon tests with Bonferroni corrections were then used to compare the performance between each pair of conditions. One-way ANOVAs were used to assess if there were statistically significant differences between the three conditions in both trust and perceived collaboration. Tukey post-hoc tests were used to perform pairwise comparisons between the conditions for trust and perceived collaboration.",TRUE,Robot-autonomy; Robot-adaptability,Robot-adaptability,Robot-autonomy,"The study manipulated the robot's behavior through different adaptation strategies, which directly influenced its autonomy and adaptability. The 'Robot-autonomy' was manipulated by varying the level of robot control and decision-making in the three conditions: no-adaptation (fixed policy), mutual-adaptation (MOMDP policy), and one-way adaptation (following user preference). The 'Robot-adaptability' was manipulated by varying the robot's ability to learn and adjust its behavior based on the user's actions and preferences. The mutual-adaptation condition allowed the robot to adapt to the user's adaptability, while the one-way adaptation condition had the robot always follow the user's preference. The no-adaptation condition had no adaptability. The results showed that 'Robot-adaptability' impacted trust, as the mutual-adaptation condition, where the robot adapted to the user's adaptability, resulted in higher trust compared to the no-adaptation condition. However, the level of autonomy itself did not significantly impact trust, as there was no significant difference in trust between the mutual-adaptation and one-way adaptation conditions, despite the different levels of autonomy in these conditions. The key factor influencing trust was the robot's ability to adapt to the user's behavior, not the level of autonomy itself.",10.1145/2909824.3020252,https://dl.acm.org/doi/10.1145/2909824.3020252,"Shared autonomy integrates user input with robot autonomy in order to control a robot and help the user to complete a task. Our work aims to improve the performance of such a human-robot team: the robot tries to guide the human towards an effective strategy, sometimes against the human’s own preference, while still retaining his trust. We achieve this through a principled human-robot mutual adaptation formalism. We integrate a bounded-memory adaptation model of the human into a partially observable stochastic decision model, which enables the robot to adapt to an adaptable human. When the human is adaptable, the robot guides the human towards a good strategy, maybe unknown to the human in advance. When the human is stubborn and not adaptable, the robot complies with the human’s preference in order to retain their trust. In the shared autonomy setting, unlike many other common human-robot collaboration settings, only the robot actions can change the physical state of the world, and the human and robot goals are not fully observable. We address these challenges and show in a human subject experiment that the proposed mutual adaptation formalism improves human-robot team performance, while retaining a high level of user trust in the robot, compared to the common approach of having the robot strictly following participants’ preference."
"Nikolaidis, Stefanos; Hsu, David; Srinivasa, Siddhartha",Human-robot mutual adaptation in collaborative tasks: Models and experiments,2017,3,69,69,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were instructed on a table-carrying task and asked to choose a preferred goal configuration. They then interacted with a robot in one of three conditions: Fixed, Mutual-adaptation, or Cross-training. Participants completed the task twice, and then answered a post-experimental questionnaire.",Participants collaborated with a simulated robot to carry a table to a goal configuration.,Unspecified,Mobile Manipulators,Research,Manipulation,Object Passing,minimal interaction,Participants interacted with the robot through a user interface.,media,Participants watched videos of the robot and table moving.,simulated,The robot was represented through video playback.,shared control (adaptive),The robot adapted its behavior based on the human's actions.,Questionnaires,,,Trust was measured using a post-experimental questionnaire.,POMDP,The robot's decision-making was based on a POMDP model that included a model of human adaptability.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's behavior was manipulated by using different policies (fixed, mutual-adaptation, cross-training), which influenced the robot's autonomy and the task difficulty for the human.",The mutual-adaptation condition led to higher trust ratings compared to the fixed condition. The robot's ability to adapt to the human's behavior increased trust.,"Participants in the Fixed condition showed a positive correlation between inferred adaptability and trust, while no such correlation was found in the Mutual-adaptation condition. Some participants in the Fixed condition had negative opinions about the robot's behavior.",A robot using a mutual adaptation strategy was perceived as more trustworthy than a robot using a fixed strategy.,"The robot and human collaborated to rotate a table to a goal configuration. The robot either followed a fixed policy, adapted to the human, or cross-trained with the human. The human chose rotation actions via a user interface.",Mann-Whitney U; Pearson correlation; Chi-squared,A two-tailed Mann-Whitney-Wilcoxon test was used to compare trust ratings between the Mutual-adaptation and Fixed conditions. Pearson's correlation was used to assess the relationship between inferred adaptability and trust within each condition. A Pearson's chi-square test was used to compare the proportion of participants who adapted to the robot's strategy in the Mutual-adaptation and Cross-training conditions.,TRUE,Robot-autonomy; Robot-adaptability,Robot-adaptability,,"The study manipulated the robot's behavior through three conditions: Fixed, Mutual-adaptation, and Cross-training. The 'Fixed' condition involved the robot using a fixed policy, which is a manipulation of 'Robot-autonomy' because the robot's decision-making was not influenced by the human. The 'Mutual-adaptation' condition involved the robot adapting its behavior based on the human's actions, which is a manipulation of 'Robot-adaptability'. The 'Cross-training' condition also involved the robot adapting to the human, but in a different way than the 'Mutual-adaptation' condition, which is also a manipulation of 'Robot-adaptability'. The study found that the 'Robot-adaptability' manipulation in the 'Mutual-adaptation' condition led to higher trust ratings compared to the 'Fixed' condition, indicating that 'Robot-adaptability' impacted trust. There was no mention of any of the other factors impacting trust.",10.1177/0278364917690593,http://journals.sagepub.com/doi/10.1177/0278364917690593,"Adaptation is critical for effective team collaboration. This paper introduces a computational formalism for mutual adaptation between a robot and a human in collaborative tasks. We propose the Bounded-Memory Adaptation Model, which is a probabilistic ﬁnite-state controller that captures human adaptive behaviors under a bounded-memory assumption. We integrate the Bounded-Memory Adaptation Model into a probabilistic decision process, enabling the robot to guide adaptable participants towards a better way of completing the task. Human subject experiments suggest that the proposed formalism improves the effectiveness of human-robot teams in collaborative tasks, when compared with one-way adaptations of the robot to the human, while maintaining the human’s trust in the robot."
"Nikolaidis, Stefanos; Hsu, David; Srinivasa, Siddhartha",Human-robot mutual adaptation in collaborative tasks: Models and experiments,2017,3,43,43,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants completed four repeated trials of the table-carrying task. After each trial, they rated their trust in the robot and their self-confidence. The robot's belief on participants' adaptability was reset after the first two trials.",Participants collaborated with a simulated robot to carry a table to a goal configuration in repeated trials.,Unspecified,Mobile Manipulators,Research,Manipulation,Object Passing,minimal interaction,Participants interacted with the robot through a user interface.,media,Participants watched videos of the robot and table moving.,simulated,The robot was represented through video playback.,shared control (adaptive),The robot adapted its behavior based on the human's actions.,Questionnaires,,,Trust was measured using a questionnaire after each trial.,POMDP,The robot's decision-making was based on a POMDP model that included a model of human adaptability.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,"The study did not directly manipulate the robot's behavior, but the repeated trials and the resetting of the robot's belief on human adaptability influenced human expectations and feedback.","Trust increased for non-adaptable participants over time, but this did not lead to increased adaptation. Participants' initial trust and self-confidence significantly affected their likelihood to adapt.","Non-adaptable participants' trust in the robot increased over time, but they remained largely non-adaptable. Participants who were initially more trusting and less confident were more likely to adapt.","Initial trust and self-confidence significantly affected the likelihood of adapting to the robot, and non-adaptable participants' trust increased over time, but they remained largely non-adaptable.",The robot and human collaborated to rotate a table to a goal configuration. The robot used a mutual adaptation strategy. The human chose rotation actions via a user interface.,Logistic regression; Wilcoxon signed-rank test; Chi-squared,A logistic regression was used to determine the effect of initial trust and self-confidence on the likelihood of adapting to the robot. A Wilcoxon signed-rank test was used to compare trust ratings of non-adaptable participants before and after the first part of the experiment. A Pearson's chi-square test was used to compare the proportion of participants who followed the robot's optimal policy in the first and second parts of the experiment.,FALSE,Task-complexity,Task-complexity,,"This study did not directly manipulate the robot's behavior, but the repeated trials and the resetting of the robot's belief on human adaptability influenced human expectations and feedback. The repeated trials and the resetting of the robot's belief on human adaptability can be considered a manipulation of 'Task-complexity' because it changed the cognitive demands on the participants as they had to adjust to the robot's behavior over time. The study found that trust increased for non-adaptable participants over time, indicating that the manipulation of 'Task-complexity' impacted trust. There was no mention of any of the other factors impacting trust.",10.1177/0278364917690593,http://journals.sagepub.com/doi/10.1177/0278364917690593,"Adaptation is critical for effective team collaboration. This paper introduces a computational formalism for mutual adaptation between a robot and a human in collaborative tasks. We propose the Bounded-Memory Adaptation Model, which is a probabilistic ﬁnite-state controller that captures human adaptive behaviors under a bounded-memory assumption. We integrate the Bounded-Memory Adaptation Model into a probabilistic decision process, enabling the robot to guide adaptable participants towards a better way of completing the task. Human subject experiments suggest that the proposed formalism improves the effectiveness of human-robot teams in collaborative tasks, when compared with one-way adaptations of the robot to the human, while maintaining the human’s trust in the robot."
"Nikolaidis, Stefanos; Hsu, David; Srinivasa, Siddhartha",Human-robot mutual adaptation in collaborative tasks: Models and experiments,2017,3,58,58,0,No participants were excluded,Online Crowdsourcing,mixed design,"Participants first completed two trials of the table-carrying task, followed by the hallway-crossing task. The robot's belief on human adaptability was reset between the two tasks.",Participants collaborated with a simulated robot in two different tasks: a table-carrying task and a hallway-crossing task.,Unspecified,Mobile Manipulators,Research,Navigation,Guiding,minimal interaction,Participants interacted with the robot through a user interface.,media,Participants watched videos of the robot and human moving.,simulated,The robot was represented through video playback.,shared control (adaptive),The robot adapted its behavior based on the human's actions.,,,,Trust was not directly measured in this study.,POMDP,The robot's decision-making was based on a POMDP model that included a model of human adaptability.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,"The study did not directly manipulate the robot's behavior, but the change in task and the resetting of the robot's belief on human adaptability influenced human expectations and task difficulty.","Participants who did not adapt in the table-carrying task were less likely to adapt in the hallway-crossing task. The study did not directly measure trust, but the results suggest that prior experience with the robot influenced subsequent behavior.","Participants who did not adapt in the first task were less likely to adapt in the second task. Some participants expected the robot to behave similarly in both tasks, based on their experience in the first task.","Adaptability in one task is predictive of adaptability in a different task, with non-adaptable participants in the first task being less likely to adapt in the second task.",The robot and human collaborated to cross a hallway. The robot used a mutual adaptation strategy. The human chose movement actions via a user interface.,Logistic regression,A logistic regression was used to determine if adaptability in the table-carrying task predicted adaptability in the hallway-crossing task.,FALSE,Task-complexity,,,"This study did not directly manipulate the robot's behavior, but the change in task and the resetting of the robot's belief on human adaptability influenced human expectations and task difficulty. The change in task from table-carrying to hallway-crossing can be considered a manipulation of 'Task-complexity' because it changed the cognitive demands on the participants as they had to adjust to a new task. The study did not directly measure trust, but the results suggest that prior experience with the robot influenced subsequent behavior. There was no mention of any of the factors impacting trust.",10.1177/0278364917690593,http://journals.sagepub.com/doi/10.1177/0278364917690593,"Adaptation is critical for effective team collaboration. This paper introduces a computational formalism for mutual adaptation between a robot and a human in collaborative tasks. We propose the Bounded-Memory Adaptation Model, which is a probabilistic ﬁnite-state controller that captures human adaptive behaviors under a bounded-memory assumption. We integrate the Bounded-Memory Adaptation Model into a probabilistic decision process, enabling the robot to guide adaptable participants towards a better way of completing the task. Human subject experiments suggest that the proposed formalism improves the effectiveness of human-robot teams in collaborative tasks, when compared with one-way adaptations of the robot to the human, while maintaining the human’s trust in the robot."
"Nikolaidis, Stefanos; Kwon, Minae; Forlizzi, Jodi; Srinivasa, Siddhartha",Planning with Verbal Communication for Human-Robot Collaboration,2018,3,200,151,49,"Participants that had as initial preference the robot goal, participants with wrong answers to the control question, incomplete data",Online Crowdsourcing,between-subjects,"Participants were familiarized with the table turning task, indicated their preferred goal configuration, and then completed two rounds of the task with the robot. Depending on the condition, the robot issued a verbal command, a state conveyance action, or no verbal action. Participants then completed a survey.","Participants collaborated with a robot to carry a table out of a room, choosing rotation actions to move the table towards their preferred goal.",HERB,Mobile Manipulators,Research,Manipulation,Object Passing,minimal interaction,"Participants interacted with the robot through a user interface, selecting actions to move a table.",media,Participants observed the robot and table through video playback.,physical,The robot was a physical robot shown in the video.,shared control (adaptive),The robot adapted its behavior based on the human's actions and the experimental condition.,Questionnaires,,,Trust was measured using a single Likert scale question.,POMDP,The robot's decision-making was based on a Partially Observable Markov Decision Process (POMDP) model that included human adaptability and compliance.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's behavior was manipulated by using different communication strategies (verbal commands, state-conveying actions, or no verbal action) to influence human adaptation and trust.",Verbal commands resulted in the highest adaptation rate and comparable trust ratings. State-conveying actions did not have the same positive effect on trust.,"The study found that verbal commands were more effective than state-conveying actions in promoting human adaptation. The initial state-conveying action, 'I think I know the best way of doing the task,' was not well-received by participants, who questioned the robot's truthfulness.",Verbal commands were the most effective way of communicating objectives and retaining user trust in a collaborative task.,"The robot attempted to guide the human to move a table towards the optimal goal, using either verbal commands, state-conveying actions, or no verbal communication. The human chose rotation actions to move the table.",Chi-squared; tost equivalence tests,"A Pearson's chi-square test was used to compare the number of participants who adapted to the robot across the three experimental conditions (Baseline, Compliance, and State-conveying). Post-hoc pairwise chi-square tests with Bonferroni corrections were used to determine which conditions differed significantly. Extended equivalence tests and pairwise TOST equivalence tests with Bonferroni corrections were used to compare the trust ratings among the three conditions.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the content of the robot's verbal communication. In one condition, the robot used verbal commands (e.g., 'Let's rotate the table clockwise'), in another, it used state-conveying actions ('I think I know the best way of doing the task'), and in a baseline condition, it used no verbal communication. The content of the verbal communication directly impacted trust, as verbal commands resulted in comparable trust ratings to the baseline, while the initial state-conveying action led to participants questioning the robot's truthfulness. This is supported by the paper content: 'Results show that adding verbal commands to the robot decision making is the most effective form of interaction... Interestingly, in the second condition state-conveying actions did not have a similar positive effect, since participants questioned whether the robot was truthful, as shown in their open-ended responses.'",10.1145/3203305,https://dl.acm.org/doi/10.1145/3203305,"Human collaborators coordinate effectively their actions through both verbal and non-verbal communication. We believe that the the same should hold for human-robot teams. We propose a formalism that enables a robot to decide optimally between taking a physical action toward task completion and issuing an utterance to the human teammate. We focus on two types of utterances: verbal commands, where the robot asks the human to take a physical action, and state-conveying actions, where the robot informs the human about its internal state, which captures the information that the robot uses in its decision making. Human subject experiments show that enabling the robot to issue verbal commands is the most effective form of communicating objectives, while retaining user trust in the robot. Communicating information about the robot’s state should be done judiciously, since many participants questioned the truthfulness of the robot statements when the robot did not provide sufficient explanation about its actions."
"Nikolaidis, Stefanos; Kwon, Minae; Forlizzi, Jodi; Srinivasa, Siddhartha",Planning with Verbal Communication for Human-Robot Collaboration,2018,3,52,52,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were familiarized with the table turning task, indicated their preferred goal configuration, and then completed two rounds of the task with the robot. The robot used the state-conveying action 'I know the best way of doing the task'. Participants then completed a survey.","Participants collaborated with a robot to carry a table out of a room, choosing rotation actions to move the table towards their preferred goal.",HERB,Mobile Manipulators,Research,Manipulation,Object Passing,minimal interaction,"Participants interacted with the robot through a user interface, selecting actions to move a table.",media,Participants observed the robot and table through video playback.,physical,The robot was a physical robot shown in the video.,shared control (adaptive),The robot adapted its behavior based on the human's actions and the experimental condition.,Questionnaires,,,Trust was measured using a single Likert scale question.,POMDP,The robot's decision-making was based on a Partially Observable Markov Decision Process (POMDP) model that included human adaptability and compliance.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's behavior was manipulated by using a state-conveying action 'I know the best way of doing the task' to influence human adaptation and trust.,The state-conveying action did not significantly improve adaptation or trust compared to the initial state-conveying action.,"The study found that removing 'I think' from the state-conveying action did not significantly improve adaptation or trust, suggesting that the issue was not the phrasing but the lack of explanation.",Simply stating that the robot knows the best way to do the task did not improve human adaptation or trust.,"The robot attempted to guide the human to move a table towards the optimal goal, using the state-conveying action 'I know the best way of doing the task'. The human chose rotation actions to move the table.",Chi-squared,A Pearson's chi-square test was used to compare the number of participants who adapted to the robot between the State-conveying I and State-conveying II conditions.,TRUE,Robot-verbal-communication-content,,Robot-verbal-communication-content,"This study manipulated the content of the robot's state-conveying action, specifically by removing 'I think' from the initial statement, changing it from 'I think I know the best way of doing the task' to 'I know the best way of doing the task'. This manipulation did not significantly impact trust or adaptation. The paper states: 'Additionally, the trust ratings between the two conditions tended to be comparable... Similarly to the initial study, users appeared not to believe the robot.' This indicates that the change in the verbal communication content did not influence trust.",10.1145/3203305,https://dl.acm.org/doi/10.1145/3203305,"Human collaborators coordinate effectively their actions through both verbal and non-verbal communication. We believe that the the same should hold for human-robot teams. We propose a formalism that enables a robot to decide optimally between taking a physical action toward task completion and issuing an utterance to the human teammate. We focus on two types of utterances: verbal commands, where the robot asks the human to take a physical action, and state-conveying actions, where the robot informs the human about its internal state, which captures the information that the robot uses in its decision making. Human subject experiments show that enabling the robot to issue verbal commands is the most effective form of communicating objectives, while retaining user trust in the robot. Communicating information about the robot’s state should be done judiciously, since many participants questioned the truthfulness of the robot statements when the robot did not provide sufficient explanation about its actions."
"Nikolaidis, Stefanos; Kwon, Minae; Forlizzi, Jodi; Srinivasa, Siddhartha",Planning with Verbal Communication for Human-Robot Collaboration,2018,3,52,52,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were familiarized with the table turning task, indicated their preferred goal configuration, and then completed two rounds of the task with the robot. The robot used the state-conveying action 'I need to be able to see the door with my forward-facing camera'. Participants then completed a survey.","Participants collaborated with a robot to carry a table out of a room, choosing rotation actions to move the table towards their preferred goal.",HERB,Mobile Manipulators,Research,Manipulation,Object Passing,minimal interaction,"Participants interacted with the robot through a user interface, selecting actions to move a table.",media,Participants observed the robot and table through video playback.,physical,The robot was a physical robot shown in the video.,shared control (adaptive),The robot adapted its behavior based on the human's actions and the experimental condition.,Questionnaires,,,Trust was measured using a single Likert scale question.,POMDP,The robot's decision-making was based on a Partially Observable Markov Decision Process (POMDP) model that included human adaptability and compliance.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's behavior was manipulated by using a state-conveying action 'I need to be able to see the door with my forward-facing camera' to influence human adaptation and trust.,"The state-conveying action with a concrete explanation significantly improved adaptation, and trust ratings tended to increase.","The study found that providing a concrete explanation for the robot's actions significantly improved human adaptation and trust, compared to the initial state-conveying action.",Providing a concrete explanation for the robot's actions significantly improved human adaptation and trust.,"The robot attempted to guide the human to move a table towards the optimal goal, using the state-conveying action 'I need to be able to see the door with my forward-facing camera'. The human chose rotation actions to move the table.",Chi-squared; Mann-Whitney U; ANOVA,A Pearson's chi-square test was used to compare the number of participants who adapted to the robot between the State-conveying I and State-conveying with Explanation conditions. A Mann-Whitney-Wilcoxon test was used to compare the trust ratings between the two conditions. A repeated measures ANOVA was used to assess the effect of the state-conveying action on participants' adaptability.,TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"This study manipulated the content of the robot's state-conveying action by providing a concrete explanation for its actions, changing it to 'I need to be able to see the door with my forward-facing camera'. This manipulation significantly impacted trust, as the new state-conveying action led to increased adaptation and a tendency for higher trust ratings. The paper states: 'Ninety-four percent of participants adapted to the robot in the State-conveying II condition, compared to 79% in the State-conveying I condition... Additionally, the trust ratings between the two conditions appear to have increased... This indicates that giving a concrete explanation on why the robot chose to follow a different goal than the person was effective in changing the participants' behavior.' This shows that the specific content of the verbal communication influenced trust.",10.1145/3203305,https://dl.acm.org/doi/10.1145/3203305,"Human collaborators coordinate effectively their actions through both verbal and non-verbal communication. We believe that the the same should hold for human-robot teams. We propose a formalism that enables a robot to decide optimally between taking a physical action toward task completion and issuing an utterance to the human teammate. We focus on two types of utterances: verbal commands, where the robot asks the human to take a physical action, and state-conveying actions, where the robot informs the human about its internal state, which captures the information that the robot uses in its decision making. Human subject experiments show that enabling the robot to issue verbal commands is the most effective form of communicating objectives, while retaining user trust in the robot. Communicating information about the robot’s state should be done judiciously, since many participants questioned the truthfulness of the robot statements when the robot did not provide sufficient explanation about its actions."
"Niu, Dongfang; Terken, Jacques; Eggen, Berry",Anthropomorphizing information to enhance trust in autonomous vehicles,2018,1,39,39,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants experienced a driving simulation with no information, then with either symbolic or symbolic + anthropomorphic information, and then completed questionnaires and an interview.",Participants experienced a driving simulation as a passenger in an autonomous vehicle and evaluated the system.,Unspecified,Autonomous Vehicles,Research,Evaluation,Survey/Questionnaire Completion,minimal interaction,Participants experienced a driving simulation without direct control of the vehicle.,simulation,Participants experienced a driving simulation with a 180-degree field of view.,simulated,The autonomous vehicle was presented as a simulation.,fully autonomous (limited adaptation),"The autonomous vehicle operated without direct human control, but with limited adaptation.",Questionnaires,Jian et al. Trust Scale,,Trust was measured using a questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the type of information presented to participants, either symbolic or symbolic with anthropomorphic features, to influence trust.",Adding anthropomorphic features to symbolic information significantly increased trust compared to symbolic information alone.,"The symbolic information condition did not significantly increase trust compared to the no information condition, which was unexpected. Participants in the anthropomorphic condition were more resilient to negative events.",Anthropomorphizing information about an autonomous vehicle's actions significantly increased trust compared to symbolic information alone.,"The robot (autonomous vehicle) drove along a pre-defined route, and the human participant was a passenger who observed the vehicle's actions and provided feedback through questionnaires and interviews.",ANOVA; t-test; Pearson correlation,"The study used independent samples ANOVAs to compare the effects of information condition (symbolic vs. symbolic + anthropomorphic) on perceived anthropomorphism, trust, and liking. One-sample t-tests were used to determine if the experimental conditions (symbolic and symbolic + anthropomorphic) were significantly different from the baseline (no information) condition. Finally, Pearson correlations were computed to assess the relationships between perceived anthropomorphism and trust, and perceived anthropomorphism and liking.",TRUE,Robot-interface-design,Robot-interface-design,,"The study manipulated the type of information presented to participants through the interface. Participants were exposed to either symbolic information (traffic icons) or symbolic information combined with anthropomorphic visualizations (animated eyes). This manipulation directly altered the visual interface of the autonomous vehicle, thus it is classified as 'Robot-interface-design'. The results showed that the symbolic + anthropomorphic information condition significantly increased trust compared to the symbolic information condition and the no information condition. Therefore, 'Robot-interface-design' is listed as a factor that impacted trust. The symbolic information condition alone did not significantly increase trust compared to the no information condition, but this is not a separate manipulation, it is a baseline condition. Therefore, there are no factors that did not impact trust.",10.1002/hfm.20745,http://doi.wiley.com/10.1002/hfm.20745,"Trust is an essential condition for accepting and relying on autonomous vehicles. One of the wellstudied factors contributing to trust in automation systems is anthropomorphism. It is expected that anthropomorphism may also enhance trust in the field of autonomous vehicles. A study is presented that investigated the effect of anthropomorphic embodiment for information about the vehicle's maneuvers on people's trust in autonomous vehicles. In a driving simulator experiment deploying a between-subjects design, participants (N = 39) were exposed to varying amounts of information displaying upcoming actions of an autonomous vehicle (symbolic information or symbolic + anthropomorphic information). Each group rated trust and liking for the test condition against a reference condition where no information about the upcoming actions was provided through questionnaires. Symbolic + anthropomorphic information resulted in significantly more trust than symbolic information. Through one-sample tests, it was found that ratings for symbolic + anthropomorphic information were significantly different from no information, while symbolic information by itself was not. Ratings for perceived anthropomorphism were positively correlated with trust and liking. It is concluded that anthropomorphizing information may foster the perception of autonomous vehicles as social agents and enhance trust in those vehicles."
"Nomura, Tatsuya; Kanda, Takayuki",Influences of Evaluation and Gaze from a Robot and Humans’ Fear of Negative Evaluation on Their Preferences of the Robot,2015,1,188,93,95,95 participants were excluded due to high self-confidence in their English skills,Controlled Lab Environment,between-subjects,"Participants completed a pre-session questionnaire, watched a video explaining robot behaviors, interacted with the robot answering questions about their English skills, and then completed a post-session questionnaire.",Participants answered questions from a robot about their English conversation skills.,Robovie-R2,Humanoid Robots,Educational; Research; Social,Social,Conversation,minimal interaction,Participants verbally interacted with the robot.,real-world,Participants interacted with a physical robot in a lab setting.,physical,A physical humanoid robot was used in the study.,wizard of oz (directly controlled),The robot's actions were controlled by a human experimenter.,Questionnaires,,Speech Data,Trust was measured using a questionnaire and speech data.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,The robot's evaluation behavior (evaluative vs. non-evaluative) and gaze behavior (normal vs. refrained) were manipulated to influence trust. Participants were also divided into high and low FNE groups.,"Participants with higher FNE tended to trust the robot more. Participants with lower FNE preferred the robot with normal gaze when it evaluated them, and preferred the robot with refrained gaze when it did not evaluate them.","Participants with higher FNE tended to trust the robot more, which is an unexpected finding. The study also found that participants spoke more when the robot did not evaluate them and when it used normal gaze.","Participants with lower FNE preferred the robot with normal gaze when it evaluated them, and preferred the robot with refrained gaze when it did not evaluate them.","The robot asked participants questions about their English skills, and the participants answered. The robot provided either evaluative or non-evaluative feedback based on the condition.",t-test; ANOVA; ANOVA; Pearson correlation,"The study used a t-test to confirm a significant difference in FNE scores between the high-FNE and low-FNE groups. Pearson's correlation coefficients were calculated to explore relationships between the subscale scores of evaluation of interaction with the robot and the numbers of utterance blocks. Two-way ANOVAs were initially conducted including gender, but no significant effects were found. Three-way ANOVAs were then used to examine the effects of evaluation, gaze, and FNE on the measured variables. MANOVAs were used as post-hoc analyses to investigate simple interaction and main effects when significant three-way interactions were found in the ANOVAs.",TRUE,Robot-verbal-communication-content; Robot-nonverbal-communication,Robot-nonverbal-communication,Robot-verbal-communication-content,"The study manipulated the robot's verbal communication by providing either evaluative or non-evaluative feedback to participants about their English skills. This is categorized as 'Robot-verbal-communication-content' because the content of the robot's feedback was directly altered. The study also manipulated the robot's gaze behavior, with the robot either gazing at participants 25% of the time (normal gaze) or 6% of the time (refrained gaze). This is categorized as 'Robot-nonverbal-communication' because it involves changes to the robot's physical movements and gaze direction. The results showed that the robot's gaze behavior ('Robot-nonverbal-communication') impacted trust, with participants having higher FNE trusting the robot more when it used normal gaze. However, the evaluation content ('Robot-verbal-communication-content') did not directly impact trust, but rather interacted with gaze and FNE to influence intention to use and likability. The main effect of evaluation was not significant on trust, but the interaction effects were significant on intention to use and likability. Therefore, 'Robot-verbal-communication-content' is listed as a factor that did not directly impact trust.",10.1007/s12369-014-0270-y,http://link.springer.com/10.1007/s12369-014-0270-y,"To investigate inﬂuences of evaluation from robots, gazes from the robots, and humans’ fear of negative evaluation (FNE) into the humans’ preferences of the robots, we conducted an experiment in which a human-sized robot is used for an advisory role, providing suggestion in foreign-language education. There were three independent variables controlled: evaluation from robot (evaluative vs. non-evaluative robot), users’ FNE (low FNE vs. high FNE), and gaze from robot (refrained gazes vs. normal gazes), and dependent variables were participants’ preferences of the robot as subjective measures and amount of utterances as an objective measure. The experimental results suggested that when the robot evaluated participants with a lower FNE, they preferred to use the robot with normal gaze behavior more than those with a higher FNE. While the effect of refrained gaze is not clear for people with higher FNE, for people with lower FNE, refrained gaze reduced their intention to participate when the robot evaluated them. Moreover, the experimental results suggested that persons having higher FNE tend to trust the robot more, and participants spoke more when the robot did not evaluate them and when it used the normal gaze. This paper discusses the implications for applications with potential evaluation capability, e.g. for healthcare and education."
"Nordqvist, Malin; Lindblom, Jessica",Operators' Experience of Trust in Manual Assembly with a Collaborative Robot,2018,1,7,7,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a manual assembly task with a collaborative robot, completing pre-test, performance test, and post-test Likert scales, followed by a semi-structured interview.","Participants collaborated with a robot to assemble a toy car, alternating between individual and joint tasks.",Unspecified,Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically interacted with the robot during the assembly task.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The study involved a physical collaborative robot.,shared control (fixed rules),"The robot and human shared control of the task, with fixed rules for task allocation.",Questionnaires; Behavioral Measures,,Video Data; Speech Data; Performance Metrics,Trust was assessed using Likert scales at different times and through observations and interviews.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the task allocation between human and robot, the interaction modes, and the complexity of the assembly instructions, which influenced the participants' ability to collaborate with the robot.",The study found that participants experienced limited trust due to communication problems and uncertainty about their ability to collaborate with the robot. Trust decreased during the task but returned to pre-test levels after the task.,"Participants initially had high trust in their ability to collaborate with the robot, but this decreased during the task due to communication issues and uncertainty, and then returned to pre-test levels after the task. The mix of interaction modes and task allocation negatively impacted trust.",Operators experienced limited trust in the collaborative robot due to communication problems and uncertainty about their own ability to collaborate with the robot.,"The robot and human alternated between assembling parts of a toy car separately and performing joint assembly tasks. The robot was controlled by voice and other interaction modes, while the human followed instructions and assembled parts.",,"No statistical tests were explicitly mentioned in the paper. The analysis was primarily qualitative, focusing on observations, interviews, and Likert scale data. The results were described in terms of the proportion of UX goals that met target levels and the overall trends in trust levels reported by participants.",TRUE,Robot-verbal-communication-content; Robot-interface-design; Robot-autonomy; Task-complexity,Robot-verbal-communication-content; Robot-interface-design; Robot-autonomy,Task-complexity,"The study manipulated several factors related to the robot's interaction and task structure. 'Robot-verbal-communication-content' was manipulated through the use of voice commands via a headset, as well as text-based and image-based instructions, which were sometimes hard to grasp, impacting the clarity of communication. 'Robot-interface-design' was manipulated by mixing several interaction modes (voice, text, image, physical), which complicated the human-robot collaboration. 'Robot-autonomy' was manipulated by varying the task allocation between the human and the robot, from the human being passive to joint collaboration, which influenced the predictability of the robot's actions. 'Task-complexity' was manipulated by the varying task allocation and the mix of interaction modes, which made the task more complex. The paper explicitly states that the communication problems and uncertainty about the ability to collaborate with the robot negatively impacted trust, which is directly linked to the 'Robot-verbal-communication-content', 'Robot-interface-design', and 'Robot-autonomy'. The paper does not explicitly state that the task complexity impacted trust, but rather that the communication and task allocation issues led to a decrease in trust. Therefore, 'Task-complexity' is included in the manipulated factors but not in the factors that impacted trust.",10.1145/3284432.3287180,https://dl.acm.org/doi/10.1145/3284432.3287180,
"Novitzky, Michael; Robinette, Paul; Benjamin, Michael R.; Gleason, Danielle K.; Fitzgerald, Caileigh; Schmidt, Henrik","Preliminary Interactions of Human-Robot Trust, Cognitive Load, and Robot Intelligence Levels in a Competitive Game",2018,1,2,2,0,No participants were excluded,Real-World Environment,within-subjects,"Participants played a capture the flag game against two different robot behaviors in a randomized order, then completed questionnaires.",Participants played a competitive capture the flag game against a robot with two different defense behaviors.,Clearpath Robotics Heron M300,Unmanned Ground Vehicles,Research,Game,Competitive Game,minimal interaction,Participants interacted with the robot in a real-world setting with verbal instructions.,real-world,The interaction took place in a real-world environment with a physical robot.,physical,The robot was a physical entity present in the real-world environment.,pre-programmed (non-adaptive),The robot followed pre-programmed behaviors without adapting to the user.,Questionnaires,,Physiological Signals,Trust was measured using questionnaires and physiological data.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's defense behavior was manipulated to be either simple or advanced, influencing its perceived competitiveness.",Participants showed higher trust in the advanced defense behavior compared to the simple defense behavior.,"The two participants overwhelmingly preferred the advanced defense behavior, indicating a strong link between perceived robot capability and trust.","Participants showed higher trust in the robot with the advanced defense behavior, suggesting that perceived robot capability influences trust.","The robot defends its flag by tagging opponents, using either a simple loiter pattern or an aggressive intercept behavior. The human attempts to score points by grabbing the robot's flag and returning it to their home zone.",,No statistical tests were explicitly mentioned in the paper. The results were based on the qualitative observation that both participants preferred the advanced defense behavior and rated it higher on a trust scale. The paper mentions that they will wait for a larger sample size before drawing conclusions regarding the cognitive load of the participants while competing against the different intelligence levels of the ASV.,TRUE,Robot-task-strategy,Robot-task-strategy,,"The study manipulated the robot's defense behavior, which is a change in the task completion strategy. The robot either loitered around its flag (simple defense) or aggressively intercepted the opponent (advanced defense). This manipulation directly impacted the perceived competitiveness and capability of the robot, which in turn influenced trust. The paper states, 'In this experiment, the task for the ASV is to defend its flag by tagging opponents... In the Simple defense behavior... the ASV traverses a loiter pattern circling around its flag without consideration of the opponent's actions... The Advanced defense behavior... is much more aggressive as the ASV will intercept and chase the opponent.' The results show that participants overwhelmingly preferred the advanced defense behavior, indicating that the change in task strategy impacted trust. There were no other factors manipulated that were found to impact trust.",10.1145/3173386.3177000,https://dl.acm.org/doi/10.1145/3173386.3177000,"This paper presents a pilot study in which we examine the interactions between human-robot teammate trust, cognitive load, and robot intelligence levels. In particular, we attempt to assess these interactions during a competitive game of capture the flag played between a human and a robot. We present results while the human plays against robots of different intelligence levels and determines their level of trust of each robot as a potential teammate through a post experiment questionnaire. We also present our exploration of heart rate measures as approximations of cognitive load. It is our goal to determine guidelines for future autonomy and interaction designers such that their systems will reduce cognitive load and increase the level of trust in robot teammates. This is an initial experiment that uses the least amount of vehicles yet still gathers competitive data on the water. Future experiments will increase in complexity to many opponents and many teammates."
"Obo, Takenori; Hase, Ryoya; Takizawa, Kazuma; Masuda, Ryusei",Effect of Human-Robot Interaction for Safety Driving on Driver’s Self-Efficacy,2021,1,14,14,0,No participants were excluded,Controlled Lab Environment,mixed design,Participants used a driving simulator while a robot provided navigation and feedback. Questionnaires were administered before and after the simulation to assess self-efficacy and robot impression.,Participants drove a car in a driving simulator while receiving navigation and feedback from a robot.,PALRO,Humanoid Robots; Expressive Robots,Social; Research,Social,Social Guidance/Coaching,minimal interaction,Participants interacted with the robot through a driving simulator and verbal feedback.,simulation,The interaction took place in a driving simulator environment.,physical,A physical robot was present and provided verbal feedback.,pre-programmed (non-adaptive),The robot followed a pre-programmed script for navigation and feedback.,Questionnaires,,,Trust was assessed using questionnaires before and after the experiment.,no modeling,No computational model of trust was used in this study.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot provided feedback and scores based on driving performance, and the robot's communication was manipulated to attribute low scores to external factors.","The effect on self-efficacy varied based on driving experience and score, with some participants interpreting the robot's comments optimistically and others negatively. The robot's feedback influenced the participants' impression of the robot.","Participants who rarely drive tended to interpret the robot's comments more optimistically, while those who drive regularly became more critical of their driving skills after the experiment. Low scores negatively impacted self-efficacy despite the robot's attempts to attribute the result to external factors.",The effect of robot feedback on driver's self-efficacy varies with not only the scoring result but also the frequency of car use in daily life and driving experience.,"The robot acted as a navigator, providing route guidance and scoring the driver's performance. The human participant drove a car in a simulator, following the robot's instructions and receiving feedback on their driving.",,No specific statistical tests were mentioned in the paper. The analysis was based on a questionnaire survey using a five-point Likert scale and semantic differential scale questions. The results were discussed qualitatively by comparing the responses of different groups based on driving experience and scores.,TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the robot's verbal communication content by attributing low scores to external factors (robot navigation or other factors) in an attempt to prevent a lack of self-efficacy. This is explicitly stated in the paper: 'In this study, we assumed that the proposed communication model can attribute not only a high score to driver's daily efforts but also a low score to robot navigation or other factors.' The impact of this manipulation on self-efficacy and robot impression was then measured. The paper states that the robot's feedback influenced the participants' impression of the robot, indicating that the manipulation of the robot's verbal communication content impacted trust. There were no other manipulations explicitly stated in the paper.",10.23919/WAC50355.2021.9559556,https://ieeexplore.ieee.org/document/9559556/,"This paper presents a robot communication system for promoting driver's motivation toward safe driving. Various navigation and control systems for safety driving have been proposed in related works. However, these don't lead to raise drivers' awareness of traffic safety. Without the feedback of own driving performance, it can be difficult for drivers to build self-confidence to achieve safe driving. In this study, we used a communication robot as a guide to navigate and score the level of safety driving. Moreover, we proposed a communication model to prevent a lack of self-efficacy to achieve safety driving, based on the theory of self-serving bias. To verify the effect of the proposed approach, we conducted some experiments and questionnaire investigation to discuss the applicability for motivating safety driving. From the experimental results, we found that the effect on driver's self-efficacy varies with not only the scoring result but also the frequency of car use in daily life and driving experience."
"Ogawa, R.; Park, S.; Umemuro, H.",How Humans Develop Trust in Communication Robots: A Phased Model Based on Interpersonal Trust,2019,1,16,16,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed questionnaires assessing trust and self-esteem at four time points: before the experiment, after watching a video of the robot, after seeing the robot in person, and after interacting with the robot. The interaction involved a conversation with the robot using the Wizard of Oz method.",Participants interacted with a robot in a conversation using the Wizard of Oz method.,Nao,Humanoid Robots; Expressive Robots,Research; Social,Social,Conversation,minimal interaction,Participants had a conversation with the robot using the Wizard of Oz method.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical robot.,wizard of oz (directly controlled),The robot's conversation was controlled by a human operator using the Wizard of Oz method.,Questionnaires,Schaefer's Trust Questionnaire/Scale,,Trust was measured using questionnaires at different phases of the experiment.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Wizard-of-Oz Studies,Indirect Manipulation,"The study indirectly manipulated trust by exposing participants to the robot in different phases: video, physical presence, and interaction. The expectation of the robot's capabilities was also influenced by the introduction video.","General and category trust increased in the early phases, while individual trust did not change significantly. Social relationship trust increased, but not significantly.","Individual trust did not change much through the connecting and interpreting phases. Social relationship trust increased, but the change was not statistically significant.",General and category trust developed in the early anticipation phase of the interaction with the robot.,"The robot initiated conversations with questions about the participant, and the conversation continued using the Wizard of Oz method. The human participant responded to the robot's questions and engaged in a dialogue.",t-test; Pearson correlation,A t-test was used to examine the change in social relationship trust before and after the interaction phase. A correlation analysis was used to assess the relationship between self-esteem and general trust.,TRUE,Robot-verbal-communication-content; Robot-autonomy,Robot-verbal-communication-content,Robot-autonomy,"The study manipulated the content of the robot's verbal communication by having the robot initiate conversations with questions about the participant, which is described in the methods section: 'NAO initiated conversations with some questions about the subject and continued dialogs by means of Wizard of Oz method.' This is categorized as 'Robot-verbal-communication-content' because the content of the conversation was varied. The study also manipulated the level of robot autonomy by using the Wizard of Oz method, where a human operator controlled the robot's responses. This is categorized as 'Robot-autonomy' because the robot's decision-making was not autonomous. The results section indicates that general and category trust developed in the early anticipation phase, which was before the interaction, and individual trust did not change much through the connecting and interpreting phases. Social relationship trust increased through the interpreting phase, which was when the robot was communicating with the participant. This indicates that the content of the robot's communication impacted trust, while the lack of autonomy did not have a significant impact on trust development. Therefore, 'Robot-verbal-communication-content' is listed as a factor that impacted trust, and 'Robot-autonomy' is listed as a factor that did not impact trust.",10.1109/HRI.2019.8673090,,"The purpose of this study was to propose a model of development of trust in social robots. Insights in interpersonal trust were adopted from social psychology and a novel model was proposed. In addition, this study aimed to investigate the relationship among trust development and self-esteem. To validate the proposed model, an experiment using a communication robot NAO was conducted and changes in categories of trust as well as self-esteem were measured. Results showed that general and category trust have been developed in the early phase. Self-esteem is also increased along the interactions with the robot."
"Oh, Seeung; Seong, Younho; Yi, Sun; Park, Sangsung",Neurological Measurement of Human Trust in Automation Using Electroencephalogram,2020,1,28,28,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed six training sessions (three for automatic control and three for manual control), 10 simulated driving trials, and a survey to evaluate trust. Trials 1-5 were designed to develop trust in automatic control, while trials 6-10 were designed to evoke mistrust.","Participants chose between manual or automatic control in a simulated driving game, aiming to achieve a high performance rate by avoiding cars before an accident.",Unspecified,Autonomous Vehicles,Research,Game,Other Game subtask: Participants chose between manual or automatic control in a simulated driving game.,minimal interaction,"Participants interacted with a simulated driving game, making choices between manual and automatic control.",simulation,The interaction was conducted in a simulated driving environment.,simulated,The robot was represented as a simulated driving system.,pre-programmed (non-adaptive),The automatic control system had a pre-programmed performance level that did not adapt to the user.,Questionnaires; Physiological Measures,,Physiological Signals; Performance Metrics,"Trust was assessed using a questionnaire and EEG signals, with performance metrics also collected.",no modeling,"The study did not use computational modeling of trust, focusing on descriptive statistics.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The performance of the automatic control was manipulated to induce trust (high performance) and mistrust (low performance). Participants were also given information about the performance of the automatic control.,"Trust increased with high automation performance and decreased with low automation performance. Alpha and beta waves were stronger under trust, while gamma waves were stronger under mistrust.","One participant consistently chose manual control regardless of automation performance, showing individual differences in trust behavior. The study also found that alpha and beta waves were associated with trust, while gamma waves were associated with mistrust.","Alpha and beta brainwaves are stronger under trust, while gamma waves are stronger under mistrust in automation.",The human participant chose between manual or automatic control in a simulated driving game. The robot (simulated automation) either performed well or poorly based on the trial.,Power spectrum analysis; standard deviation,"The study used power spectrum analysis to analyze EEG data, specifically focusing on alpha, beta, and gamma waves. This analysis was used to evaluate the state of trust and mistrust by examining the power of these brainwaves. Additionally, standard deviation was used to compare the differences in alpha, beta, and gamma waves under states of trust and mistrust relative to a baseline.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the performance of the automatic control system. In trials 1-5, the automatic control had 100% performance, designed to build trust. In trials 6-10, the automatic control had a low performance (under 36%), designed to evoke mistrust. This manipulation of the robot's success rate directly influenced the task performance metrics, making 'Robot-accuracy' the most appropriate category. The paper explicitly states, 'Trials 1-5 were designed to develop trust in automatic control by applying 100% automation, whereas trials 6-10 were designed to evoke mistrust in automation by applying a low performance of under 36%...'. The results showed that trust increased with high automation performance and decreased with low automation performance, indicating that 'Robot-accuracy' impacted trust. There were no other factors manipulated in the study.",10.5391/IJFIS.2020.20.4.261,http://www.ijfis.org/journal/view.html?uid=929&sort=&scale=&key=year&keyword=&s_v=20&s_n=4&pn=vol&year=2020&vmd=Full,"In modern society, automation is sufﬁciently complex to conduct advanced tasks. The role of the human operator in controlling a complex automation is crucial for avoiding failures, reducing risk, and preventing unpredictable situations. Measuring the level of trust of human operators is vital in predicting their acceptance and reliance on automation. In this study, an electroencephalogram (EEG) is used to identify speciﬁc brainwaves under trusted and mistrusted cases of automation. A power spectrum analysis was used for a brainwave analysis. The results indicate that the power of the alpha and beta waves is stronger for a trusted situation, whereas the power of gamma waves was stronger for a mistrusted situation. When the level of human trust in automation increases, the use of automatic control increases. Therefore, the ﬁndings of this research will contribute to utilizing a neurological technology to measure the level of trust of the human operator, which can affect the decision-making and the overall performance of automation used in industries."
"Okada, Yuka; Kimoto, Mitsuhiko; Iio, Takamasa; Shimohara, Katsunori; Shiomi, Masahiro",Two is better than one: Apologies from two robots are preferred,2023,2,203,168,35,"35 participants were excluded for incorrectly answering dummy questions, 35 participants were excluded for not carefully watching the videos and instructions",Online Crowdsourcing,within-subjects,"Participants watched videos of a robot making a mistake and apologizing, then answered questionnaires. They watched two videos, one with one robot apologizing and one with two robots apologizing, in a counterbalanced order. Participants also answered a three-choice question about their preferred number of apologizing robots and provided free-text feedback.",Participants watched videos of a robot dropping an ice cream cone and then evaluated the robot's apology.,Pepper,Humanoid Robots,Social; Service and Assistive Robots,Evaluation,Survey/Questionnaire Completion,passive observation,Participants watched videos of the robot interaction.,media,Participants watched videos of the robot interaction.,physical,The robot was a physical robot shown in the video.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires; Multidimensional Measures,Forgiveness Scale; Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using questionnaires and a multi-dimensional trust scale.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The number of robots apologizing was manipulated to see if it affected trust.,"Apologies from two robots were preferred over one, increasing trust scores.","Participants significantly preferred apologies from two robots over one, and the effect size was particularly large for the competent, ethical, and benevolent subscales of trust.",Apologies from two robots are preferred over apologies from one robot in a service failure scenario.,The robot dropped an ice cream cone and apologized. The human participant watched the video and answered questionnaires.,Chi-squared; ryan's tests; paired-t test,"A chi-square test was used to analyze the preferred number of apologizing robots. Ryan's tests were used for multiple comparisons following the chi-square test. Paired-t tests were used to compare the questionnaire results between the one-robot and two-robot apology conditions for forgiveness, negative word-of-mouth, trust (reliable, competent, ethical, transparent, and benevolent subscales), and intention to use.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the number of robots apologizing, which directly changes the content of the verbal communication. In the one-apology condition, only the main robot apologized, while in the two-apology condition, both the main and sub-robot apologized. This manipulation directly affects the content of the verbal communication received by the participants. The results showed that the two-apology condition led to higher trust scores, indicating that the manipulated factor, Robot-verbal-communication-content, impacted trust. There were no other factors manipulated in this study.",10.1371/journal.pone.0281604,https://dx.plos.org/10.1371/journal.pone.0281604,"Although the capabilities of service robots are increasing, avoiding any mistakes is difficult. Therefore, strategies for mitigating mistakes, such as apology behavior designs, are essential for service robots. Past studies reported that costly apology is perceived as more sincere than non-costly ones and more acceptable. To increase the apology cost in robot service situations, we thought that using multiple robots would increase the perceived costs in the of financial, physical, and time costs. Therefore, we focused on the number of robots who apologize for their mistakes as well as their individual, specific roles and behaviors during such apologies. We investigated the differences in perceived impressions toward apologies from two robots (the main robot that makes a mistake and apologizes and a sub-robot that also apologizes) and an apology from just one robot (only the main robot) through a web survey with 168 valid participants. The experiment results showed that the participants significantly preferred and positively evaluated apologies from two robots more than one robot in the context of forgiveness, negative word-of-mouth, trust, and intention to use. We also conducted another web survey with 430 valid participants to investigate the effects of different roles for the sub-robot: apologize-only, cleaning-up-only, and both actions. The experimental results showed that the participants significantly preferred and positively evaluated both actions in the context of forgiveness and reliable/competent perspectives."
"Okada, Yuka; Kimoto, Mitsuhiko; Iio, Takamasa; Shimohara, Katsunori; Shiomi, Masahiro",Two is better than one: Apologies from two robots are preferred,2023,2,609,430,179,"179 participants were excluded for incorrectly answering dummy questions, 179 participants were excluded for not carefully watching the videos and instructions",Online Crowdsourcing,within-subjects,"Participants watched videos of a robot making a mistake and a second robot performing different actions (apologizing, cleaning, or both), then answered questionnaires. Participants watched all three videos in a counterbalanced order.","Participants watched videos of a robot dropping an ice cream cone and a second robot performing different actions, then evaluated the robot's apology.",Pepper,Humanoid Robots,Social; Service and Assistive Robots,Evaluation,Survey/Questionnaire Completion,passive observation,Participants watched videos of the robot interaction.,media,Participants watched videos of the robot interaction.,physical,The robot was a physical robot shown in the video.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires; Multidimensional Measures,Forgiveness Scale; Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using questionnaires and a multi-dimensional trust scale.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The sub-robot's behavior was manipulated to see if it affected trust, with conditions including apologizing, cleaning, or both.","The mixed condition (apologizing and cleaning) was preferred over the other conditions, increasing forgiveness and trust scores.",The mixed condition (apologizing and cleaning) was significantly preferred over the apologize-only and cleaning-only conditions for forgiveness and reliable/competent trust subscales. The cleaning-only condition did not show significant differences compared to the apologize-only condition.,A sub-robot that both apologizes and cleans up after a mistake is preferred over a sub-robot that only apologizes or only cleans up.,"The main robot dropped an ice cream cone and apologized. The sub-robot either apologized, cleaned up, or did both. The human participant watched the video and answered questionnaires.",repeated anova; Bonferroni correction,"A repeated measures ANOVA was used to analyze the questionnaire results for forgiveness, negative word-of-mouth, trust (reliable, competent, ethical, transparent, and benevolent subscales), and intention to use across the three sub-robot behavior conditions (apologize-only, cleaning-up-only, and mixed). The Bonferroni method was used for post-hoc multiple comparisons following the ANOVA.",TRUE,Robot-verbal-communication-content; Robot-task-strategy,Robot-verbal-communication-content; Robot-task-strategy,,"In this study, the researchers manipulated the sub-robot's behavior after the main robot's apology. The sub-robot either apologized (apologize-only), cleaned up (cleaning-up-only), or both apologized and cleaned up (mixed condition). The 'apologize-only' condition changes the content of the verbal communication, as the sub-robot provides an additional apology. The 'cleaning-up-only' and 'mixed' conditions change the task strategy of the sub-robot, as it performs a cleaning action. The results showed that the mixed condition, where the sub-robot both apologized and cleaned up, led to higher trust scores compared to the other conditions, indicating that both Robot-verbal-communication-content and Robot-task-strategy impacted trust. There were no other factors manipulated in this study.",10.1371/journal.pone.0281604,https://dx.plos.org/10.1371/journal.pone.0281604,"Although the capabilities of service robots are increasing, avoiding any mistakes is difficult. Therefore, strategies for mitigating mistakes, such as apology behavior designs, are essential for service robots. Past studies reported that costly apology is perceived as more sincere than non-costly ones and more acceptable. To increase the apology cost in robot service situations, we thought that using multiple robots would increase the perceived costs in the of financial, physical, and time costs. Therefore, we focused on the number of robots who apologize for their mistakes as well as their individual, specific roles and behaviors during such apologies. We investigated the differences in perceived impressions toward apologies from two robots (the main robot that makes a mistake and apologizes and a sub-robot that also apologizes) and an apology from just one robot (only the main robot) through a web survey with 168 valid participants. The experiment results showed that the participants significantly preferred and positively evaluated apologies from two robots more than one robot in the context of forgiveness, negative word-of-mouth, trust, and intention to use. We also conducted another web survey with 430 valid participants to investigate the effects of different roles for the sub-robot: apologize-only, cleaning-up-only, and both actions. The experimental results showed that the participants significantly preferred and positively evaluated both actions in the context of forgiveness and reliable/competent perspectives."
"Okamura, Kazuo; Yamada, Seiji",Adaptive Trust Calibration for Supervised Autonomous Vehicles,2018,1,60,60,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants first learned to use a drone simulator, then were trained to trust the drone at 100% reliability. Finally, the drone's reliability was decreased to 50%, and participants were divided into four groups, each receiving a different trust calibration cue. The participants' reliance behavior was measured.",Participants were asked to fly a drone along a predefined route and decide whether to rely on the drone's automatic pothole detection or manually check the road image.,Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a drone simulator, making decisions based on the drone's reports.",simulation,Participants used a 3D drone simulator to complete the task.,simulated,The robot was represented as a simulated drone in a virtual environment.,shared control (fixed rules),The drone operated autonomously but its reliability was pre-set and changed according to the experimental design.,Behavioral Measures; Questionnaires,,Performance Metrics,Trust was assessed through reliance behavior and a post-experiment trust survey.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The drone's reliability was manipulated to induce over-trust, and trust calibration cues were introduced to help participants recover from over-trust. Participants were also told the average success rate of manual pothole detection to influence their self-confidence.",The study aimed to decrease trust in the drone by reducing its reliability and then to increase trust calibration through the use of trust calibration cues.,,The study explores a framework to detect over-trust and under-trust based on reliance behavior and examines the effectiveness of trust calibration cues in restoring appropriate trust.,"The drone automatically detects potholes, and the human decides whether to rely on the drone's report or manually check the road image.",chi-square frequency test,The chi-square frequency test was used to determine if the counts of the observed reliance behavior significantly differed from what would be expected by chance. This test was used to identify when participants were in an over-trust state.,TRUE,Robot-accuracy; Robot-verbal-communication-content; Robot-interface-design,Robot-accuracy; Robot-verbal-communication-content,,"The study manipulated 'Robot-accuracy' by changing the drone's pothole detection reliability from 100% to 50% to induce over-trust. This is explicitly stated in the 'Procedure' section: 'The P auto is artificially decreased from 100% to 50%'. The study also manipulated 'Robot-verbal-communication-content' by providing different trust calibration cues (TCCs) which are described as 'cognitive cues' intended to 'trigger the user to promptly notice what has been happening in the environment and to calibrate the trust'. These cues included visual signs, audible cues, anthropomorphic animations, and verbal warnings. The 'Procedure' section also mentions that participants were told the average success rate of manual pothole detection (75%), which is a form of verbal communication that influences self-confidence and thus indirectly impacts trust. The 'Robot-interface-design' was manipulated by the presence of the pop-up notification windows and the TCCs presented on the screen. The study found that the manipulation of 'Robot-accuracy' and 'Robot-verbal-communication-content' impacted trust, as the change in reliability led to over-trust, and the TCCs were intended to help participants recover from over-trust. The study does not explicitly state that the interface design had an impact on trust, but it was a necessary component for the TCCs to be presented.",10.1145/3239092.3265948,https://dl.acm.org/doi/10.1145/3239092.3265948,"Poor trust calibration in autonomous vehicles often degrades total system performance in safety or efﬁciency. Existing studies have primarily examined the importance of system transparency of autonomous systems to maintain proper trust calibration, with little emphasis on how to detect over-trust and under-trust nor how to recover from them. With the goal of addressing these research gaps, we ﬁrst provide a framework to detect a calibration status on the basis of the user’s behavior of reliance. We then propose a new concept with cognitive cues called trust calibration cues (TCCs) to trigger the user to quickly restore appropriate trust calibration. With our framework and TCCs, a novel method of adaptive trust calibration is explored in this study. We will evaluate our framework and examine the effectiveness of TCCs with a newly developed online drone simulator."
"Okamura, Kazuo; Yamada, Seiji",Adaptive trust calibration for human-AI collaboration,2020,1,194,116,78,"78 participants unintentionally moved the drone far from the area where the CKPs were located, and they failed to complete the tasks within the time limit.",Online Crowdsourcing,mixed design,"Participants were randomly assigned to one of five groups (NoTCC, visual, audio, verbal, and anthro.). The experiment consisted of three phases: instruction, training, and main. In the main phase, participants performed a pothole inspection task using a drone simulator. The reliability of the automatic inspection fluctuated depending on the weather conditions. When over-trust was detected, a TCC was presented to the user.","Participants performed a pothole inspection task using a drone simulator, choosing between automatic inspection or manual inspection.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a drone simulator through a screen, making choices about automatic or manual inspection.",simulation,Participants used a 3D drone simulator to perform the pothole inspection task.,simulated,The robot was a simulated drone within the 3D environment.,shared control (fixed rules),"The drone's automatic inspection had a fixed reliability that changed based on weather conditions, and the user chose between automatic or manual inspection.",Behavioral Measures; Performance-Based Measures,,Performance Metrics,Trust was assessed using behavioral measures (manual choice rate) and performance metrics (sensitivity d' and accuracy).,no modeling,Trust was not modeled computationally; the study focused on detecting over-trust based on behavioral observations.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the automatic inspection (performance) was manipulated by changing the weather conditions, which also influenced task difficulty. The presentation of TCCs was also manipulated as a form of feedback.","Adaptively presented TCCs significantly affected the choice behavior of the participants, leading to increased manual choice rates and improved sensitivity d' in the bad weather period. The NoTCC group did not change their behavior despite the system information indicating the reliability becoming worse.","The verbal TCC group showed the most significant increase in manual choice rates and sensitivity d' during the bad weather period, suggesting that the specific wording of the cue was more effective than visual or auditory cues. The NoTCC group did not change their behavior despite the system information indicating the reliability becoming worse, which is not in line with previous studies that emphasized the effectiveness of continuous trust calibration with system transparency.","Adaptively presenting trust calibration cues (TCCs) can effectively mitigate over-trust in human-AI collaboration, while continuous system transparency alone may not be sufficient.","The robot (simulated drone) performed automatic pothole inspections, and the human participant chose whether to rely on the automatic inspection or to manually inspect the road images. The human monitored the drone's performance and made decisions based on the reliability of the automatic inspection and the presence of TCCs.",ANOVA; t-test; t-test,"The study used a two-factor mixed ANOVA to analyze the effects of TCC groups and CKP periods on manual choice rates, sensitivity d', and accuracy. Post-hoc comparisons with Holm-Bonferroni adjustments were conducted to investigate the effects of TCCs. One-sample t-tests were used to compare Pman with Pauto. A Welch's t-test was used to compare Pman in the good weather period and in the bad weather period.",TRUE,Robot-accuracy; Robot-verbal-communication-content; Task-environment,Robot-accuracy; Robot-verbal-communication-content,Task-environment,"The study manipulated 'Robot-accuracy' by changing the weather conditions, which directly affected the reliability of the automatic pothole inspection (Pauto). This is explicitly stated in the paper: 'Under good weather conditions, P auto and the corresponding sensitivity d 0 were 90%... Under bad weather conditions, P auto dropped to 50%'. The study also manipulated 'Robot-verbal-communication-content' by presenting different types of trust calibration cues (TCCs), including verbal cues, which provided specific warnings to the user. The paper states: 'Verbal TCC is a tooltip balloon with the warning message ""This choice might not be a good idea.""'. The 'Task-environment' was also manipulated by changing the weather conditions, which affected the visibility and the sounds of a thunderstorm. The paper mentions: 'Immediately after the 6th CKP was inspected, sounds of a thunderstorm began. The visibility of the field also became very low'. The results showed that changes in 'Robot-accuracy' and 'Robot-verbal-communication-content' significantly impacted trust, as evidenced by changes in manual choice rates and sensitivity d'. However, the 'Task-environment' manipulation, while intended to influence trust, did not significantly impact the NoTCC group's behavior, indicating that the change in environment alone was not sufficient to alter trust calibration. The NoTCC group continued to rely on the drone despite the degraded performance and the change in environment, as stated in the paper: 'Nevertheless, the participants of the NoTCC group continued to rely on the drone's automatic pothole inspection...'. Therefore, 'Task-environment' is listed as a factor that did not impact trust.",10.1371/journal.pone.0229132,https://dx.plos.org/10.1371/journal.pone.0229132,"Safety and efficiency of human-AI collaboration often depend on how humans could appropriately calibrate their trust towards the AI agents. Over-trusting the autonomous system sometimes causes serious safety issues. Although many studies focused on the importance of system transparency in keeping proper trust calibration, the research in detecting and mitigating improper trust calibration remains very limited. To fill these research gaps, we propose a method of adaptive trust calibration that consists of a framework for detecting the inappropriate calibration status by monitoring the user’s reliance behavior and cognitive cues called “trust calibration cues” to prompt the user to reinitiate trust calibration. We evaluated our framework and four types of trust calibration cues in an online experiment using a drone simulator. A total of 116 participants performed pothole inspection tasks by using the drone’s automatic inspection, the reliability of which could fluctuate depending upon the weather conditions. The participants needed to decide whether to rely on automatic inspection or to do the inspection manually. The results showed that adaptively presenting simple cues could significantly promote trust calibration during over-trust."
"Okamura, Kazuo; Yamada, Seiji",Calibrating Trust in Human-Drone Cooperative Navigation,2020,1,36,32,4,4 participants were excluded due to large deviations from the course,Online Crowdsourcing,between-subjects,"Participants were instructed to fly a drone along a 10km course within 10 minutes, with a higher score for more accurate flight. They experienced both auto-pilot and manual-pilot in a training phase. The main phase consisted of three 3.3km sections with varying weather conditions (A1, B, A2). The TCC group received verbal cues when over/under-trust was detected, while the NoTCC group did not. Participants completed a post-experiment questionnaire.","Participants flew a drone along a course, using either auto-pilot or manual-pilot, and were instructed to stay on the course.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Navigation,Path Following,minimal interaction,"Participants interacted with a drone simulator through a screen, using keyboard inputs to control the drone.",simulation,The interaction was through a drone simulator displayed on a screen.,simulated,The robot was a simulated drone in a virtual environment.,shared control (fixed rules),"The drone had an auto-pilot mode with fixed rules, and participants could take over control manually.",Behavioral Measures; Performance-Based Measures,,Performance Metrics,Trust was assessed through behavioral measures (manual-pilot rates) and performance metrics (cross-track errors).,no modeling,Trust was not modeled computationally; the study focused on detecting over/under-trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the auto-pilot was manipulated by simulating different weather conditions, and verbal trust calibration cues were presented to the TCC group when over/under-trust was detected.","The TCC group showed increased manual-pilot rates when over-trust was detected and decreased manual-pilot rates when under-trust was detected, indicating that the TCCs helped calibrate trust. The NoTCC group did not show these changes.","The study found that participants in the NoTCC group tended to interrupt the auto-pilot even when it was highly reliable, suggesting that the drone's behavior had a greater impact on trust than the reliability indicator. The task setting induced under-trust more than over-trust.","The study demonstrated that the proposed framework with TCCs could promote trust calibration in a continuous real-time task, and that adaptively presenting TCCs was more effective than continuous system transparency.","The robot (simulated drone) autonomously followed a predefined path, and the human participant could take over manual control at any time to correct the drone's trajectory.",ANOVA; t-test,"The study used a one-way ANOVA to analyze the effect of the ABA conditions (different weather conditions) on the TCC rates. Post-hoc analysis using the Holm-Bonferroni method was used to compare TCC rates between conditions. One-sample t-tests were used to compare the means of the success rates of the manual-pilot with a hypothesized value to validate the assumptions about P_auto and P_man in different weather conditions. Multiple comparisons using the Holm-Bonferroni method were also used to compare cross-track errors between the NoTCC group, the TCC group, and the manual-pilot only group.",TRUE,Robot-accuracy; Robot-verbal-communication-content; Task-environment,Robot-accuracy; Robot-verbal-communication-content,Task-environment,"The study manipulated 'Robot-accuracy' by changing the weather conditions, which directly affected the auto-pilot's performance (P_auto). The paper states, ""In the B condition, a thunderstorm was simulated... The cross-track errors... were artificially distorted to simulate the deteriorated sensing accuracy... This made P auto deteriorate to 0.69..."" This directly impacts the robot's ability to perform the task. 'Robot-verbal-communication-content' was manipulated through the use of verbal trust calibration cues (TCCs) presented to the TCC group when over/under-trust was detected. The paper mentions, ""The verbal TCC was presented in front of the drone... when over-trust and under-trust were detected..."" The content of these cues was designed to encourage participants to reconsider their decisions. 'Task-environment' was manipulated by changing the weather conditions, which included screen brightness and sound effects. The paper states, ""In the A condition, good weather conditions were simulated... In the B condition, a thunderstorm was simulated with a blurred and dark (40% brightness) screen and with sound effects."" While the task environment was manipulated, the study found that the drone's behavior had a greater impact on trust than the reliability indicator, suggesting that the environmental changes did not directly impact trust as much as the robot's accuracy and the TCCs. The TCCs and the robot's accuracy were found to impact trust, as the TCC group showed increased manual-pilot rates when over-trust was detected and decreased manual-pilot rates when under-trust was detected, and the NoTCC group did not show these changes. The study also found that participants tended to interrupt the auto-pilot even when it was highly reliable, suggesting that the drone's behavior had a greater impact on trust than the reliability indicator.",10.1109/RO-MAN47096.2020.9223509,https://ieeexplore.ieee.org/document/9223509/,"Trust calibration is essential to successful cooperation between humans and autonomous systems such as those for self-driving cars and autonomous drones. If users over-estimate the capability of autonomous systems, over-trust occurs, and the users rely on the systems even in situations in which they could outperform the systems. On the contrary, if users under-estimate the capability of a system, undertrust occurs, and they tend not to use the system. Since both situations hamper cooperation in terms of safety and efﬁciency, it would be highly desirable to have a mechanism that facilitates users in keeping the appropriate level of trust in autonomous systems. In this paper, we ﬁrst propose an adaptive trust calibration framework that can detect over/under-trust from users’ behaviors and encourage them to keep the appropriate trust level in a “continuous” cooperative task. Then, we conduct experiments to evaluate our method with semi-automatic drone navigation. In experiments, we introduce ABA situations of weather conditions to investigate our method in bidirectional trust changes. The results show that our method adaptively detected trust changes and encouraged users to calibrate their trust in a continuous cooperative task. We believe that the ﬁndings of this study will contribute to better user-interface designs for collaborative systems."
"Okamura, Kazuo; Yamada, Seiji",Calibrating Trust in Autonomous Systems in a Dynamic Environment,2020,1,70,70,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants performed a pothole inspection task using a drone simulator, choosing between manual or automatic inspection. The experiment included ABA/BAB scenarios with varying weather conditions to induce over-trust and under-trust. A verbal trust calibration cue (TCC) was presented adaptively based on the participant's choices. The experiment had a training phase followed by the main phase with 24 checkpoints.","Participants were asked to inspect road images from a drone to check for potholes, choosing between manual or automatic inspection.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a drone simulator through a screen, making choices about manual or automatic inspection.",simulation,"The interaction was through a drone simulator, providing a virtual environment for the task.",simulated,The robot was represented as a simulated drone in the virtual environment.,shared control (fixed rules),"The drone performed automatic inspection based on pre-set rules, and participants could choose to rely on it or perform manual inspection.",Behavioral Measures; Performance-Based Measures,,Performance Metrics,Trust was assessed through behavioral measures (manual choice rates) and performance metrics (sensitivity d').,no modeling,Trust was not modeled computationally; the study focused on detecting and mitigating miscalibration.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the weather conditions to change the drone's automatic inspection reliability, and provided a verbal trust calibration cue (TCC) when miscalibration was detected.","The TCC increased manual choice rates in over-trust scenarios and decreased them in under-trust scenarios, promoting better trust calibration and performance.","The TCC rates decreased over time, indicating that the participants' trust was being calibrated. The NoTCC groups did not significantly change their choice behaviors when the automatic reliability deteriorated, which is not in line with previous studies that emphasized the importance of continuous system transparency.","Adaptively presenting a simple verbal cue (TCC) significantly promoted trust calibration in both over-trust and under-trust cases, leading to better performance.","The drone performed automatic pothole inspection, and the human participant chose to either rely on the automatic inspection or manually inspect the road images. The human's primary task was to monitor the drone's performance and make decisions about reliance.",t-test; ANOVA,"The study used one-sample t-tests to validate the estimation of Pman and Pauto in a pre-experiment. One-way ANOVAs were conducted to compare the manual rates and sensitivity d' across different conditions (A, B, and A or B, A, and B) within each group (TCC and NoTCC). Post-hoc analyses using the Holm-Bonferroni method were performed to identify specific differences between conditions.",TRUE,Robot-accuracy; Robot-verbal-communication-content; Task-environment,Robot-accuracy; Robot-verbal-communication-content,Task-environment,"The researchers manipulated the weather conditions, which directly impacted the reliability of the drone's automatic pothole inspection (Robot-accuracy). Specifically, the weather conditions were changed to simulate good and bad weather, which altered the success rate of the automatic inspection. The study also introduced a verbal trust calibration cue (TCC) that was presented to participants when miscalibration was detected (Robot-verbal-communication-content). This cue was designed to influence the participants' trust in the system. The weather conditions also changed the visual and auditory environment of the task (Task-environment), but this was not found to directly impact trust, as the TCC was the primary driver of trust calibration. The study explicitly states that the TCC increased manual choice rates in over-trust scenarios and decreased them in under-trust scenarios, promoting better trust calibration and performance. The manipulation of the weather conditions (Task-environment) was primarily to change the robot's accuracy, and the TCC was the primary manipulation to impact trust.",,,"Appropriately calibrating trust in autonomous systems is essential for successful collaboration between humans and the systems. Over-trust and under-trust often happen in dynamically changing environments, and they can be major causes of serious issues with safety and efﬁciency. Many studies have examined the role of continuous system transparency in keeping proper trust calibration; however, not many studies have focused on how to ﬁnd poor trust calibration nor how to mitigate it. In our proposed method of trust calibration, a behaviorbased approach is used to detect improper trust calibration, and cognitive cues called “trust calibration cues” are presented to users as triggers for trust calibration. We conducted an online experiment with a drone simulator. Seventy participants performed pothole inspection tasks manually or relied on the drone’s automatic inspection. The results demonstrated that adaptively presenting a simple cue could signiﬁcantly promote trust calibration in both over-trust and under-trust cases."
"Li, Yugang; Wu, Baizhou; Huang, Yuqi; Luan, Shenghua","Developing trustworthy artificial intelligence: insights from research on interpersonal, human-automation, and human-AI trust",2024,1,1077,1077,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of six groups and played a trust game where they decided how much of $1000 to give to a hypothetical opponent described as either a robot, AI, or control (human name or nickname).",Participants played a trust game where they decided how much of $1000 to give to a hypothetical opponent.,Unspecified,Other,Research,Game,Economic Game,passive observation,Participants only read about the hypothetical interaction scenario.,media,The interaction was described through text only.,hypothetical,"The robot was only described in text, without any visual representation.",not autonomous,The robot's actions were hypothetical and did not involve any real autonomy.,Behavioral Measures,Big Five Inventory Scale,,Trust was measured by the amount of money given to the opponent in the trust game.,"parametric models (e.g., regression)",Ordinary least squares regression was used to analyze the correlations of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the description of the opponent as either a robot, AI, or control (human name or nickname) to influence trust.","Opponent type (robot, AI, or control) had no significant effect on trust, but opponents named jdrx894 were trusted more than those named Michael.","The most trusted opponent was the robot jdrx894, and the least trusted was Michael in the control group, which contradicts the similarity-attraction hypothesis.","Opponent type (robot, AI, or control) had no significant effect on trust, but opponents named jdrx894 were trusted more than those named Michael.","The human participant decided how much of $1000 to give to a hypothetical opponent, and the robot did not perform any actions.",ANOVA; tukey's honest significant difference test; Kruskal-Wallis; ordinary least squares regression; breusch-pagan test; Robust regression,The study used a one-way ANOVA to compare the means of the six experimental groups in the trust game. A two-way ANOVA was used to analyze the effect of opponent name and type. Tukey's honest significant difference test was used for pairwise comparisons. A Kruskal-Wallis H test was used as a robustness check. Ordinary least squares regression was used to analyze the correlations of trust with sociodemographic and social-psychological factors. A Breusch-Pagan test was used to check for heteroskedasticity. Robust regression was used to address outliers.,TRUE,Teaming,Teaming,,"The study manipulated the description of the opponent as either a robot, AI, or control (human name or nickname). This manipulation falls under the 'Teaming' category because it changes the nature of the interaction from a human-human interaction to a human-robot or human-AI interaction, which is a form of collaboration or competition in a game setting. The study found that the specific type of opponent (robot, AI, or control) did not significantly impact trust, but the name of the opponent did. The name 'jdrx894' was trusted more than 'Michael'. Therefore, the 'Teaming' factor was found to impact trust, as the type of opponent was a manipulation of the team composition. The study did not manipulate any other factors from the provided list.",10.3389/fpsyg.2024.1382693,https://www.frontiersin.org/articles/10.3389/fpsyg.2024.1382693/full,"The rapid advancement of artificial intelligence (AI) has impacted society in many aspects. Alongside this progress, concerns such as privacy violation, discriminatory bias, and safety risks have also surfaced, highlighting the need for the development of ethical, responsible, and socially beneficial AI. In response, the concept of trustworthy AI has gained prominence, and several guidelines for developing trustworthy AI have been proposed. Against this background, we demonstrate the significance of psychological research in identifying factors that contribute to the formation of trust in AI. Specifically, we review research findings on interpersonal, human-automation, and human-AI trust from the perspective of a three-dimension framework (i.e., the trustor, the trustee, and their interactive context). The framework synthesizes common factors related to trust formation and maintenance across different trust types. These factors point out the foundational requirements for building trustworthy AI and provide pivotal guidance for its development that also involves communication, education, and training for users. We conclude by discussing how the insights in trust research can help enhance AI’s trustworthiness and foster its adoption and application."
"Oliveira, Luis; Proctor, Karl; Burns, Christopher G.; Birrell, Stewart",Driving Style: How Should an Automated Vehicle Behave?,2019,1,43,41,2,2 participants were excluded due to technical mishaps,Controlled Lab Environment,mixed design,Participants were passengers in autonomous vehicles with two different driving styles. They completed surveys after each of four journeys and participated in a semi-structured interview.,Participants rode in an autonomous vehicle and were asked to evaluate their trust in the vehicle.,Unspecified,Autonomous Vehicles,Research,Evaluation,Survey/Questionnaire Completion,minimal interaction,Participants rode in the vehicle but had no direct control over it.,real-world,Participants experienced the autonomous vehicle in a real-world setting.,physical,Participants interacted with a physical autonomous vehicle.,fully autonomous (limited adaptation),"The vehicles operated autonomously within a predefined area, but with limited adaptation.",Questionnaires,Trust in Automated Systems Scale,,Trust was measured using the Trust in Automated Systems Scale questionnaire.,no modeling,"Trust was not modeled computationally, only descriptive statistics were used.",Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The driving style of the autonomous vehicle was manipulated to be either human-like or machine-like, influencing how the vehicle behaved at junctions.","There were no significant differences in trust between the two driving styles, but trust increased over time for both.","Trust scores increased over the four journeys, regardless of the driving style. Distrust scores were more stable in the first two runs, then decreased, and then stabilized again. The human-like driving style showed a steeper change in trust scores between runs 2 and 3.","There were no significant differences in trust between the human-like and machine-like driving styles, but trust increased over time for both.","The robot drove autonomously, following a pre-defined path, and the human participant was a passenger in the vehicle, completing surveys after each journey.",ANOVA; Kolmogorov-Smirnov; t-test,"A 4x2 repeated-measures ANOVA was conducted to analyze the trust scores across the four trips, with driving style as a between-groups factor. The Kolmogorov-Smirnov test was used to check for normality of the trust variables. Paired t-tests were used for post-hoc analysis to identify differences in trust scores between the four trips.",TRUE,Robot-task-strategy,,Robot-task-strategy,"The study manipulated the driving style of the autonomous vehicle, which is a task strategy. One style was 'human-like' where the vehicle would slow down and 'peek' at junctions, and the other was 'machine-like' where the vehicle would proceed without slowing down if it knew the junction was clear. This manipulation did not influence trust levels, as there were no significant differences in trust between the two driving styles. The paper states: 'There were no group differences between the human-like or machine-like driving styles (F(1,39) = 1.711, p = 0.20)'. Therefore, 'Robot-task-strategy' is the manipulated factor, and it did not impact trust.",10.3390/info10060219,https://www.mdpi.com/2078-2489/10/6/219,"This article reports on a study to investigate how the driving behaviour of autonomous vehicles inﬂuences trust and acceptance. Two diﬀerent designs were presented to two groups of participants (n = 22/21), using actual autonomously driving vehicles. The ﬁrst was a vehicle programmed to drive similarly to a human, “peeking” when approaching road junctions as if it was looking before proceeding. The second design had a vehicle programmed to convey the impression that it was communicating with other vehicles and infrastructure and “knew” if the junction was clear so could proceed without ever stopping or slowing down. Results showed non-signiﬁcant diﬀerences in trust between the two vehicle behaviours. However, there were signiﬁcant increases in trust scores overall for both designs as the trials progressed. Post-interaction interviews indicated that there were pros and cons for both driving styles, and participants suggested which aspects of the driving styles could be improved. This paper presents user information recommendations for the design and programming of driving systems for autonomous vehicles, with the aim of improving their users’ trust and acceptance."
"Onnasch, Linda; Hildebrandt, Clara Laudine",Impact of Anthropomorphic Robot Design on Trust and Attention in Industrial Human-Robot Interaction,2022,1,40,40,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were randomly assigned to either an anthropomorphic or non-anthropomorphic robot condition. They performed a collaborative task with the robot, which involved the robot handing over boxes containing LEGO bricks. The task was performed over four blocks, with a robot failure introduced in the fourth block. Trust and attention were measured at different points during the experiment.",Participants assembled LEGO circuit boards using components delivered by the robot.,Sawyer,Industrial Robot Arms; Collaborative Robots,Industrial; Research,Manipulation,Object Passing,direct-contact interaction,Participants physically interacted with the robot by taking boxes from it.,real-world,The study was conducted in a real-world lab setting with a physical robot.,physical,Participants interacted with a physical industrial robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions without adapting to the user.,Behavioral Measures; Custom Scales; Questionnaires,Negative Attitude towards Robots Scale (NARS); Godspeed Questionnaire; Affinity for Technological Interaction (ATI) Scale,Eye-tracking Data; Video Data,"Trust was assessed using questionnaires, behavioral measures, and eye-tracking data.",no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's appearance was manipulated by adding a face to the display in one condition, and the robot's performance was manipulated by introducing a failure in the fourth block. The face was intended to influence trust through anthropomorphism, and the failure was intended to influence trust dissolution.","The anthropomorphic design did not increase trust and even decreased perceived reliability. The robot failure decreased trust in both conditions, but the anthropomorphic robot showed a steeper decrease in trust behavior after the failure.","The study found a negative impact of anthropomorphism on perceived reliability, which is contrary to many findings in social HRI. The anthropomorphic robot also showed a steeper decline in trust behavior after a failure. The manipulation check did not show a significant difference in perceived anthropomorphism between the two conditions, which is unexpected.","Anthropomorphic design of an industrial robot did not increase trust and even decreased perceived reliability, and it also led to a distracting effect on visual attention.",The robot grasped boxes from a shelf and handed them to the participant. The participant assembled the components inside the box and performed a quality check.,t-test; Mann-Whitney U; ANOVA; mixed anovas,"The study used t-tests for independent samples to compare the anthropomorphic and non-anthropomorphic groups on control variables and manipulation checks. Mann-Whitney U tests were used as a non-parametric alternative to t-tests when assumptions were violated. Mixed ANOVAs were used to analyze the impact of anthropomorphism (between-subject factor) and interaction experience (within-subject factor) on trust and visual attention during fault-free interactions. Additionally, ANOVAs were used to test the impact of anthropomorphism and failure experience on trust, with Bonferroni corrections applied for multiple tests. Post-hoc tests with Bonferroni corrections were used for pairwise comparisons of the independent variable interaction experience. The Greenhouse-Geisser adjustment was used for corrections in case of violations of Mauchly's sphericity test.",TRUE,Robot-aesthetics; Robot-accuracy,Robot-accuracy,Robot-aesthetics,"The study manipulated the robot's appearance by adding a face to the display in one condition, which falls under 'Robot-aesthetics' as it changes the visual appeal of the robot. The study also manipulated the robot's performance by introducing a failure in the fourth block, which directly impacts the robot's success rate and thus is categorized as 'Robot-accuracy'. The results showed that the robot's failure (Robot-accuracy) significantly impacted trust, leading to a decrease in trust ratings and an increase in handover times. However, the anthropomorphic design (Robot-aesthetics) did not significantly increase trust and even decreased perceived reliability, indicating that it did not positively impact trust. The study also found that the anthropomorphic design led to a distracting effect on visual attention, but this is not a direct impact on trust, but rather a consequence of the manipulation. Therefore, 'Robot-aesthetics' is listed as a factor that did not impact trust.",10.1145/3472224,https://dl.acm.org/doi/10.1145/3472224,"The application of anthropomorphic features to robots is generally considered beneficial for               human-robot interaction (HRI               ). Although previous research has mainly focused on social robots, the phenomenon gains increasing attention in industrial human-Robot interaction as well. In this study, the impact of anthropomorphic design of a collaborative industrial robot on the dynamics of trust and visual attention allocation was examined. Participants interacted with a robot, which was either anthropomorphically or non-anthropomorphically designed. Unexpectedly, attribute-based trust measures revealed no beneficial effect of anthropomorphism but even a negative impact on the perceived reliability of the robot. Trust behavior was not significantly affected by an anthropomorphic robot design during faultless interactions, but showed a relatively steeper decrease after participants experienced a failure of the robot. With regard to attention allocation, the study clearly reveals a distracting effect of anthropomorphic robot design. The results emphasize that anthropomorphism might not be an appropriate feature in industrial HRI as it not only failed to reveal positive effects on trust, but distracted participants from relevant task areas which might be a significant drawback with regard to occupational safety in HRI."
"Onyeoru, Happy Chidi; Wirth, Christopher; Giles, Joshua; Arvaneh, Mahnaz",Analysing and Modelling Human Trust to a Navigation Robot,2023,1,10,10,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants observed a simulated robotic arm performing a factory task, with varying error rates and types across three scenarios. They reported their trust level after observing a set number of runs in each scenario. In the third scenario, participants could intervene to correct errors.","Participants observed a simulated robotic arm navigating through objects to pick up a target object, and rated their trust in the robot's actions.",Unspecified,Industrial Robot Arms,Industrial; Research,Supervision,Monitoring,passive observation,Participants passively observed the robot's actions on a screen.,simulation,The interaction was presented as a simulation on a computer screen.,simulated,The robot was a simulated robotic arm.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions with fixed error rates.,Custom Scales,,Performance Metrics,Trust was measured using a custom 5-point scale and performance metrics were collected.,"parametric models (e.g., regression)",A regression model was used to estimate trust based on error rates.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers directly manipulated the robot's error rates, the speed and accuracy of the robot, and the ability of the human to intervene and correct errors, which was intended to affect trust.",Increased error rates decreased trust. Lower speed but higher accuracy led to higher trust. Human intervention to correct errors increased trust.,"The study found that WTI errors had a significant negative correlation with reported trust, but this correlation disappeared when humans could correct the errors. This suggests that the ability to correct errors can restore trust.","Human trust in a machine is significantly affected by changes in error rates, and the ability of humans to correct severe errors increases their trust in the machine.","The robot moved a simulated arm to pick up a target object, and the human observed the robot's actions and rated their trust. In one scenario, the human could intervene to correct errors.",Pearson correlation; t-test; Linear regression,"The study used Pearson correlation to examine the relationship between error rates (MA and WTI errors) and reported trust values. Paired t-tests were used to compare mean trust values between different experimental conditions (e.g., low speed/high accuracy vs. high speed/low accuracy). Regression analysis was employed to develop a model for estimating trust based on error rates (MA and WTI errors).",TRUE,Robot-accuracy; Robot-autonomy,Robot-accuracy; Robot-autonomy,,"The study manipulated the robot's error rates (MA, SOT, WTI) across different scenarios, which directly impacts the robot's accuracy in performing the task. This is a manipulation of 'Robot-accuracy' because it directly influences the robot's success in reaching the target and grasping it. The study also manipulated the ability of the human to intervene and correct errors, which is a manipulation of 'Robot-autonomy' because it changes the level of control the robot has over its actions and the level of human intervention allowed. The results showed that changes in error rates (specifically MA and WTI errors) significantly impacted trust, indicating that 'Robot-accuracy' influenced trust. The ability of the human to intervene and correct errors also impacted trust, indicating that 'Robot-autonomy' influenced trust. There were no factors that were manipulated that did not impact trust.",10.1109/MetroXRAINE58569.2023.10405803,https://ieeexplore.ieee.org/document/10405803/,"Human trust plays a crucial role in Human-Machine Interactions (HMIs) within autonomous systems. This paper delves into the factors that inﬂuence human trust in machines, including varying error rates and types made by the machine, as well as the human’s ability to intervene and rectify errors. To explore these factors, we conducted three scenarios involving a simulated claw robot navigating through multiple objects to detect and locate a target object. The ﬁrst scenario examined the effect of changing error rates on human trust in the machine. In the second scenario, we investigated how variability in speed and accuracy of reaching the target impacted human trust. Lastly, we explored whether human trust in the machine changed when individuals had the capability to intervene and correct severe errors made by the machine. We then proposed a regression model to estimate human trust. Our results showed that human trust is signiﬁcantly affected by changes in error rates. Our participants reported a higher trust on robot with low speed but higher accuracy in performing the task than the robot with high speed but lower accuracy. Interestingly, the ability to intervene and correct the robot’s errors improved the participants’ trust in the robot. Our regression result showed that we can estimate trust using different type of errors committed by machine which can be applied in real world scenarios."
"Oros, Marina; Nikolic, Milutin; Borovac, Branislav; Jerkovic, Ivan",Children's preference of appearance and parents' attitudes towards assistive robots,2014,2,155,155,0,No participants were excluded,Educational Setting,between-subjects,"Children were shown five black and white sketches of robots and asked to choose the most likable, then color it. Examiners asked about the chosen sketch, its gender, and traits.",Children chose and colored a robot sketch and answered questions about it.,Unspecified,Other,Research,Evaluation,Rating,passive observation,Children observed and selected from robot sketches.,media,The interaction was based on static images of robot sketches.,simulated,The robots were represented as sketches.,not autonomous,The robots were static images with no autonomous behavior.,,,,Trust was not directly measured in this study.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The study manipulated the visual appearance of the robot sketches to see which designs children preferred.,"The study did not directly measure trust, but found that children preferred robots with round edges, compact bodies, and feminine characteristics, suggesting these features may influence positive perceptions.",Boys showed stronger gender-typing in ascribing gender to the robot than girls.,"Children prefer robot sketches with round edges, compact bodies, and feminine characteristics, and the dominant color chosen was blue.",Children were asked to select a robot sketch they liked the most and then color it. The robot was a static image.,chi-squared (χ 2 ) test,"The chi-squared test was used to examine if there were statistically significant differences in the frequency of children choosing different robot sketches, and to examine gender differences in sketch preference. The test compared observed frequencies of sketch choices with expected frequencies if there was no preference.",TRUE,Robot-aesthetics,Robot-aesthetics,,"The study manipulated the visual appearance of the robot sketches (e.g., round vs. sharp edges, compact vs. elongated bodies, human-like vs. animal-like features) to see which designs children preferred. This falls under 'Robot-aesthetics' as it directly relates to the visual appeal and design of the robots. The study found that children preferred robots with round edges, compact bodies, and feminine characteristics, suggesting these aesthetic features influence positive perceptions, which can be linked to trust. Therefore, 'Robot-aesthetics' is also listed as a factor that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1109/HUMANOIDS.2014.7041385,http://ieeexplore.ieee.org/document/7041385/,"The paper aims to examine children’s preference of visual appearance and parents’ attitudes towards assistive robots. Results show that children do prefer visual appearances of robots with round and smooth edges, compact and stocky body and with feminine characteristics. Gender differences have been found in robot sketch preference, as well as in ascribing gender to robots – boys tend to be more gendertyped. The dominant colour preffered for the robot is blue, which is a colour associated with positive emotional states, trust, and stability. Based on the results, the paper provides general suggestions and guidelines for creating a robot that the children would find likable. Parents have shown more positive than negative attitudes towards robots in general, as well as towards robots in the children’s therapy. Gender differences have been found in potentially negative aspects of child-robot interaction, with mothers more concerned about negative consequences. Also, differences in educational levels have been found, with parents of higher educational level showing more positive attitudes towards robots."
"Oros, Marina; Nikolic, Milutin; Borovac, Branislav; Jerkovic, Ivan",Children's preference of appearance and parents' attitudes towards assistive robots,2014,2,130,130,0,No participants were excluded,Educational Setting,within-subjects,Parents completed two questionnaires: one on general attitudes towards robots and another on attitudes towards robots in children's therapy.,Parents completed two questionnaires about their attitudes towards robots.,Unspecified,Other,Care; Research,Evaluation,Survey/Questionnaire Completion,passive observation,Parents only read and responded to statements about robots.,media,The interaction was based on text descriptions of robots and scenarios.,hypothetical,The robots were hypothetical and not physically present.,not autonomous,The robots were hypothetical and had no autonomous behavior.,Questionnaires,,,Trust was measured using questionnaires.,no modeling,No computational model of trust was used.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,"The study did not manipulate any factors, but assessed parents' attitudes towards robots.","The study found that parents generally have positive attitudes towards robots, especially in therapeutic settings, and that mothers are more concerned about negative aspects of child-robot interaction than fathers.","Mothers showed more concern about negative aspects of child-robot interaction, and parents with higher education had more positive attitudes towards robots.",Parents have more positive attitudes towards robots in children's therapy than towards robots in general.,Parents completed two questionnaires about their attitudes towards robots in general and in therapeutic settings. The robot was hypothetical.,t-test for dependent samples; one-way analysis of variance (anova) f-test,A t-test for dependent samples was used to compare the means of parents' attitudes towards robots in general and their attitudes towards robots in children's therapy. The ANOVA F-test was used to examine gender differences and differences between parents with different educational levels in their attitudes towards robots in children's therapy.,FALSE,,,,"This study did not manipulate any factors. It was an observational study where parents completed questionnaires about their attitudes towards robots. Therefore, there are no factors to list under 'factors_manipulated', 'factors_that_impacted_trust', or 'factors_that_did_not_impact_trust'.",10.1109/HUMANOIDS.2014.7041385,http://ieeexplore.ieee.org/document/7041385/,"The paper aims to examine children’s preference of visual appearance and parents’ attitudes towards assistive robots. Results show that children do prefer visual appearances of robots with round and smooth edges, compact and stocky body and with feminine characteristics. Gender differences have been found in robot sketch preference, as well as in ascribing gender to robots – boys tend to be more gendertyped. The dominant colour preffered for the robot is blue, which is a colour associated with positive emotional states, trust, and stability. Based on the results, the paper provides general suggestions and guidelines for creating a robot that the children would find likable. Parents have shown more positive than negative attitudes towards robots in general, as well as towards robots in the children’s therapy. Gender differences have been found in potentially negative aspects of child-robot interaction, with mothers more concerned about negative consequences. Also, differences in educational levels have been found, with parents of higher educational level showing more positive attitudes towards robots."
"Orozco, Jesus A.; Artemiadis, Panagiotis",Extracting Human Levels of Trust in Human–Swarm Interaction Using EEG Signals,2024,1,7,6,1,1 participant was excluded due to a hardware malfunction,Controlled Lab Environment,within-subjects,"Participants observed three different swarm behaviors (explore, surround, and flock) five times each, while EEG data was recorded. Participants were asked to think about each swarm's ability to reach the goal. After the experiment, participants were asked about their impressions of the experiment.",Participants observed a swarm of virtual agents attempting to reach a target point on a computer screen and were asked to consider the swarm's ability to complete the task.,Unspecified,Swarm Robots,Research,Supervision,Monitoring,passive observation,Participants passively observed the swarm behavior on a computer screen.,simulation,The interaction was through a simulation of virtual agents on a computer screen.,simulated,The robots were simulated virtual agents displayed on a computer screen.,pre-programmed (non-adaptive),The swarm behaviors were pre-programmed and did not adapt to the user.,Physiological Measures,,Physiological Signals,Trust was assessed using EEG data to identify brain activity related to different levels of trust.,"deep learning (e.g., neural networks, reinforcement learning)",Machine learning was used to classify EEG features related to low and high trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the swarm behavior, with some behaviors (flock and surround) always reaching the goal and one behavior (explore) never reaching the goal, to influence the perceived trustworthiness of the swarm.","The flock and surround behaviors were associated with high trust, while the explore behavior was associated with low trust.","The study found that different brain regions were activated differently between low and high trust scenarios, with most differences observed in the frontal and occipital regions. There was no single feature shared among all six subjects, but there were six features that at least four subjects shared.","EEG correlates of swarm trust exist and are distinguishable in machine learning feature classification, with high prediction accuracy using a k-NN classifier.","The robot swarm moved on a screen, attempting to reach a target point. The human participant observed the swarm and was asked to think about the swarm's ability to complete the task.",t-test; k-nn; Discriminant analysis; svm,"The study used a t-test to compare the power spectrum density of low-trust (explore behavior) and high-trust (flock and surround behaviors) EEG features across all subjects and subbands. The k-NN classifier was used for feature classification, with a fivefold cross-validation. The performance of the k-NN classifier was compared with LDA and SVM classifiers.",TRUE,Robot-accuracy,Robot-accuracy,,"The researchers manipulated the swarm behavior such that some behaviors (flock and surround) always reached the goal, while one behavior (explore) never reached the goal. This directly impacts the task performance metrics (success/failure rates) of the robot swarm, thus it is classified as 'Robot-accuracy'. The paper explicitly states, 'To note, the explore swarm never completes the task, whereas the flock and surround behaviors do in the allotted time. This is by design as we want to have a clear-cut definition for low-(explore) and high-trust (flock and surround) scenarios.' The different success rates of the swarms directly influenced the participants' trust, as stated in the results: 'All six subjects mentioned trusting the red (surround) and blue (flock) swarm to reach the goal on the screen while distrusting the green (explore) swarm.' Therefore, 'Robot-accuracy' is the factor that impacted trust. There were no other factors manipulated, so there are no factors that did not impact trust.",10.1109/THMS.2024.3356421,https://ieeexplore.ieee.org/document/10423920/,"Trust is an essential building block of human civilization. However, when it relates to artiﬁcial systems, it has been a barrier to intelligent technology adoption in general. This article addresses the gap in determining levels of trust in scenarios that include humans interacting with a swarm of robots. Electroencephalography (EEG) recordings of the human observers of the different swarms allow for extracting speciﬁc EEG features related to different trust levels. Feature selection and machine learning methods comprise a classiﬁcation system that would allow recognition of different levels of human trust in those human–swarm interaction scenarios. The results of this study suggest that EEG correlates of swarm trust exist and are distinguishable in machine learning feature classiﬁcation with very high accuracy. Moreover, comparing common EEG features across all human subjects used in this study allows for the generalization of the classiﬁcation method, providing solid evidence of speciﬁc areas and features of the human brain where activations are related to levels of human–swarm trust. This work has direct implications for effective human–machine teaming with applications to many ﬁelds, such as exploration, search and rescue operations, surveillance, environmental monitoring, and defense. In these applications, quantifying levels of human trust in the deployed swarm is of utmost importance because it can lead to swarm controllers that adapt their output based on the human’s perceived trust level."
"Paddeu, Daniela; Parkhurst, Graham; Shergold, Ian",Passenger comfort and trust on first-time use of a shared autonomous shuttle vehicle,2020,1,56,55,1,1 participant was excluded because they did not attend the trial as planned,Real-World Environment,within-subjects,"Participants experienced four trips in a shared autonomous vehicle (SAV) with varying speeds (8/16 km/h) and seating directions (forwards/backwards). They rated trust, comfort, and nausea after each run and completed pre- and post-experiment questionnaires.","Participants rode in an SAV and rated their trust, comfort, and nausea levels.",Unspecified,Autonomous Vehicles,Research,Evaluation,Survey/Questionnaire Completion,minimal interaction,"Participants rode in the SAV and provided ratings, with limited direct interaction beyond the ride itself.",real-world,Participants experienced the SAV in a real-world setting on a test track.,physical,Participants interacted with a physical SAV.,fully autonomous (limited adaptation),The SAV operated autonomously on a predefined route with limited adaptation.,Questionnaires; Custom Scales,Jian et al. Trust Scale,,Trust was measured using a custom 11-point Likert scale and the Trust in Automation Checklist (TAC).,no modeling,Trust data was analyzed statistically but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The researchers manipulated the speed of the SAV and the direction of the participants' seating to influence trust and comfort.,Trust was significantly lower at higher speeds and when participants were seated facing backwards.,"Participants who were daily car drivers showed lower initial trust and comfort ratings but became more favorable after the experiment. There was a significant combined effect of speed and direction of face on nausea, with the highest nausea ratings when participants were traveling at lower speed and looking backwards.","Trust in the SAV was significantly influenced by both speed and direction of face, with lower trust at higher speeds and when facing backwards.","The robot (SAV) moved along a predefined route at varying speeds. The human participants rode in the SAV and provided ratings of trust, comfort, and nausea.",ANOVA; ANOVA; t-test; Pearson correlation,"The study used a two-way repeated measures ANOVA to analyze the effects of speed and direction of face (DoF) on trust, comfort, and nausea. One-way repeated measures ANOVA was used to examine changes in trust and comfort ratings over the four experimental runs. Paired t-tests were used to compare pre- and post-experiment ratings of comfort, trust, and nausea. Correlation analysis was used to explore relationships between pre- and post-experiment ratings, as well as between initial trust, opinions about AVs, and interest in AVs.",TRUE,Task-environment; Task-constraints,Task-environment; Task-constraints,,"The researchers manipulated the 'speed of travel' and 'direction of face' (seating direction) of the SAV. 'Speed of travel' is categorized as 'Task-constraints' because it directly imposed a limit on the performance of the task (riding in the SAV), influencing the experience of the participants. 'Direction of face' is categorized as 'Task-environment' because it altered the physical conditions of the task, specifically the orientation of the participant within the vehicle, which influenced their perception of the environment and their experience of the ride. The study found that both speed and direction of face significantly impacted trust levels, with lower trust at higher speeds and when facing backwards. Therefore, both 'Task-environment' and 'Task-constraints' are listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1016/j.trc.2020.02.026,https://linkinghub.elsevier.com/retrieve/pii/S0968090X19301901,"Autonomous Vehicles (AV) may become widely diffused as a road transport technology around the world. However, two conditions of successful adoption of AVs are that they must be synchronously shared, to avoid negative transport network and environmental consequences, and that high levels of public acceptance of the technology must exist. The implications of these two conditions are that travellers must accept sharing rides with unfamiliar others in Shared Autonomous Vehicles (SAV). Two factors that have been identified as being positive influencers of acceptance are comfort and trust. The present paper undertakes a novel examination as to how comfort and trust ratings are affected by specific attributes of the ride experience of travelling in a fully-automated real-world, shared vehicle. To this end, 55 participants experienced riding in an SAV shuttle under experimental conditions at a test facility. Each experimental run involved two unrelated participants, accompanied by a safety operative and a researcher, undertaking four trips in the SAV, during which two conditions were presented for each of the independent variables of ‘direction of face’ (forwards/backwards) and ‘maximum vehicle speed’ (8/16 km/h). Order of presentation was varied between pairs of participants. After each run, participants rated the dependent variables ‘trust’ and ‘comfort’ (the latter variable comprised by six comfort factors). Expected and evaluative ratings were also obtained during pre-experimental orientation and debriefing sessions. Statistically significant relationships (p < .001) were found between trust and each of the independent variables, but for neither variable in the case of perceived comfort. A strong correlation was found between comfort and trust, interpreted as indicating trust in the SAV as an important predictor of perceived comfort. The before and after-experiment ratings for both variables showed statistically significant increases, and particularly for daily car drivers."
"Paeng, Erin; Wu, Jane; Jr, James C Boerkoel",Human-Robot Trust and Cooperation Through a Game Theoretic Framework,2016,1,229,229,0,No participants were excluded,Online Crowdsourcing,mixed design,"Participants played 16 rounds of a coin entrustment game against a computer algorithm, with half being told they were playing against a robot and the other half against a human, followed by a post-game survey.","Participants played a coin entrustment game, deciding how many coins to entrust to their opponent and whether to return coins entrusted to them.",Unspecified,Other,Research,Game,Economic Game,minimal interaction,Participants interacted with a computer algorithm through a game interface.,simulation,The interaction was through a simulated game environment.,hypothetical,"The robot was not physically present, and was described as a computer algorithm.",pre-programmed (non-adaptive),The robot's behavior was based on a pre-programmed algorithm.,Behavioral Measures; Questionnaires,Jian et al. Trust Scale,Performance Metrics,Trust was measured through the number of coins entrusted and a post-game survey.,no modeling,"Trust was not modeled computationally, and the analysis was based on statistical tests.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"Participants were told their opponent was either a robot or a human, influencing their expectations and perceptions of the opponent.",Participants showed higher levels of trust towards the robot opponent compared to the human opponent.,Participants regained trust in robot opponents more quickly after a betrayal than they did with human opponents. Participants reported different motivations when playing against a robot (coin maximization) versus a human (victory).,"Humans tend to trust robots to a greater degree than other humans in a coin entrustment game, while cooperating equally well with both.","The robot, a computer algorithm, played a coin entrustment game, deciding how many coins to entrust and whether to return coins. The human participant made the same decisions, interacting through a computer interface.",ANOVA,"A mixed ANOVA was used to evaluate the results. The between-subjects factor was the opponent type (robot or human), and the within-subjects factor was the 16 rounds of the game. The ANOVA was used to determine if there were statistically significant differences in the number of coins entrusted across all rounds based on the opponent type.",TRUE,Teaming,Teaming,,"The study manipulated the perceived opponent type (robot or human), which directly influences whether the participant is in a collaborative or competitive setting. This is best categorized as 'Teaming' because the manipulation is about whether the participant believes they are interacting with a robot or a human, which changes the nature of the interaction from a human-robot interaction to a human-human interaction. The paper states, 'While all participants played against the same strategy algorithm, we told half that their opponent was a robot and the other half that their their opponent was a human.' This manipulation directly impacted trust, as the results showed that 'players trust robots more than humans in across the rounds.' The study found that the cooperation rate was not impacted by the manipulation, as 'participants cooperated at nearly identical rates regardless of opponent type.'",,,"Trust and cooperation are fundamental to human interactions. How much we trust other people directly influences the decisions we make and our willingness to cooperate. It thus seems natural that trust be equally important in successful human-robot interaction (HRI), since how much a human trusts a robot affects how they might interact with it. As a result, considerable research has been done to explore factors that influence trust in HRI, particularly using game theory (Lee and Hwang 2008; Mathur and Reichling 2009). Unfortunately, these approaches lack a comparative analysis of trust and cooperation as distinct qualities. We propose using a coin entrustment game (explained later), a variant of prisoner’s dilemma, to measure trust and cooperation as separate phenomenon between human and robot agents. With this game, we test the following hypotheses: (1) Humans will achieve and maintain higher levels of trust when interacting with what they believe to be a robot than with another human; and (2) humans will cooperate more readily with robots and will maintain a higher level of cooperation. This work contributes an experimental paradigm that uses the coin entrustment game as a way to test our hypotheses. Our empirical analysis shows that humans tend to trust robots to a greater degree than other humans, while cooperating equally well in both."
"Palmarini, Riccardo; del Amo, Iñigo Fernandez; Bertolino, Guglielmo; Dini, Gino; Erkoyuncu, John Ahmet; Roy, Rajkumar; Farnsworth, Michael",Designing an AR interface to improve trust in Human-Robots collaboration,2018,1,15,15,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were asked to complete a maintenance task using an AR-HRC system, which provided a virtual animation of the robot's movements before the robot started moving. After completing the task, participants filled out a Likert scale questionnaire to assess their trust in the system.","Participants performed a pick-and-place task of an electronic card using a robot, with AR providing a preview of the robot's movements.",TurtleBot,Mobile Robots; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Passing,minimal interaction,Participants interacted with the robot by observing its movements and completing a pick-and-place task.,real-world,Participants used a tablet to view an augmented reality overlay of the robot's movements.,physical,"A physical robot was present in the study, and participants interacted with it.",pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of movements.,Questionnaires,,,Trust was measured using a Likert scale questionnaire.,no modeling,No computational model of trust was used in this study.,Empirical HRI Studies,Mixed Reality Studies,Direct Manipulation,"The study manipulated the robot's behavior by providing a preview of its movements through AR, which was intended to increase the user's sense of safety and trust.",The AR preview of the robot's movements positively affected the participants' trust in the robot.,"The study found that providing context-awareness through AR, specifically by showing the robot's movements in advance, increased trust in the HRC system.",Providing a preview of the robot's movements through AR enhances human trust in human-robot collaboration.,"The robot performed a pick-and-place task of an electronic card, while the human observed the robot's movements through an AR interface and completed the task.",,"No specific statistical tests were mentioned in the paper. The study used a Likert scale questionnaire to measure trust, and the results were presented as averages. However, no statistical analysis was performed to compare different conditions or groups.",TRUE,Robot-interface-design,Robot-interface-design,,"The study manipulated the presentation of the robot's movements through an AR interface, which provided a preview of the robot's actions before they occurred. This falls under 'Robot-interface-design' because it involves changes to the interactive elements (the AR display) that provide information about the robot's behavior. The paper states, 'The AR-HRC has been developed to provide accurate context awareness to the technician by giving him the information about the robot movements in advance.' and 'The user can start the animation of the cobot to understand what movements it is going to do once started.' This manipulation of the interface was found to positively impact trust, as the results showed that 'the utilisation of the proposed AR design for understanding the cobot movements in the real environment has positively affected the trust of the participants in collaborating with the cobot.' Therefore, 'Robot-interface-design' is also listed as a factor that impacted trust. There were no other factors manipulated, and no factors that were manipulated that did not impact trust.",10.1016/j.procir.2018.01.009,https://linkinghub.elsevier.com/retrieve/pii/S2212827118300155,"In a global, c-commerce marketplace, product customisation is driven towards manufacturing flexibility. Conventional caged robots are designed for high volume and low mix production cannot always comply with the increasing low volume and high customisation requirements. In this scenario, the interest in collaborative robots is growing. A critical aspect of Human-Robot Collaboration (HRC) is human trust in robots. This research focuses on increasing the human confidence and trust in robots by designing an Augmented Reality (AR) interface for HRC. The variable affecting the trust involved in HRC have been estimated. These have been utilised for designing the AR-HRC. The proposed design aims to provide situational awareness and spatial dialog. The AR-HRC developed has been tested on 15 participants which have performed a ""pick-and-place"" task. The results show that the utilisation of AR in the proposed scenario positively affects the human trust in robot. The human-robot collaboration enhanced by AR are more natural and effective. The trust has been measured through an empirical psychometric method also presented in this paper."
"Palmer, Samson; Richards, Dale; Shelton-Rayner, Graham; Izzetoglu, Kurtulus; Inch, David",Assessing Variable Levels of Delegated Control – A Novel Measure of Trust,2020,1,23,23,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants supervised unmanned vehicles via a Ground Control Station (GCS) with varying levels of autonomous support. fNIRS data was recorded throughout the study, with a 5-min baseline recorded before the task. Participants completed several scenarios with different GCS types, including Assisted Manual Mode (AMM), Assisted Autonomous Mode (AAM), and two Fully Autonomous Modes (FAM), one of which was flawed. System exposures were counterbalanced.","Participants were tasked with defending military bases using unmanned vehicles, responding to various scenarios, and interrogating unknown signals. They used command systems with varying levels of autonomous support to protect the bases from enemy units, escort civilians, or escort VIPs.",Unspecified,Unmanned Aerial Vehicles (UAVs); Unmanned Ground Vehicles (UGVs),Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the system through a GCS interface, with no direct physical contact with the robots.",simulation,"The interaction took place through a simulated GCS interface, without physical embodiment of the robots.",simulated,"The robots were represented through a simulated interface, without any physical presence.",shared control (fixed rules),"The robots operated with varying levels of autonomy, including assisted modes with fixed rules and fully autonomous modes.",Physiological Measures,,Physiological Signals,Trust was assessed through fNIRS measurements of prefrontal cortical activity.,no modeling,"The study did not use computational modeling of trust, focusing on statistical analysis of physiological data.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers manipulated the level of autonomy of the GCS and introduced a flawed autonomous system to influence trust.,"The flawed autonomous system led to increased activity in the ventrolateral prefrontal cortex, suggesting distrust. The assisted manual task also showed increased activity in the same region, suggesting uncertainty in decision making.","The study found that the anterior prefrontal cortex (aPFC) is associated with uncertainty of the decision-making abilities of an autonomous system, while the ventrolateral prefrontal cortex (vlPFC) is associated with the development of distrust as a result of poor decision making.","The study found that the anterior prefrontal cortex (aPFC) is associated with uncertainty of the decision-making abilities of an autonomous system, while the ventrolateral prefrontal cortex (vlPFC) is associated with the development of distrust as a result of poor decision making.","Participants monitored a GCS interface, using unmanned vehicles to defend military bases, respond to scenarios, and interrogate unknown signals. The robots were simulated and performed actions based on the GCS mode, with varying levels of autonomy.",t-test,"Independent T-tests were used to compare the mean OXY (oxygenated hemoglobin to deoxygenated hemoglobin) values for each optode grouping (hemispheres, groups of four, and pairs) across different task conditions (Assisted Manual, Assisted Auto, Full Auto Incorrect) against the baseline. This was done to determine if there were significant differences in prefrontal cortical activity, as measured by fNIRS, between the different levels of autonomy and the baseline.",TRUE,Robot-autonomy; Robot-accuracy,Robot-autonomy; Robot-accuracy,,"The study manipulated the level of autonomy of the Ground Control Station (GCS), which is categorized as 'Robot-autonomy'. The GCS had four modes: Assisted Manual Mode (AMM), Assisted Autonomous Mode (AAM), and two Fully Autonomous Modes (FAM). The FAM modes differed in their decision-making approach, with one being flawed and leading to mission failure, which is categorized as 'Robot-accuracy'. The paper explicitly states that the flawed autonomous system was designed to elicit uncertainty and distrust, and the results showed increased activity in the vlPFC, which is associated with distrust. The different levels of autonomy and the flawed system directly impacted trust levels, as evidenced by the changes in brain activity. The study did not explicitly manipulate any other factors from the provided list.",,http://link.springer.com/10.1007/978-3-030-60128-7_16,"Autonomous cars are set to drastically change the driving environment. The promise of a safer and more efﬁcient driving experience has led to a signiﬁcant rise in research surrounding human interaction with autonomous systems, however we must investigate ways to effectively integrate these systems and develop the partnership between human and autonomous system. In particular, understanding the nature of human-automation trust will ensure safe and efﬁcient integration of these systems, and therefore investing in new measures of trust is key to the development of the human-automation partnership. This paper discusses ﬁndings of an experiment that examines the nature of human-automation interaction and the neural correlates associated with trust. Participants were asked to interact with unmanned vehicle control stations of varying levels of control and integrity, whilst prefrontal cortical activity was monitored using functional Near Infrared spectroscopy. The ﬁndings of this study suggest that the anterior prefrontal cortex (aPFC) is associated with uncertainty of the decision-making abilities of an autonomous system, whilst the ventrolateral prefrontal cortex (vlPFC) has been implicated in the development of distrust as a result of poor decision making. The ﬁndings present a new opportunity to develop a reliable measure of humanautomation trust that could inform future system design and facilitate a safer and more effective human automation partnership."
"Pan, Ye; Steed, Anthony","A Comparison of Avatar-, Video-, and Robot-Mediated Interaction on Users’ Trust in Expertise",2016,1,24,24,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants answered 30 general knowledge questions, seeking advice from one of two advisors (robot, avatar, or video) for each question. They then answered a final high-stakes question and completed a post-experimental questionnaire.",Participants were asked to answer difficult general-knowledge questions and seek advice from one of two advisors presented via different media.,RoboThespian,Humanoid Robots,Research,Social,Social Guidance/Coaching,minimal interaction,"Participants observed and interacted with advisors through video, avatar, or robot representations.",media,The interaction was presented through video clips of the advisors.,physical,"The robot was a physical entity, while the avatar and video were presented on a screen.",pre-programmed (non-adaptive),The robot's actions were pre-recorded and did not adapt to the user.,Behavioral Measures; Questionnaires,,,Trust was measured using advice-seeking behavior and a post-experimental questionnaire.,no modeling,The study did not use computational models of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the expertise of the advisors and the media through which they were presented (robot, avatar, video) to influence trust.","Participants generally trusted the expert advisor more, but the media representation influenced their choices, with avatars being less trusted than robots or video.","Participants showed a bias against the avatar, even when it was the expert, and a preference for video, which sometimes led them to choose non-expert advice. There was no significant difference in advice-seeking behavior between video and robot.","Media representation significantly impacts trust formation, with avatars being less trusted than robots or video, even when the avatar is an expert.","The robot, avatar, and video presented pre-recorded advice to participants, who then chose which advisor to follow for each question.",ANOVA; Tukey HSD; t-test; Shapiro-Wilk; Levene's test; mauchly's test,"The study used a mixed-design ANOVA to analyze the mean expert advice-seeking rate over time, with experimental conditions as a between-subjects factor and time as a within-subjects factor. Post-hoc comparisons were performed using the Tukey HSD test. One-sample t-tests were used to compare non-expert advice-seeking rates against a value of 0.5. Paired samples t-tests were used to compare self-reported trust scores between expert and non-expert advisors within each experimental condition. Shapiro-Wilk's test was used to assess normality, Levene's test for homogeneity of variances, and Mauchly's test for sphericity.",TRUE,Robot-nonverbal-communication; Robot-accuracy,Robot-nonverbal-communication,Robot-accuracy,"The study manipulated the media through which advice was presented (robot, avatar, video), which directly influenced the nonverbal cues available to participants. This is categorized as 'Robot-nonverbal-communication' because the different media (robot, avatar, video) inherently alter the presentation of nonverbal cues such as body language, facial expressions, and gaze. The study also manipulated the accuracy of the advice given by the advisors, with one being an expert and the other a non-expert. This is categorized as 'Robot-accuracy' because it directly impacts the success rate of the task. The study found that the media representation (nonverbal cues) significantly impacted trust, with avatars being less trusted than robots or video, even when the avatar was an expert. This is why 'Robot-nonverbal-communication' is listed as a factor that impacted trust. While the accuracy of the advice was manipulated, the study found that participants were generally able to identify the expert advisor, but their choices were biased by the media representation. This indicates that the accuracy manipulation did not impact trust as much as the media representation, and thus 'Robot-accuracy' is listed as a factor that did not impact trust.",10.3389/frobt.2016.00012,http://journal.frontiersin.org/Article/10.3389/frobt.2016.00012/abstract,"Communication technologies are becoming increasingly diverse in form and functionality. A central concern is the ability to detect whether others are trustworthy. Judgments of trustworthiness rely, in part, on assessments of non-verbal cues, which are affected by media representations. In this research, we compared trust formation on three media representations. We presented 24 participants with advisors represented by two of the three alternate formats: video, avatar, or robot. Unknown to the participants, one was an expert, and the other was a non-expert. We observed participants’ advice-seeking behavior under risk as an indicator of their trust in the advisor. We found that most participants preferred seeking advice from the expert, but we also found a tendency for seeking robot or video advice. Avatar advice, in contrast, was more rarely sought. Users’ self-reports support these findings. These results suggest that when users make trust assessments, the physical presence of the robot representation might compensate for the lack of identity cues."
"Pan, Jiahe; Eden, Jonathan; Oetomo, Denny; Johal, Wafa",Effects of Shared Control on Cognitive Load and Trust in Teleoperated Trajectory Tracking,2024,1,24,24,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a teleoperation task with varying levels of robot autonomy. They completed a training phase followed by a main phase, with each phase consisting of multiple rounds of trials. During each trial, participants performed a primary trajectory tracking task and a secondary rhythmic tapping task. Questionnaires were administered after each round to assess cognitive load and trust.",Participants teleoperated a robot arm to track a 3D trajectory while simultaneously performing a rhythmic tapping task.,Franka Emika Panda,Industrial Robot Arms; Collaborative Robots,Research,Manipulation,Remote Manipulation,minimal interaction,"Participants interacted with the robot through a teleoperation interface, with no direct physical contact.",simulation,Participants interacted with a virtual rendering of the robot and its environment on a computer screen.,physical,"The study used a physical robot, but participants interacted with it through a virtual interface.",shared control (fixed rules),The robot's autonomy was varied using a blending control scheme with fixed parameters.,Questionnaires; Multidimensional Measures,Multi-Dimensional Measure of Trust (MDMT),Performance Metrics,"Trust was assessed using the MDMT questionnaire and a single-scale trust question, with performance metrics also collected.","parametric models (e.g., regression)","Multiple regression was used to analyze the relationship between cognitive load, autonomy, and trust.",Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's autonomy level was directly manipulated using a blending control scheme, which influenced the robot's performance and the perceived task difficulty.","Trust increased with higher robot autonomy, which was also associated with better task performance.","The subjective cognitive load measure (NASA-TLX) showed a decrease with higher autonomy, while objective measures (tapping error and pupil diameter) did not show a clear effect. There was a potential trend of lower trust with higher cognitive load under high autonomy, and constant trust under low autonomy.",Participants' trust in the robot increased with higher levels of robot autonomy during a teleoperation task.,"The robot tracked a 3D trajectory, and the human teleoperated the robot using a haptic controller to follow the same trajectory while also performing a rhythmic tapping task.",ANOVA; t-test; Pearson correlation; Linear regression,"The study used repeated-measures ANOVA to examine the effects of robot autonomy on perceived autonomy, cognitive load (NASA-TLX, tapping error, pupil diameter), and trust (MDMT, single-scale trust). A paired t-test was used to compare tracking performance in baseline and no autonomy rounds. Pearson correlation analysis was used to assess the relationship between MDMT and single-scale trust, and between MDMT and trajectory tracking error. Multiple regression was used to examine the collective effect of cognitive load, autonomy, and trust, and to test for an interaction between cognitive load and autonomy on trust.",TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,,"The study explicitly manipulated 'Robot-autonomy' by varying the level of shared control between the human operator and the robot's autonomous controller using a blending control scheme (Equation 3). The autonomy level, represented by γ, ranged from 0 (complete human control) to 1 (complete robot control). This is described in section III-B: 'The scalar γ represents the level of robot autonomy, where γ = 1 corresponds to complete robot control (full autonomy) and γ = 0 corresponds to complete human control (no autonomy). By altering γ the relative autonomy between the human and robot was varied.' The study also implicitly manipulated 'Task-complexity' through the dual-task paradigm, where participants had to perform a primary trajectory tracking task and a secondary rhythmic tapping task concurrently. This is described in section III: 'In addition to this primary task, we employed a dual-task methodology to measure cognitive load, with a secondary rhythmic tapping task.' The results showed that 'Robot-autonomy' impacted trust levels, as stated in section IV-C: 'The MDMT (Fig. 4(b)) averaged across the Reliable and Capable sub-categories (Cronbach's α = .968) indicated a clear autonomy effect (F (1, 22) = 41.418, p < .001, η 2 = .248). Similarly, the autonomy level was also observed to affect the single-scale trust measure (F (1, 22) = 33.169, p < .001, η 2 = .234).'",10.1109/LRA.2024.3396111,https://ieeexplore.ieee.org/document/10517390/,"Teleoperation is increasingly recognized as a viable solution for deploying robots in hazardous environments. Controlling a robot to perform a complex or demanding task may overload operators resulting in poor performance. To design a robot controller to assist the human in executing such challenging tasks, a comprehensive understanding of the interplay between the robot’s autonomous behavior and the operator’s internal state is essential. In this letter, we investigate the relationships between robot autonomy and both the human user’s cognitive load and trust levels, and the potential existence of three-way interactions in the robot-assisted execution of the task. Our user study (N = 24) results indicate that while the autonomy level inﬂuences the teleoperator’s perceived cognitive load and trust, there is no clear interaction between these factors. Instead, these elements appear to operate independently, thus highlighting the need to consider both cognitive load and trust as distinct but interrelated factors in varying the robot autonomy level in shared-control settings. This insight is crucial for the development of more effective and adaptable assistive robotic systems."
"Panchetti, Tommaso; Pietrantoni, Luca; Puzzo, Gabriele; Gualtieri, Luca; Fraboni, Federico","Assessing the Relationship between Cognitive Workload, Workstation Design, User Acceptance and Trust in Collaborative Robots",2023,1,14,14,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a training session without the robot, then performed an assembly task in three sequential scenarios with increasing levels of robot speed, autonomy, and notifications. Questionnaires were administered before the experiment and between each scenario. Gaze behavior was measured throughout the experiment.",Participants worked with a collaborative robot to build a simplified version of a pneumatic cylinder.,UR3,Collaborative Robots (Cobots); Industrial Robot Arms,Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically interacted with the robot to complete an assembly task.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,shared control (fixed rules),"The robot's behavior was pre-programmed with fixed rules, and the human and robot shared control over the task.",Questionnaires,Trust in Industrial Human-Robot Interaction questionnaire,Eye-tracking Data,Trust was measured using a questionnaire and eye-tracking data was collected.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers manipulated robot speed, autonomy, trajectories, user commands, notifications, and safety training across three scenarios to influence cognitive workload and trust.",The study found no significant evidence that the manipulations of the robot's features influenced trust.,"The study found a strong negative correlation between subjective and objective measures of cognitive workload in the first trial, but no correlation in the subsequent trials. The study also found that acceptance predicted perceived stress, but not cognitive workload. There was no significant moderating effect of trust on the relationship between cognitive workload and perceived stress.","Manipulating features of the workstation and robot can reduce cognitive workload, and user acceptance is associated with perceived stress in collaborative tasks.","The robot picked up and released components for the assembly of a pneumatic cylinder, while the human assembled the components using a screwdriver and interacted with the robot through commands and notifications.",Friedman test; Wilcoxon signed-rank test; Spearman correlation; Linear regression; Moderation analysis,"The study used Friedman's test to compare cognitive workload scores across three scenarios. Wilcoxon signed-rank tests with Bonferroni correction were used for post-hoc analysis of eye-tracking data (number of fixations and fixation duration). Spearman correlation analysis was used to assess the relationship between subjective (NASA-TLX) and objective (eye-tracking) measures of cognitive workload. Linear regression analysis was used to test if acceptance predicted cognitive workload and perceived stress. Finally, moderated regression analysis was used to investigate the moderating effect of trust on the relationship between cognitive workload and perceived stress.",TRUE,Robot-autonomy; Robot-accuracy; Robot-interface-design; Task-complexity,,Robot-autonomy; Robot-accuracy; Robot-interface-design; Task-complexity,"The study manipulated several factors across three scenarios. 'Robot-autonomy' was manipulated by increasing the robot's level of autonomy in each scenario, as stated in the paper: 'The guidelines were gradually implemented across the scenarios, with increasing levels of robot speed, autonomy, trajectories, user commands, notifications to the users and safety training measures.' 'Robot-accuracy' was manipulated through changes in robot speed and trajectories, which would affect the robot's performance in the task. 'Robot-interface-design' was manipulated by changing the user commands and notifications displayed on the LCD screen. 'Task-complexity' was manipulated by increasing the number of steps and cognitive demands in each scenario. The paper explicitly states that 'no significant evidence was found in the three different trials, meaning that we found no support that trust towards robots could influence how cognitive workload and perceived stress were related to each other.' This indicates that none of the manipulated factors had a significant impact on trust.",10.3390/app13031720,https://www.mdpi.com/2076-3417/13/3/1720,"Collaborative robots are revolutionising the manufacturing industry and the way workers perform their tasks. When designing shared workspaces between robots and humans, human factors and ergonomics are often overlooked. This study assessed the relationship between cognitive workload, workstation design, user acceptance and trust in collaborative robots. We combined subjective and objective data to evaluate the cognitive workload during an assembly task in three different scenarios in which we manipulated various features of the workstation and interaction modalities. Our results showed that participants experienced a reduction in cognitive workload in each of the three trials, indicating an improvement in cognitive performance. Additionally, we found that user acceptance predicted perceived stress across the trials but did not signiﬁcantly impact the cognitive workload. Trust was not found to moderate the relationship between cognitive workload and perceived stress. This study has the potential to make a signiﬁcant contribution to the ﬁeld of collaborative assembly systems by providing valuable insights and helping to bridge the gap between researchers and practitioners. This study can potentially impact companies looking to improve safety, productivity and efﬁciency."
"Pang, Yijiang; Huang, Chao; Liu, Rui",Synthesized Trust Learning from Limited Human Feedback for Human-Load-Reduced Multi-Robot Deployments,2021,1,10,10,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a tutorial with video examples, then observed a series of sequential and paired task trajectories, providing trust feedback after each.",Participants observed simulated multi-robot search and rescue tasks and provided trust ratings and preferences for different robot trajectories.,Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,passive observation,Participants observed robot behavior through a customized interface.,simulation,Participants viewed a simulated environment of a city with robots performing search and rescue tasks.,simulated,The robots were represented as virtual entities within the simulation.,pre-programmed (non-adaptive),The robots followed pre-programmed paths to target locations.,Custom Scales,,Performance Metrics,Trust was measured using discrete trust level ratings and preference comparisons between trajectories.,"deep learning (e.g., neural networks, reinforcement learning)",A deep neural network was used to model trust based on trajectory features and human feedback.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's trajectory features (velocity, formation, heading direction variance) were manipulated to influence task performance and difficulty, which in turn affected trust.","The study showed that the model trained with active learning and heterogeneous data (trust level and preference) achieved higher accuracy in predicting trust, indicating that these manipulations influenced trust outcomes.","The active learning method improved model performance with less data, and the heterogeneous data learning (trust level and preference) further improved model performance.","The Synthesized Trust Learning (STL) method, which combines active learning and heterogeneous data, effectively models human trust in multi-robot systems, reducing the need for human feedback and improving prediction accuracy.","The robots autonomously flew to target locations, and the human participants monitored the robot's performance through an interface, providing trust ratings and preferences for different trajectories.",Kolmogorov-Smirnov,The study used a two-dimensional Kolmogorov-Smirnov test to measure the difference in data distribution between raw trajectory features and generated trajectory features by the active learning method. This test was used to validate the effectiveness of the active learning method in enriching data diversity.,TRUE,Robot-accuracy; Robot-task-strategy,Robot-accuracy; Robot-task-strategy,,"The study manipulated the robot's trajectory features (velocity, formation, heading direction variance) to influence task performance and difficulty. These manipulations directly affected the robot's accuracy in reaching target locations and the strategy it employed to do so. The paper states, 'In each task, one or two target locations were randomly generated... and another parameters, ψ c , were randomly generated in the preset range to control the above-mentioned three features of the robot team.' This indicates a direct manipulation of robot behavior that impacts both the success of the task (accuracy) and the way the task is carried out (strategy). The paper also states, 'The robot team was initialized to a same starting location then flew to the assigned target locations automatically supported by an integrated path planning algorithm.' This shows that the robots were not simply moving randomly, but following a strategy, which was manipulated by the researchers. The study found that these manipulations influenced trust outcomes, as the model trained with active learning and heterogeneous data achieved higher accuracy in predicting trust. Therefore, both 'Robot-accuracy' and 'Robot-task-strategy' are listed as factors that impacted trust.",10.1109/RO-MAN50785.2021.9515509,https://ieeexplore.ieee.org/document/9515509/,"Human multi-robot system (MRS) collaboration is demonstrating potentials in wide application scenarios due to the integration of human cognitive skills and a robot team’s powerful capability introduced by its multi-member structure. However, due to limited human cognitive capability, a human cannot simultaneously monitor multiple robots and identify the abnormal ones, largely limiting the efﬁciency of the humanMRS collaboration. There is an urgent need to proactively reduce unnecessary human engagements and further reduce human cognitive loads. Human trust in human MRS collaboration reveals human expectations on robot performance. Based on trust estimation, the work between a human and MRS will be reallocated that an MRS will self-monitor and only request human guidance in critical situations. Inspired by that, a novel Synthesized Trust Learning (STL) method was developed to model human trust in the collaboration. STL explores two aspects of human trust (trust level and trust preference), meanwhile accelerates the convergence speed by integrating active learning to reduce human workload. To validate the effectiveness of the method, tasks ”searching victims in the context of city rescue” were designed in an open-world simulation environment, and a user study with 10 volunteers was conducted to generate real human trust feedback. The results showed that by maximally utilizing human feedback, the STL achieved higher accuracy in trust modeling with a few human feedback, effectively reducing human interventions needed for modeling an accurate trust, therefore reducing human cognitive load in the collaboration."
"Pang, Yijiang; Liu, Rui",Trust-Aware Emergency Response for A Resilient Human-Swarm Cooperative System,2021,1,50,50,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants completed a tutorial, then watched videos of robot swarm performance in two task scenarios with four randomized conditions each. After each video, they answered questions about fault detection, robot identification, and trust levels.","Participants watched videos of a robot swarm performing two tasks: transitioning between area inspection tasks and responding to an emergent target (car accident). They then rated the performance of the swarm and individual robots, and identified any faults.",Unspecified,Unmanned Aerial Vehicles (UAVs); Swarm Robots,Research,Supervision,Monitoring,passive observation,Participants watched videos of the robot swarm.,media,Participants observed the robot swarm through video recordings.,simulated,The robots were simulated in a virtual environment.,shared control (fixed rules),"The robot swarm's behavior was controlled by a fixed algorithm, with some influence from the trust-aware control method.",Questionnaires,,Video Data; Performance Metrics,"Trust was measured using a questionnaire with a 5-point scale, and performance metrics were also collected.","parametric models (e.g., regression)",The study used statistical analysis (Mann-Whitney U test) to analyze the effect of experimental factors on human trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the robot swarm's behavior by applying a trust-aware control method (Trust-R) to correct faulty behaviors, which influenced the swarm's performance and autonomy.","The trust-aware control method (Trust-R) significantly increased trust levels compared to the traditional control method, which did not correct faulty behaviors. The trust was effectively restored when faulty behavior was corrected.","The severity of faults did not affect the proportion of human volunteers in reporting faulty behaviors in the first scenario, but it did affect the chance that they were reported in the second scenario. In the second scenario, the severity of faults also affected trust ratings in the faulty condition, but not in the repaired condition.",The trust-aware reflective control method (Trust-R) effectively restored human trust in a robot swarm by correcting faulty behaviors during emergency response tasks.,"The robot swarm performed area inspection and emergency response tasks, while the human participant monitored the swarm's performance and rated their trust in the robots.",Mann-Whitney U,"The Mann-Whitney U test was used to analyze the effect of experimental factors on human trust. Specifically, it was used to compare trust levels between different conditions (e.g., faulty vs. repaired) and to assess the impact of fault severity on fault reporting and trust ratings.",TRUE,Robot-accuracy; Robot-autonomy,Robot-accuracy,,"The study manipulated 'Robot-accuracy' by introducing a faulty robot with degraded motor issues, which directly impacted the swarm's performance (e.g., deviation from the expected trajectory, speed, and spatial distribution). The Trust-R method was then applied to correct these faulty behaviors, thus manipulating the accuracy of the robot swarm's performance. The study also manipulated 'Robot-autonomy' by implementing the Trust-R method, which allowed the swarm to adjust its behavior based on estimated trust levels, effectively changing the level of autonomy in the swarm's response. The results showed that the 'Robot-accuracy' significantly impacted trust levels, with higher trust reported when the Trust-R method corrected faulty behaviors. The severity of the fault (motor degradation level) also impacted the chance of reporting a fault and trust ratings in the second scenario, but not in the first scenario. There was no factor that was manipulated that did not impact trust.",10.1109/SSRR53300.2021.9597682,https://ieeexplore.ieee.org/document/9597682/,"A human-swarm cooperative system, which mixes multiple robots and a human supervisor to form a mission team, has been widely used for emergent scenarios such as criminal tracking and victim assistance. These scenarios are related to human safety and require a robot team to quickly transit from the current undergoing task into the new emergent task. This sudden mission change brings difﬁculty in robot motion adjustment and increases the risk of performance degradation of the swarm. Trust in human-human collaboration reﬂects a general expectation of the collaboration; based on the trust humans mutually adjust their behaviors for better teamwork. Inspired by this, in this research, a trust-aware reﬂective control (Trust-R), was developed for a robot swarm to understand the collaborative mission and calibrate its motions accordingly for better emergency response. Typical emergent tasks “transit between area inspection tasks”, “response to emergent target – car accident” in social security with eight fault-related situations were designed to simulate robot deployments. A human user study with 50 volunteers was conducted to model trust and assess swarm performance. Trust-R’s effectiveness in supporting a robot team for emergency response was validated by improved task performance and increased trust scores."
"Paradeda, Raul Benites; Hashemian, Mojgan; Rodrigues, Rafael Afonso; Paiva, Ana",How Facial Expressions and Small Talk May Influence Trust in a Robot,2016,1,42,41,1,1 participant was excluded because their native language was English,Controlled Lab Environment,between-subjects,"Participants filled a pre-questionnaire, interacted with a robot in one of four scenarios (small talk/no small talk, sad/joyful expression), and then filled a post-questionnaire. The robot told a story and asked for donations.",Participants interacted with a robot that told a story and asked for donations.,Emys,Expressive Robots,Social; Research,Social,Persuasion,minimal interaction,Participants had verbal interaction with the robot.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,The robot was a physical entity.,pre-programmed (non-adaptive),The robot followed a pre-set script and did not adapt to the user.,Questionnaires,,,Trust was measured using pre and post questionnaires.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's facial expression (sad or joyful) and the presence of small talk were manipulated to influence trust.,"Small talk enhanced trust, especially when combined with a sad facial expression. Facial expression alone did not significantly impact trust.","The robot's joyful facial expression was not clearly perceived by participants, which may have affected the results related to the second hypothesis. The donation amounts were higher when the robot performed small talk, but this was not statistically significant.","Starting a conversation with small talk enhances the level of trust in a robot, especially when combined with a sad facial expression.","The robot tells a story and asks for donations, while the human listens and decides whether to donate. The robot also engages in small talk in some conditions.",t-test; Mann-Whitney U; Kolmogorov-Smirnov; Kruskal-Wallis,"The study used a variety of statistical tests to analyze the impact of small talk and facial expressions on trust. T-tests were used to compare pre- and post-questionnaire data and to compare trust levels between groups. U Mann-Whitney tests were used to compare groups with non-normal distributions, specifically to assess the impact of small talk within facial expression conditions and donation amounts. Kolmogorov-Smirnov tests were used to check for differences in the distribution of trust levels. Kruskal-Wallis test was used to compare the distribution of trust levels across different facial expression conditions. These tests were used to determine if the manipulated variables had a statistically significant effect on the level of trust reported by participants.",TRUE,Robot-emotional-display; Robot-social-attitude,Robot-social-attitude,Robot-emotional-display,"The study manipulated the robot's facial expression (sad or joyful), which falls under 'Robot-emotional-display'. The study also manipulated whether the robot engaged in small talk before the story, which is categorized as 'Robot-social-attitude' because it changes the social approach of the robot. The results showed that small talk (Robot-social-attitude) significantly impacted trust, while the facial expression (Robot-emotional-display) did not have a significant impact on trust, although it was intended to. The paper states, 'The results showed the highest level of trust gained when the robot starts with small talk and expresses facial expression in the same direction of storytelling expected emotion.' and 'The most significant difference happens in the case of small talk together with the sad face.' and 'Although small talk seems to play a vital role in trust, however, it was not clear in the case when the robot expresses the joyful facial expression.' and 'Hence, hypothesis 2 could not be evaluated well in the current setting.' This indicates that the small talk manipulation had an impact on trust, while the facial expression manipulation did not have a significant impact.",,http://link.springer.com/10.1007/978-3-319-47437-3_17,"In this study, we address the level of trust that a human being displays during an interaction with a robot under diﬀerent circumstances. The inﬂuencing factors considered are the facial expressions of a robot during the interactions, as well as the ability of making small talk. To examine these inﬂuences, we ran an experiment in which a robot tells a story to a participant, and then asks for help in form of donations. The experiment was implemented in four diﬀerent scenarios in order to examine the two inﬂuencing factors on trust. The results showed the highest level of trust gained when the robot starts with small talk and expresses facial expression in the same direction of storytelling expected emotion."
"Park, Eui; Jenkins, Quaneisha; Jiang, Xiaochun",MEASURING TRUST OF HUMAN OPERATORS IN NEW GENERATION RESCUE ROBOTS,2008,1,20,20,0,No participants were excluded,Survey/Interview,,"Participants were briefed on the experiment, provided informed consent, given a USAR scenario, and asked to complete a questionnaire containing the trust measurement instrument.",Participants completed a questionnaire based on a provided USAR scenario.,Unspecified,Mobile Robots,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read a description of a USAR scenario.,media,The interaction was based on a text-based scenario.,hypothetical,The robot was only described in a text-based scenario.,not autonomous,The robot's actions were only described in a text-based scenario.,Questionnaires,,,Trust was measured using a questionnaire.,"parametric models (e.g., regression)",A stepwise regression model will be used to develop a trust model.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,The study assessed trust without intentionally altering any specific factors.,,,The study aims to develop an instrument to measure trust in human-robot interaction for urban search and rescue missions.,The robot is described as a compact rescue crawler used in USAR missions. The human participant reads a scenario and completes a questionnaire.,Pearson correlation; stepwise regression model,"The study will use descriptive statistics (mean and standard deviation) and inferential statistics, specifically correlation analysis, to analyze the data collected from the questionnaires. A stepwise regression model will then be used to develop a trust model and identify the important dimensions of HRI.",FALSE,,,,"The study did not manipulate any factors. The participants were given a USAR scenario and asked to complete a questionnaire. The study focused on developing a trust measurement instrument rather than testing the impact of specific factors on trust. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust.",10.5739/isfp.2008.489,http://joi.jlc.jst.go.jp/JST.Journalarchive/isfp1989/2008.489?from=CrossRef,"The utilization of mobile rescue robots in dynamic environments decreases the risk to emergency personnel in the field. Recent efforts to improve rescue robot design by using new fluid-power technology provide opportunities of studying the changes in metrics of human-robot interaction (HRI), such as trust. Trust is one of the most critical factors in urban search and rescue missions because it can impact the decisions human make in uncertain conditions. This research is to develop an instrument that can be used to measure trust in human-robotic interaction, which will allow us to collect data for building a quantitative model of trust in HRI. As the first step in this effort, a pilot study was conducted to determine the validity of an instrument to measure the appropriate dimensions of trust in this new human-robot system."
"Park, Corey; Shahrdar, Shervin; Nojoumian, Mehrdad",EEG-Based Classification of Emotional State Using an Autonomous Vehicle Simulator,2018,1,1,1,0,No participants were excluded,Controlled Lab Environment,within-subjects,"A single participant experienced two driving scenarios in a simulator while EEG data was collected. Scenario 1 involved smooth driving, and Scenario 2 involved erratic driving. The beta to alpha power ratio was calculated for each scenario.",Participants experienced two driving scenarios in a simulator while EEG data was collected.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,The participant experienced a simulated driving scenario with no direct physical interaction with the vehicle.,simulation,The participant was immersed in a virtual reality driving simulation.,simulated,The robot was a simulated autonomous vehicle in a virtual environment.,pre-programmed (non-adaptive),The autonomous vehicle followed pre-programmed driving scenarios without adapting to the participant.,Physiological Measures,,Physiological Signals,Trust was assessed using the beta to alpha power ratio from EEG data.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The driving behavior of the autonomous vehicle was manipulated to induce different emotional states in the participant, with one scenario being smooth and the other erratic.","The study did not directly measure trust, but the results showed that the erratic driving scenario elicited a higher beta to alpha power ratio, indicating a negative emotional response.","The study found that the beta to alpha power ratio increased during the erratic driving scenario, which was expected. Spikes in the ratio were correlated with specific events in the simulation, such as running a stop sign.","The beta to alpha power ratio is an effective indicator of emotional state in a driving simulator, with a higher ratio indicating a negative emotional response.","The participant was passively observing the autonomous vehicle in a driving simulator. The robot drove in two different scenarios, one smooth and one erratic. The participant's EEG data was recorded during the simulation.",,"The study calculated the beta to alpha power ratio from EEG data for two different driving scenarios. While the study compares the average beta to alpha power ratio between the two scenarios, no formal statistical tests were explicitly mentioned or used to compare the means. The analysis focused on observing the difference in the ratio between the two scenarios and correlating spikes in the ratio with specific events in the simulation.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the driving behavior of the autonomous vehicle, with one scenario involving smooth driving and the other involving erratic driving, including running a stop sign and nearly colliding with another vehicle. This manipulation directly impacts the robot's accuracy in following traffic rules and maintaining safe driving behavior, which is a key factor in how a user would perceive the robot's reliability and thus their trust in it. The paper states, 'In the first scenario (Scenario 1), the SDC performed smooth highway driving... In the second scenario (Scenario 2), the SDC drove erratically around a residential neighborhood violating common rules of the road. The SDC ran through a stop sign and nearly collided with another vehicle.' The change in driving behavior directly influences the robot's performance on the task of driving safely, thus 'Robot-accuracy' is the most appropriate category. The study found that the erratic driving scenario elicited a higher beta to alpha power ratio, indicating a negative emotional response, which is linked to a decrease in trust. Therefore, 'Robot-accuracy' is also listed as a factor that impacted trust.",10.1109/SAM.2018.8448945,https://ieeexplore.ieee.org/document/8448945/,"Societal acceptance of self-driving cars (SDC) is predicated on a level of trust between humans and the autonomous vehicle. Although the performance of SDCs has improved dramatically, the question of mainstream acceptance and requisite trust is still open. We are exploring this question through integration of virtual reality SDC simulator and an electroencephalographic (EEG) recorder. In order for a passenger to build and maintain trust, the SDC will need to operate in a manner that elicits positive emotional response and avoids negative emotional response. In our experiment, a test subject was exposed to scenarios designed to induce positive and negative emotional responses, quantiﬁed by the EEG beta wave to alpha wave power ratio. As predicted, an increase in the beta to alpha power ratio was observed when the test subject was exposed to stress inducing situations inside the SDC simulator. Our results are expected to inform the design and operation of an EEG-based supervisory feedback control module or artiﬁcial intelligence (AI) that monitors the emotional state of passengers and adjusts the AI control parameters accordingly."
"Park, Sangwon",Multifaceted trust in tourism service robots,2020,2,202,202,0,No participants were excluded,Online Crowdsourcing,,Participants viewed images and a video of a Pepper robot serving a customer and then completed an online questionnaire.,Participants were asked to complete a questionnaire regarding their trust in service robots after viewing images and a video of a robot.,Pepper,Humanoid Robots; Expressive Robots,Social; Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants observed the robot through images and a video.,media,Participants viewed a video of the robot interacting with a person.,physical,Participants viewed images and a video of a physical robot.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires; Multidimensional Measures,,,Trust was measured using a questionnaire with multiple dimensions.,"parametric models (e.g., regression)",The study used partial least squares analysis to test the relationships between trust dimensions.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,"The study did not manipulate any specific factors related to trust, but measured trust after participants viewed a video of a robot.",,"The performance construct was the most critical dimension to define trust in service robots, followed by purpose and process.",The study identified a higher-order formative construct of trust in service robots with the highest importance for a performance construct.,"The robot was shown serving a customer at a restaurant, and the human participant completed a questionnaire about their trust in the robot.",Partial least squares; Multilevel Model; multitrait-multimethod analysis; redundancy analysis; harman's single-factor test; Factor analysis,"The study used partial least squares analysis to test a higher-order formative model of trust, examining the relationships between the dimensions of performance, process, and purpose. Confirmatory factor analysis was used to validate the measurement of first-order constructs. Multitrait-multimethod analysis and heterotrait-monotrait ratio of correlations were used to assess the validity of the formative constructs. Redundancy analysis was used to assess the quality of the formative measurement model. Harman's single-factor test and exploratory factor analysis were used to test for common method bias. The study also examined the relationship between trust, perceived risk, and behavioral intention.",FALSE,,,,"The study did not manipulate any factors. Participants were shown images and a video of a Pepper robot serving a customer and then completed a questionnaire. There was no manipulation of any kind, so no factors are listed.",10.1016/j.annals.2020.102888,https://linkinghub.elsevier.com/retrieve/pii/S0160738320300323,"In recognizing the increase in the use of service robots by service industries, identifying the structure of trust in intelligent robots is crucial for tourism studies. This paper ﬁrst proposes a model of multifaceted trust in service robots comprised of three constructs – performance, process, and purpose – and, second, tests the trust model that considers institution-based trust, trusting belief, and intention. As a result, this paper identiﬁed a higher-order formative construct of trust in service robots with the highest importance for a performance construct (Study 1). The antecedents of the multifaceted trust in tourism service robots are then identiﬁed (Study 2). This study provides important theoretical and methodological contributions to the ﬁelds of information technology and tourism."
"Park, Sangwon",Multifaceted trust in tourism service robots,2020,2,406,406,0,No participants were excluded,Online Crowdsourcing,,Participants viewed images and a video of an NAO robot at a hotel and then completed an online questionnaire.,Participants were asked to complete a questionnaire regarding their trust in service robots after viewing images and a video of a robot.,Nao,Humanoid Robots; Expressive Robots,Social; Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants observed the robot through images and a video.,media,Participants viewed a video of the robot interacting with a person.,physical,Participants viewed images and a video of a physical robot.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires; Multidimensional Measures,,,Trust was measured using a questionnaire with multiple dimensions.,"parametric models (e.g., regression)",The study used partial least squares analysis to test the relationships between trust dimensions.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,"The study did not manipulate any specific factors related to trust, but measured trust after participants viewed a video of a robot.",,"The performance construct was the most critical factor explaining trust in service robots, followed by purpose and process.",The study identified the linear relationships of multidimensional trust in service robots to institution-based trust and a behavioral intention to purchase tourism services.,"The robot was shown serving a customer at a hotel, and the human participant completed a questionnaire about their trust in the robot.",Partial least squares; Multilevel Model; multitrait-multimethod matrix analysis; redundancy analysis; harman's single factor analysis; Factor analysis,"The study used partial least squares analysis to examine the constructs of trust in service robots and to test the proposed relationships of the trust model, including institution-based trust, trusting beliefs, and a behavioral intention. Confirmatory factor analysis was used to estimate the measurement model for understanding the structures of first-order constructs. Multitrait-multimethod matrix analysis and heterotrait-monotrait test were used to assess the validity of the formative constructs. Redundancy analysis was used to assess the quality of the formative measurement model. Harman's single factor analysis and exploratory factor analysis were used to test for common method bias.",FALSE,,,,"The study did not manipulate any factors. Participants were shown images and a video of an NAO robot at a hotel and then completed a questionnaire. There was no manipulation of any kind, so no factors are listed.",10.1016/j.annals.2020.102888,https://linkinghub.elsevier.com/retrieve/pii/S0160738320300323,"In recognizing the increase in the use of service robots by service industries, identifying the structure of trust in intelligent robots is crucial for tourism studies. This paper ﬁrst proposes a model of multifaceted trust in service robots comprised of three constructs – performance, process, and purpose – and, second, tests the trust model that considers institution-based trust, trusting belief, and intention. As a result, this paper identiﬁed a higher-order formative construct of trust in service robots with the highest importance for a performance construct (Study 1). The antecedents of the multifaceted trust in tourism service robots are then identiﬁed (Study 2). This study provides important theoretical and methodological contributions to the ﬁelds of information technology and tourism."
"Parron, Jesse; Nguyen, Thai Thao; Wang, Weitian",Development of A Multimodal Trust Database in Human-Robot Collaborative Contexts,2023,1,65,65,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants wore physiological sensors and collaborated with a robot on a manufacturing task, receiving a part from the robot and then rating their trust on a Likert scale after each interaction. The robot's speed, distance, and height were varied across 27 trials.","Participants collaborated with a robot to assemble a model car, receiving parts from the robot.",Franka Emika Panda,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Passing,direct-contact interaction,Participants physically received parts from the robot during the task.,real-world,Participants wore an AR headset while interacting with the robot.,physical,Participants interacted with a physical robot arm.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions.,Questionnaires; Physiological Measures,,Physiological Signals; Eye-tracking Data,Trust was measured using a questionnaire and physiological data.,no modeling,The study collected data but did not model trust.,Empirical HRI Studies,Mixed Reality Studies,Direct Manipulation,"The robot's speed, distance, and height were varied to influence the task difficulty and the user's perception of the robot's performance, and the interaction was conducted in AR.","Trust decreased when the robot's speed increased or when the robot was at a far distance and high height, and increased when the robot was at a slower speed, closer distance, and lower height.","The study found that participants' heart rate decreased and their trust increased over time, suggesting adaptation to the robot. There were also spikes in EEG data that correlated with lower trust ratings.","The study developed a multimodal trust database (TrustBase) by collecting physiological data and trust ratings during human-robot collaboration, showing that robot performance factors such as speed, distance, and height influence trust.","The robot picked up a part of a model car and handed it to the human participant, who then retrieved the part from the robot's grippers. The human then rated their trust level on a questionnaire.",,"No statistical tests were explicitly mentioned in the paper. The analysis focused on descriptive statistics, such as calculating means and identifying trends in physiological data (EEG, EMG, ECG, eye-tracking) and trust ratings over time. The paper describes the data collection and visualization process, but does not include any inferential statistical analysis.",TRUE,Robot-accuracy,Robot-accuracy,,"The study explicitly manipulated the robot's speed, distance, and height during the part hand-off task. These manipulations directly affected the robot's performance in the task, influencing how easily the participant could receive the part. Specifically, the paper states, 'The positioning of the robot arm encompassed a range of heights including low, medium, and high. Furthermore, different distances -close, medium, and far -were incorporated into the task. The robot's speed was also manipulated during the experiment, with three different settings: slow, medium, and fast.' These changes in speed, distance, and height directly impact the robot's accuracy in performing the task, making 'Robot-accuracy' the most appropriate category. The paper also states, 'Trust decreased when the robot's speed increased or when the robot was at a far distance and high height, and increased when the robot was at a slower speed, closer distance, and lower height.' This clearly indicates that the manipulated factors (speed, distance, height) which are categorized under 'Robot-accuracy' had a direct impact on trust levels. There were no other factors manipulated that were found to not impact trust.",10.1109/UEMCON59035.2023.10316014,https://ieeexplore.ieee.org/document/10316014/,"Robots are gradually being incorporated into the workforce to assist with labor-intensive and repetitive tasks, especially in smart manufacturing contexts. This leads to increased human-robot collaboration, which may be an unfamiliar, distrustful, and uncomfortable situation for inexperienced people to navigate. Motivated by these issues and aiming to have a comprehensive understanding of the factors that affect people’s trust in robots, we developed a new trust database by investigating the trust between human collaborators wearing four biological sensors and a robot performing collaborative tasks. Using these sensors, we collected trust-related physiological human factors from the brain (EEG), heart (ECG), forearm (EMG), and eyes during human-robot collaborative tasks. As well as a trust rating through a questionnaire, this allows for the creation of a multimodal human-robot trust database (TrustBase). TrustBase provides insightful guidance to optimize and improve the environment deployment and robot configuration in human-robot partnerships within smart manufacturing contexts."
"Patacchiola, M.; Cangelosi, A.",A Developmental Cognitive Architecture for Trust and Theory of Mind in Humanoid Robots,2020,2,25,25,0,No participants were excluded,Controlled Lab Environment,between-subjects,"The robot learned object names from reliable and unreliable informants, then its ability to identify the unreliable informant and endorse the correct name was assessed.",The robot had to learn object names from reliable and unreliable informants and then identify the unreliable informant and endorse the correct name.,iCub,Humanoid Robots,Research,Social,Tutoring,minimal interaction,The robot interacted with human informants in a lab setting.,real-world,The interaction took place in a real-world lab setting with a physical robot.,physical,The robot was a physical humanoid robot.,shared control (fixed rules),The robot followed pre-programmed rules to interact with the informants.,Behavioral Measures,,,Trust was assessed by measuring the robot's ability to identify the unreliable informant and endorse the correct name.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The reliability of the informants was directly manipulated by having one informant provide correct object names and the other provide incorrect object names.,"The robot was able to correctly identify the unreliable informant and endorse the correct name when using the unbiased model, but not when using the biased model.","The results showed that the model with a ToM bias performed similarly to 3-year-olds, while the unbiased model performed similarly to 4-year-olds.","The model was able to replicate the behavior of 4-year-olds in trust-based learning, and a bias in the model replicated the behavior of 3-year-olds.","The robot learned object names from human informants, and then had to identify the unreliable informant and endorse the correct name for a new object.",t-test,An independent sample t-test (two-tailed) was used to compare the robot's performance in the explicit judgment and endorsement trials. The t-test was used to determine if the robot's performance was significantly different from chance in identifying the unreliable informant and endorsing the correct name.,TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the reliability of the informants by having one informant provide correct object names and the other provide incorrect object names. This directly impacts the robot's accuracy in learning the correct object names, which is a key factor in trust. The robot's ability to identify the unreliable informant and endorse the correct name was directly influenced by the accuracy of the information it received. Therefore, 'Robot-accuracy' is the most appropriate category.",10.1109/TCYB.2020.3002892,,"As artificial systems are starting to be widely deployed in real-world settings, it becomes critical to provide them with the ability to discriminate between different informants and to learn from reliable sources. Moreover, equipping an artificial agent to infer beliefs may improve the collaboration between humans and machines in several ways. In this article, we propose a hybrid cognitive architecture, called Thrive, with the purpose of unifying in a computational model recent discoveries regarding the underlying mechanism involved in trust. The model is based on biological observations that confirmed the role of the midbrain in trial-and-error learning, and on developmental studies that indicate how essential is a theory of mind in order to build empathetic trust. Thrive is build on top of an actor-critic framework that is used to stabilize the weights of two self-organizing maps. A Bayesian network embeds prior knowledge into an intrinsic environment, providing a measure of cost that is used to boostrap learning without an external reward signal. Following a developmental robotics approach, we embodied the model in the iCub humanoid robot and we replicated two psychological experiments. The results are in line with real data, and shed some light on the mechanisms involved in trust-based learning in children and robots."
"Patacchiola, M.; Cangelosi, A.",A Developmental Cognitive Architecture for Trust and Theory of Mind in Humanoid Robots,2020,2,600,600,0,No participants were excluded,Simulation,between-subjects,"Participants were exposed to a reliable and unreliable informant in a sticker-finding task, then their perception of the informants was assessed.",Participants had to identify the reliable and unreliable informants in a sticker-finding task.,Unspecified,Other,Research,Evaluation,Rating,minimal interaction,Participants interacted with virtual agents in a simulated environment.,simulation,The interaction took place in a simulated environment.,simulated,The robots were represented as virtual agents.,pre-programmed (non-adaptive),The virtual agents followed a pre-scripted series of actions.,Behavioral Measures,,,Trust was assessed by measuring the participants' ability to identify the reliable informant.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The reliability of the informants was directly manipulated by having one informant provide correct information and the other provide incorrect information.,Participants were able to correctly identify the reliable informant.,The simulation results were consistent with the real data from the original study.,The model was able to correctly differentiate between reliable and unreliable informants.,"The virtual agents provided labels for the location of stickers, and the participants had to identify the reliable informant.",mcnemar's χ²,McNemar's chi-squared test was used to determine if the model's performance in identifying the reliable informant was significantly different from chance. This test was used to compare the proportion of times the model correctly identified the reliable informant.,TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the reliability of the virtual informants by having one informant provide correct information about the location of stickers and the other provide incorrect information. This directly impacts the virtual agents' accuracy in guiding participants to the correct sticker location. The participants' ability to identify the reliable informant was directly influenced by the accuracy of the information they received. Therefore, 'Robot-accuracy' is the most appropriate category.",10.1109/TCYB.2020.3002892,,"As artificial systems are starting to be widely deployed in real-world settings, it becomes critical to provide them with the ability to discriminate between different informants and to learn from reliable sources. Moreover, equipping an artificial agent to infer beliefs may improve the collaboration between humans and machines in several ways. In this article, we propose a hybrid cognitive architecture, called Thrive, with the purpose of unifying in a computational model recent discoveries regarding the underlying mechanism involved in trust. The model is based on biological observations that confirmed the role of the midbrain in trial-and-error learning, and on developmental studies that indicate how essential is a theory of mind in order to build empathetic trust. Thrive is build on top of an actor-critic framework that is used to stabilize the weights of two self-organizing maps. A Bayesian network embeds prior knowledge into an intrinsic environment, providing a measure of cost that is used to boostrap learning without an external reward signal. Following a developmental robotics approach, we embodied the model in the iCub humanoid robot and we replicated two psychological experiments. The results are in line with real data, and shed some light on the mechanisms involved in trust-based learning in children and robots."
"Patel, Jayam; Pinciroli, Carlo",Improving Human Performance Using Mixed Granularity of Control in Multi-Human Multi-Robot Interaction,2020,1,28,28,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed two scenarios (MGOC and SGOC) in a randomized order, with a 5-minute exploration period before each scenario, and completed a questionnaire after each scenario.",Participants supervised 8 robots to construct a simple structure by moving two objects to target positions.,Unspecified,Mobile Robots; Swarm Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the robots through a handheld device.,real-world,Participants used an augmented reality application to interact with the robots.,physical,Participants interacted with real robots in a physical environment.,shared control (fixed rules),The robots had pre-programmed behaviors and responded to user input through the AR interface.,Questionnaires,Human-Robot Trust Scale/Questionnaire (HRTS/HRITS); Interpersonal Trust Scale/Questionnaire,,Trust was measured using questionnaires.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the granularity of control and role assignment, with one condition allowing mixed granularity and free role assignment, and the other forcing single granularity and specific roles.",Trust was higher in the mixed granularity of control condition compared to the single granularity of control condition.,"Operators in the mixed granularity condition naturally adopted a parallel work strategy, which led to better performance and higher trust. The forced role assignment in the single granularity condition resulted in lower engagement and trust for the operator with lower workload.",Mixed granularity of control and flexible role assignment in multi-human multi-robot interaction leads to higher trust and better system performance compared to forced specialization.,"The robots autonomously moved to transport objects, and the human participants used an AR interface to supervise the robots, manipulate objects, and assign robots to tasks.",Friedman test,"The Friedman test was used to analyze the differences in the number of interactions and activity periods between operators in the Single Granularity of Control (SGOC) and Mixed Granularity of Control (MGOC) conditions. Specifically, it was used to determine if the differences in these metrics were statistically significant. The test was applied because the data was paired and non-parametric.",TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,Task-complexity,"The study manipulated the level of robot autonomy by comparing two conditions: Mixed Granularity of Control (MGOC) and Single Granularity of Control (SGOC). In MGOC, operators had full control over the robots and could choose how to interact with the system, effectively allowing for a more flexible and shared control approach. In SGOC, the operators were assigned specific roles and modalities of interaction, limiting their control and autonomy. Specifically, one operator was assigned to object manipulation (transport) and the other to robot control (placement), which reduced the overall autonomy of the system and the operators. This manipulation of control granularity and role assignment directly impacted trust, as evidenced by the higher trust reported in the MGOC condition. The task complexity was also implicitly manipulated by the different control conditions. In the MGOC condition, the operators could choose to work in parallel or sequentially, which could lead to different levels of task complexity. In the SGOC condition, the task was divided into two distinct phases, each assigned to a specific operator, which could be considered a manipulation of task complexity. However, the study did not find that this manipulation of task complexity had a direct impact on trust. The primary driver of trust was the level of control and autonomy afforded to the operators, not the inherent complexity of the task itself. Therefore, 'Robot-autonomy' is the most appropriate category for the primary manipulation, and 'Task-complexity' is a secondary factor that was implicitly manipulated but did not directly impact trust.",10.1109/RO-MAN47096.2020.9223553,https://ieeexplore.ieee.org/document/9223553/,"Due to the potentially large number of units involved, the interaction with a multi-robot system is likely to exceed the limits of the span of apprehension of any individual human operator. In previous work, we studied how this issue can be tackled by interacting with the robots in two modalities — environment-oriented and robot-oriented. In this paper, we study how this concept can be applied to the case in which multiple human operators perform supervisory control on a multirobot system. While the presence of extra operators suggests that more complex tasks could be accomplished, little research exists on how this could be achieved efﬁciently. In particular, one challenge arises — the out-of-the-loop performance problem caused by a lack of engagement in the task, awareness of its state, and trust in the system and in the other operators. Through a user study involving 28 human operators and 8 real robots, we study how the concept of mixed granularity in multi-human multi-robot interaction affects user engagement, awareness, and trust while balancing the workload between multiple operators."
"Payre, William; Perelló-March, Jaume; Birrell, Stewart",Under pressure: Effect of a ransomware and a screen failure on trust and driving performance in an automated car simulation,2023,1,38,37,1,1 participant withdrew due to simulator sickness,Controlled Lab Environment,within-subjects,"Participants completed a familiarization trial, then two experimental conditions (silent and explicit failure) and one control condition in a counterbalanced order, each lasting 12 minutes, while performing a non-driving related task (NDRT). The failures occurred during automated driving mode. Participants could take over control at any time.","Participants drove a simulated car in automated mode while performing a word search task, and experienced either a silent failure (no turn signals) or an explicit failure (ransomware attack) during automated driving.",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the automated vehicle in a driving simulator, with the option to take over control.",simulation,The study used a high-fidelity driving simulator with realistic graphics and motion.,simulated,The automated vehicle was simulated within the driving simulator.,shared control (fixed rules),"The automated vehicle operated with fixed rules, allowing participants to take over control at any time.",Questionnaires; Behavioral Measures; Performance-Based Measures,Trust in Automation Scale (TAS); Situational Trust Scale -Automated Driving,Performance Metrics,"Trust was assessed using questionnaires, behavioral measures (takeover rate, time driving manually, resuming NDRT), and performance metrics (speed homogeneity, steering wheel angle).",no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the presence of system failures (silent and explicit) during automated driving, which influenced the perceived reliability of the system and the user's decision to take over control.","Trust decreased after both silent and explicit failures, with a greater negative effect after the explicit failure. Drivers who resumed control had lower trust scores.","The study found that younger participants were more inclined to pay the ransom, and that the type of system failure had no significant effect on two out of three measures of driving behavior related to trust. The study also found that drivers who did not resume control after the explicit failure had higher trust in automation scores.","Cyberattacks and screen failures negatively affect drivers' trust and performance in automated vehicles, with explicit failures having a more detrimental effect on trust than silent failures. Engagement in a non-driving related task is an indicator of trust in the system.","The robot (simulated automated vehicle) drove autonomously on a motorway, while the human participant performed a word search task and could take over control at any time. The robot experienced either a silent failure (no turn signals) or an explicit failure (ransomware attack).",ANOVA; ANOVA; Mann-Whitney U; Mann-Whitney U; pair-wise t-tests,"The study used repeated measures ANOVA to analyze self-reported data from the STS-AD, TAS, and two bespoke trust items to test for variations in trust across conditions. One-way ANOVAs were conducted to test the effect of resuming control after a failure on situational trust. Independent-samples Mann-Whitney U tests were used to analyze the TAS and bespoke items that did not meet normality assumptions, followed by Wilcoxon tests for pair-wise comparisons. Pair-wise t-tests were used to test the effect of failures on taking over manual control, time driving manually, and resuming the NDRT. Additionally, Mann-Whitney U tests were used to analyze the effect of resuming control on driving performance measures (speed homogeneity and steering wheel angle).",TRUE,Robot-interface-design; Robot-accuracy,Robot-interface-design; Robot-accuracy,,"The study manipulated the presence of system failures, which directly relates to the robot's accuracy in performing its task. The silent failure (no turn signals) and the explicit failure (ransomware attack) both represent a deviation from the expected behavior of the automated system, thus impacting its accuracy. The ransomware attack also manipulated the robot interface design by displaying a ransomware message on the in-vehicle touchscreen. The study found that both types of failures impacted trust, with the explicit failure having a greater negative effect. The study did not find any factors that were manipulated that did not impact trust.",10.3389/fpsyg.2023.1078723,https://www.frontiersin.org/articles/10.3389/fpsyg.2023.1078723/full,"One major challenge for automated cars is to not only be safe, but also secure. Indeed, connected vehicles are vulnerable to cyberattacks, which may jeopardize individuals’ trust in these vehicles and their safety. In a driving simulator experiment, 38 participants were exposed to two screen failures:               silent               (i.e., no turn signals on the in-vehicle screen and instrument cluster) and               explicit               (i.e., ransomware attack), both while performing a non-driving related task (NDRT) in a conditionally automated vehicle. Results showed that objective trust decreased after experiencing the failures. Drivers took over control of the vehicle and stopped their NDRT more often after the explicit failure than after the silent failure. Lateral control of the vehicle was compromised when taking over control after both failures compared to automated driving performance. However, longitudinal control proved to be smoother in terms of speed homogeneity compared to automated driving performance. These findings suggest that connectivity failures negatively affect trust in automation and manual driving performance after taking over control. This research posits the question of the importance of connectivity in the realm of trust in automation. Finally, we argue that engagement in a NDRT while riding in automated mode is an indicator of trust in the system and could be used as a surrogate measure for trust."
"Pearson, Carl J",Who Should I Trust (Human vs. Automation)?: The Effects of Pedigree in a Dual Advisor Context,2019,1,200,200,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were given instructions to choose the safest route for a military convoy. They were then randomly assigned to an advisor pair and the order of the pair was also randomized. Participants completed a pedigree rating scale for each advisor. They then completed eight map trials, with two trials having conflicting recommendations. Finally, they completed a trust scale for each advisor and provided demographic information.",Participants were asked to choose the safest route for a military convoy based on information provided by two advisors (human and automated) with varying levels of pedigree.,Unspecified,Other,Research,Evaluation,Text Evaluation,minimal interaction,"Participants interacted with the decision aids through text and audio, with no physical interaction.",media,The interaction was presented through static satellite view maps and text-based profiles.,hypothetical,"The robot was presented as a text-based profile, without any visual representation.",pre-programmed (non-adaptive),The automated advisor provided pre-programmed recommendations without adapting to the user.,Questionnaires; Custom Scales,,,Trust was measured using a custom trust scale adapted from Merritt (2011).,"parametric models (e.g., regression)",Multinomial logistic regressions were used to examine the effect of trust preference scores on behavioral reliance decisions.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The pedigree of the human and automated advisors was manipulated through text-based profiles, influencing the perceived expertise of each advisor.","Participants generally preferred the human advisor, but this preference was reversed when the automated advisor had a much higher pedigree. Trust preferences significantly predicted reliance behaviors.","The study found a general preference for human advisors, which was only reversed when the automated advisor had a much higher pedigree. This contradicts some previous findings that suggested a general preference for automation.","Pedigree perceptions in decision aids are an important trait in the formation of trust, and a general preference for human advisors was only overridden when automation pedigree far outweighed the human pedigree.",The human participant was tasked with selecting the safest route for a military convoy based on recommendations from a human and an automated advisor. The advisors provided recommendations via text and audio.,ANOVA; games-howell post hoc test; Logistic regression,"An ANOVA was used to examine the effect of advisor pedigree treatment group on advisor trust preference. A Games-Howell post hoc test was conducted to control for unequal cell sizes. Multinomial logistic regressions were used to examine the effect of trust preference scores on the likelihood of behavioral reliance decisions (between self-reliance, automation, and human reliance).",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the perceived expertise (pedigree) of the human and automated advisors through text-based profiles. This manipulation directly influenced the content of the information presented about the advisors, which is why 'Robot-verbal-communication-content' is the most appropriate category. The profiles described the advisors' backgrounds and qualifications, which directly impacted how participants perceived their expertise. The study found that this manipulation of pedigree significantly impacted trust, as participants showed a preference for the human advisor unless the automated advisor had a much higher pedigree. Therefore, 'Robot-verbal-communication-content' is also listed as a factor that impacted trust. There were no factors that were manipulated that did not impact trust.",,,"Source type bias (human vs automation) may inﬂuence the development of trust in decision aids. Situations involving two decision-aids may depend on the inﬂuence of pedigree (perceived expertise) such that decision making or reliance behavior is affected. In this task, the Convoy Leader decision-making paradigm developed by Lyons and Stokes (2012) was adapted to address advisor pedigree such that the human and automated information sources could be of high or low pedigree. Two hundred participants were asked to make eight decisions regarding the route taken by a military convoy based on intelligence (e.g., past insurgent attacks, Improvised Explosive Devices (IEDs) detected, etc.) provided by two information sources (human and automation) of varying degrees of pedigree. In two of these eight decisions, the decision-aids provided conﬂicting information. Results indicated that participants were likely to demonstrate a bias such that they were more likely to trust the information coming from the human advisor regardless of pedigree. This bias towards the human was only reversed when the automated decision aid was presented as having far greater pedigree. Measures of trust attitudes were highly indicative of decision making behaviors. The ﬁndings are addressed in terms of design within a dual-advisor context where human operators may receive conﬂicting information from advisors of different source types."
"Pedersen, Bjarke Kristian Maigaard Kjær; Andersen, Kamilla Egedal; Köslich, Simon; Weigelin, Bente Charlotte; Kuusinen, Kati",Simulations and Self-Driving Cars: A Study of Trust and Consequences,2018,1,121,121,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants interacted with a car simulator in six different conditions. In some conditions, participants were told they would receive an electrical shock if the simulated car crashed. Participants completed a structured interview after the simulation.",Participants were asked to monitor a simulated self-driving car and assume control if necessary.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a simulated car, with no physical contact.",simulation,The interaction took place in a car simulator.,simulated,The robot was a simulated self-driving car.,fully autonomous (limited adaptation),"The simulated car operated autonomously, but with limited adaptation.",Behavioral Measures,,Video Data,Trust was measured by whether participants assumed control of the car.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"Participants were told that they would receive an electrical shock if the simulated car crashed, which was intended to influence their trust in the system.",Participants were significantly less trusting of the autonomous system when real-life consequences were involved.,"Participants were significantly less trusting of the autonomous system when real-life consequences were involved, which is a notable trend.",Participants are significantly less trusting of an autonomous system when real-life consequences are involved.,"The simulated self-driving car navigated through a series of scenarios, and the human participant was tasked with monitoring the car's behavior and taking control if they deemed it necessary.",Fisher's exact test,"Fisher's Exact test was used to find the level of significance between the experimental conditions, specifically comparing the percentage of participants assuming control of the simulated car in different conditions.",TRUE,Task-constraints; Robot-verbal-communication-content,Task-constraints,Robot-verbal-communication-content,"The study manipulated task constraints by introducing the potential for a painful electrical shock in some conditions (C2, C4-C6) if the simulated self-driving car crashed. This was intended to simulate real-life consequences and influence participants' trust in the system. The paper states, 'In C2 and C4-C6 participants were falsely informed that should anything happen to the SDC, the simulator would administer a painful electrical shock through a device attached to their right hand index finger.' This manipulation of consequences directly impacted trust, as participants were significantly less trusting of the autonomous system when real-life consequences were involved. The study also manipulated the robot's verbal communication content by providing incorrect information about the number of cars in the second intersection in condition C6, stating 'In C5 the SDCs interface provides correct audible information about the state of traffic ""5 cars"", while in C6 the information was wrong ""2 cars""'. However, this manipulation did not significantly impact the percentage of participants assuming control, indicating that it did not impact trust levels in this study. The results section states, 'The percentage of participants assuming control, i.e. braked in the second intersection did not differ significantly between C5 and C6 (35% and 40% respectively, p = 1).'",10.1145/3173386.3176987,https://dl.acm.org/doi/10.1145/3173386.3176987,"Trust plays an essential role in ensuring safe and robust humanrobot interaction. Recent work suggests that people can be too trusting of technology, leading to potential dangerous situations. We carried out a series of experiments in an autonomous car simulator, in order to test if there is a difference in people’s behavior when real-life consequences are applied, compared to pure simulation. The study was carried out with six experimental conditions in a between-subject design in which participants (N = 121) interacted with the simulator and were told they could assume control of the autonomous car at any point during the simulation. Results show that participants are significantly less trusting of the autonomous system, when real-life consequences were involved (p = .014)."
"Perdomo, Maria Elena; Chang López, Roberto; Ordoñez-Avila, Jose Luis",Students level of trust in robotic systems,2024,1,213,213,0,No participants were excluded,Survey/Interview,between-subjects,"Participants completed a questionnaire with eleven items related to trust and mistrust in robots. The data was analyzed using normality tests, sign tests, and correlation analysis to compare responses across different academic grades.",Participants answered a questionnaire about their trust and mistrust in robots.,Unspecified,Other,Other,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about robots and answered a questionnaire.,media,The interaction was based on text descriptions of robots.,hypothetical,"The robots were only described in text, without any visual representation.",not autonomous,"The robot's actions were not part of the study, and the robot was only described in text.",Questionnaires; Custom Scales,,,Trust was measured using a custom questionnaire with eleven items.,no modeling,No computational model of trust was used; the study used statistical tests to compare responses.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study did not manipulate any factors; it measured trust levels across different academic grades.,The study found that doctoral students had greater confidence in robots compared to other academic grades.,"The study found that doctoral students showed the highest level of trust in robots, while undergraduate students showed no significant difference between trust and mistrust, suggesting confusion or lack of interest in robotics.","Doctoral students have greater confidence in robots compared to other academic grades, suggesting that higher levels of knowledge increase trust in robotic systems.",The robot was not physically present; participants answered a questionnaire about their trust and mistrust in robots based on their general understanding of robots.,Shapiro-Wilk; Sign test; Pearson correlation; cronbach's alpha; t-test; Spearman correlation,"The study used several statistical tests. Normality tests were performed to determine if the data followed a normal distribution, which informed the choice between parametric and non-parametric tests. Sign tests were used to compare the responses on trust and mistrust across different academic grades. Correlation analysis was used to examine the relationship between academic grades and the trust/mistrust variables. Cronbach's alpha was used to assess the reliability of the questionnaire. A paired sample t-test was mentioned in the abstract but not in the results section, and Spearman's correlation test was used to assess the relationship between trust and mistrust items.",FALSE,,,,"The study did not manipulate any factors. It measured trust levels across different academic grades using a questionnaire. The study aimed to measure the confidence of students of different grades in robots, but did not manipulate any variables to influence trust. The study used statistical tests to compare responses across different academic grades, but there was no manipulation of any factor related to the robot or the task itself. The study is observational and survey-based, not experimental.",10.1145/3664934.3664953,https://dl.acm.org/doi/10.1145/3664934.3664953,"Nowadays there are many mechatronic systems that interact with organic systems, among them robots have been enhanced by advances in artificial intelligence and their mechanical design. This interaction between mechatronic systems and organic systems is currently under study since many occasions mechatronic systems can be unreliable. Therefore, the objective of this work is to measure the confidence of students of different grades in mechatronic systems known as robots. For this purpose, an instrument for measuring confidence in automation of eleven questions was used, to which the normality test and the paired sample t-test were performed to demonstrate the significant differences between the grades of study. Finally, it was concluded that there are significant differences with the PhD students and most of the other degrees. Therefore, the level of knowledge generates greater confidence in the robots."
"Perello-March, Jaume R.; Burns, Christopher G.; Woodman, Roger; Elliott, Mark T.; Birrell, Stewart A.",Driver State Monitoring: Manipulating Reliability Expectations in Simulated Automated Driving Scenarios,2022,1,27,27,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were divided into two groups with different reliability expectations about the automated vehicle. They completed a series of driving scenarios in a simulator, including a highway with a 2-back task, interurban, urban low complexity, urban high complexity, and a risky maneuver. Physiological data and trust questionnaires were collected at different points.",Participants experienced different driving scenarios while their physiological responses were recorded. They also completed a 2-back task during one of the scenarios and filled out trust questionnaires at different points.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the automated vehicle in a driving simulator, but did not have direct physical contact.",simulation,The study used a driving simulator to create an immersive experience for the participants.,simulated,The robot was a simulated automated vehicle within the driving simulator.,fully autonomous (limited adaptation),"The automated vehicle operated without direct human control, but with limited adaptation to unexpected scenarios.",Questionnaires,Trust in Automated Systems Scale,Physiological Signals,Trust was assessed using a questionnaire and physiological data was collected.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"Participants were given different expectations about the reliability of the automated vehicle, and the driving scenarios varied in complexity. The interaction medium was a driving simulator.","The low reliability group showed increased distrust and decreased total trust scores, while the high reliability group showed increased total trust scores. The reliability expectations had an effect on self-reported trust.","The study found that the 2-back task generated comparable arousal to the urban high complexity scenario. The risk scenario did not produce significant physiological effects, but did show high SCR amplitudes and magnitudes. There were no physiological differences between the reliability groups, but self-reported trust differed between groups. The highway scenario unexpectedly generated higher heart rate than interurban and urban low complexity.","The study found that reliability expectations influenced self-reported trust, with the low reliability group showing increased distrust and decreased total trust scores, and the high reliability group showing increased total trust scores. The 2-back task generated comparable arousal to the urban high complexity scenario.","The automated vehicle drove through different scenarios, and the human participant monitored the vehicle and performed a 2-back task during one of the scenarios. The human was instructed not to take control of the vehicle.",ANOVA,"The study used mixed repeated measures ANOVAs to analyze the effects of driving conditions and reliability expectations on physiological measures (ECG and EDA features) and trust scores. Post-hoc comparisons were used to further investigate significant effects. Specifically, ANOVAs were used to examine differences in heart rate, heart rate variability (LF/HF ratio and RMSSD), skin conductance response (SCR) count, amplitude, and magnitude across different driving scenarios (pre-drive, 2-back, highway, interurban, urban low complexity, urban high complexity, and risky maneuver) and between the two reliability groups (high and low). ANOVAs were also used to analyze changes in trust and distrust scores over time within each reliability group.",TRUE,Task-complexity; Robot-autonomy,Robot-autonomy,Task-complexity,"The study manipulated task complexity by varying the driving scenarios (highway, interurban, urban low complexity, urban high complexity, and a risky maneuver) and including a 2-back task during one of the scenarios. This is described in the 'III. OBJECTIVES' section, where the authors state that they designed the study to provide a variety of driving scenarios with varying traffic density and complexity. The study also manipulated robot autonomy by providing different reliability expectations to the participants about the automated vehicle. The low reliability group was told the vehicle was a prototype and not fully reliable, while the high reliability group was told the vehicle was fully reliable. This is described in the 'IV. METHOD' section, specifically in the 'A. Participants' subsection. The study found that the reliability expectations (Robot-autonomy) had an impact on self-reported trust, with the low reliability group showing increased distrust and decreased total trust scores, and the high reliability group showing increased total trust scores. This is described in the 'V. RESULTS' section, specifically in the 'E. Hypothesis 5', 'F. Hypothesis 6', and 'G. Hypothesis 7' subsections. The study did not find that the different levels of task complexity had an impact on trust, as described in the 'VI. DISCUSSION' section, where the authors state that 'the lack of physiological effects here does not disregard the need to further explore the effects of changing traffic complexity scenarios for HAVs'. Although the task complexity did impact physiological measures, it did not impact trust.",10.1109/TITS.2021.3050518,https://ieeexplore.ieee.org/document/9334426/,"Highly Automated Driving technology will be facing major challenges before being pervasively integrated across production vehicles. One of them will be monitoring drivers’ state and determining whether they are ready to take over control under certain circumstances. Thus, we have explored their physiological responses and the effects on trust of different scenarios with varying trafﬁc complexity in a driving simulator. Using a mixed repeated measures design, twenty-seven participants were divided in two reliability groups with opposite induced automation reliability expectations -low and high-. We hypothesized that expectations would modulate participants’ trust in automation, and consequently, their physiological responses across different scenarios. That is, increasing trafﬁc complexity would also increase participants’ arousal, and this would be accentuated or mitigated by automation reliability expectations. Although reliability group differences could not be observed, our results show an increase of physiological activation within high complexity driving conditions (i.e., a mentally demanding non-driving related task and urban scenarios). In addition, we observed a modulation of trust in automation according to the group expectations delivered. These ﬁndings provide a background methodology from which further research in driver monitoring systems can beneﬁt and be used to train machine learning methods to classify drivers’ state in changing scenarios. This would potentially help mitigate inappropriate take-overs, calibrate trust and increase users’ comfort and safety in future Highly Automated Vehicles."
"Perkins, LeeAnn; Miller, Janet E; Hashemi, Ali; Burns, Gary",Designing for Human-Centered Systems: Situational Risk as a Factor of Trust in Automation,2010,1,95,95,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a practice route and four experimental routes with varying levels of risk using a GPS simulator. They made decisions at seven points in each route, choosing between the suggested route and a manual route. Subjective trust was measured using a 5-point Likert scale for five trust factors.","Participants were tasked with navigating from point A to point B on a map using a GPS simulator, adhering to different risk conditions and making route choices at seven decision points.",Unspecified,Other,Research,Navigation,Path Following,minimal interaction,"Participants interacted with a GPS simulator, making route choices based on displayed information.",simulation,"The interaction was through a GPS simulator, providing a virtual environment for route planning.",simulated,The robot was represented as a simulated GPS interface on a screen.,pre-programmed (non-adaptive),"The GPS provided pre-programmed routes, with no adaptation to user input beyond route selection.",Questionnaires; Behavioral Measures,,Performance Metrics,Trust was assessed using a questionnaire measuring five factors and by observing route choices.,"parametric models (e.g., regression)",Linear models were used to correlate trust factors with the use of the suggested route.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The level of risk was manipulated by varying the type of hazards and time constraints in the route planning scenarios, influencing the user's expectations of the system.","Subjective trust increased with higher risk, but the use of the suggested route decreased with higher risk hazards. Positive correlations were found between trust and use of the suggested route in the control and high risk conditions.","Subjective trust increased with higher risk, but the use of the suggested route decreased with higher risk hazards. There was a conflict between subjective trust measures and objective behavioral measures, where participants rated the automation favorably but did not use it more in high-risk situations. The study also found that participants were more likely to use the suggested route after hitting a car accident versus a traffic jam, and less likely to use the suggested route after hitting uncommon hazards.","The study found that while subjective trust increased with higher risk, the actual use of the automated system decreased when faced with high-risk hazards, indicating a disconnect between perceived trust and actual behavior.","The robot (GPS simulator) provided route suggestions, and the human participant chose between the suggested route and a manual route at seven decision points. The human's task was to navigate from point A to point B while adhering to different risk conditions.",ANOVA; ANOVA,A one-way within-subjects ANOVA was used to compare the effect of the level of risk on the usage of the suggested route. Linear models were used to examine correlations between subjective trust measures (overall and individual trust factors) and the use of the suggested route for each risk condition.,TRUE,Task-constraints; Task-environment,Task-constraints; Task-environment,,"The study manipulated 'Task-constraints' by varying the time pressure (30 minutes or less in low risk, 20 minutes or less in high risk) and the presence of hazards. The 'Task-environment' was manipulated by changing the type of hazards present in the route (common hazards like traffic jams and car accidents in medium risk, uncommon hazards like burning buildings and drive-by shootings in high risk). The paper states, 'Each participant went through a total of five routes, a practice route and four experimental routes, each with a different level of risk.' and 'The four risk conditions were as followed: Control Condition... Low Risk Condition... Medium Risk Condition... High Risk Condition...'. These manipulations of time constraints and hazard types directly impacted the participants' experience and perception of risk, which in turn influenced their trust in the GPS system. The results section indicates that the use of the suggested route decreased with higher risk hazards, and that participants were more likely to use the suggested route after hitting a car accident versus a traffic jam, and less likely to use the suggested route after hitting uncommon hazards. This shows that both the time constraints and the type of hazards (environment) impacted trust.",,,"As society continues to become advanced in technology, automation is increasingly implemented in systems reducing the need for human intervention. Although the system is the main focus in rendering the cognitive workload, the human’s use in the system is the main component for successful performance. Trust is a prime factor in building a symbiotic relationship between human-automation interaction and further empirical research is needed to develop appropriate methods for designing trusted systems. These methods can be defined as identifying the prime factors of trust that influence the user’s decision making. The focus of this paper looks at the factor of risk to determine if and how risk in a situational environment is an influencing factor of trust in automation. A Global Positioning System (GPS) was used as the automated platform, and participants went through a series of route planning sessions with varying levels of risk and hazards. It was found that as the level of risk increased on hazards, the use and trust of automation decreased."
"Petersen, Luke; Zhao, Huajing; Tilbury, Dawn M; Yang, X Jessie; Jr, Lionel P Robert",THE INFLUENCE OF RISK ON DRIVER TRUST IN AUTONOMOUS DRIVING SYSTEMS,2018,1,36,27,0,11 participants were excluded because they completed surveys via paper-form due to software constraints,Controlled Lab Environment,within-subjects,"Participants completed a pre-experiment survey, a training session, and four test sessions with different risk conditions, followed by post-condition surveys.",Participants drove a simulated semi-autonomous vehicle while performing a secondary target detection task.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated vehicle and a secondary task.,simulation,The study used a driving simulator to create a virtual environment.,simulated,The robot was a simulated vehicle in a virtual environment.,shared control (fixed rules),"The simulated vehicle had lane keeping, cruise control, and automatic emergency braking, but required human intervention for some actions.",Questionnaires; Behavioral Measures; Physiological Measures,Trust in Automation Scale (TAS); NASA Task Load Index (NASA-TLX),Eye-tracking Data; Physiological Signals; Performance Metrics; robot data,"Trust was assessed using questionnaires, behavioral data, and physiological measures.",no modeling,The study did not implement a computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"Internal risk was manipulated by varying the reliability of the collision warning system, and external risk was manipulated by varying the driving visibility using fog.","Internal risk reduced trust in the ADS, while external risk had a minor impact on self-reported trust.",The study found that internal risk (reliability of the warning system) had a stronger negative impact on trust than external risk (visibility).,"Internal risk, manipulated through the reliability of the warning system, reduces trust in autonomous driving systems.","The human drove a simulated vehicle, while also performing a secondary target detection task. The simulated vehicle had some autonomous features, such as lane keeping and emergency braking. The human had to take over control when needed.",,"The paper mentions preliminary analysis of self-reported trust, reliance, risk, and workload using survey data, and secondary task performance. However, it does not specify any particular statistical tests used for this preliminary analysis. The paper mentions that further analysis will be conducted using physiological and simulation data, but does not specify the statistical tests that will be used.",TRUE,Robot-accuracy; Task-environment,Robot-accuracy,Task-environment,"The study manipulated the reliability of the forward collision warning system, which directly impacts the accuracy of the robot's actions, thus it is classified as 'Robot-accuracy'. The study also manipulated the visibility of the driving environment using fog, which is a change to the working conditions, thus it is classified as 'Task-environment'. The results showed that the reliability of the warning system (internal risk) had a negative impact on trust, so 'Robot-accuracy' is listed as a factor that impacted trust. The visibility of the driving environment (external risk) had a minor impact on self-reported trust, so 'Task-environment' is listed as a factor that did not impact trust.",,,"Autonomous driving systems (ADS) in autonomous and semi-autonomous vehicles have the potential to improve driving safety and enable drivers to perform non-driving tasks concurrently. Drivers sometimes fail to fully leverage a vehicle’s autonomy because of a lack of trust. To address this issue, the present study examined the influence of risk on drivers’ trust. Subject tests were conducted to evaluate the effects of combined internal and external risk, where participants drove a simulated semi-autonomous vehicle and completed a secondary task at the same time. Results of this study are expected to provide new insights into promoting trust and acceptance of autonomy in both military and civilian settings."
"Phillips, Elizabeth; Ososky, Scott; Jentsch, Florian",An Investigation of Human Decision-Making in a Human—Robot Team Task,2014,1,55,55,0,No participants were excluded,Educational Setting,within-subjects,"Participants completed biographical questionnaires, a robot sketch, and a trust in robots questionnaire. They then read hypothetical scenarios, made decisions at each node of a decision tree, and chose a tactical decision. They also ranked the four tactical decisions and answered a free response question.","Participants made decisions in hypothetical foot pursuit scenarios involving a soldier and a robot, deciding who should engage in the pursuit.",Unspecified,Mobile Robots; Unmanned Ground Vehicles,Research,Evaluation,Ranking,passive observation,Participants only read about the robot and interaction scenarios.,media,The interaction was based on text descriptions of scenarios.,hypothetical,"The robot was only described in text, without any visual representation.",not autonomous,"The robot's actions were described in text, without any real autonomy.",Questionnaires,Trust in Automated Systems Scale,,Trust was measured using a modified version of the Trust in Automated Systems Scale.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Indirect Manipulation,"The scenarios varied in terms of weather, terrain, presence of civilians, sensor reliability, backup availability, and suspect features, which indirectly influenced trust by altering the perceived risk and robot capabilities.","Higher trust in robots was associated with a tendency to rank robot-only pursuit as the best option, while lower trust was associated with ranking soldier-only pursuit as the best option.","Participants showed a preference for keeping the team together, even when the decision tree logic suggested sending only one member. Some participants made illogical decisions, possibly due to fatigue or lack of effort. There was a significant relationship between trust in robots and the tactical decisions made by participants.","Trust in robots was significantly associated with the decision to send the robot on a foot pursuit, while low trust was associated with sending the soldier.","The robot was described as an advanced intelligent team member with GPS, radio communication, and various sensors. The human participant read scenarios and made decisions about who should engage in a foot pursuit, considering factors like terrain, sensor reliability, and threat level.",Fisher's exact test; spearman's rho rank-order correlation coefficient,The Fisher-Freeman-Halton test was used to examine the association between participants' binary decisions at each node of the decision tree and their tactical decisions (choosing who should engage in the foot pursuit). Spearman's rho rank-order correlation coefficient was used to investigate the relationship between participants' trust in robots scores and their rankings of the four tactical decisions.,TRUE,Task-environment; Task-complexity,Task-environment,Task-complexity,"The study manipulated the Task-environment by varying scenario details such as weather conditions, terrain surfaces, the presence of civilians, sensor and communication functionality, available backup units, and descriptive features of the person of interest. These variations in the environment indirectly influenced trust by altering the perceived risk and robot capabilities. The study also manipulated Task-complexity by presenting a decision tree with multiple nodes, requiring participants to make decisions at each node before making a final tactical decision. However, the study found that the individual decision nodes (related to task complexity) were not significantly associated with decision outcomes, while the environmental factors did impact trust. Specifically, the scenarios varied in terms of risk and robot capabilities, which indirectly affected participants' trust perceptions. The paper states, 'Each scenario presented additional details concerning weather conditions, terrain surfaces, the presence of civilians in the vicinity, sensor and communication functionality, available backup units, and descriptive features of the person of interest.' This shows the manipulation of the task environment. The paper also states, 'Participants were asked to consider the details of the scenario, indicate a ""yes"" or ""no"" response for each of the questions corresponding to each node of the decision tree, and make a decision concerning who (if anyone) should pursue the suspected criminal.' This shows the manipulation of task complexity through the decision tree.",10.1177/1541931214581065,http://journals.sagepub.com/doi/10.1177/1541931214581065,"This paper presents initial insights from an exploratory analysis of human decision making in a human—robot teaming scenario. A cognitive model in the form of a decision tree was developed using local and national police foot pursuit protocols. Participants were asked to read through a series of hypothetical scenarios involving a Soldier and a robot engaging in a foot pursuit of a person of interest. Participants made decisions at each node of the decision tree and then a tactical decision concerning which member of the team should engage in the pursuit. Initial results revealed that individual decision nodes were not associated with participants’ choice in who should engage in the pursuit. Trust in robots, however, was significantly associated with the participants’ choices."
"Pilacinski, Artur; Pinto, Ana; Oliveira, Soraia; Araújo, Eduardo; Carvalho, Carla; Silva, Paula Alexandra; Matias, Ricardo; Menezes, Paulo; Sousa, Sonia",The robot eyes don't have it. The presence of eyes on collaborative robots yields marginally higher user trust but lower performance,2023,1,38,38,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed questionnaires, were equipped with VR headset and sensors, and then completed four blocks of trials with a virtual robot. The order of the four test conditions was randomized and balanced within the group. Heart rate, head and eye movement were measured continuously during the experiment.",Participants were instructed to grab a tool passed by a virtual robot.,Baxter,Humanoid Robots; Collaborative Robots (Cobots),Research,Manipulation,Object Passing,minimal interaction,Participants interacted with a virtual robot in a simulated environment.,simulation,The interaction took place in a virtual reality environment.,simulated,The robot was a digital twin of a real robot.,pre-programmed (non-adaptive),The robot moved autonomously using a pre-programmed motion plan.,Questionnaires; Physiological Measures; Performance-Based Measures,Human-Robot Trust Scale/Questionnaire (HRTS/HRITS),Physiological Signals; Eye-tracking Data; Performance Metrics,"Trust was assessed using questionnaires, physiological measures, and task performance.","parametric models (e.g., regression)",The study used ANOVA to analyze the effects of the experimental manipulations on trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The presence or absence of eyes on the robot and the distance between the robot and the user were manipulated to influence trust.,"Subjective trust ratings were slightly higher for robots with eyes, but objective measures indicated less comfort and slower task completion with eyed robots.","The study found a disparity between subjective and objective measures of trust, with subjective ratings indicating higher trust for eyed robots, while objective measures suggested the opposite. There was also a negative correlation between perceived ease of use of technology and pupil size.","The presence of eyes on a collaborative robot had a minimal positive effect on subjective trust ratings, but objective measures indicated less comfortable cooperation with eyed robots.","The robot passed a tool to the human, and the human grabbed the tool. The robot's arm movement was controlled by a motion planner, and the human's hand movement was controlled by a VR controller.",ANOVA,"The study used a 2x2x2 ANOVA to test the impact of cobot eyes (on/off), distance (near/far), and participant gender (male/female) on perceived trust. Additionally, a 2x2 ANOVA was used to test the effects of cobot eyes (on/off) and distance (near/far) on objective variables such as pupil size, task duration, and pulse rate. Correlations between variables were also investigated.",TRUE,Robot-interface-design; Task-environment,Robot-interface-design,Task-environment,"The study manipulated the presence or absence of eyes on the robot's display, which is a change to the robot's interface design. This is explicitly stated in the paper: 'We used virtual reality scenes in which we manipulated distance and the presence of eyes on a robot's display during simple collaboration scenes.' The distance between the robot and the user was also manipulated ('Baxter was placed at either 120/130 cm or 160-170 cm of distance to the operator, conditions named as ""Near"" and ""Far"" respectively.'), which changes the task environment by altering the user's proximity to the robot and whether the robot could potentially reach the user. The presence of eyes on the robot (Robot-interface-design) was found to have a weak impact on subjective trust ratings, as stated in the results: 'We found that eyed cobots yielded a significant, very weak effect of higher trust'. The distance between the robot and the user (Task-environment) did not significantly affect trust, as stated in the results: 'Cobot distance did not significantly affect trust (p > 0.8)'.",10.1016/j.heliyon.2023.e18164,https://linkinghub.elsevier.com/retrieve/pii/S2405844023053720,"Eye gaze is a prominent feature of human social lives, but little is known on whether fitting eyes on machines makes humans trust them more. In this study we compared subjective and objective markers of human trust when collaborating with eyed and non-eyed robots of the same type. We used virtual reality scenes in which we manipulated distance and the presence of eyes on a robot’s display during simple collaboration scenes. We found that while collaboration with eyed cobots resulted in slightly higher subjective trust ratings, the objective markers such as pupil size and task completion time indicated it was in fact less comfortable to collaborate with eyed robots. These findings are in line with recent suggestions that anthropomorphism may be actually a detrimental feature of collaborative robots. These findings also show the complex relationship between human objective and subjective markers of trust when collaborating with artificial agents."
"Pinney, Joel; Carroll, Fiona; Newbury, Paul",Human-robot interaction: the impact of robotic aesthetics on anticipated human trust,2022,1,74,74,0,No participants were excluded,Online Crowdsourcing,within-subjects,Participants completed an online questionnaire where they were presented with images of a robot with different aesthetic modifications and asked to rate their trust in the robot.,"Participants were asked to rate their trust in a robot based on its visual appearance, including different facial expressions, colors, and chest screen imagery.",Canbot U03,Expressive Robots,Research,Evaluation,Rating,passive observation,Participants observed images of the robot without direct interaction.,media,Participants viewed static images of the robot.,simulated,"The robot was presented as images, not a physical robot.",not autonomous,The robot's actions were simulated and did not involve any autonomous behavior.,Questionnaires,,,Trust was assessed using a questionnaire with rating scales.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's facial expressions, colors, and chest screen imagery were manipulated to influence participants' trust perceptions. The task was framed as a trust evaluation based on visual appearance.","Certain facial aesthetics, such as cartoon faces, were perceived as more trustworthy than human-like faces. Blurry visuals decreased trust. The cohesion between facial expressions and chest screen imagery also impacted trust.","Anthropomorphism did not consistently increase trust; in some cases, it decreased trust, aligning with the uncanny valley theory. Participants were more likely to trust a robot with a clear, non-blurry face and a cohesive message between the face and chest screen. Participants often selected the robot with the most basic visual appearance as the most trustworthy, even when it provided incorrect information.","Robot aesthetics, particularly facial expressions, color, and the cohesion of visual elements, significantly influence human trust in robots. Blurry visuals and conflicting messages between facial expressions and chest screen imagery decreased trust.",The robot was presented as a series of images with different visual modifications. The human participant was asked to rate their trust in the robot based on these visual cues and to identify the robot's emotion.,,The study primarily used descriptive statistics to summarize the quantitative data collected from the online questionnaire. The analysis focused on frequencies and percentages of responses related to trust ratings and emotional perceptions of the robot's visual modifications. No specific statistical tests were mentioned in the paper.,TRUE,Robot-aesthetics; Robot-emotional-display; Robot-interface-design,Robot-aesthetics; Robot-emotional-display; Robot-interface-design,,"The study manipulated the robot's visual appearance, including facial expressions, colors, and chest screen imagery, which falls under 'Robot-aesthetics'. The changes in facial expressions to portray different emotions are categorized as 'Robot-emotional-display'. The manipulation of the chest screen imagery, which is part of the robot's interface, is categorized as 'Robot-interface-design'. The paper explicitly states that these manipulations impacted trust levels, as certain facial aesthetics (cartoon faces) were perceived as more trustworthy than human-like faces, blurry visuals decreased trust, and the cohesion between facial expressions and chest screen imagery also impacted trust. Therefore, all three factors are listed under 'factors_that_impacted_trust'. There were no factors that were manipulated that did not impact trust, so 'factors_that_did_not_impact_trust' is empty.",10.7717/peerj-cs.837,https://peerj.com/articles/cs-837,"Background. Human senses have evolved to recognise sensory cues. Beyond our perception, they play an integral role in our emotional processing, learning, and interpretation. They are what help us to sculpt our everyday experiences and can be triggered by aesthetics to form the foundations of our interactions with each other and our surroundings. In terms of Human-Robot Interaction (HRI), robots have the possibility to interact with both people and environments given their senses. They can offer the attributes of human characteristics, which in turn can make the interchange with technology a more appealing and admissible experience. However, for many reasons, people still do not seem to trust and accept robots. Trust is expressed as a person’s ability to accept the potential risks associated with participating alongside an entity such as a robot. Whilst trust is an important factor in building relationships with robots, the presence of uncertainties can add an additional dimension to the decision to trust a robot. In order to begin to understand how to build trust with robots and reverse the negative ideology, this paper examines the influences of aesthetic design techniques on the human ability to trust robots. Method. This paper explores the potential that robots have unique opportunities to improve their facilities for empathy, emotion, and social awareness beyond their more cognitive functionalities. Through conducting an online questionnaire distributed globally, we explored participants ability and acceptance in trusting the Canbot U03 robot. Participants were presented with a range of visual questions which manipulated the robot’s facial screen and asked whether or not they would trust the robot. A selection of questions aimed at putting participants in situations where they were required to establish whether or not to trust a robot’s responses based solely on the visual appearance. We accomplished this by manipulating different design elements of the robots facial and chest screens, which influenced the human-robot interaction. Results. We found that certain facial aesthetics seem to be more trustworthy than others, such as a cartoon face versus a human face, and that certain visual variables (i.e., blur) afforded uncertainty more than others. Consequentially, this paper reports that participant’s uncertainties of the visualisations greatly influenced their willingness to accept and trust the robot. The results of introducing certain anthropomorphic characteristics emphasised the participants embrace of the uncanny valley theory, where pushing the degree of human likeness introduced a thin line between participants accepting robots and not. By understanding what manipulation of design elements created the aesthetic effect that triggered the affective processes, this paper further enriches our knowledge of how we might design for certain emotions, feelings, and ultimately more socially acceptable and trusting robotic experiences."
"Pinto, Ana; Sousa, Sonia; Silva, Crist&oacute;v&atilde;o; Coelho, Pedro",Adaptation and validation of the HCTM Scale into Human-robot interaction Portuguese context: A study of measuring trust in human-robot interactions,2020,1,243,243,0,No participants were excluded,Educational Setting,,"Participants were presented with a video of a COBOT in an industrial context, then completed the HCTM scale adapted for HRI.",Participants completed a questionnaire about their trust in COBOTS after watching a video.,Unspecified,Collaborative Robots (Cobots),Industrial,Evaluation,Survey/Questionnaire Completion,passive observation,"Participants did not interact directly with a robot, but completed a questionnaire after watching a video.",media,Participants watched a video of a COBOT in an industrial context.,physical,Participants saw a video of a physical robot.,not autonomous,"The robot's actions were shown in a video, but the robot was not acting autonomously in the study.",Questionnaires,Human-Computer Trust Scale/Questionnaire (HCT/HCTM),,Trust was measured using the adapted Human-Computer Trust Model Scale (HCTM).,no modeling,"The study did not model trust, but rather assessed the reliability and validity of the HCTM scale.",Observational & Survey Studies,Quantitative Surveys,No Manipulation,"The study did not manipulate any factors related to trust, but rather focused on validating a trust scale.",,"The Dillon-Goldstein's rho measures for Competency, Benevolence, and Reciprocity constructs were close to 0.70, indicating acceptable but poor internal consistency.","The Portuguese version of the HCTM scale is acceptable for measuring trust in HRI, but has poor internal consistency.","The robot was shown in a video performing industrial tasks, and the human participant completed a questionnaire about their trust in the robot.",Partial least squares; dillon-goldstein's rho; fornell-lacker criterion,"The study used Partial Least Squares Structural Equation Modeling (PLS-SEM) to assess the reliability and validity of the Human-Computer Trust Model Scale (HCTM) in the context of human-robot interaction. Dillon-Goldstein's rho was used to measure the internal consistency of the constructs, and the Fornell-Lacker criterion was used to assess discriminant validity. The analysis examined the loadings of items on their respective constructs, average variance extracted (AVE), composite reliability (CR), and cross-loadings to determine the psychometric properties of the scale.",FALSE,,,,"The study did not manipulate any factors related to trust. The study focused on adapting and validating the Human-Computer Trust Model Scale (HCTM) for human-robot interaction in a Portuguese context. Participants watched a video of a COBOT in an industrial setting and then completed a questionnaire. There was no manipulation of the robot's behavior, appearance, or task. The study's goal was to assess the psychometric properties of the HCTM scale, not to investigate the impact of specific factors on trust. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust in the context of this study.",10.1145/3419249.3420087,https://dl.acm.org/doi/10.1145/3419249.3420087,"As robots become increasingly common in a wide variety of domains, there is an increasing need to assess the trust humans have when interacting with robots. Specifically are two of the main objectives in this study: 1) to adapt and validate the Human-Computer Trust Model Scale (HCTM) to human-robot interaction (HRI), in a Portuguese population. This need is because trust in automation has been understood through its analogy to interpersonal trust. With the HCTM we intend to suggest an alternative to that. This measure of trust is a very recent scale, 2019, and based on the present context and research. More, this scale was subjected to robust statistical analysis tests and was tested at different scenarios of computing with successful. The final motive is that authors are not aware of a similar scale in the Portuguese context that can measure trust in HRI with COBOTS. The growth of this market is notable. For that, we used 243 undergraduate students with backgrounds on Management and Engineering and Industrial Management. Results, indicate a good measurement of the latent constructs, convergent reliability, internal consistency and discriminate validity. However, DillonGoldstein’s rho measures for both Competency (0.696), Benevolence (0.686) and Reciprocity (0.604) constructs present scores close to 0.70. According to Hair et al [11] the composite reliability (internal consistency) should be higher than 0.7 (or >0.6 in exploratory research). This means that the model is acceptable, and the Portuguese version of the model satisfies the criteria for measure trust in human-robot interaction (HRI), however, have a poor internal consist."
"Pinto, Ana; Sousa, Sónia; Simões, Ana; Santos, Joana","A Trust Scale for Human-Robot Interaction: Translation, Adaptation, and Validation of a Human Computer Trust Scale",2022,1,239,239,0,No participants were excluded,Educational Setting,,Participants were asked to watch a video illustrating the use of cobots in industrial settings and read a user story before completing a survey. The survey included the Human-Robot Interaction Trust Scale (HRITS).,Participants completed a questionnaire assessing their trust in collaborative robots (cobots) after watching a video and reading a user story.,Unspecified,Collaborative Robots (Cobots),Industrial,Evaluation,Survey/Questionnaire Completion,passive observation,Participants watched a video of a robot interaction.,media,Participants watched a video of a robot interaction.,physical,Participants watched a video of a physical robot.,,The level of autonomy of the robot in the video was not specified.,Questionnaires; Custom Scales,Human-Computer Trust Scale/Questionnaire (HCT/HCTM); Human-Robot Trust Scale/Questionnaire (HRTS/HRITS),,Trust was measured using the HRITS questionnaire.,"parametric models (e.g., regression)",Confirmatory factor analysis was used to validate the HRITS scale.,Observational & Survey Studies,Quantitative Surveys,Indirect Manipulation,"Participants were shown a video of a cobot in an industrial setting, which indirectly influenced their expectations and perceptions of the robot and the interaction medium.","The study found that males had more trust in cobots than females, and that benevolence was the subscale with the highest scores.","The study found statistically significant differences in trust perceptions by gender, with males reporting higher trust in cobots than females. The benevolence subscale had the highest scores.",The Human-Robot Interaction Trust Scale (HRITS) is a useful and psychometrically sound instrument to assess users' trust in HRI context.,The robot was shown in a video performing assembly and packaging tasks in an industrial context. The human participant watched the video and then completed a questionnaire.,Multilevel Model; t-test,"Confirmatory Factor Analysis (CFA) was used to assess the construct validity of the Human-Robot Interaction Trust Scale (HRITS), including a second-order CFA to test a general 'trust' factor. An independent t-test was used to investigate differences in trust perceptions between genders.",FALSE,,,,"The study did not explicitly manipulate any factors. Participants were shown a video of a cobot in an industrial setting and completed a survey. While the video and user story might have influenced their perceptions, this was not an intentional manipulation of specific factors to measure their impact on trust. The study focused on validating a trust scale (HRITS) and examining differences in trust perceptions based on gender, not on manipulating any specific robot or task-related factors. Therefore, no factors were manipulated.",10.1155/2022/6437441,https://www.hindawi.com/journals/hbet/2022/6437441/,"Recently there has been an increasing demand for technologies (automated and intelligent machines) that brings benefits to organizations and society. Similar to the widespread use of personal computers in the past, today’s needs are towards facilitating human-machine technology appropriation, especially in highly risky and regulated industries like robotics, manufacturing, automation, military, finance, or healthcare. In this context, trust can be used as a critical element to instruct how human-machine interaction should occur. Considering the context-dependency and multidimensional trust, this study seeks to find a way to measure the effects of perceived trust in a collaborative robot (cobot), regardless of its literal credibility as a real person. This article aims at translating, adapting, and validating a Human-Computer Trust Scale (HCTM) in human-robot interaction (HRI) context and its application to cobots. The Human-Robot Interaction Trust Scale (HRITS) involved 239 participants and included eleven items. The 2nd order CFA with a general factor called “trust” have proven to be empirically robust (                                                   CFI                   =                   .94                                               ;                                                   TLI                   =                   .93                                               ;                                                   SRMR                   =                   .04                                               ; and                                                   RMSEA                   =                   .05                                               ) [                                                   CR                   =                   .84                                               ;                                                   AVE                   =                   .58                                               , and                                                   MaxR                                                               H                                                           =                   .92                                               ]; results indicated a good measurement of the general factor trust, and the model satisfied the criteria for measure trust. An analysis of the differences in perceptions of trust by gender was conducted using a                                                   t                                               -test. This analysis showed that statistical differences by gender exist (                                                   p                   =                   .04                                               ). This study’s results allowed for a better understanding of trust in HRI, specifically regarding cobots. The validation of a Portuguese scale for trust assessment in HRI can give a valuable contribution to designing collaborative environments between humans and robots."
"Pipitone, Arianna; Geraci, Alessandro; D’Amico, Antonella; Seidita, Valeria; Chella, Antonio",Robot’s Inner Speech Effects on Human Trust and Anthropomorphism,2023,1,51,51,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were randomly assigned to either an experimental group or a control group. Both groups completed questionnaires before and after interacting with a robot. The experimental group interacted with a robot using inner speech, while the control group interacted with a robot using only outer speech. The interaction involved a table setting task on a tablet.",Participants were asked to cooperate with a robot to set a virtual table following an etiquette schema. They could either move objects themselves or ask the robot to do it.,Pepper,Humanoid Robots; Expressive Robots,Research; Social,Manipulation,Table Setting,minimal interaction,"Participants interacted with the robot through a tablet interface, with verbal instructions but no physical touch.",simulation,"The interaction took place in a virtual environment on a tablet, simulating a table setting task.",physical,"The robot was physically present, but the interaction was mediated through a tablet interface.",shared control (fixed rules),"The robot followed pre-defined rules for its actions, responding to user requests and events on the tablet.",Questionnaires,Trust Perception Scale - HRI; Godspeed Questionnaire,,Trust was measured using the Trust Perception Scale-HRI questionnaire.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's behavior was manipulated by implementing an inner speech system in the experimental group, while the control group only had outer speech. This was intended to influence the perception of the robot's animacy and intelligence.","Trust increased for all participants, but the inner speech manipulation did not have a specific effect on trust levels. However, the inner speech manipulation did increase the perception of animacy and intelligence in the experimental group.","The study found that while trust increased for all participants, the inner speech manipulation only affected the perception of animacy and intelligence, not trust directly. This suggests that the inner speech system may not be a strong enough factor to influence trust in this context.","The main finding is that a robot equipped with an inner speech system is perceived as more animated and intelligent, but this does not directly translate to increased trust.","The robot provides verbal feedback and performs actions on a virtual table based on user requests. The human participant interacts with the robot through a tablet interface, selecting actions and moving virtual objects.",ANOVA; ANCOVA,"The study used 2x2 factorial ANOVAs and ANCOVAs with repeated measures to analyze the data. ANOVAs were used to examine the effects of 'Group' (experimental vs. control) and 'Time' (pre-test vs. post-test) on trust and the different dimensions of robot perception (anthropomorphism, animacy, likeability, perceived intelligence, and perceived safety). ANCOVAs were used to examine the influence of participants' self-talk on the effect of robot inner speech on trust and robot perception, using self-talk scores as a covariate.",TRUE,Robot-verbal-communication-content,,Robot-verbal-communication-content,"The study manipulated the robot's verbal communication by implementing an 'inner speech' system in the experimental group, while the control group only had 'outer speech'. This manipulation falls under 'Robot-verbal-communication-content' because it changes the content of what the robot communicates, adding internal thoughts and comments in the experimental condition, which is not present in the control condition. The results showed that this manipulation did not have a significant impact on trust levels, as trust increased for all participants regardless of the group. Therefore, 'Robot-verbal-communication-content' is listed as a factor that did not impact trust. The study did find that the inner speech manipulation increased the perception of animacy and intelligence, but this is not related to trust, so it is not included in the factors that impacted trust.",10.1007/s12369-023-01002-3,https://link.springer.com/10.1007/s12369-023-01002-3,"Inner Speech is an essential but also elusive human psychological process that refers to an everyday covert internal conversation with oneself. We argued that programming a robot with an overt self-talk system that simulates human inner speech could enhance both human trust and users’ perception of robot’s anthropomorphism, animacy, likeability, intelligence and safety. For this reason, we planned a pre-test/post-test control group design. Participants were divided in two different groups, one experimental group and one control group. Participants in the experimental group interacted with the robot Pepper equipped with an over inner speech system whereas participants in the control group interacted with the robot that produces only outer speech. Before and after the interaction, both groups of participants were requested to complete some questionnaires about inner speech and trust. Results showed differences between participants’ pretest and post-test assessment responses, suggesting that the robot’s inner speech inﬂuences in participants of experimental group the perceptions of animacy and intelligence in robot. Implications for these results are discussed."
"Pipkorn, Linda",Driver conflict response during supervised automation: Do hands on wheel matter?,2021,1,76,76,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants drove with supervised automation for 30 minutes on a test track. Then, they encountered a conflict object revealed by a lead-vehicle cut-out and had to avoid it.",Participants were required to supervise automated driving and intervene when needed to avoid a collision with a static object.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants supervised automation and intervened when needed.,real-world,The study was conducted on a test track with a real vehicle.,physical,Participants interacted with a physical vehicle.,shared control (fixed rules),The vehicle used supervised automation with fixed rules.,Questionnaires,,Video Data; Eye-tracking Data; Performance Metrics,Trust was measured using a post-experiment questionnaire and behavioral measures.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the hands-on-wheel requirement, the type of conflict object, and the level of trust in automation, influencing the driver's response.","High trust was associated with delayed responses and crashes, while low trust was associated with appropriate responses. The hands-on-wheel requirement did not affect response times.",Crashers generally responded later than non-crashers. High-trust drivers responded later than low-trust drivers. The hands-on-wheel requirement did not influence driver response. A larger conflict object triggered an earlier surprise reaction.,"High trust in automation is associated with late response and crashing, whereas low trust is associated with appropriate driver response.","The robot (automated vehicle) maintained lane and speed, while the human supervised the driving and intervened to avoid a collision with a static object.",Mann-Whitney U,"The Mann-Whitney U test was used to compare the response times (SRT, HOW, and DS) between different groups. These groups were formed based on conflict outcome (crashers vs. non-crashers), hands-on-wheel requirement (with vs. without), conflict object type (garbage bag vs. stationary vehicle), and reported trust level (high vs. low). The test was used because the data distributions for SRT, HOW, and DS were observed to be non-normal.",TRUE,Robot-autonomy; Task-environment,Robot-autonomy,Task-environment,"The study manipulated the hands-on-wheel requirement, which is a form of shared control and thus falls under 'Robot-autonomy'. The study also manipulated the type of conflict object (garbage bag vs. stationary vehicle), which changes the visual environment and thus falls under 'Task-environment'. The results showed that the level of trust in automation (high vs. low) significantly impacted the driver's response, with high trust leading to delayed responses and crashes, indicating that the level of 'Robot-autonomy' (specifically, the driver's perception of the automation's capabilities) influenced trust. The type of conflict object ('Task-environment') did not significantly impact the driver's response time or the likelihood of a crash, indicating that this factor did not impact trust.",,,"Securing appropriate driver responses to conﬂicts is essential in automation that is not perfect (because the driver is needed as a fall-back for system limitations and failures). However, this is recognized as a major challenge in the human factors literature. Moreover, in-depth knowledge is lacking regarding mechanisms affecting the driver response process. The ﬁrst aim of this study was to investigate how driver conﬂict response while using highly reliable (but not perfect) supervised automation differ for drivers that (a) crash or avoid a conﬂict object and (b) report high trust or low trust in automation to avoid the conﬂict object. The second aim was to understand the inﬂuence on the driver conﬂict response of two speciﬁc factors: a hands-on-wheel requirement (with vs. without), and the conﬂict object type (garbage bag vs. stationary vehicle). Seventy-six participants drove with highly reliable but supervised automation for 30 min on a test track. Thereafter they needed to avoid a static object that was revealed by a lead-vehicle cutout. The driver conﬂict response was assessed through the response process: timepoints for driver surprise reaction, hands-on-wheel, driver steering, and driver braking. Crashers generally responded later in all actions of the response process compared to non-crashers. In fact, some crashers collided with the conﬂict object without even putting their hands on the wheel. Driver conﬂict response was independent of the hands-on-wheel requirement. High-trust drivers generally responded later than the low-trust drivers or not at all, and only high trust drivers crashed. The larger stationary vehicle triggered an earlier surprise reaction compared to the garbage bag, while hands-on-wheel and steering response were similar for the two conﬂict object types. To conclude, crashing is associated with a delay in all actions of the response process. In addition, driver conﬂict response does not change with a hands-on-wheel requirement but changes with trust-level and conﬂict object type. Simply holding the hands on the wheel is not sufﬁcient to prevent collisions or elicit earlier responses. High trust in automation is associated with late response and crashing, whereas low trust is associated with appropriate driver response. A larger conﬂict object trigger earlier surprise reactions."
"Pippin, Charles; Christensen, Henrik",Trust modeling in multi-robot patrolling,2014,1,8,8,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants observed a robot team performing a patrolling task. One robot was designated as a poor performer, and the system dynamically reallocated tasks based on a trust model. Two different task reassignment methods were tested: centralized and local.",Participants observed a team of robots performing a patrolling task in a simulated museum environment.,TurtleBot,Mobile Robots,Research,Supervision,Monitoring,passive observation,Participants observed the robots' behavior without direct interaction.,real-world,The study was conducted in a real-world environment with physical robots.,physical,The study used physical robots.,fully autonomous (limited adaptation),"The robots operated autonomously, but with limited adaptation to unexpected scenarios.",Performance-Based Measures,,"Performance Metrics; robot data (sensor data, etc.)",Trust was assessed based on robot performance metrics and sensor data.,"parametric models (e.g., regression)",A probability-based trust model using the beta distribution was used to model trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The performance of one robot was intentionally degraded by reducing its maximum speed, which influenced the trust model and task reassignment.","The trust score of the poor performing robot decreased, leading to task reassignment to other robots.","The trust scores of some robots dipped briefly due to localization errors, but the model was tolerant to noise and the scores recovered. The local assignment approach resulted in multiple robots assisting the poor performer.",The use of a trust model improved performance in a multi-robot patrolling task by dynamically reassigning tasks from poorly performing robots to trusted ones.,"The robots autonomously patrolled a museum environment, and the human participants observed the robots' performance. The robots reported node visits to a central monitor, which updated the trust model and reallocated tasks.",,"No statistical tests were explicitly mentioned in the paper. The analysis focused on the performance of the robots and the trust model, with comparisons made based on observed refresh times and task reassignments. The paper presents graphs of trust scores and refresh times, but does not mention any specific statistical tests used to analyze these results.",TRUE,Robot-accuracy,Robot-accuracy,,"The study explicitly manipulated the performance of one robot by reducing its maximum speed, which directly impacted its ability to complete the patrolling task efficiently. This manipulation is categorized as 'Robot-accuracy' because it directly affects the robot's task performance metrics (speed and refresh time). The paper states, 'The performance for this type of robot is affected by randomly adjusting the maximum forward velocity of the robot after each visit to a patrol node.' This intentional degradation of performance led to a decrease in the robot's trust score, as described in the results section, 'After the robot 3 was observed performing poorly, its trust score decreased until it reached the low threshold (0.5) for trust and became untrusted.' Therefore, 'Robot-accuracy' is also the factor that impacted trust. There were no other factors manipulated in the study.",10.1109/ICRA.2014.6906590,http://ieeexplore.ieee.org/document/6906590/,"On typical multi-robot teams, there is an implicit assumption that robots can be trusted to effectively perform assigned tasks. The multi-robot patrolling task is an example of a domain that is particularly sensitive to reliability and performance of robots. Yet reliable performance of team members may not always be a valid assumption even within homogeneous teams. For instance, a robot’s performance may deteriorate over time or a robot may not estimate tasks correctly. Robots that can identify poorly performing team members as performance deteriorates, can dynamically adjust the task assignment strategy. This paper investigates the use of an observation based trust model for detecting unreliable robot team members. Robots can reason over this model to perform dynamic task reassignment to trusted team members. Experiments were performed in simulation and using a team of indoor robots in a patrolling task to demonstrate both centralized and decentralized approaches to task reassignment. The results clearly demonstrate that the use of a trust model can improve performance in the multi-robot patrolling task."
"Pompe, Babiche L.; Velner, Ella; Truong, Khiet P.",The Robot That Showed Remorse: Repairing Trust with a Genuine Apology,2022,1,25,23,2,2 participants were removed due to technical malfunctions,Online Crowdsourcing,between-subjects,"Participants first played a trust game with the robot, then the robot made a speech recognition error, and finally the robot either apologized with remorse, without remorse, or not at all, followed by another trust game.","Participants played a trust game with a robot, answered IQ questions, and then played another trust game.",Nao,Humanoid Robots; Expressive Robots,Research; Social,Game,Economic Game,minimal interaction,Participants interacted with the robot remotely via video call.,media,Participants interacted with the robot through a video call.,physical,Participants interacted with a physical robot via video.,wizard of oz (directly controlled),The robot's actions were controlled by a human operator.,Behavioral Measures; Questionnaires,Trust Perception Scale - HRI; Godspeed Questionnaire,,Trust was measured using a questionnaire and a behavioral measure of tokens invested.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's apology behavior (no apology, apology without remorse, apology with remorse) was manipulated after a speech recognition error to influence trust.","Apologies, especially with remorse, showed a trend towards restoring trust, with a significant effect on trusting behavior (number of tokens invested).","Perceived intelligence and number of tokens increased in the apology conditions after an error, which was unexpected. Some participants also apologized to the robot, showing varied attitudes towards the robot.","An apology with remorse from a robot after a speech recognition error showed a trend towards restoring trust, with a significant effect on trusting behavior.","The robot asked trivia questions and participants bet tokens on the robot answering correctly. The robot then asked IQ questions and misrecognized the answers. Finally, the robot either apologized or did not, and the participants played another trust game.",Kruskal-Wallis; Dunn test; Wilcoxon rank sum,"The study used Kruskal-Wallis tests to compare the differences in perceived intelligence, perceived trust, and number of tokens across the three apology conditions (no apology, apology, apology with remorse). A post-hoc Dunn test was used to examine pairwise differences between conditions. Wilcoxon rank sum tests were used to compare the pooled apology conditions (apology and apology with remorse) against the no apology condition for the same dependent variables. These tests were used to determine if the type of apology had a significant effect on trust and perceived intelligence after a robot error.",TRUE,Robot-emotional-display; Robot-verbal-communication-content,Robot-emotional-display; Robot-verbal-communication-content,,"The study manipulated the robot's apology behavior after a speech recognition error. The robot either gave no apology, an apology without remorse, or an apology with remorse. The remorse was conveyed through nonverbal cues (looking down, avoiding eye contact, covering face), which falls under 'Robot-emotional-display'. The content of the apology itself (e.g., 'Oh, I am sorry. I might have misunderstood. My speech recognition engine must have failed') is a manipulation of 'Robot-verbal-communication-content'. The results showed that the type of apology (including the presence of remorse) significantly impacted trusting behavior (number of tokens invested), indicating that both 'Robot-emotional-display' and 'Robot-verbal-communication-content' influenced trust. The study did not find any factors that did not impact trust.",10.1109/RO-MAN53752.2022.9900860,https://ieeexplore.ieee.org/document/9900860/,"In the current state-of-the-art, robots are bound to make errors in a human-robot interaction (HRI). Trust is one of the important concepts in HRI that is often lowered by these errors. Fortunately, research has shown there are strategies that can help rebuild trust. An apology made by the robot is one of those strategies. However, apologies can take different forms. We designed a study in which Nao first built trust with the users, then violated that trust by making a speech recognition error, and then tried to restore it by either an apology with display of remorse, without remorse, or no apology at all. The results showed expected trends; an apology with remorse in most cases rebuilt trust the strongest. Although the effect of the type of apology on the trusting beliefs were not significant, the effect on the trusting behaviours was found to be just significant. Suggestions for future research include repeating the study without its current limitations (small sample size, offline) and investigating the accuracy of the portrayed remorse by the robot."
"Poornikoo, Mehdi; Gyldensten, William; Vesin, Boban; Øvergård, Kjell Ivar","Trust in Automation (TiA): Simulation Model, and Empirical Findings in Supervisory Control of Maritime Autonomous Surface Ships (MASS)",2024,1,30,28,2,2 participants did not register the introduced error,Controlled Lab Environment,within-subjects,"Participants were introduced to the study, signed consent forms, completed questionnaires, calibrated eye-tracking, trained on the task, and then monitored an autonomous vessel with a system malfunction introduced mid-session. Self-report trust measures were collected before and after the error.","Participants monitored an autonomous vessel, ensuring its safe passage, and were expected to respond to alarms and execute corrective maneuvers in the event of a system malfunction.",Unspecified,Unmanned Ground Vehicles,Research,Supervision,Monitoring,passive observation,Participants observed the autonomous vessel through a simulator interface.,simulation,Participants interacted with a simulated environment of an autonomous vessel.,simulated,The robot was represented as a simulated vessel in a maritime simulator.,fully autonomous (limited adaptation),The vessel navigated autonomously but with limited adaptation to unexpected scenarios.,Questionnaires,,Eye-tracking Data,Trust was measured using questionnaires and eye-tracking data was collected for analysis.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"A system malfunction was introduced, causing a deviation from the vessel's course, which was intended to decrease trust.",The system malfunction led to a significant decline in perceived reliability and trust in automation.,"Two participants were excluded because they did not register the introduced error, leading to an anomalous increase in their trust in the automated system.","A system malfunction led to a significant decline in perceived reliability and trust in automation, and the timing of the malfunction significantly influenced trust recovery.","The robot, a simulated autonomous vessel, navigated a predefined course. The human participant monitored the vessel's performance, responding to alarms and taking corrective actions when a malfunction occurred.",paired samples t-test; Pearson correlation,"A paired samples t-test was used to assess the differences in gaze metrics, perceived reliability, and trust in automation before and after the error occurrence. A correlation analysis was performed to delineate the relationships between perceived reliability, trust in automation, and various personality traits across the two time points. The correlation analysis also examined the relationship between changes in trust and changes in perceived reliability.",TRUE,Robot-accuracy,Robot-accuracy,,"The study introduced a system malfunction that caused the vessel to deviate from its course. This directly impacts the robot's accuracy in performing its task, which is to navigate the vessel along a predefined path. The malfunction was an intentional manipulation to observe its effect on trust. The paper states, 'During the latter half of the session, the system was programmed to introduce a malfunction by halting one of the steering gear pumps. This malfunction led to the vessel's deviation from its charted course, subsequently activating an alarm to notify the participant of the issue.' This clearly indicates a manipulation of the robot's accuracy. The results showed that this manipulation significantly decreased trust, as stated in the paper: 'Results not only validated the proposed model but demonstrated a significant decline in perceived reliability and trust in automation as well as the monitoring strategy after the automation malfunction.' Therefore, 'Robot-accuracy' is the factor that impacted trust. The paper also states, 'Notably, the study reveals that personality traits do not directly influence perceived reliability or variations in trust.' This indicates that no other factors were found to impact trust.",10.1080/10447318.2024.2399439,https://www.tandfonline.com/doi/full/10.1080/10447318.2024.2399439,"Over the past three decades, Trust in Automation (TiA) has been the subject of extensive research. However, a large portion of the research takes a “static” approach to modeling trust and views trust as a linear unidirectional phenomenon. This view fails to recognize that trust is a dynamic construct that changes over time as an outcome of prolonged interaction with automation. The present study aims to address this gap and explore the nonlinear dynamic nature of trust by developing a simulation model of Trust in Automation (TiA) that can demonstrate trust evolution, deterioration, and recovery within the context of supervisory control of Maritime Autonomous Surface Ships (MASS). Employing System Dynamics (SD) approach, the model captures trust’s nonlinear and reciprocal nature through dynamic feedback loops, producing behavioral patterns con­ sistent with empirical observations of trust. The simulation results showcase the crucial role of ini­ tial trust conditions and the alignment of expectations with system performance in fostering trust and effective automation use. The study also explores the timing of system malfunctions, revealing that early faults have a greater negative impact on trust compared to later faults of the same magnitude. We tested a segment of the proposed model in an experimental study involving 30 human participants to investigate the effects of automation malfunctions on operators’ trust and behavioral responses during the supervisory control of MASS. Results not only validated the pro­ posed model but demonstrated a significant decline in perceived reliability and trust in automa­ tion as well as the monitoring strategy after the automation malfunction."
"Pynadath, David V.; Wang, Ning; Kamireddy, Sreekar",A Markovian Method for Predicting Trust Behavior in Human-Agent Interaction,2019,1,53,53,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants completed a background survey and then interacted with a virtual robot across eight reconnaissance missions. In each mission, the robot provided recommendations on whether to wear protective gear before entering buildings, and participants decided whether to follow or ignore the robot's advice.","Participants worked with a virtual robot to search buildings for potential danger, deciding whether to follow the robot's recommendations on wearing protective gear.",Unspecified,Mobile Robots; Legged Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a virtual robot in a simulated environment.,simulation,The interaction took place in a simulated environment.,simulated,The robot was a virtual representation in the simulation.,shared control (fixed rules),"The robot followed a policy generated from a POMDP, with fixed rules for recommendations.",Behavioral Measures; Questionnaires,Negative Attitude towards Robots Scale (NARS); Propensity to Trust Scales; Disposition to Trust Questionnaire,Performance Metrics,Trust was assessed using behavioral measures of following or ignoring the robot's advice and pre-survey questionnaires.,POMDP,The study used a POMDP to model the robot's decision-making process and predict human trust behavior.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's confidence-level elaboration, acknowledgement of mistakes, and embodiment were manipulated to influence trust. The robot provided or withheld confidence information, acknowledged errors, and had different appearances.","The robot's confidence-level elaboration had a significant impact on trust, while acknowledgement and embodiment had negligible impact.","The study found that initial trust behavior was influenced by pre-survey measures, but direct experience with the robot became the primary determinant of trust as the interaction progressed. The robot's confidence-level elaboration had a significant impact on trust, while acknowledgement and embodiment had negligible impact.",A probabilistic model can summarize and predict human trust behaviors based on background personality traits and limited history information from an HAI scenario.,The robot provided recommendations on whether to wear protective gear before entering buildings. The human participant decided whether to follow or ignore the robot's advice.,,"The study primarily used descriptive statistics, such as frequencies and percentages, to analyze the participants' trust behavior. The analysis involved calculating the frequency of following or ignoring the robot's recommendations under different conditions (e.g., robot's assessment accuracy, confidence level, and previous actions). The study also compared the frequency of following behavior between groups based on their pre-survey scores (e.g., propensity to trust) using median splits. No specific inferential statistical tests (e.g., t-tests, ANOVA) were explicitly mentioned.",TRUE,Robot-verbal-communication-content; Robot-aesthetics; Robot-accuracy,Robot-verbal-communication-content,Robot-aesthetics,"The study manipulated several factors related to the robot. 'Robot-verbal-communication-content' was manipulated by having half of the robots provide confidence-level elaborations on their assessments, while the other half did not. This is explicitly stated in the paper: 'Half of the robots augment their decisions with additional information that should help its teammate better understand its ability (e.g., decision-making)... These robots give a confidence-level elaboration that augments the decision message with additional information about the robot's uncertainty in its decision.' The paper also states that 'Half of the robots send an additional message every time they make an assessment that turns out to be incorrect; the other half do not send any such message.' However, this manipulation is not considered as a factor that impacted trust, as the paper states that 'The acknowledgment and embodiment had negligible impact in general.' 'Robot-aesthetics' was manipulated by having half of the robots look like a robotic dog and the other half look like a stereotypical robot. This is described in the paper: 'Half of the robots look like a robotic dog... The other half look like a stereotypical ""robot-looking"" robot'. The paper explicitly states that 'The acknowledgment and embodiment had negligible impact in general', so 'Robot-aesthetics' did not impact trust. 'Robot-accuracy' was implicitly manipulated by the study design, as the robot's assessment of the safety/danger of a building was correct in only 12 out of 15 buildings in each mission. This is described in the paper: 'We give the robot noisy observations, so that its assessment of the safety/danger of a building is correct in only 12 out of the 15 buildings in each mission.' While the robot's accuracy was not directly manipulated as a variable, it was a factor that influenced the participant's trust in the robot. The paper states that 'For the purposes of this investigation, the critical difference is whether or not the robot provides the confidence-level elaboration on its assessment. The acknowledgment and embodiment had negligible impact in general.' This indicates that 'Robot-verbal-communication-content' was the only factor that impacted trust, while 'Robot-aesthetics' did not.",10.1145/3349537.3351905,https://dl.acm.org/doi/10.1145/3349537.3351905,"Trust calibration is critical to the success of human-agent interaction (HAI). However, individual differences are ubiquitous in people’s trust relationships with autonomous systems. To assist its heterogeneous human teammates calibrate their trust in it, an agent must first dynamically model them as individuals, rather than communicating with them all in the same manner. It can then generate expectations of its teammates’ behavior and optimize its own communication based on the current state of the trust relationship it has with them. In this work, we examine how an agent can generate accurate expectations given observations of only the teammate’s trust-related behaviors (e.g., did the person follow or ignore its advice?). In addition to this limited input, we also seek a specific output: accurately predicting its human teammate’s future trust behavior (e.g., will the person follow or ignore my next suggestion?). In this investigation, we construct a model capable of generating such expectations using data gathered in a humansubject study of behavior in a simulated human-robot interaction (HRI) scenario. We first analyze the ability of measures from a presurvey on trust-related traits to accurately predict subsequent trust behaviors. However, as the interaction progresses, this effect is dwarfed by the direct experience. We therefore analyze the ability of sequences of prior behavior by the teammate to accurately predict subsequent trust behaviors. Such behavioral sequences have shown to be indicative of the subjective beliefs of other teammates, and we show here that they have a predictive power as well."
"Qu, Jianhong; Zhou, Ronggang; Zhang, Yaping; Ma, Qianli","Understanding trust calibration in automated driving: the effect of time, personality, and system warning design",2023,1,64,64,0,No participants were excluded,Controlled Lab Environment,within-subjects,Participants completed questionnaires on trust propensity and neuroticism before the experiment. They then participated in a driving simulator experiment with automated driving and takeover scenarios. Trust was measured at multiple points during and after the experiment. Participants were randomly assigned to one of two studies based on warning information design (content or countdown).,"Participants were asked to drive in a simulator, using automated driving mode and taking over control when prompted by the system. They also played a Tetris game on an iPad during automated driving.",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the automated driving system in a simulator, taking over control when prompted.",simulation,The study used a driving simulator to create an interactive virtual environment.,simulated,The robot was a simulated automated driving system within the driving simulator.,shared control (fixed rules),The automated driving system operated independently but required human takeover at specific points based on pre-set rules.,Questionnaires,Jian et al. Trust Scale; Propensity to Trust Scales,,Trust was measured using questionnaires at multiple points in the experiment.,"parametric models (e.g., regression)",The study used regression analysis to model the influence of various factors on trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The study manipulated the warning stage (one-stage or two-stage) and the warning information (content or countdown) to influence trust. The system also required the user to take over control at specific points.,"Trust increased during the experiment but decreased after four months, although it remained higher than the initial trust level. Two-stage warning designs increased trust compared to one-stage designs. Neuroticism had a significant effect on the countdown warning.","Trust increased during the experiment but decreased after four months, although it remained higher than the initial trust level. The study found that initial trust plays a crucial role in long-term trust, and that trust propensity has a stable effect. Neuroticism had a significant effect on the countdown warning, but not on the content warning.","Initial trust has a lasting impact on trust, and the trust propensity trait has a stable effect. Compared with the one-stage warning system, the driver places greater trust in the two stages.","The automated driving system drove the vehicle, and the human participant monitored the system and took over control when prompted by the system's warning. The human also played a Tetris game during automated driving.",ANOVA; Linear regression; structural equation models,"The study used repeated-measures ANOVA to examine the effect of time on trust and to compare trust levels across different groups (high vs. low trust propensity, high vs. low neuroticism). Hierarchical regression analysis was used to assess the influence of human-related factors (trust propensity, neuroticism), system-related factors (warning stage, warning information), and time-related factors on trust during the experiment. Structural equation modeling was used to explore the relationships between trust propensity, initial trust, trust during the experiment, and post-experiment trust.",TRUE,Robot-verbal-communication-content; Robot-autonomy,Robot-verbal-communication-content; Robot-autonomy,,"The study manipulated the warning stage (one-stage or two-stage) and the warning information (content or countdown) to influence trust. The warning stage manipulation is categorized as 'Robot-autonomy' because it directly affects the level of control the automated system has before requiring human intervention. Specifically, the two-stage warning (MR + TOR) provides an earlier monitoring request, which can be seen as a form of shared control where the system signals the need for human attention before a full takeover. The warning information manipulation (content or countdown) is categorized as 'Robot-verbal-communication-content' because it changes the specific information communicated to the driver during the takeover request. The content warning provides a description of the hazard, while the countdown warning provides time-related information. Both of these are changes to the content of the verbal communication from the system. The study found that both the warning stage (two-stage design increased trust) and the warning information (content or countdown) impacted trust, although the impact of the warning information was not significant. Therefore, both 'Robot-verbal-communication-content' and 'Robot-autonomy' are listed as factors that impacted trust. The study did not find any manipulated factors that did not impact trust.",10.1080/00140139.2023.2191907,https://www.tandfonline.com/doi/full/10.1080/00140139.2023.2191907,"Under the human-automation codriving future, dynamic trust should be considered. This paper explored how trust changes over time and how multiple factors (time, trust propensity, neuroti­ cism, and takeover warning design) calibrate trust together. We launched two driving simulator experiments to measure drivers’ trust before, during, and after the experiment under takeover scenarios. The results showed that trust in automation increased during short-term interactions and dropped after four months, which is still higher than pre-experiment trust. Initial trust and trust propensity had a stable impact on trust. Drivers trusted the system more with the twostage (MR þ TOR) warning design than the one-stage (TOR). Neuroticism had a significant effect on the countdown compared with the content warning."
"Quinn, Daniel B.; Pak, Richard; de Visser, Ewart J.",Testing the Efficacy of Human-Human Trust Repair Strategies with Machines,2017,1,160,160,0,Individuals that score three or more standard deviations from the mean will be considered outliers and will be excluded from future analysis.,Controlled Lab Environment,mixed design,"Participants will complete a taxi dispatching task with an automated system. They will also complete a secondary task of monitoring a communications panel. The automation will sometimes fail, and after a failure, will provide either an apology or a denial. Participants will complete two blocks of trials, one with competency failures and one with integrity failures. Trust will be measured after each block.","Participants will play the role of a taxi driver for an app-based ride-share company, selecting customers to maximize income and rating. They will also monitor a communications panel for a target call sign.",Unspecified,Other,Research,Supervision,Monitoring,minimal interaction,Participants interact with a simulated automation system through a computer interface.,simulation,The interaction takes place in a simulated environment on a computer screen.,simulated,The robot is represented as a simulated automation system on a computer screen.,shared control (fixed rules),The automation system operates independently but follows fixed rules for task execution and responses.,Questionnaires,Muir's Trust Questionnaire,,Trust is measured using a questionnaire adapted from Lee and Moray (1994).,no modeling,The study does not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulates system reliability, failure type (competence or integrity), and repair strategy (apology or denial) to influence trust. The system's reliability is manipulated by varying the frequency of failures, and the type of failure is manipulated by the error message shown to the user. The repair strategy is manipulated by the message shown after a failure.","The study hypothesizes that trust will be lower in less reliable automation. It also expects that repair strategies will only have significantly different effects on trust when automation reliability is high. Specifically, apologies will better repair trust for competency-based failures and denials will better repair trust for integrity-based failures when reliability is high.","The study anticipates a three-way interaction between reliability, repair strategy, and failure type, suggesting that the effectiveness of repair strategies is contingent on the reliability of the system and the type of failure.","The study aims to determine whether apologies and denials are effective at repairing trust in human-automation interaction, and how this is affected by failure type and system reliability.","The automation system provides calculations of the closest fares, estimated fare, and traffic information. The human participant selects customers to maximize income and rating, and monitors a communications panel for a target call sign.",ANOVA,"The study will use a repeated measures analysis of variance (ANOVA) to analyze the history-based trust data. The ANOVA will examine the effects of repair strategy (apology, denial), failure type (competence, integrity), and reliability (low, high) on trust.",TRUE,Robot-verbal-communication-content; Robot-accuracy; Task-complexity,Robot-verbal-communication-content; Robot-accuracy,,"The study manipulates 'Robot-verbal-communication-content' by varying the error messages displayed after a system failure, specifically using apologies or denials. This is explicitly stated in the 'Design and Procedure' section: 'To manipulate failure type and repair strategy, we will show participants an error message immediately following system failures. Each of these messages will be composed of one failure type (competency or integrity) and one repair strategy (apology or denial).' The study also manipulates 'Robot-accuracy' by varying the reliability of the automation system (60% or 80% success rate), as described in the 'Design and Procedure' section: 'System reliability will be manipulated between subjects using two levels: low and high...low reliability conditions will be set to 60%...and high reliability conditions will be set to 80%'. The study also manipulates 'Task-complexity' by adding a secondary task of monitoring a communications panel, as described in the 'Experimental tasks' section: 'To increase the general level of workload and ensure that participants use the automation...participants will also complete a secondary task. Participants will monitor the communications panel for the target call sign ""MASON""'. The study anticipates that both 'Robot-verbal-communication-content' (apology vs. denial) and 'Robot-accuracy' (high vs. low reliability) will impact trust, as stated in the 'ANTICIPATED RESULTS' section: 'We anticipate a main effect for reliability such that participants will exhibit lower trust in less reliable automation. Next, we expect that there will be a three-way interaction of reliability, repair strategy, and failure type such that repair strategies will only have significantly different effects on trust when automation reliability is high.' There is no indication that the secondary task of monitoring the communications panel impacted trust, it was only used to increase workload.",10.1177/1541931213601930,http://journals.sagepub.com/doi/10.1177/1541931213601930,"Trust is a critical component to both human-automation and human-human interactions. Interface manipulations, such as visual anthropomorphism and machine politeness, have been used to affect trust in automation. However, these design strategies are meant to primarily facilitate initial trust formation but have not been examined as a means to actively repair trust that has been violated by a system failure. Previous research has shown that trust in another party can be effectively repaired after a violation using various strategies, but there is little evidence substantiating such strategies in human-automation context. The current study will examine the effectiveness of trust repair strategies, derived from a human-human or human-organizational context, in human-automation interaction."
"Rae, Irene; Takayama, Leila; Mutlu, Bilge","In-body experiences: embodiment, control, and trust in robot-mediated communication",2013,1,58,58,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were paired and completed a museum tour task and a daytrader game across three conditions: non-embodied local control, embodied local control, and embodied remote control.",Participants completed a museum tour task and a daytrader game.,Texai Alpha,Telepresence Robots,Research,Game,Economic Game,minimal interaction,Participants interacted with each other through a telepresence robot or a tablet.,real-world,Participants interacted through a physical robot or a tablet in a real-world setting.,physical,The robot was physically present in the interaction.,wizard of oz (directly controlled),The robot was directly controlled by a human operator.,Behavioral Measures; Questionnaires,Specific Interpersonal Trust Scale,Performance Metrics,Trust was measured using a game and a questionnaire.,"parametric models (e.g., regression)",The study used t-tests and multilevel regression models to analyze trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The study manipulated the physical embodiment of the communication system (tablet vs. robot) and who controlled the robot (local vs. remote user) to see how it affected trust.,Trust increased more when participants used a physically embodied system and when the local participant controlled the system.,"The study found that local control of the robot led to higher trust, which was contrary to the initial hypothesis that remote control would increase trust.","Physical embodiment increased trust, and local control of the robot led to higher trust than remote control.","The robot was used as a communication medium, and the human participants completed a museum tour and a daytrader game, with one participant giving a tour and both participants playing the game.",t-test; multilevel regression model; cohen's κ,"The study used independent-samples t-tests to compare measurements between individuals in similar control conditions or roles across the three experimental conditions. A multilevel regression model was used for dyadic analysis to compare data between and within dyads, accounting for the non-independence of measurements within pairs. The model treated participants as individuals nested within pairs and measurements from individuals as repeated measures within pairs. Cohen's kappa was used to assess inter-rater reliability for the museum recall task.",TRUE,Robot-embodiment; Robot-autonomy,Robot-embodiment; Robot-autonomy,,"The study manipulated two main factors: the physical embodiment of the communication system (tablet vs. robot) and who controlled the robot (local vs. remote user). The manipulation of physical embodiment is categorized as 'Robot-embodiment' because it directly changes the physical presence of the communication medium. The manipulation of control is categorized as 'Robot-autonomy' because it changes the decision authority over the robot's movements. The results showed that both 'Robot-embodiment' and 'Robot-autonomy' impacted trust levels. Specifically, trust increased more when participants used a physically embodied system and when the local participant controlled the system. There were no factors that were manipulated that did not impact trust.",10.1145/2470654.2466253,https://dl.acm.org/doi/10.1145/2470654.2466253,"Communication technologies are becoming increasingly diverse in form and functionality, making it important to identify which aspects of these technologies actually improve geographically distributed communication. Our study examines two potentially important aspects of communication technologies which appear in robot-mediated communication—physical embodiment and control of this embodiment. We studied the impact of physical embodiment and control upon interpersonal trust in a controlled laboratory experiment using three different videoconferencing settings: (1) a handheld tablet controlled by a local user, (2) an embodied system controlled by a local user, and (3) an embodied system controlled by a remote user (n = 29 dyads). We found that physical embodiment and control by the local user increased the amount of trust built between partners. These results suggest that both physical embodiment and control of the system inﬂuence interpersonal trust in mediated communication and have implications for future system designs."
"Rahman, S. M. Mizanoor; Sadrfaridpour, Behzad; Wang, Yue",Trust-Based Optimal Subtask Allocation and Model Predictive Control for Human-Robot Collaborative Assembly in Manufacturing,2015,1,15,15,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a collaborative assembly task with a robot in two conditions: with and without Model Predictive Control (MPC). Each participant completed the assembly task for 30 minutes in each condition. Human and robot performance were measured in real-time, and faults were assessed using a Likert scale every 4 minutes. Participants also rated their likeability of the collaboration using a 5-point Likert scale.",Participants collaborated with a robot to assemble LEGO blocks into a final product following specific instructions and sequence.,Baxter,Humanoid Robots; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically collaborated with the robot to assemble objects.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The study used a physical robot for the interaction.,shared control (adaptive),"The robot adapted its speed based on the human's speed, using a Model Predictive Control (MPC) approach.",Behavioral Measures; Performance-Based Measures; Real-time Trust Measures,,"Performance Metrics; robot data (sensor data, etc.)","Trust was measured using real-time performance data, behavioral measures, and a Likert scale for faults.","parametric models (e.g., regression)",Time-series models were used to model human-robot bilateral trust dynamics.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers manipulated the robot's behavior by implementing a Model Predictive Control (MPC) to adjust the robot's speed based on the human's speed, and provided feedback on performance and faults.","The MPC increased trust by minimizing speed differences between the human and the robot, and by maximizing the overall trust values.","The study found that the human naturally changes their speed based on their trust in the robot. The subtask re-allocation was not necessary for any of the trials, indicating the effectiveness of the feedforward optimization strategy.","The trust-triggered MPC minimized speed differences between the human and the robot, which maximized the overall trust values and increased the human's likeability of the collaboration.","The robot manipulated LEGO parts from different positions to a central assembly point, while the human assembled the parts following a specific sequence. The robot's speed was adjusted based on the human's speed using MPC.",Linear regression; time-series model,"The study used a regression model to identify trust factors and a time-series model to investigate human's trust dynamics. The time-series models were also used to model human-robot bilateral trust dynamics, specifically the dynamic evolution of human-to-robot trust and robot-to-human trust. The models included factors such as robot performance, robot fault, human performance, and human fault.",TRUE,Robot-autonomy; Robot-accuracy,Robot-autonomy; Robot-accuracy,,"The study manipulated the robot's autonomy by implementing a Model Predictive Control (MPC) that adjusted the robot's speed based on the human's speed. This is described in the 'THE MODEL PREDICTIVE CONTROL (MPC)' section, where it states 'a trust-based MPC [38] is designed so that if 2 ( ) increases and it exceeds a threshold, the MPC acts to minimize the difference between 2 ( ) and 2 ( )'. This directly changes the robot's decision-making authority, fitting the 'Robot-autonomy' category. The study also manipulated the robot's accuracy by using the MPC to minimize speed differences between the human and the robot, which directly impacts the robot's performance on the task. This is supported by the text 'The MPC minimizes variations in 2 as well as maximizes the 2 through minimizing the difference between human speed and robot speed, and maximizing the robot speed'. The results showed that the MPC, which is a manipulation of both robot autonomy and accuracy, increased trust by minimizing speed differences and maximizing overall trust values, as stated in the 'EXPERIMENT RESULTS' section: 'the MPC reduces the differences in assembly times (hence, in assembly speeds) between the human and the robot. This also matches with the corresponding trust values in Fig. 12, which shows that the differences in the assembly speeds (Fig. 11) follow the trust differences (Fig. 12). If we compare Fig. 12 with Fig. 9, we see that the trust magnitudes have also increased in Fig. 12 due to the MPC.' Therefore, both 'Robot-autonomy' and 'Robot-accuracy' are listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1115/DSCC2015-9850,"https://asmedigitalcollection.asme.org/DSCC/proceedings/DSCC2015/57250/Columbus,%20Ohio,%20USA/230595","We develop a one human-one robot hybrid cell for collaborative assembly in manufacturing. The selected task is to assemble a few LEGO parts into a final assembled product following specified instructions and sequence in collaboration between the human and the robot. We develop a two-level feedforward optimization strategy that determines the optimal subtask allocation between the human and the robot for the selected assembly before the assembly starts. We derive dynamics models for human’s trust in the robot and the robot’s trust in the human for the assembly and estimate the trusts. The aim is to maintain satisfactory trust levels between the human and the robot through the application of the optimal subtask allocation. Again, subtask re-allocation is proposed to regain trusts if the trusts reduce to below the specified levels. Furthermore, it is hypothesized that fluctuations in human’s trust in the robot may cause fluctuations in human’s speeds and the human may appreciate if the robot adjusts its speeds with changes in human speeds. Hence, trustbased Model Predictive Control (MPC) is proposed to minimize the variations between human and robot speeds and to maximize the trusts. Experiment results prove the effectiveness of the hybrid cell, the feedforward optimal subtask allocation and of the trust-based MPC. The results also show that the overall assembly performance can be enhanced and the performance status can be monitored through a single dynamic parameter, i.e. the trust."
"Rahman, S.M. Mizanoor; Wang, Yue",Mutual trust-based subtask allocation for human–robot collaboration in flexible lightweight assembly in manufacturing,2018,1,20,20,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants practiced the assembly task, then performed the task under three conditions: two-way trust, one-way trust, and no trust. Data was collected on performance, trust, workload, and team fluency.",Participants collaborated with a robot to assemble LEGO blocks following a specific sequence.,Baxter,Humanoid Robots; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically collaborated with the robot to assemble parts.,real-world,The interaction took place in a real-world lab setting with a physical robot.,physical,The robot was a physical Baxter robot.,shared control (fixed rules),The robot followed pre-programmed motions but its speed was adjusted based on human performance.,Behavioral Measures; Performance-Based Measures; Real-time Trust Measures,NASA Task Load Index (NASA-TLX); Situational Awareness Rating Technique (SART),"Performance Metrics; robot data (sensor data, etc.)","Trust was measured using real-time performance and fault data, and questionnaires were used to assess workload and situational awareness.","parametric models (e.g., regression)",Trust was modeled using time-series models based on robot and human performance and fault data.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the presence of mutual trust by displaying or not displaying trust values, and by using trust to trigger subtask reallocation. The robot's speed was also adjusted based on human performance.","Two-way trust resulted in higher trust, team fluency, situation awareness, productivity, quality, utilization ratio and likeability, and lower cognitive workload than one-way or no trust conditions.","The study found that two-way trust was more effective than one-way trust or no trust. The robot's difficulty in performing one specific subtask led to subtask reallocation, highlighting the importance of feedback in the system.",The integrated (feedforward + feedback) optimum subtask allocation scheme triggered by mutual trust was effective in maintaining satisfactory trust levels and improving HRI and assembly performance.,"The robot manipulated parts from input locations to a central assembly location, and the human assembled the parts. The robot's speed was adjusted to match the human's pace, and the robot displayed trust values on its screen.",ANOVA,"Analyses of Variances (ANOVAs) were used to determine if there were statistically significant differences in situational awareness, cognitive workload, likeability, team fluency, productivity, and quality among the three experimental conditions: no trust, one-way trust, and two-way trust. The ANOVAs were used to compare the means of these dependent variables across the different trust conditions.",TRUE,Robot-verbal-communication-content; Robot-autonomy,Robot-verbal-communication-content; Robot-autonomy,,"The study manipulated the presence of mutual trust by displaying or not displaying trust values, which falls under 'Robot-verbal-communication-content' as it changes what information is communicated to the human. The study also manipulated the robot's speed based on human performance and subtask reallocation based on trust levels, which is a change in decision authority and thus falls under 'Robot-autonomy'. The results showed that the presence of two-way trust (displaying both human's trust in robot and robot's trust in human) resulted in higher trust, team fluency, situation awareness, productivity, quality, utilization ratio and likeability, and lower cognitive workload than one-way or no trust conditions. This indicates that both the display of trust values ('Robot-verbal-communication-content') and the subtask reallocation based on trust ('Robot-autonomy') impacted trust levels. The study did not explicitly state any factors that were manipulated but did not impact trust.",10.1016/j.mechatronics.2018.07.007,https://linkinghub.elsevier.com/retrieve/pii/S0957415818301211,"A human–robot hybrid cell is developed for ﬂexible assembly in manufacturing through the collaboration between a human and a robot. The selected task is to assemble a few LEGO blocks (parts) into a ﬁnal product following speciﬁed sequence and instructions. The task is divided into several subtasks. A two-level feedforward optimization strategy is developed that determines optimum subtask allocation between the human and the robot before the assembly starts. Human's trust in robot and robot's trust in human are considered, computational models of the trust are derived and real-time trust measurement and display methods are developed. A feedback approach is integrated into the feedforward subtask allocation in the form of subtask re-allocation if trust levels reduce to below speciﬁed thresholds. It is hypothesized that subtask re-allocation may help regain trust and maintain satisfactory performance. Experiment results prove that (i) the integrated (feedforward + feedback) optimum subtask allocation is eﬀective to maintain satisfactory trust levels of human and robot that result in satisfactory human–robot interactions (HRI) and assembly performance, and (ii) consideration of two-way trust (human's trust in robot and robot's trust in human) produces better HRI and assembly performance than that produced when one-way trust (human's trust in robot) or no trust is considered."
"Rahman, S.M. Mizanoor",Mixed-initiative collaboration between a humanoid robot and a virtual human through a common platform for a real-world common task: Evaluation and benchmarking,2019,1,52,52,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were divided into five groups. Four groups interacted with different master agents (human, Skyped-human, virtual human, robot) to find a hidden object. The fifth group evaluated the collaboration between the robot and virtual human.",Participants were asked to collaborate with a master agent to find a hidden object in a room.,Nao,Humanoid Robots,Research; Social,Social,Social Guidance/Coaching,minimal interaction,Participants received verbal instructions from the master agent and interacted with the robot or virtual human.,real-world,"The robot was physically present in the room, and the virtual human was displayed on a screen.",physical,The robot was a physical entity in the real-world environment.,shared control (fixed rules),The robot followed pre-programmed rules and responded to the virtual human's instructions.,Questionnaires; Performance-Based Measures; Real-time Trust Measures,NASA Task Load Index (NASA-TLX),"Performance Metrics; robot data (sensor data, etc.)","Trust was measured using questionnaires, performance metrics, and real-time trust measures based on robot and virtual human performance.","parametric models (e.g., regression)",Trust was modeled using a time-series model based on performance and faults of the robot and virtual human.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the type of master agent (human, Skyped-human, virtual human, robot) and the roles of the agents (master/follower) based on trust.","Trust was influenced by the type of master agent, with human agents generally eliciting higher trust. The robot and virtual human switched roles based on their mutual trust.","The robot's trust in the virtual human was higher than the virtual human's trust in the robot, leading to the virtual human acting as the master agent in most trials. The robot's lower stability and vulnerability to disturbances might have reduced its overall performance and trustworthiness.","The study demonstrated a successful real-world collaboration between a humanoid robot and a virtual human through a common platform, with trust-triggered mixed-initiatives, and high success rate in the collaborative task.","The robot and virtual human collaborated to find a hidden object, with one agent acting as the master and the other as the follower. The human participants evaluated the interaction and the service provided.",ANOVA,"Analyses of Variances (ANOVAs) were used to determine if variations in evaluation scores of agent attributes, interaction quality, follower agent's workload, follower human's situation awareness, and service beneficiary's likeability due to subjects were statistically significant. The results showed that the variations were statistically non-significant (p > 0.05 for each case), indicating the generality of the results.",TRUE,Robot-autonomy; Robot-verbal-communication-content,Robot-autonomy; Robot-verbal-communication-content,,"The study manipulated the type of master agent (human, Skyped-human, virtual human, robot) and the roles of the agents (master/follower) based on trust. This manipulation of roles based on trust directly relates to 'Robot-autonomy' as it changes the decision-making authority of the robot and virtual human. The study also manipulated the verbal instructions given by the master agent, which falls under 'Robot-verbal-communication-content'. The paper states, 'The virtual human tells, ""hi robot! I will help you find out the hidden object, follow me""' and 'the robot uses some verbal expressions such as tells, ""hi virtual human! thank you for instructing the location of the object, now I can try to find it""'. These are examples of the content of the verbal communication being manipulated. The results showed that the type of master agent influenced trust, with human agents generally eliciting higher trust. The robot and virtual human switched roles based on their mutual trust, indicating that the manipulation of autonomy impacted trust. The verbal communication content also impacted trust as the clarity and helpfulness of the instructions influenced the follower agent's ability to find the object, which in turn affected trust.",10.3233/AIS-190535,https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/AIS-190535,"This article presents mixed-initiative collaboration between two artiﬁcial agents of heterogeneous realities (a humanoid robot and a virtual human) for a real-world social task communicated through a common platform. A detailed framework for the collaboration is developed. Based on the framework, a representative real-world task to be performed through collaboration between the artiﬁcial agents for the wellbeing of humans is decided, which is to search for a missing (hidden) object in a homely environment. The agents are enriched with various similar intelligence and autonomy, functionalities and interaction modalities, and are integrated through a common communication platform based on a collaboration scheme so that they can collaborate (assist each other) to perform the real-world task. As a part of the collaboration scheme, a robot-virtual human bilateral trust model is derived and a real-time trust measurement method is developed so that the role of taking initiative in the collaboration can be switched between the agents triggered by the bilateral trust, which results in a mixed-initiative collaboration. An evaluation scheme is developed to evaluate the performance of the agents for the collaborative task. To benchmark their performance in the collaboration, human’s collaborations with the artiﬁcial agents and with some other allied agents for the same task are studied. The evaluation and benchmarking results show that both the robot and the virtual human perform satisfactorily in their collaboration, which proves the effectiveness of the real-world collaboration between the artiﬁcial agents of heterogeneous realities as well as justiﬁes the effectiveness of the common platform and of the bilateral trust-based mixed-initiatives between the artiﬁcial agents. The results can be used to develop intelligent agents of heterogeneous realities to assist humans in various realworld tasks or help humans get real-world tasks performed in cooperation between artiﬁcial agents of heterogeneous realities."
"Razin, Yosef S.; Feigh, Karen M.",Hitting the Road: Exploring Human-Robot Trust for Self-Driving Vehicles,2020,1,29,29,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a pre-experiment questionnaire, then drove a simulated course with both human-driven and self-driving cars, and finally completed a post-task questionnaire.","Participants drove a simulated car around a virtual campus, encountering both human-driven and self-driving cars at intersections.",Unspecified,Autonomous Vehicles,Research,Navigation,Street Crossing,minimal interaction,Participants interacted with simulated vehicles in a driving simulator.,simulation,Participants drove in an immersive driving simulator.,simulated,The robots were simulated vehicles in a virtual environment.,pre-programmed (non-adaptive),The self-driving cars followed a pre-programmed behavior.,Questionnaires; Custom Scales,Jian et al. Trust Scale; Interpersonal Trust Scale/Questionnaire,Performance Metrics,Trust was assessed using pre- and post-experiment questionnaires and performance metrics.,"parametric models (e.g., regression)",Linear mixed models were used to analyze the effect of car type on trust dimensions.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the perceived type of vehicle (human-driven vs. self-driving) while keeping their behavior identical, influencing expectations and perceptions of trust.","The perceived 'Ability' of self-driving cars was rated lower than human-driven cars, despite identical behavior.","Participants rated human-driven cars as more trustworthy, less irritating, and more familiar than self-driving cars, despite both behaving identically. There was a significant effect of car type on the 'Ability' dimension of trust, but not on 'Integrity'.","Perceived 'Ability' of self-driving cars was rated lower than human-driven cars, despite identical behavior, revealing an anti-automation bias.","Participants drove a simulated car, and the simulated vehicles (both human-driven and self-driving) behaved according to pre-programmed rules, interacting with the participant at intersections.",Spearman correlation; cronbach α; Factor analysis; kaiser-meyer-olkin (kmo) test; bartlett's test of sphericity; Fisher's exact test; Wilcoxon signed-rank test; linear mixed models,"The study used Spearman's rank correlation to assess relationships between driving experience and other variables. Cronbach's alpha was used to assess the internal consistency of a scale related to attitudes towards automation. Exploratory factor analysis (EFA) was employed to determine the underlying structure of trust-related questions, with the Kaiser-Meyer-Olkin (KMO) test and Bartlett's test of sphericity used to assess the suitability of the data for factor analysis. Fisher's exact test was used to compare binary judgements of trustworthiness and irritation between human-driven and self-driving cars. A paired Wilcoxon signed-rank test was used to compare perceived familiarity between car types. Finally, linear mixed models were used to analyze the effect of car type on the 'Ability' and 'Integrity' dimensions of trust, accounting for within-subject variance.",TRUE,Robot-aesthetics,Robot-aesthetics,,"The study manipulated the perceived type of vehicle (human-driven vs. self-driving) by changing the visual appearance of the cars, while their behavior was identical. This falls under 'Robot-aesthetics' because the manipulation was purely visual, changing the style of the car to represent either a human-driven or self-driving vehicle. The paper states, 'While the two categories of cars appeared different (see Fig. 1), they were programmed to behave exactly the same.' This manipulation of visual appearance impacted trust, as the study found that 'the human-driven car being rated more favorably along both factors' of trustworthiness and irritation, and that the 'Ability' construct was rated higher for human-driven cars. The study did not manipulate any other factors from the provided list.",10.1109/ICHMS49158.2020.9209525,https://ieeexplore.ieee.org/document/9209525/,"With self-driving cars making their way on to our roads, we ask not what it would take for them to gain acceptance among consumers, but what impact they may have on other drivers. How they will be perceived and whether they will be trusted will likely have a major effect on trafﬁc ﬂow and vehicular safety. This work ﬁrst undertakes an exploratory factor analysis to validate a trust scale for human-robot interaction and shows how previously validated metrics and general trust theory support a more complete model of trust that has increased applicability in the driving domain. We experimentally test this expanded model in the context of human-automation interaction during simulated driving, revealing how using these dimensions uncovers signiﬁcant biases within human-robot trust that may have particularly deleterious effects when it comes to sharing our future roads with automated vehicles."
"Rehm, Matthias; Pontikis, Ioannis; Hald, Kasper",Automatic Trust Estimation From Movement Data in Industrial Human-Robot Collaboration Based on Deep Learning,2024,1,20,20,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed two collaborative tasks (transport and draping) with a robot, each for ten minutes. Half started with transport, half with draping. They wore a motion tracking suit. After each session, they completed a trust questionnaire.",Participants collaborated with a robot on two tasks: transporting fabric and draping fabric.,Unspecified,Industrial Robot Arms,Industrial; Research,Manipulation,Object Passing,direct-contact interaction,Participants physically collaborated with the robot on material handling tasks.,real-world,The study involved real-world interaction with a physical robot.,physical,Participants interacted with a physical industrial robot arm.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions without adapting to the user.,Questionnaires; Behavioral Measures,Trust Perception Scale - HRI,robot data; Video Data,Trust was measured using a questionnaire and behavioral data from motion tracking.,"deep learning (e.g., neural networks, reinforcement learning)",Deep learning models were used to classify trust levels based on motion tracking data.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's speed was manipulated to induce different levels of trust, with slow and fast speeds used for both transport and draping tasks.","Robot speed influenced trust levels, with slower speeds generally leading to higher trust.",The study found that models trained on data that took the dynamic development of trust into account yielded higher accuracy and fostered more robust generalization. There was a large drop in accuracy when models were validated on unseen data.,"Automatic trust assessment from motion tracking data is feasible, with the best results achieved using LMA features and considering the dynamics of trust development.",The robot either transported fabric to a second table or positioned fabric on a table for draping. The human followed the robot during transport and draped the fabric after the robot positioned it.,,"The paper focuses on using deep learning models (FCNN and CNN) to classify trust levels based on motion tracking data. The models were trained and validated using different approaches, including splitting the data randomly and validating on unseen subjects. The performance of the models was evaluated using metrics such as precision, recall, F1-score, and accuracy. No traditional statistical tests like t-tests or ANOVA were explicitly mentioned in the paper.",TRUE,Robot-accuracy,Robot-accuracy,,"The paper explicitly states that 'Robot speed was used to induce different levels of trust.' and that 'Data was collected in the transport and draping tasks with both a slow and fast robot.' This indicates a manipulation of the robot's speed, which directly impacts the robot's performance on the task, thus falling under the category 'Robot-accuracy'. The paper also mentions 'We have shown earlier that speed is a good predictor of trust levels [15].' and 'Robot speed influenced trust levels, with slower speeds generally leading to higher trust.' This confirms that the manipulated factor, robot speed, impacted trust levels. There were no other factors manipulated in the study.",10.1109/ICRA57147.2024.10610822,https://ieeexplore.ieee.org/document/10610822/,"Trust in automation is usually assessed with postinteraction questionnaires. For human robot collaboration it would be beneficial to assess the trust level during the interaction to adjust the robot’s collaboration behavior to the user expectations. In this paper we investigate if trust can be estimated from observable behavior like movements during the interaction with a large industrial manipulator. To this end, we report on a data collection for two tasks during collaborative draping, the transport of large cut pieces and the actual draping process in close proximity to the robot. The data is used to train and compare different deep learning models. Results show that automatic trust estimation is feasible, which opens up to using trust as a parameter for informing the interaction with robots."
"Rehm, Matthias; Hald, Kasper; Pontikis, Ioannis",Benchmark Movement Data Set for Trust Assessment in Human Robot Collaboration,2024,1,20,20,0,No participants were excluded,Controlled Lab Environment,between-subjects,Participants performed collaborative transport and draping tasks with a robot at either a slow or fast speed. They completed trust questionnaires before and after each task.,Participants collaborated with a robot to transport and drape textile pieces.,KUKA KR 300 R2500 ultra,Industrial Robot Arms,Industrial; Research,Manipulation,Object Passing,direct-contact interaction,Participants physically collaborated with the robot to move textile pieces.,real-world,The study involved real-world interaction with a physical robot.,physical,Participants interacted with a physical industrial robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions without adapting to the user.,Questionnaires,Schaefer's Trust Questionnaire/Scale,Video Data; robot data,Trust was measured using questionnaires and movement data was collected for trust modeling.,"deep learning (e.g., neural networks, reinforcement learning)",A deep learning algorithm was trained to assess trust based on motion descriptors.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's speed was manipulated to influence trust levels, with faster speeds expected to lower initial trust.","Robot speed influenced trust levels, with lower trust reported at higher speeds.","The study observed different walking styles during the transport task, suggesting a link between movement patterns and trust.","Robot speed significantly impacts trust levels in human-robot collaboration, with higher speeds leading to lower initial trust.","The robot moved textile pieces, and the human participant collaborated by holding the other end of the textile and moving it to a designated location.",,"No statistical tests were explicitly mentioned in the paper. The study focused on collecting movement and trust data, and training a deep learning model for trust assessment. The analysis primarily involved descriptive statistics such as average age, height, and trial durations, but no inferential statistical tests were reported.",TRUE,Robot-task-strategy,Robot-task-strategy,,"The study explicitly manipulated the robot's speed during the collaborative tasks. The paper states, 'the experimental setup manipulated this factor across participants and tasks (see Section 2).' and 'Participants performed either the transportation or the draping task frst. Second, during the transportation task, the robot would move at either a slow speed setting or a fast speed setting.' This manipulation of speed directly relates to how the robot executes the task, which falls under the category of 'Robot-task-strategy'. The paper also notes that 'From previous experiments [2], we know that speed of the robot plays a crucial role in perceived trust levels as well as in trust dynamics, e.g. for a robot with high speed starting trust levels are lower and it takes longer to reach a stable trust rating.' and 'Robot speed significantly impacts trust levels in human-robot collaboration, with higher speeds leading to lower initial trust.' This indicates that the manipulated factor, robot speed, did impact trust levels. Therefore, 'Robot-task-strategy' is included in 'factors_that_impacted_trust'. There were no other factors manipulated, and no factors that were manipulated that did not impact trust.",10.1145/3610977.3637472,https://dl.acm.org/doi/10.1145/3610977.3637472,"Trust is a factor that is becoming more prominent in human robot interaction research. Only few approaches so far tackle the challenge of data-driven trust assessment. In this paper, we present a data set consisting of motion tracking data from an industrial human robot collaboration task. The data is collected during a trust manipulation experiment that has been designed to elicit diferent trust levels in the participants. Additionally, participants flled out a standard trust questionnaire. The data set allows for developing and testing data-driven trust assessment algorithms."
"Rehm, Matthias; Krummheuer, Antonia L.; Cubero, Carlos G.",The effect of rejection strategy on trust and shopping choices in robot-assisted shopping ,2024,1,31,31,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants interacted with a robot using two different rejection strategies (positive and negative) in a counterbalanced order, completing a shopping task with a predefined list of items. They filled out questionnaires after each interaction.","Participants were asked to select items from a shopping list while the robot provided decision support, sometimes rejecting their choices based on a sustainability agenda.",Sawyer,Industrial Robot Arms; Collaborative Robots,Research; Social,Social,Persuasion,minimal interaction,"Participants interacted with the robot verbally and observed its actions, but there was no physical touch.",real-world,The interaction took place in a lab setting with a physical robot.,physical,The robot was a physical industrial robot arm.,wizard of oz (directly controlled),The robot's actions were controlled by a human operator using a Wizard of Oz setup.,Questionnaires,Schaefer's Trust Questionnaire/Scale,,Trust was measured using a questionnaire.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's rejection strategy was manipulated, with positive rejection offering an alternative and negative rejection only providing arguments against the chosen item, influencing perceived autonomy and behavior.",The positive rejection strategy elicited higher trust compared to the negative rejection strategy.,"The study found that a positive rejection strategy, where the robot suggests an alternative, led to higher trust and more sustainable choices compared to a negative rejection strategy, where the robot only criticizes the user's choice.","A positive rejection strategy, where the robot suggests an alternative, elicits more trust in the system compared to a negative rejection strategy.",The robot fetched products from a shelf and provided decision support based on a sustainability agenda. The human selected items from a shopping list and interacted with the robot verbally.,Levene's test; Wilcoxon signed-rank test,"Levene tests were used to check for normality of the data from the Trust-HRI questionnaire, the number of accepted items, and the usability data from the QUESI questionnaire. Since the data was not normally distributed, Wilcoxon signed-rank tests were used to compare trust ratings, the number of accepted items, and usability ratings between the positive and negative rejection strategies.",TRUE,Robot-verbal-communication-content; Robot-autonomy,Robot-verbal-communication-content; Robot-autonomy,,"The study manipulated the robot's rejection strategy, which directly impacts the content of the robot's verbal communication. In the positive rejection strategy, the robot provides an alternative product and explains why it's a better choice, while in the negative rejection strategy, the robot only criticizes the user's choice without offering an alternative. This difference in content is a manipulation of 'Robot-verbal-communication-content'. Additionally, the positive rejection strategy involves the robot selecting an alternative product, which increases the robot's decision-making authority and thus manipulates 'Robot-autonomy'. The paper explicitly states that the positive rejection strategy, which includes both the alternative suggestion and the explanation, elicited higher trust. Therefore, both 'Robot-verbal-communication-content' and 'Robot-autonomy' impacted trust. There were no factors that were manipulated that did not impact trust.",10.1109/ICRA57147.2024.10611366,https://ieeexplore.ieee.org/document/10611366/,"In this paper, we investigate how a customer-facing service robot can support decision making in shopping interactions. In this role, a robot needs sometimes to reject a customer’s choice. Thus, we investigate different rejection strategies with the goal of changing customer behavior. The implemented strategies have been developed based on an ethnographic study on assisted shopping and tested in a lab experiment with 31 participants. The experiment showed significant differences in trust ratings and decision-making depending on the employed strategy."
"Reichenbach, Juliane; Onnasch, Linda; Manzey, Dietrich",Misuse of Automation: The Impact of System Experience on Complacency and Automation Bias in Interaction with Automated Aids,2010,1,88,88,0,No participants were excluded,Controlled Lab Environment,between-subjects,Participants completed a supervisory control task with an automated aid. They were divided into four groups based on the timing of automation failures. Trust ratings and system parameters were collected before each block and at the end of the session. Participants were asked about their approach to automation verification after the final automation failure.,"Participants performed a supervisory control task of a simulated life support system, monitoring for faults and using an automated aid for diagnosis and repair.",Unspecified,Other,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated system through a computer interface.,simulation,The interaction was through a computer simulation of a life support system.,simulated,The automated aid was a simulated entity within the software.,shared control (fixed rules),"The automated aid provided diagnoses and recommendations, but the human had to confirm them.",Behavioral Measures; Questionnaires,,Performance Metrics,Trust was assessed using subjective ratings and behavioral measures of automation verification.,no modeling,Trust was not modeled computationally; the study used statistical analysis.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The timing and occurrence of automation failures were manipulated to influence trust. Participants were also warned about the aid's reliability.,Negative experiences with automation failures had a stronger negative impact on trust than positive experiences. Early failures led to lower trust and increased verification behavior.,"The study found that negative experiences with automation failures had a much stronger impact on trust than positive experiences. Also, some participants followed the wrong advice despite checking all the necessary information, suggesting a 'looking-but-not-seeing' effect.","Negative experiences with automation failures have a stronger impact on trust than positive experiences, and can lead to a reduction in complacency and automation bias.","The automated aid provided diagnoses and repair recommendations for system faults. The human operator monitored the system, verified the aid's diagnoses, and initiated repairs.",ANOVA; χ 2,"The study used ANOVA to analyze the effects of group and block on subjective trust and automation verification behavior. Specifically, a 2(Groups) x 5(Block) ANOVA was used to analyze subjective trust, and a 2(Group) x 4(Block) ANOVA was used to analyze automation verification information sampling (AVIS). A chi-squared test (χ 2) was used to compare the proportion of participants committing commission errors between groups.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy,Task-complexity,"The study manipulated the timing and occurrence of automation failures, which directly impacts the 'Robot-accuracy' as the automated aid sometimes provided incorrect diagnoses. The study also manipulated the number of blocks participants worked with the system before a failure occurred, which can be seen as a manipulation of 'Task-complexity' as the number of events and the timing of failures could influence the cognitive load. The study found that the 'Robot-accuracy' (i.e., the experience of automation failures) had a significant impact on trust, with negative experiences leading to a decrease in trust. The number of valid diagnoses prior to the automation failure (which is related to the 'Task-complexity' manipulation) did not have a significant impact on automation bias, suggesting it did not impact trust as much as the accuracy of the aid.",,,"The study investigates how complacency and automation bias effects in interaction with automated aids are moderated by system experience. Participants performed a supervisory control task supported by an aid for fault identification and management. Groups differed with respect to how long they worked with the aid until eventually an automation failure occurred, and whether this failure was the first or second one the participants were exposed to. Results show that negative experiences, i.e., automation failures, entail stronger effects on subjective trust in automation as well as the level of complacency and automation bias than positive experiences (correct recommendations of the aid). Furthermore, results suggest that commission errors may be due to three different sorts of effects: (1) a withdrawal of attention in terms of incomplete cross-checks of information, (2) an active discounting of contradictory system information, and (3) an inattentive processing of contradictory information analogue to a “looking-but-not-seeing” effect."
"Reidy, Kaitlyn; Markin, Kristy; Kohn, Spencer; Wiese, Eva",Effects of Perspective Taking on Ratings of Human Likeness and Trust,2015,1,80,74,6,"6 participants were excluded because the stories were not completed, 2 additional individual responses were disregarded from the perspective-taking robot condition for not properly completing that specific story",Online Crowdsourcing,mixed design,"Participants completed a storytelling task from either their own or the agent's perspective, then rated the human-likeness and trustworthiness of both a human and a robot agent.","Participants were asked to complete a story from either their own perspective or the perspective of a robot or human agent, and then complete surveys about the agents.",EDDIE,Humanoid Robots,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read stories and rated agents.,media,Participants viewed images of the agents.,physical,Participants viewed images of a physical robot.,not autonomous,"The robot's actions were described in a story, but the robot did not act autonomously.",Questionnaires; Custom Scales,Trust in Automation Scale (TAS); Human-Robot Trust Scale/Questionnaire (HRTS/HRITS),,Trust was measured using augmented versions of the Trust Scale and Human Likeness Posttask Survey.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"Participants were asked to complete stories from either their own perspective or the agent's perspective, which was intended to influence their perception of the agent's humanness and trustworthiness.",Perspective taking increased ratings of human-likeness but did not significantly affect trust ratings.,"Perspective taking increased ratings of human-likeness but not trust. The order in which the agents were presented affected the results, with the robot yielding lower scores when presented first in the non-perspective taking condition.","Taking the perspective of a robot increased the perceived human-likeness of the robot, but did not increase trust.",Participants read a story about a human or robot agent and completed the story from either their own or the agent's perspective. They then rated the agent's human-likeness and trustworthiness.,ANOVA; ANOVA,"The study used a 2x2 mixed ANOVA to analyze the quantitative data, with perspective taking (yes vs. no) as a between-subjects factor and agent type (human vs. robot) as a within-subjects factor. This analysis was used to examine the effects of perspective taking and agent type on ratings of humanness and trust. Post-hoc analyses were conducted to determine whether the order in which the agents were presented affected the results, using additional ANOVAs.",TRUE,Task-complexity,,Task-complexity,"The study manipulated the perspective from which participants completed a story, either their own or the agent's (human or robot). This manipulation is categorized as 'Task-complexity' because it changes the cognitive demands of the task. Completing a story from another's perspective requires more cognitive effort and mentalizing than completing it from one's own perspective. The paper states, 'Perspective taking was manipulated within the framework of a storytelling paradigm in which participants had to complete a given story either from their own perspective (condition 1) or from the perspective of another agent (condition 2).' The results showed that this manipulation impacted ratings of human-likeness but not trust. Therefore, 'Task-complexity' is listed as a factor that did not impact trust.",,http://link.springer.com/10.1007/978-3-319-25554-5_56,"The effects of perspective taking on ratings of human-likeness and trust are investigated. Seventy-four participants were shown pictures of two agents (human and robot) and storytelling narratives, which they had to complete. Afterwards, participants completed augmented versions of the Trust Scale and Human-Likeness Posttask Survey. Half of the participants were given stories using the perspective of the agent (perspective taking condition) and the other half was given stories using a third-person perspective (non-perspective taking condition). It was hypothesized that participants in the perspective taking condition would rate the agent higher on human-likeness and trust compared to the non-perspective taking condition. Interestingly, the results support our hypothesis for human-likeness but not for trust. The findings have important implications for the design of social robots by demonstrating the importance of perspective taking exercises on perception of humanness. Future studies need to validate the effects of perspective taking on human-robot interaction in various contexts and with different robot agents."
"Rezaei Khavas, Zahra; Kotturu, Monish Reddy; Ahmadzadeh, S. Reza; Robinette, Paul",Do Humans Trust Robots that Violate Moral Trust?,2024,1,100,84,16,16 participants were excluded as they did not provide correct responses to the manipulation check questions,Online Crowdsourcing,between-subjects,"Participants completed a consent form, a three-step tutorial, a quiz, played 7 rounds of a collaborative search game with a robot, answered end-of-round questions, and completed a post-survey questionnaire.","Participants played an online collaborative search game with a robot teammate, searching for targets and making trust decisions about sharing scores.",Unspecified,Other,Research,Game,Cooperative Game,minimal interaction,Participants interacted with a simulated robot through an online game.,simulation,The interaction took place in a simulated online game environment.,simulated,The robot was a simulated entity within the online game.,pre-programmed (non-adaptive),The robot's behavior was pre-programmed and did not adapt to the participant's actions.,Questionnaires; Behavioral Measures; Multidimensional Measures,Multi-Dimensional Measure of Trust (MDMT),Performance Metrics,"Trust was assessed using questionnaires, behavioral measures, and performance metrics.",no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's behavior was manipulated to either violate performance trust by performing poorly or violate moral trust by acting selfishly, influencing the participant's trust.",Moral trust violations led to a more significant decrease in trust compared to performance trust violations.,"Participants exhibited retaliatory behavior towards the robot that violated moral trust, while sympathizing with the robot that violated performance trust. The reliability trust dimension was influenced more by moral trust than performance trust.",Moral trust violations by a robot have a more significant negative impact on human trust than performance trust violations.,"The robot searched for targets and either shared its score or kept it for itself, while the human searched for targets and decided whether to combine their score with the robot's.",Mann-Whitney U; Kruskal-Wallis; Logistic regression; z-test; t-test,"The study used several statistical tests to analyze the data. Mann-Whitney tests were used to compare trust scores and ratings between the two experimental conditions (moral trust violation vs. performance trust violation) across different trust dimensions and rounds. Kruskal-Wallis tests were used to compare the percentages of participants who added to the individual score across different rounds and conditions. Binomial tests were used to compare the number of N/A responses in different trust dimensions. T-tests were used to compare the time-to-respond (TTR) values between the two conditions. The Z-test was mentioned as being used, but the specific context was not provided.",TRUE,Robot-morality; Robot-accuracy; Teaming,Robot-morality; Robot-accuracy,,"The study manipulated the robot's behavior in a collaborative search game. In one condition (performance trust violation), the robot acted morally by always offering to share scores but had poor performance (gained no points) in the last four rounds. This is categorized as 'Robot-accuracy' because it directly impacts the robot's ability to perform the task effectively. In the other condition (moral trust violation), the robot performed well (gained points) but acted immorally by choosing to keep its score for itself in the last four rounds. This is categorized as 'Robot-morality' because it involves a breach of expected cooperative behavior, which is a moral violation in the context of the game. The study also manipulated whether the human and robot were working together towards a team score or individual scores, which is categorized as 'Teaming'. Both 'Robot-morality' and 'Robot-accuracy' were found to impact trust levels, with moral violations having a more significant negative impact. The 'Teaming' manipulation was a constant factor in the study design and was not directly tested for its impact on trust, but it was a necessary component of the experimental design to allow for the manipulation of the other two factors. The study did not explicitly test for factors that did not impact trust.",10.1145/3651992,https://dl.acm.org/doi/10.1145/3651992,"The increasing use of robots in social applications requires further research on human-robot trust. The research on human-robot trust needs to go beyond the conventional definition that mainly focuses on how human-robot relations are influenced by robot performance. The emerging field of social robotics considers optimizing a robot’s personality a critical factor in user perceptions of experienced human-robot interaction (HRI). Researchers have developed trust scales that account for different dimensions of trust in HRI. These trust scales consider one performance aspect (i.e., the trust in an agent’s competence to perform a given task and their proficiency in executing the task accurately) and one moral aspect (i.e., trust in an agent’s honesty in fulfilling their stated commitments or promises) for human-robot trust. The question that arises here is to what extent do these trust aspects affect human trust in a robot? The main goal of this study is to investigate whether a robot’s undesirable behavior due to the performance trust violation would affect human trust differently than another similar undesirable behavior due to a moral trust violation. We designed and implemented an online human-robot collaborative search task that allows distinguishing between performance and moral trust violations by a robot. We ran these experiments on Prolific and recruited 100 participants for this study. Our results showed that a moral trust violation by a robot affects human trust more severely than a performance trust violation with the same magnitude and consequences."
"Richert, Anja; Shehadeh, Mohammad A.; Muller, Sarah L.; Schroder, Stefan; Jeschke, Sabina",Socializing with robots: Human-robot interactions within a virtual environment,2016,1,162,162,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were immersed in a virtual environment using an Oculus Rift and Virtual Theatre. They were tasked with commanding a virtual robot partner to complete a rope-pulling task. The robot's appearance (humanoid or industrial) and behavior (perfect, error-prone, or human-controlled) were manipulated. Participants completed pre- and post-experiment questionnaires, and their skin conductance was measured.",Participants collaborated with a virtual robot to pull a rope within a limited time. The task required coordinating two sub-tasks simultaneously: activating a timer and pulling the rope by alternating turns with the robot.,Unspecified,Humanoid Robots; Industrial Robot Arms,Research; Industrial,Manipulation,Other Manipulation subtask: The task involves pulling a rope by alternating turns with the robot.,minimal interaction,"Participants interacted with a virtual robot in a simulated environment, giving verbal commands but without physical touch.",simulation,Participants were immersed in a virtual environment using an Oculus Rift and Virtual Theatre.,simulated,The robot was a virtual representation within the virtual reality environment.,wizard of oz (directly controlled),"The robot's behavior was either pre-programmed, error-prone, or directly controlled by a human, without the participant's knowledge in one condition.",Questionnaires,Mayer and Davis' Trust/Trustworthiness Scales (1999); Mayo High Performance Teamwork Scale,Video Data; Physiological Signals; Performance Metrics,"Trust was measured using questionnaires, skin conductance, and video recordings of the interaction.",no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's appearance (humanoid or industrial) and behavior (perfect, error-prone, or human-controlled) were directly manipulated to observe their impact on trust and performance.","The study hypothesized that a flawless robot would be the control, an error-prone robot would cause distrust, and a human-controlled robot would improve performance over time. The impact on trust was not explicitly stated, but it was expected to be influenced by the robot's behavior.","The study hypothesizes that an error-prone robot will cause distrust, and a human-controlled robot will improve performance over time. The study also mentions that the human will initially blame themselves for the mistakes of the robot.","The study aims to investigate how the appearance and accuracy of a robot influence human stress, cooperation, and trust in a virtual collaborative task.","The human commands the robot to activate a timer and then to pull a rope. The human and robot alternate pulling the rope, with the human also needing to activate a floor switch to enable the rope to be pulled in the desired direction. The robot responds to the human's commands.",,"The paper mentions that completed questionnaires will be assessed descriptively. However, no specific statistical tests are mentioned. The paper also mentions that data collected during the study will be analyzed using data mining methods, but no specific statistical tests are named.",TRUE,Robot-accuracy; Robot-aesthetics; Robot-autonomy,Robot-accuracy,,"The study manipulated the robot's accuracy by having it perform perfectly, make mistakes, or be controlled by a human. This directly impacts the robot's task performance, thus 'Robot-accuracy' is the most appropriate category. The robot's appearance was also manipulated by using either a humanoid or an industrial robot, which is a change in visual appeal, thus 'Robot-aesthetics'. The robot's autonomy was manipulated by having it be either pre-programmed, error-prone, or human-controlled, which is a change in decision authority, thus 'Robot-autonomy'. The paper hypothesizes that the robot's accuracy (perfect, error-prone, or human-controlled) will impact trust, with the error-prone robot causing distrust. The paper does not explicitly state that the robot's appearance or autonomy impacted trust, but it does state that the robot's behavior (which is tied to accuracy and autonomy) will impact trust. Therefore, only 'Robot-accuracy' is listed as a factor that impacted trust. The paper does not explicitly state that any of the manipulated factors did not impact trust.",10.1109/ARSO.2016.7736255,http://ieeexplore.ieee.org/document/7736255/,"Robots are already physically supporting humans within multiple processes, but as a step further, the robots will be able to identify and adapt to any individual strengths and become the flawless co-workers needed. One of the questions is whether in other fields of social robotics, e.g. in ergonomics, existing knowledge about human teams can be transferred into the design of hybrid teams and the shaping of human-computer interactions. These developments serve the appearance of Industry 4.0, which is directly correlated to the modernization of productions that are directly involved in defining the concept of hybrid human-robot-teams."
"Robinette, Paul; Wagner, Alan R; Howard, Ayanna M",Building and Maintaining Trust Between Humans and Guidance Robots in an Emergency,2013,1,15,15,0,No participants were excluded,Simulation,within-subjects,Fifteen volunteers each performed seven evacuation scenarios both with and without robotic guidance.,Participants completed simulated evacuation scenarios with and without robot guidance.,Unspecified,Mobile Robots,Research,Navigation,Guiding,minimal interaction,Participants interacted with the robot in a simulated environment.,simulation,The interaction took place in a simulated 3D environment.,simulated,The robot was a virtual representation in the simulation.,pre-programmed (non-adaptive),The robot followed a pre-set path in the simulation.,Behavioral Measures,,,Trust was measured by whether participants followed the robot.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The presence or absence of robot guidance was manipulated to see if participants would follow the robot.,"All participants followed the robot in the first scenario, but the simulation was low stress.","All participants followed the robot in the first scenario, which may be due to the low-stress nature of the simulation.","Participants followed the robot in the first simulated evacuation scenario, but the simulation was low stress.","The robot guided participants to an exit in a simulated environment, and participants were asked to follow the robot.",,"No statistical tests were explicitly mentioned in the paper. The study observed whether participants followed the robot in a simulated evacuation scenario, but no statistical analysis was performed on this data.",TRUE,Robot-aesthetics; Robot-nonverbal-communication; Robot-verbal-communication-content,Robot-aesthetics; Robot-nonverbal-communication; Robot-verbal-communication-content,,"The study manipulated several factors related to the robot's appearance and behavior. 'Robot-aesthetics' was manipulated through the design of the robot, including its color, markings (e.g., fire department seal, 'Emergency Evacuation Robot' text), and overall visual style (e.g., red and white stripes, cylindrical shape). The paper mentions that these design choices were made to increase the evacuee's confidence in the robot. 'Robot-nonverbal-communication' was manipulated by adding articulated arms to the robot, which could perform standard gestures used by emergency responders to guide individuals. The paper states that these arms were intended to provide further clarity of direction and make the robot appear more active. 'Robot-verbal-communication-content' was manipulated by having the robot ask questions to the evacuee to find out why the evacuee will not follow, and by providing assurances, such as informing the evacuees of the estimated time to exit and giving status updates on the arrival of emergency personnel. The paper states that by acting predictably and explaining its behavior the robot will allow evacuees to form a simple model of the robot's decision processes. All of these factors were found to impact trust, as the paper discusses how these changes were made to increase trust and how the robot's communication and actions can influence whether or not an evacuee follows the robot.",,,Emergency evacuations are dangerous situations for both evacuees and first responders. The use of automation in the form of guidance robots can reduce the danger to humans by both aiding evacuees and assisting first responders. This presents an interesting opportunity to explore the trust dynamic between frightened evacuees and automated robot guides. We present our work so far on designing robots to immediately generate trust as well as our initial concept of an algorithm for maintaining trust through interaction.
"Robinette, Paul; Howard, Ayanna M.",Trust in emergency evacuation robots,2012,1,15,15,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants completed seven scenarios in a random order, one without a robot and six with one of two robot designs guiding them to different exits. User and robot positions were recorded at 0.5 second intervals. After all scenarios, users completed a survey.","Participants were asked to navigate a simulated mall environment and evacuate during a fire emergency, following a robot guide in some scenarios.",Unspecified,Mobile Robots; Mobile Robots,Research; Research,Navigation,Guiding,minimal interaction,"Participants interacted with the robot in a simulated environment, following its guidance to different exits.",simulation,The study used a three-dimensional computer simulation of an emergency environment.,simulated,The robots were virtual representations within the simulation.,pre-programmed (non-adaptive),The robots followed a pre-programmed path to guide users to the exits.,Questionnaires; Behavioral Measures,,Performance Metrics,Trust was assessed using a post-experiment questionnaire and by observing whether participants followed the robot.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The study manipulated the robot's appearance and behavior by using two different robot designs and having them guide participants to different exits.,"The study found no significant difference in trust between the two robot designs, but participants followed the robot in at least two scenarios, indicating some level of trust.","Users tended to follow the robot initially, but then often chose the left exit, possibly due to frustration or perceived proximity. Some users noted differences in robot behavior despite identical programming, and some lost trust when the robot passed a closer exit.","Participants were willing to follow the robot in at least two scenarios, and one-third followed the robot in all scenarios, suggesting a general acceptance of robots in emergency situations.","The robot guided participants to different exits in a simulated mall environment, while the participants navigated the environment and decided whether to follow the robot's guidance.",t-test,"A paired t-test was used to compare evacuation times in scenarios with and without a robot. Specifically, the test was used to determine if there was a statistically significant difference in evacuation times when a robot was present, compared to when no robot was present. The analysis focused on three of the robot scenarios, and the p-value was used to determine statistical significance.",TRUE,Robot-aesthetics; Robot-task-strategy,Robot-task-strategy,Robot-aesthetics,"The study manipulated the robot's appearance by using two different robot designs (Robot 1 and Robot 2), which is categorized as 'Robot-aesthetics'. The robots also had different strategies for guiding the user to the exit, sometimes passing closer exits to guide the user to a further one, which is categorized as 'Robot-task-strategy'. The study found that the robot's task strategy (passing closer exits) impacted trust, as users lost trust when the robot passed a closer exit. However, the study found no significant difference in trust between the two robot designs, indicating that 'Robot-aesthetics' did not impact trust.",10.1109/SSRR.2012.6523903,http://ieeexplore.ieee.org/document/6523903/,"Would you trust a robot to lead you to safety in an emergency? What design would best attract your attention in a smoke-ﬁlled environment? How should the robot behave to best increase your trust? To answer these questions, we have created a three dimensional environment to simulate an emergency and determine to what degree an individual will follow a robot to a variety of exits. Survey feedback and quantitative scenario results were gathered on two different robot designs. Fifteen volunteers completed a total of seven scenarios each: one without a robot and one with each robot pointing to each of three exits in the environment. Robots were followed by each volunteer in at least two scenarios. One-third of all volunteers followed the robot in each robot-guided scenario."
"Robinette, Paul; Howard, Ayanna M.; Wagner, Alan R.",Timing Is Key for Robot Trust Repair,2015,1,480,319,161,"30 submissions were excluded because they had taken similar surveys in the past, because they had mistakenly taken multiple conditions of this experiment, or because they failed to answer at least half of the survey questions, 131 participants failed the comprehension check",Online Crowdsourcing,between-subjects,"Participants were greeted with a screen and an image of the robot, then practiced moving in the simulation. They followed the robot to a meeting room, answered a question about the robot's guidance, and were then informed of an emergency. Participants had 30 seconds to find an exit, and were then asked to complete a survey.",Participants were asked to follow a robot to a meeting room and then find an exit during a simulated emergency.,Unspecified,Mobile Robots,Research,Navigation,Guiding,minimal interaction,Participants interacted with the robot in a simulation environment.,simulation,The interaction took place in a 3D simulation of an office environment.,simulated,The robot was a simulated Turtlebot in a virtual environment.,pre-programmed (non-adaptive),The robot followed pre-set waypoints without adapting to the user.,Behavioral Measures; Questionnaires,,,Trust was measured by the participant's decision to follow the robot and a self-report question.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's guidance behavior (efficient or circuitous), timing of trust repair messages (immediately after violation or during emergency), and content of trust repair messages (apology, promise, or information) were manipulated to influence trust.","Trust was significantly reduced by circuitous guidance. Apologies, promises, and information were effective at repairing trust when given during the emergency, but not immediately after the violation.","The timing of the trust repair was critical, with messages being effective only when given during the emergency, not immediately after the violation. This was unexpected, as participants were shown to remember the messages.","The timing of trust repair attempts is critical; apologies, promises, and additional information are effective when given at the time of a trust decision, but not immediately after a trust violation.","The robot guided participants to a meeting room, either efficiently or circuitously. Then, during a simulated emergency, the robot guided participants to an exit, while participants could choose to follow the robot or find their own way out.",χ 2,"Chi-squared tests were used to compare the proportion of participants who followed the robot in different experimental conditions. Specifically, the tests compared the effect of efficient vs. circuitous robot guidance, the impact of different trust repair messages (apology, promise, information) given at different times (immediately after violation vs. during emergency), and the effect of no message. The tests aimed to determine if these manipulations had a statistically significant impact on participants' decisions to follow the robot during the emergency phase.",TRUE,Robot-accuracy; Robot-verbal-communication-content; Robot-social-timing,Robot-verbal-communication-content; Robot-social-timing,Robot-accuracy,"The study manipulated the robot's guidance behavior (efficient or circuitous), which directly impacts the robot's accuracy in guiding the participant to the meeting room. This is classified as 'Robot-accuracy' because it directly influences the task performance. The study also manipulated the content of the robot's verbal communication (apology, promise, or information) and the timing of these messages (immediately after the violation or during the emergency). The content of the messages is classified as 'Robot-verbal-communication-content' because it changes what the robot communicates to the participant. The timing of the messages is classified as 'Robot-social-timing' because it changes when the robot attempts to repair trust. The results showed that the timing of the trust repair messages and the content of the messages impacted trust, while the initial accuracy of the robot's guidance (efficient vs. circuitous) was used to break trust but did not impact trust repair. The initial accuracy was a manipulation to create a trust violation, but the manipulation itself did not impact trust repair, only the timing and content of the repair attempts did. Therefore, 'Robot-accuracy' is listed as a manipulated factor but not as a factor that impacted trust.",,http://link.springer.com/10.1007/978-3-319-25554-5_57,"Even the best robots will eventually make a mistake while performing their tasks. In our past experiments, we have found that even one mistake can cause a large loss in trust by human users. In this paper, we evaluate the eﬀects of a robot apologizing for its mistake, promising to do better in the future, and providing additional reasons to trust it in a simulated oﬃce evacuation conducted in a virtual environment. In tests with 319 participants, we ﬁnd that each of these techniques can be successful at repairing trust if they are used when the robot asks the human to trust it again, but are not successful when used immediately after the mistake. The implications of these results are discussed."
"Robinette, Paul; Wagner, Alan R.; Howard, Ayanna M.",Investigating Human-Robot Trust in Emergency Scenarios: Methodological Lessons Learned,2016,2,128,128,0,No participants were excluded,Online Crowdsourcing,within-subjects,Participants read 12 narratives describing trust scenarios and answered questions about each scenario.,Participants evaluated whether the actions in the narratives indicated trust.,Unspecified,,,Evaluation,Text Evaluation,passive observation,Participants only read text-based narratives.,media,The interaction was based solely on text descriptions.,hypothetical,The robot was only described in text.,not autonomous,"The robot's actions were described in text, but no actual robot was present.",Questionnaires,,,Trust was measured using participant responses to questions about the narratives.,no modeling,No computational model of trust was used.,Observational & Survey Studies,Quantitative Surveys,Direct Manipulation,The narratives were manipulated to present different scenarios that either met or violated the conditions for trust.,"Participants agreed that the Trust Matrix narratives required trust, but had some disagreements about situations that did not require trust.","Participants sometimes invented reasons for the trustor's actions when the narrative did not make sense, suggesting a limitation of the narrative approach. There was also an anchoring bias where participants focused on key words.",There was a strong correlation between the predictions of the researchers' conditions for trust and the evaluations made by participants.,"The robot's actions were described in the narratives, and the human participant's task was to read the narratives and evaluate whether the actions indicated trust.",Pearson correlation,"A correlation analysis was used to assess the relationship between the researchers' predictions of trust conditions and the participants' evaluations of those conditions based on the narratives. Specifically, the correlation coefficient (D C0:592) was calculated to determine the strength and direction of the relationship between the predicted and observed trust evaluations.",TRUE,Task-complexity,Task-complexity,,"The study manipulated the complexity of the trust scenarios presented in the narratives. The narratives were designed to either meet or violate the conditions for trust, which directly impacted the cognitive load required to evaluate the situation. The paper states, 'We were able to further divide the matrices that violated the definition of trust into sub-categories based on the way the definition was violated.' This indicates that the researchers intentionally varied the scenarios to test different aspects of their trust definition, thus manipulating the task complexity. The different matrix types (Trust Matrix, Equal Outcomes, etc.) each presented a different level of complexity in terms of evaluating whether trust was required. The study found that participants agreed that the Trust Matrix narratives required trust, but had some disagreements about situations that did not require trust, indicating that the manipulation of task complexity impacted trust evaluations.",,http://link.springer.com/10.1007/978-1-4899-7668-0_8,
"Robinette, Paul; Wagner, Alan R.; Howard, Ayanna M.",Investigating Human-Robot Trust in Emergency Scenarios: Methodological Lessons Learned,2016,2,770,770,0,No participants were excluded,Online Crowdsourcing,mixed design,"Participants navigated a virtual maze and chose whether to use a robot for guidance in one or two rounds, and completed surveys after each round.",Participants decided whether to follow a robot guide in a virtual maze during an emergency evacuation.,Unspecified,Mobile Robots,Research,Navigation,Guiding,minimal interaction,Participants interacted with a virtual robot in a simulation.,simulation,Participants interacted with a virtual robot in a simulated maze environment.,simulated,The robot was a virtual representation in the simulation.,pre-programmed (non-adaptive),The robot followed a pre-defined path without adapting to the user's behavior.,Questionnaires; Behavioral Measures,,,Trust was measured using self-report questionnaires and the participant's decision to follow the robot.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's navigation behavior was manipulated to be either successful or unsuccessful, and the task was framed as either a bonus-based or emergency evacuation scenario.",Participants reported a decrease in self-reported trust when the robot performed poorly. The emergency framing resulted in self-reports of trust that closely matched the participant's decision to follow the robot.,"Participants often continued to follow the robot even when it performed poorly, indicating a tendency to over-trust robots. The type of unsuccessful navigation behavior did not significantly affect the decision to follow the robot or self-reported trust.",The framing of the task as an emergency evacuation resulted in self-reports of trust that closely matched the participant's decision to follow the robot.,"The robot guided the participant through a virtual maze, and the human participant's task was to decide whether to follow the robot to the exit.",Pearson correlation,"Correlation analyses were used to examine the relationship between participants' self-reported trust and their decision to follow the robot in the virtual maze. Specifically, correlation coefficients (D C0:661 for round 1 and D C0:745 for round 2) were calculated to determine the strength and direction of the relationship between these two measures of trust. Additionally, the study mentions a significant decrease in self-reported trust (53% decrease) when the robot performed poorly, although the specific statistical test used to determine this significance is not explicitly stated.",TRUE,Robot-accuracy; Task-constraints,Robot-accuracy; Task-constraints,,"The study manipulated the robot's navigation behavior, which directly affected its accuracy in guiding participants through the maze. The paper states, 'The robot's guidance performance in the second round always matched its performance in the first round.' This indicates that the robot's accuracy was intentionally varied to be either successful or unsuccessful. The paper also states, 'Instead of receiving a bonus for a fast completion, they were told that they would only survive if they found the exit in time.' This indicates that the task constraints were manipulated by framing the task as either a bonus-based or emergency evacuation scenario, which introduced time pressure. The study found that participants reported a decrease in self-reported trust when the robot performed poorly and that the emergency framing resulted in self-reports of trust that closely matched the participant's decision to follow the robot. This indicates that both robot accuracy and task constraints impacted trust.",,http://link.springer.com/10.1007/978-1-4899-7668-0_8,
"Robinette, Paul; Howard, Ayanna M.; Wagner, Alan R.",Effect of Robot Performance on Human–Robot Trust in Time-Critical Situations,2017,2,106,106,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants navigated a virtual maze with or without robot guidance in two rounds, with robot behavior varied in the first round and kept consistent in the second round. They completed surveys after each round and a final demographic survey.",Participants navigated a virtual maze and chose whether to use a robot for guidance in each of two rounds.,TurtleBot,Mobile Robots,Research,Navigation,Guiding,minimal interaction,Participants interacted with the robot in a simulation environment.,simulation,Participants navigated a virtual maze environment.,simulated,The robot was a virtual representation in the simulation.,pre-programmed (non-adaptive),The robot followed a pre-defined path without adapting to the user.,Behavioral Measures; Questionnaires,,,Trust was measured using self-reports and the decision to follow the robot.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's navigation performance was manipulated to be either efficient, circuitous, or incorrect, influencing the user's trust.","Self-reported trust decreased significantly when the robot performed poorly, but the decision to use the robot was not significantly affected.","Participants reported a significant decrease in self-reported trust after a robot failure, but their decision to use the robot was not significantly affected, suggesting a disconnect between reported trust and behavior in this low-risk scenario.","A single failure by the robot significantly decreased self-reported trust, but did not significantly impact the decision to use the robot in a low-risk scenario.","The robot guided the participant through a maze, either efficiently, circuitously, or incorrectly, while the participant navigated the maze and decided whether to follow the robot.",Chi-squared; phi correlation,The study used chi-squared tests to compare the changes in self-reported trust and the decision to use the robot between different robot behavior conditions (efficient vs. circuitous/incorrect) across the two rounds. Phi correlation was used to assess the relationship between the decision to follow the robot and self-reported trust in both rounds.,TRUE,Robot-accuracy; Task-constraints,Robot-accuracy,,"The study manipulated 'Robot-accuracy' by having the robot navigate the maze efficiently, circuitously, or incorrectly (stopping before the exit). This directly impacted the robot's performance in guiding the participant to the exit, which is a measure of accuracy. The study also manipulated 'Task-constraints' by introducing a time limit and a monetary bonus that decreased over time, creating time pressure for the participants to complete the maze quickly. The results showed that 'Robot-accuracy' significantly impacted trust, as participants reported lower trust after experiencing a circuitous or incorrect robot. The type of failure (circuitous vs. incorrect) did not have a significant impact on trust, but the fact that the robot failed did. The time constraint was present in all conditions and did not vary, so it was not a factor that impacted trust.",10.1109/THMS.2017.2648849,http://ieeexplore.ieee.org/document/7828078/,"Robots have the potential to save lives in high-risk situations, such as emergency evacuations. To realize this potential, we must understand how factors such as the robot’s performance, the riskiness of the situation, and the evacuee’s motivation inﬂuence his or her decision to follow a robot. In this paper, we developed a set of experiments that tasked individuals with navigating a virtual maze using different methods to simulate an evacuation. Participants chose whether or not to use the robot for guidance in each of two separate navigation rounds. The robot performed poorly in two of the three conditions. The participant’s decision to use the robot and self-reported trust in the robot served as dependent measures. A 53% drop in self-reported trust was found when the robot performs poorly. Self-reports of trust were strongly correlated with the decision to use the robot for guidance (φ(90) = +0.745). We conclude that a mistake made by a robot will cause a person to have a signiﬁcantly lower level of trust in it in later interactions."
"Robinette, Paul; Howard, Ayanna M.; Wagner, Alan R.",Effect of Robot Performance on Human–Robot Trust in Time-Critical Situations,2017,2,129,129,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants navigated a virtual maze with or without robot guidance in two rounds, with robot behavior varied in the first round and kept consistent in the second round. They completed surveys after each round and a final demographic survey.",Participants navigated a virtual maze and chose whether to use a robot for guidance in each of two rounds.,TurtleBot,Mobile Robots; Mobile Manipulators,Research,Navigation,Guiding,minimal interaction,Participants interacted with the robot in a simulation environment.,simulation,Participants navigated a virtual maze environment.,simulated,The robot was a virtual representation in the simulation.,pre-programmed (non-adaptive),The robot followed a pre-defined path without adapting to the user.,Behavioral Measures; Questionnaires,,,Trust was measured using self-reports and the decision to follow the robot.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's navigation performance was manipulated to be either efficient, circuitous, or incorrect, and the task was framed as an emergency, influencing the user's trust.",Both self-reported trust and the decision to follow the robot significantly decreased after a robot failure.,"In contrast to the bonus scenario, both self-reported trust and the decision to follow the robot significantly decreased after a failure, indicating that the framing of the task as an emergency increased the impact of robot failure on trust and behavior.",A single failure by the robot significantly decreased both self-reported trust and the decision to use the robot in a simulated emergency scenario.,"The robot guided the participant through a maze, either efficiently, circuitously, or incorrectly, while the participant navigated the maze and decided whether to follow the robot.",Chi-squared; phi correlation,The study used chi-squared tests to compare the changes in self-reported trust and the decision to use the robot between different robot behavior conditions (efficient vs. circuitous/incorrect) across the two rounds. Phi correlation was used to assess the relationship between the decision to follow the robot and self-reported trust in both rounds.,TRUE,Robot-accuracy; Task-constraints,Robot-accuracy,,"Similar to the first study, 'Robot-accuracy' was manipulated by having the robot navigate the maze efficiently, circuitously, or incorrectly. This directly impacted the robot's performance in guiding the participant to the exit. 'Task-constraints' were manipulated by framing the task as an emergency evacuation with a countdown timer, creating time pressure and a sense of survival risk. The results showed that 'Robot-accuracy' significantly impacted trust, as participants reported lower trust and were less likely to follow the robot after experiencing a circuitous or incorrect robot. The type of failure (circuitous vs. incorrect) did not have a significant impact on trust, but the fact that the robot failed did. The time constraint was present in all conditions and did not vary, so it was not a factor that impacted trust.",10.1109/THMS.2017.2648849,http://ieeexplore.ieee.org/document/7828078/,"Robots have the potential to save lives in high-risk situations, such as emergency evacuations. To realize this potential, we must understand how factors such as the robot’s performance, the riskiness of the situation, and the evacuee’s motivation inﬂuence his or her decision to follow a robot. In this paper, we developed a set of experiments that tasked individuals with navigating a virtual maze using different methods to simulate an evacuation. Participants chose whether or not to use the robot for guidance in each of two separate navigation rounds. The robot performed poorly in two of the three conditions. The participant’s decision to use the robot and self-reported trust in the robot served as dependent measures. A 53% drop in self-reported trust was found when the robot performs poorly. Self-reports of trust were strongly correlated with the decision to use the robot for guidance (φ(90) = +0.745). We conclude that a mistake made by a robot will cause a person to have a signiﬁcantly lower level of trust in it in later interactions."
"Robinette, Paul; Novitzky, Michael; Fitzgerald, Caileigh; Benjamin, Michael R.; Schmidt, Henrik",Exploring Human-Robot Trust During Teaming in a Real-World Testbed,2019,1,7,7,0,No participants were excluded,Real-World Environment,within-subjects,"Participants were briefed, practiced, played against two robot behaviors in random order, completed surveys, and chose their preferred robot.",Participants played a capture the flag game against a robot with two different defense behaviors.,Clearpath M300 Heron,Unmanned Ground Vehicles,Research,Game,Competitive Game,direct-contact interaction,Participants directly interacted with the robot in a real-world game.,real-world,The interaction took place in a real-world environment with physical robots.,physical,The robot was a physical entity present in the real-world environment.,pre-programmed (non-adaptive),The robot followed pre-programmed behaviors without adapting to the participant.,Questionnaires; Behavioral Measures,,Performance Metrics,Trust was assessed using a Likert scale questionnaire and behavioral measures of game performance.,no modeling,No computational model of trust was used in this study.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's defense behavior was manipulated to be either passive or aggressive, which influenced the task difficulty for the participant.","Participants reported higher trust in the aggressive robot behavior, despite scoring fewer points against it.","Participants preferred the more aggressive robot despite it being more difficult to score against, suggesting that perceived competence may be more important than ease of interaction for trust.",Participants indicated more trust in the robot exhibiting more aggressive behaviors.,"The robot defended its flag using either a passive circling behavior or an aggressive intercepting behavior, while the human participant attempted to grab the robot's flag and return it to their own flag.",t-test; Wilcoxon rank sum,A t-test was used to compare the number of points scored by participants in the passive and aggressive robot behavior conditions. A Wilcoxon Signed Rank test was used to compare the Likert scale ratings of willingness to risk their flag to the robot in the passive and aggressive behavior conditions.,TRUE,Robot-task-strategy,Robot-task-strategy,,"The study manipulated the robot's defense behavior, which is a change in the robot's task strategy. In the passive condition, the robot circled the flag, while in the aggressive condition, it intercepted the participant. This manipulation directly impacted trust, as participants reported higher trust in the aggressive behavior despite scoring fewer points against it. The paper states, 'We varied the robot's capability and had the human compete against each variation.' This clearly indicates a manipulation of the robot's task strategy. The results section also states, 'Participants reported higher Likert values when asked if they would risk their flag to a defender exhibiting the aggressive behavior... which, according to our definition of situational trust, means that they are more likely to trust the aggressive robot.' This shows that the manipulation of the robot's task strategy directly impacted trust. There were no other factors manipulated that were found to impact trust.",10.1109/HRI.2019.8673134,https://ieeexplore.ieee.org/document/8673134/,"Project Aquaticus is a human-robot teaming competition on the water involving autonomous surface vehicles and human operated motorized kayaks. Teams composed of both humans and robots share the same physical environment to play capture the ﬂag. In this paper, we present results from seven competitions of our half-court (one participant versus one robot) game. We found that participants indicated more trust in more aggressive behaviors from robots."
"Rodriguez, Sebastian; Deep, Harsh; Asher, Drshika; Schaffer, James; Kirlik, Alex","Validating Trust in Human-Robot Interaction through Virtual Reality: Comparing Embodied and ""Behind-the-Screen"" Interactions",2023,1,119,119,0,No participants were excluded,Online Crowdsourcing,mixed design,Participants completed a quality assurance task in either a screen or VR condition. They were paired with a robotic teammate that provided recommendations on whether to send or reject packages. The reliability of the robot's recommendations varied across blocks. Participants received feedback on their performance after each block of trials.,"Participants were tasked with a quality assurance role, deciding whether to send or reject packages based on a scanner and a robot's recommendation.",Unspecified,Other: automated robotic crane,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a virtual robot in a simulated environment.,simulation,"The interaction was conducted in a simulated environment, either on a screen or in VR.",simulated,"The robot was a simulated entity, presented either on a screen or in VR.",shared control (fixed rules),"The robot provided recommendations based on pre-set rules, but the human made the final decision.",Behavioral Measures; Performance-Based Measures,,Performance Metrics,"Trust was assessed through behavioral measures such as reliance and compliance, and performance metrics.",no modeling,No computational model of trust was used; the study focused on statistical analysis of behavioral data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the robot's recommendations was directly manipulated, and the interaction medium was varied between screen and VR. Participants were also told that the robot was imperfect.","Reliance increased at 91% reliability, and deferrals increased with embodied agents, indicating that embodiment changes trust calibration when an agent is imperfect.","The study found that embodiment influenced how decisions were made (deferrals), but not what decisions were made (reliance and compliance), which was primarily influenced by reliability. There was a near-significant interaction between deferrals and representation, with deferrals being higher in the VR condition.","Embodiment changes trust calibration when an agent is presented as imperfect, leading to greater deferral to the agent, regardless of reliability.","The robot provided recommendations on whether to send or reject packages. The human participant made the final decision, using a scanner and the robot's recommendation as a guide.",ANOVA,"The study used ANOVA to analyze the effects of Reliability, Time, and Representation on reliance, compliance, and deferral behaviors. Specifically, ANOVAs were used to examine main effects and interactions between these factors on the dependent variables. The analysis aimed to determine how robot reliability, time, and the interaction medium (screen vs. VR) influenced participants' decision-making and trust-related behaviors.",TRUE,Robot-accuracy; Task-constraints,Robot-accuracy,,"The study manipulated 'Robot-accuracy' by varying the reliability of the robot's recommendations across four blocks (100%, 91%, 75%, and 50%). This is explicitly stated in the 'Experimental Design' section: 'Reliability then describes the accuracy of recommendations participants receive from the robotic teammate: 100% (Perfect), 91% (Ideal), 75% (Good Enough), 50% (No Info)'. The study also manipulated 'Task-constraints' by introducing time pressure, as described in the 'Simulation Design' section: 'As time progresses, the reward and penalty are linearly scaled accordingly to de-incentivize participants from delaying their decision, minimizing rewards and maximizing penalties at 10 seconds'. The results section indicates that 'Robot-accuracy' impacted trust, as evidenced by changes in reliance and compliance based on the reliability condition: 'For reliance, the ANOVA revealed a main effect in the Reliability...factor...For compliance, the ANOVA revealed a main effect in the Reliability factor'. The study did not find any factors that did not impact trust.",10.54941/ahfe1004408,https://openaccess.cms-conferences.org/publications/book/978-1-958651-93-3/article/978-1-958651-93-3_9,"Human-agent interaction is commonplace in our daily lives, manifesting in forms ranging from virtual assistants on websites to embodied agents like robots that we might encounter in a physical setting. Previous research has largely been focused on “behind-the-screen” interactions, but these might not fully encapsulate the nuanced responses humans exhibit towards physically embodied agents. To address this gap, we use virtual reality to examine how simulated physical embodiment and the reliability of an agent (automated robotic crane) inﬂuence trust and performance in a task simulating a quality assurance role and compare it to a “behind-the-screen” interaction. Out of 119 participants, the data revealed there is a marked behavioral shift observed when reliability hits a 91% threshold, with no inﬂuence from embodiment. Furthermore, participants displayed a tendency to trust and defer to the decisions of embodied agents more, especially when these agents were not infallible. This study accentuates the need for transparency about an agent’s capabilities and emphasizes the signiﬁcance of ensuring that the agent’s representation is congruent with the nature of the interaction. Our ﬁndings pave the way for a deeper understanding of human-agent interactions, suggesting a future where these interactions might seamlessly blend the virtual and physical realms."
"Rodriguez Rodriguez, Lucero; Bustamante Orellana, Carlos; Landfair, Jayci; Magaldino, Corey; Demir, Mustafa; Amazeen, Polemnia G.; Metcalfe, Jason S.; Huang, Lixiao; Kang, Yun",Dynamics of Trust in Automation and Interactive Decision Making during Driving Simulation Tasks,2021,1,16,4,12,12 participants were excluded because the analysis focused on the two lowest and two highest-performing participants,Controlled Lab Environment,within-subjects,"Participants completed a practice lap without automation, followed by four experimental trials with different automation types and levels. The order of automation conditions was counterbalanced. Participants were incentivized based on driving performance.","Participants performed a simulated driving task while following a lead vehicle, using an automated driving assistant. They had to manage the automation and respond to perturbations such as changes in lead vehicle speed and the appearance of pedestrians.",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the automated driving system in a simulated environment.,simulation,The study used a driving simulator with a three-screen visual presentation system.,simulated,The automated driving system was a simulated representation within the driving simulator.,shared control (fixed rules),"The automated driving system had fixed rules for lane-keeping and cruise control, with the human able to engage and disengage it.",Behavioral Measures; Performance-Based Measures,,Performance Metrics,Trust was assessed through behavioral measures such as reliance on automation and performance metrics.,no modeling,"The study did not use computational models of trust, focusing on descriptive analysis.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the type and reliability of the automation, and introduced perturbations to the driving task, influencing the driver's reliance on the automation.","Higher levels of automation performance elicited greater reliance on automation. Low-performing participants showed either too low or too high reliance on automation, while high-performing participants showed a more balanced and consistent reliance.","Low-performing participants tended to rely on automation for longer periods or not at all, while high-performing participants toggled automation on and off more consistently. The appearance of dynamic pedestrians influenced the reliance on automation for low-performing participants.","High-performing participants showed a balanced and uniformly distributed reliance on automation, suggesting that strategic use of automation leads to better performance.","The robot (automated driving system) assisted with speed and lane-keeping. The human participant was responsible for monitoring the system, engaging/disengaging the automation, and responding to perturbations.",t-test,"A one-tailed t-test was used to determine the statistical significance of the relationship between automation performance and operator reliance. The test was applied separately for low-performing and high-performing participants, and for both full and speed automation types. The null hypothesis was that automation performance has no effect on operator reliance. The t-test was used to compare the mean reliance scores across different automation conditions for each performance group.",TRUE,Robot-autonomy; Robot-accuracy; Task-environment,Robot-accuracy,Robot-autonomy; Task-environment,"The study manipulated 'Robot-autonomy' by varying the type of automation (speed control only vs. speed and lane-keeping) and the level of automation (low vs. high reliability). This is described in the 'Condition' section: 'The experiment had a 2 (automation type: speed, full) × 2 (automation level: low, high) within-participants design.' The 'Robot-accuracy' was manipulated by varying the reliability of the automation, with low reliability having higher variance in maintaining distance/lane position compared to high reliability. This is also described in the 'Condition' section: 'A low reliability has a higher variance in the maintenance of distance/lane position, as opposed to the high reliability automation which has lower variance.' The 'Task-environment' was manipulated by introducing perturbations such as changes in lead vehicle speed and the appearance of pedestrians, as described in the 'Perturbations' section: 'The first type of perturbation was a change in the velocity of the lead vehicle... The second type of perturbation was wind gusts... The third type of perturbation was the sudden appearance of pedestrians...'. The study found that 'Robot-accuracy' impacted trust, as evidenced by the finding that higher levels of automation performance (high reliability) elicited greater reliance on automation, as stated in the 'Relationship between automation performance and operator's reliance' section: 'Not surprisingly, higher levels of automation performance elicited greater reliance on automation.' The study did not find that 'Robot-autonomy' or 'Task-environment' directly impacted trust, as the analysis focused on the relationship between automation performance and reliance, not the type of automation or the specific perturbations. While these factors were manipulated, the study's analysis did not directly link them to changes in trust levels, instead focusing on the impact of automation performance on reliance.",10.1177/1071181321651288,https://doi.org/10.1177/1071181321651288,"As technological advancements and lowered costs make self-driving cars available to more people, it becomes important to understand the dynamics of human-automation interactions for safety and efficacy. We used a dynamical approach to examine data from a previous study on simulated driving with an automated driving assistant. To maximize effect size in this preliminary study, we focused the current analysis on the two lowest and two highest-performing participants. Our visual comparisons were the utilization of the automated system and the impact of perturbations. Low-performing participants toggled and maintained reliance either on automation or themselves for longer periods of time. Decision making of high-performing participants was using the automation briefly and consistently throughout the driving task. Participants who displayed an early understanding of automation capabilities opted for tactical use. Further exploration of individual differences and automation usage styles will help to understand the optimal human-automation-team dynamic and increase safety and efficacy."
"Roesler, Eileen; Vollmann, Meret; Manzey, Dietrich; Onnasch, Linda",The dynamics of human–robot trust attitude and behavior — Exploring the effects of anthropomorphism and type of failure,2024,2,56,43,13,13 participants needed to be excluded due to technical problems or failures in the experimental procedure,Controlled Lab Environment,mixed design,"Participants completed questionnaires, were shown a robot (either anthropomorphic or technical), completed a pre-interaction questionnaire, then performed a sorting task with the robot over three blocks, with a failure in the second block. Questionnaires were completed after each block.",Participants handed over colored balls to a robot and instructed the robot via voice command where to sort the ball (yellow or blue).,Pepper; Panda,Humanoid Robots; Industrial Robot Arms; Collaborative Robots,Research,Manipulation,Object Passing,minimal interaction,Participants interacted with the robot through object handovers and voice commands.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions.,Behavioral Measures; Questionnaires,Godspeed Questionnaire,Speech Data,Trust was measured using questionnaires and speech analysis.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's appearance (anthropomorphic vs. technical) and performance (failure vs. no failure) were directly manipulated to influence trust.,Trust was higher for the technical robot compared to the anthropomorphic robot. Trust decreased after a failure and increased after a fault-free interaction. Behavioral trust only changed during the trust formation phase.,"The study found that trust attitude and perceived reliability did not perfectly correspond. Trust attitude seemed to completely recover to the pre-failure level, while perceived reliability only partially recovered. There was no behavioral adaptation following a failure experience.","Trust attitude showed the expected dynamics of formation, dissolution, and restoration, but this was not clearly reflected in behavioral measures.","The robot moved its arm forward to receive a ball, then moved the ball to a designated basket based on the participant's voice command. The human handed the ball to the robot and gave a voice command.",t-test; ANOVA,"Independent sample t-tests were used to analyze control variables (affinity towards technology and tendency to anthropomorphize) and manipulation checks (anthropomorphism and human likeness scores). Mixed ANOVAs with repeated measures were used to analyze the dependent variables (trust attitude, perceived reliability, mean volume, maximum volume, and word length) across the different interaction experiences and robot designs. Post hoc tests with Bonferroni correction were used to further analyze the main effect of interaction experience.",TRUE,Robot-aesthetics; Robot-accuracy,Robot-aesthetics; Robot-accuracy,,"The study manipulated the robot's appearance by using either an anthropomorphic robot (Pepper) or a technical robot (Panda), which falls under 'Robot-aesthetics'. The study also manipulated the robot's performance by introducing a failure in the second block, which is categorized as 'Robot-accuracy'. The results showed that both the robot's aesthetics and accuracy impacted trust levels. Specifically, the technical robot was trusted more than the anthropomorphic one, and trust decreased after a failure. Therefore, both 'Robot-aesthetics' and 'Robot-accuracy' are listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1016/j.chb.2023.108008,https://linkinghub.elsevier.com/retrieve/pii/S074756322300359X,"In two experiments centered around trust dynamics in human–robot interaction, we explored the interplay between attribute-based characteristic of robots (i.e., anthropomorphism and type of failure) and trust attitude, perceived reliability, as well as trust behavior. In general, trust attitude is considered to be one of the most important factors for successful collaboration. However, although previous research has shown that performance and attribute-based characteristics of robots lead to changes in trust attitudes, the question remains whether this is also linked to behavioral adaptations. To investigate this relationship, participants collaborated with robots via voice commands that failed. In the first experiment, participants worked with either an anthropomorphic or technical robot. In the second experiment, all participants worked with the technical robot, which made a failure in either information acquisition/processing or action implementation. Participants trusted the technical robot significantly more than the anthropomorphic one. Moreover, failures in information acquisition/processing seemed to lead to more trust dissolution than failures in action implementation. In both experiments, the dynamics of trust attitudes as a function of interaction experience were shown, in particular, trust dissolution due to failure. However, there was no evidence of behavioral adaptation following a failure experience. In general, the effects on the attitudinal level did not clearly translate into behavioral adaptations. For trust attitude, purely decorative anthropomorphic robot design and failures related to the cognitive abilities of robots should be avoided."
"Roesler, Eileen; Vollmann, Meret; Manzey, Dietrich; Onnasch, Linda",The dynamics of human–robot trust attitude and behavior — Exploring the effects of anthropomorphism and type of failure,2024,2,48,40,8,8 participants needed to be excluded due to technical problems or failures in the experimental procedure,Controlled Lab Environment,mixed design,"Participants were assigned to one of two failure conditions (information or action failure), completed questionnaires, then performed a sorting task with the robot over three blocks, with a failure in the second block. Questionnaires were completed after each block.",Participants handed over colored balls to a robot and instructed the robot via voice command where to sort the ball (yellow or blue).,Franka Emika Panda,Industrial Robot Arms; Collaborative Robots,Research,Manipulation,Object Passing,minimal interaction,Participants interacted with the robot through object handovers and voice commands.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions.,Behavioral Measures; Questionnaires,,Speech Data,Trust was measured using questionnaires and speech analysis.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The type of failure (information acquisition/processing vs. action implementation) was directly manipulated to influence trust.,"Trust dynamics were different for the two failure conditions. The information failure condition showed the classical trust dynamic, while the action failure condition did not. Behavioral trust only changed during the trust formation phase.",The study found that trust attitude and perceived reliability did not show the expected dynamics for the action failure condition. The information failure condition showed the classical trust dynamic. There was no behavioral adaptation following a failure experience.,"The type of failure (information vs. action) significantly impacted trust dynamics, with information failures leading to more pronounced trust dissolution.","The robot moved its arm forward to receive a ball, then moved the ball to a designated basket based on the participant's voice command. The human handed the ball to the robot and gave a voice command.",ANOVA,"Mixed ANOVAs with repeated measures were used to analyze the dependent variables (trust attitude, perceived reliability, mean volume, maximum volume, and word length) across the different interaction experiences and failure conditions. Post hoc tests with Bonferroni correction were used to further analyze the main effect of interaction experience.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the type of failure the robot experienced, either an information acquisition/processing failure or an action implementation failure. Both of these failures directly impact the robot's ability to perform the task correctly, thus falling under 'Robot-accuracy'. The results showed that the type of failure impacted trust dynamics, with information failures leading to more pronounced trust dissolution. Therefore, 'Robot-accuracy' is listed as a factor that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1016/j.chb.2023.108008,https://linkinghub.elsevier.com/retrieve/pii/S074756322300359X,"In two experiments centered around trust dynamics in human–robot interaction, we explored the interplay between attribute-based characteristic of robots (i.e., anthropomorphism and type of failure) and trust attitude, perceived reliability, as well as trust behavior. In general, trust attitude is considered to be one of the most important factors for successful collaboration. However, although previous research has shown that performance and attribute-based characteristics of robots lead to changes in trust attitudes, the question remains whether this is also linked to behavioral adaptations. To investigate this relationship, participants collaborated with robots via voice commands that failed. In the first experiment, participants worked with either an anthropomorphic or technical robot. In the second experiment, all participants worked with the technical robot, which made a failure in either information acquisition/processing or action implementation. Participants trusted the technical robot significantly more than the anthropomorphic one. Moreover, failures in information acquisition/processing seemed to lead to more trust dissolution than failures in action implementation. In both experiments, the dynamics of trust attitudes as a function of interaction experience were shown, in particular, trust dissolution due to failure. However, there was no evidence of behavioral adaptation following a failure experience. In general, the effects on the attitudinal level did not clearly translate into behavioral adaptations. For trust attitude, purely decorative anthropomorphic robot design and failures related to the cognitive abilities of robots should be avoided."
"Rogers, Hunter; Khasawneh, Amro; Bertrand, Jeffery; Madathil, Kapil Chalil",An Investigation of the Effect of Latency on the Operator’s Trust and Performance for Manual Multi-robot Teleoperated Tasks,2017,1,40,40,0,"Some outliers were removed from the analysis based on studentized deleted residual and Cook's distance, but the number of participants excluded was not specified.",Controlled Lab Environment,mixed design,"Participants completed a demographic survey, read an informed consent form, were trained on the system, and then completed the simulated rescue task with one or two robots, with or without latency. They then completed trust, usability, and workload questionnaires.",Participants used a joystick to control one or two robots to rescue 10 victims in a simulated building after an earthquake.,Unspecified,Mobile Robots,Research,Navigation,Remote Navigation,minimal interaction,Participants interacted with the robots through a simulation using a joystick.,simulation,The interaction took place in a simulated environment.,simulated,The robots were virtual representations in the simulation.,wizard of oz (directly controlled),The robots were directly controlled by the participants using a joystick.,Questionnaires; Real-time Trust Measures,IBM Computer System Usability Questionnaire Scale (IBM-CSUQ); NASA Task Load Index (NASA-TLX); Jian et al. Trust Scale,Performance Metrics,"Trust was measured using questionnaires, real-time ratings, and performance metrics.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated latency and the number of robots to influence task difficulty and robot performance, which was expected to affect trust.","Increased latency decreased dependability and reliability when using two robots, and increased the perception of harmful outcomes. Frustration was higher when using two robots. The study found evidence to support that increased dependability and reliability are linked to increased trust, and increased possibility of harmful outcomes is linked to mistrust.","There was a significant two-way interaction between latency level and number of robots on dependability, reliability, and harmful outcomes. The simple main effects showed that dependability was not significantly different for the nonlatency group compared to the latency group when using one robot. However, dependability was statistically significantly greater in the non-latency group compared to the latency group when using two robots. The same pattern was observed for reliability and harmful outcomes. There was no significant effect of either independent variable on the time to complete the task.","Increased latency decreased dependability and reliability when using two robots, and increased the perception of harmful outcomes, suggesting a negative impact on trust. The number of robots had a significant effect on the subjective rating of frustration.",The human participant used a joystick to control one or two robots in a simulated environment. The robots moved through the environment to find and rescue victims. The human's task was to navigate the robots to the victims as quickly as possible.,ANOVA; simple main effects test; mauchly's test,"The study used a repeated measure two-way ANOVA to check the significant effects of the independent variables (latency and number of robots) on the dependent variables (performance, workload, trust). Mauchly's test was used to assess the assumption of sphericity. Simple main effects tests were conducted to further investigate significant interaction effects found in the ANOVA.",TRUE,Task-complexity; Robot-accuracy,Robot-accuracy,Task-complexity,"The study manipulated task complexity by varying the number of robots (one or two) that participants had to control, which increased the cognitive load and task switching demands. This is classified as 'Task-complexity' because it directly changes the cognitive demands of the task. The study also manipulated latency, which directly impacted the robot's accuracy in responding to user inputs, leading to more errors (e.g., the robot getting stuck on walls). This is classified as 'Robot-accuracy' because it directly affects the robot's performance in completing the task. The results showed that the latency manipulation (and thus robot accuracy) impacted trust, as participants reported lower dependability and reliability and higher perception of harmful outcomes when using two robots with latency. The number of robots (task complexity) did not directly impact trust, but did impact frustration levels.",10.1177/1541931213601579,http://journals.sagepub.com/doi/10.1177/1541931213601579,"Latency is an important factor when conducting teleoperated missions. This study investigates the effects of latency on a set of dependent variables: performance (measured by time and number of errors), subjective workload, trust, and usability. These measures were tested in a simulated search-and-rescue mission over two levels of two independent variables. One independent variable was the number of robots – one or two (within-subject), and the other independent variable was latency – simulations with and without latency (between-subject.) The significant effect of the independent variables on the dependent variables were checked using repeated measure two-way ANOVA with a confidence level of 95%. The data determined any significant effects that latency and/or the number of robots had on such factors as errors, dependability, reliability, harmful outcomes, temporal demand, and frustration."
"Rogers, Hunter; Khasawneh, Amro; Bertrand, Jeffery; Madathil, Kapil Chalil",An Investigation of the Effect of Latency on the Operator’s Trust and Performance for Manual Multi-robot Teleoperated Tasks,2017,1,40,40,0,"Some outliers were removed from the analysis based on studentized deleted residual and Cook's distance, but the number of participants excluded was not specified.",Controlled Lab Environment,mixed design,"Participants completed a demographic survey, read an informed consent form, were trained on the system, and then completed the simulated rescue task with one or two robots, with or without latency. They then completed trust, usability, and workload questionnaires.",Participants used a joystick to control one or two robots to rescue 10 victims in a simulated building after an earthquake.,Unspecified,Mobile Robots,Research,Navigation,Remote Navigation,minimal interaction,Participants interacted with the robots through a simulation using a joystick.,simulation,The interaction took place in a simulated environment.,simulated,The robots were virtual representations in the simulation.,wizard of oz (directly controlled),The robots were directly controlled by the participants using a joystick.,Questionnaires; Real-time Trust Measures,IBM Computer System Usability Questionnaire Scale (IBM-CSUQ); NASA Task Load Index (NASA-TLX); Jian et al. Trust Scale,Performance Metrics,"Trust was measured using questionnaires, real-time ratings, and performance metrics.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated latency and the number of robots to influence task difficulty and robot performance, which was expected to affect trust.","Increased latency decreased dependability and reliability when using two robots, and increased the perception of harmful outcomes. Frustration was higher when using two robots. The study found evidence to support that increased dependability and reliability are linked to increased trust, and increased possibility of harmful outcomes is linked to mistrust.","There was a significant two-way interaction between latency level and number of robots on dependability, reliability, and harmful outcomes. The simple main effects showed that dependability was not significantly different for the nonlatency group compared to the latency group when using one robot. However, dependability was statistically significantly greater in the non-latency group compared to the latency group when using two robots. The same pattern was observed for reliability and harmful outcomes. There was no significant effect of either independent variable on the time to complete the task.","Increased latency decreased dependability and reliability when using two robots, and increased the perception of harmful outcomes, suggesting a negative impact on trust. The number of robots had a significant effect on the subjective rating of frustration.",The human participant used a joystick to control one or two robots in a simulated environment. The robots moved through the environment to find and rescue victims. The human's task was to navigate the robots to the victims as quickly as possible.,ANOVA; simple main effects test; mauchly's test,"The study used a repeated measure two-way ANOVA to check the significant effects of the independent variables (latency and number of robots) on the dependent variables (performance, workload, trust). Mauchly's test was used to assess the assumption of sphericity for the ANOVA. Simple main effects tests were conducted to further investigate significant interaction effects found in the ANOVA.",TRUE,Task-complexity; Robot-accuracy,Robot-accuracy,Task-complexity,"The study manipulated the number of robots (one or two), which directly impacts the complexity of the task, as participants had to manage either one or two robots simultaneously. This is categorized as 'Task-complexity' because it changes the cognitive demands on the participant. The study also manipulated latency (0ms or 500ms), which directly impacts the robot's performance and the number of errors made by the robot. This is categorized as 'Robot-accuracy' because it directly affects the robot's ability to perform the task effectively. The results showed that latency (and thus robot accuracy) had a significant impact on trust, as increased latency decreased dependability and reliability, and increased the perception of harmful outcomes. The number of robots (task complexity) did not have a significant impact on trust, but did impact frustration levels.",10.1177/1541931213601579,http://journals.sagepub.com/doi/10.1177/1541931213601579,"Latency is an important factor when conducting teleoperated missions. This study investigates the effects of latency on a set of dependent variables: performance (measured by time and number of errors), subjective workload, trust, and usability. These measures were tested in a simulated search-and-rescue mission over two levels of two independent variables. One independent variable was the number of robots – one or two (within-subject), and the other independent variable was latency – simulations with and without latency (between-subject.) The significant effect of the independent variables on the dependent variables were checked using repeated measure two-way ANOVA with a confidence level of 95%. The data determined any significant effects that latency and/or the number of robots had on such factors as errors, dependability, reliability, harmful outcomes, temporal demand, and frustration."
"Rogers, Kantwon; Bryant, De'Aira; Howard, Ayanna","Robot Gendering: Influences on Trust, Occupational Competency, and Preference of Robot Over Human",2020,1,150,150,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of three conditions (male, female, or neutral robot voice), watched a short video of the robot introducing itself, and then completed a questionnaire.","Participants watched a video of a robot and then completed a questionnaire assessing their perception of the robot's occupational competency, trust, and preference over a human for 14 different occupations.",Pepper,Humanoid Robots; Expressive Robots,Research; Social,Evaluation,Survey/Questionnaire Completion,passive observation,Participants watched a video of the robot.,media,Participants watched a video of the robot introducing itself.,physical,Participants watched a video of a physical robot.,pre-programmed (non-adaptive),The robot performed a pre-scripted introduction.,Questionnaires,,,Trust was measured using a questionnaire with Likert scale questions.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's voice was manipulated to be male, female, or neutral, and the robot was given a male, female, or no name, which was intended to influence the perceived gender of the robot.","The perceived gender of the robot did not significantly impact trust in most cases, but participant comfort level with new humanoid robots did significantly influence trust.","Participants were more likely to ascribe a gender to a robot designed to be gender neutral. There were some statistically significant differences in trust based on participant demographics, but these were weakly correlated. Comfort level with new humanoid robots had a larger impact on trust than robot gender.",Participant comfort level with new humanoid robots significantly influences their beliefs and trust in the competency of a robot in particular occupations.,"The robot introduced itself in a video, and participants completed a questionnaire about the robot's occupational competency, trust, and preference over a human for various occupations.",Kruskal-Wallis; Chi-squared,"The study used Kruskal-Wallis tests to examine the differences between robot-gender groups for metrics of occupational competency, trust in occupation, and preference of robot over male/female human for all 14 occupations. Kruskal-Wallis tests were also used to investigate the differences between participant comfort level with new humanoid robots and the metrics of occupational competency, trust in occupation, and preference for the robot over a human male/female. Additionally, Kruskal-Wallis tests were used to examine if gendering tendency had any influence over participants ratings of the robot's occupational competency, trust in occupation, or preferring the robot over a human male/female. Chi-square tests of independents were performed to examine the relationship between participant demographics and occupational competency, trust in occupation, and preference for robot over human for each of the 14 occupations.",TRUE,Robot-verbal-communication-content,,Robot-verbal-communication-content,"The study manipulated the robot's voice and name to be male, female, or neutral. This is a manipulation of the content of the robot's verbal communication, as the name and voice were intended to convey gender. The paper states, 'Two separate videos were generated in which Pepper was filmed acting out the script, ""Hi, my name is {James/Mary} and I am a humanoid robot..."" where James and Mary were used for the male and female robot conditions respectively. A third video was also created with a gender-neutral robot and the script ""Hi, I am a humanoid robot,"" omitting the presentation of a formal name... Computer-generated male, female, and neutral voices were used in each video'. The results section states, 'For nearly all cases, robot gender did not produce statistically significant differences for any of the metrics.' This indicates that the manipulation of the robot's verbal communication content (specifically, the name and voice) did not significantly impact trust. The paper also states, 'participant comfort level when using new humanoid robots does significantly influence their beliefs and trust in the competency of a robot in particular occupations.' However, participant comfort level is not a manipulated factor, but rather a measured individual difference.",10.1145/3334480.3382930,https://dl.acm.org/doi/10.1145/3334480.3382930,"This paper presents an investigation into the differences found in participants’ comfort levels with using new humanoid robots in addition to the tendency to give a gender to a robot designed to be gender neutral. These factors were used to examine participants’ perception of occupational competency, trust, and preference for a humanoid robot over a human male or female for various occupations. Our results suggest that comfort level influences these metrics but does not cause a person to ascribe a gender to a gender-neutral robot. These findings suggest that there is no need to perpetuate societal gender norms onto robots. However, even when designing for robot gender neutrality people are still more likely to ascribe a gender to the robot, but this gendering does not significantly impact occupational judgements."
"Rogers, Kantwon; Webber, Reiden John Allen; Howard, Ayanna",Lying About Lying: Examining Trust Repair Strategies After Robot Deception in a High-Stakes HRI Scenario,2023,1,675,341,334,334 participants were excluded for failing attention checks,Online Crowdsourcing,between-subjects,Participants were asked to drive a simulated car with a robotic assistant that provided false information about police presence. Participants were then presented with one of five different apologies from the robot and completed a post-assessment of trust.,Participants drove a simulated car while receiving advice from a robotic assistant.,Unspecified,Autonomous Vehicles,Social; Research,Social,Social Guidance/Coaching,minimal interaction,Participants interacted with the robot through a driving simulation.,simulation,The interaction was conducted in a simulated driving environment.,simulated,The robot was presented as a simulated assistant within the driving simulation.,pre-programmed (non-adaptive),The robot's behavior was pre-programmed and did not adapt to the user.,Questionnaires,Schaefer's Trust Questionnaire/Scale,,Trust was measured using a pre- and post-interaction questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's behavior was manipulated by providing false information and different types of apologies. The task was framed as a high-stakes scenario, and feedback was manipulated through the robot's responses.",Trust decreased in all conditions after the robot's deception. An apology without admitting deception resulted in a smaller decrease in trust compared to apologies that admitted deception.,Participants were more likely to follow the robot's advice to not speed compared to instructions alone. The apology that did not admit deception was more effective at mitigating trust loss than apologies that admitted deception. Participants interpreted the robot's deception as a malfunction when no admission of deception was given.,An apology without acknowledging intentional deception was most effective at mitigating negative influences on trust after robot deception.,The robot provided advice to the human about the speed limit. The human drove a simulated car and followed or ignored the robot's advice.,Fisher's exact test; Kruskal-Wallis; Dunn test,The study used Fisher's exact test to compare the proportion of participants who did not speed when advised by a robot versus when given instructions alone. A Kruskal-Wallis test was used to determine if there was a significant difference in the change in trust based on the type of apology given by the robot. Post-hoc pairwise comparisons using Dunn's test were then conducted to identify which specific apology conditions differed significantly in terms of trust change.,TRUE,Robot-verbal-communication-content; Robot-morality,Robot-verbal-communication-content; Robot-morality,,"The study manipulated the content of the robot's verbal communication by providing false information about police presence and then varying the type of apology given (or no apology). This falls under 'Robot-verbal-communication-content' because the core manipulation is in the information being conveyed by the robot. The robot's act of lying also constitutes a violation of moral codes, which is why 'Robot-morality' is also included as a manipulated factor. The study found that the type of apology (or lack thereof) significantly impacted trust levels, indicating that both 'Robot-verbal-communication-content' and 'Robot-morality' influenced trust. Specifically, apologies that did not admit deception were more effective at mitigating trust loss, and the explanatory apology was better than the baseline of no apology. The study did not find any factors that were manipulated that did not impact trust.",10.1145/3568294.3580178,https://dl.acm.org/doi/10.1145/3568294.3580178,"This work presents an empirical study into robot deception and its effects on changes in behavior and trust in a high-stakes, timesensitive human-robot interaction scenario. Specifically, we explore the effectiveness of different apologies to repair trust in an assisted driving task after participants realize they have been lied to by a robotic assistant. Our results show that participants are significantly more likely to change their speeding behaviors when driving advice is framed as coming from a robotic assistant. Our results also suggest an apology without acknowledging intentional deception is best at mitigating negative influences on trust. These results add much needed knowledge to the understudied area of robot deception and could inform designers and policy makers of future practices when considering deploying robots that may learn to deceive."
"Romeo, Marta; McKenna, Peter E.; Robb, David A.; Rajendran, Gnanathusharan; Nesset, Birthe; Cangelosi, Angelo; Hastie, Helen",Exploring Theory of Mind for Human-Robot Collaboration,2022,1,706,706,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were assigned to one of three groups (Baseline, Sensor, ToM). They watched a priming video of Pepper either passing or failing a ToM test. Then, they completed a maze task where Pepper gave advice on which path to take. Participants indicated their path choice and confidence. After a mistake by Pepper, it apologized. Finally, participants completed questionnaires.",Participants had to navigate a maze by solving 10 mini-mazes with the help of Pepper's advice. They had to choose between two exit paths for each mini-maze.,Pepper,Humanoid Robots; Expressive Robots,Research; Social,Navigation,Guiding,passive observation,"Participants watched videos of the robot giving advice, without direct interaction.",media,The interaction was presented through videos of the robot.,physical,"The robot was a physical robot, but presented through videos.",pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Behavioral Measures; Multidimensional Measures; Questionnaires,Negative Attitude towards Robots Scale (NARS); Disposition to Trust Questionnaire; Multi-Dimensional Measure of Trust (MDMT),Performance Metrics,"Trust was assessed using questionnaires, behavioral measures, and a multidimensional trust scale.",no modeling,The study did not use computational models of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's utterances were manipulated to mimic Theory of Mind (ToM), sensor-based reasoning, or a baseline. The task difficulty was also manipulated by introducing a trial where the robot made a mistake.","The ToM manipulation led to more cautious decision-making, with participants taking more time and expressing lower confidence after the robot's error. The ToM group showed a more appropriate level of trust, being more sensitive to the robot's performance.","The ToM group showed lower confidence and longer decision times, especially after the robot's error, indicating a more cautious approach. The Sensor group showed the highest confidence overall, while the Baseline group showed over-trust in the robot's suggestions until the final trial.","Mimicking Theory of Mind in a robot did not increase overall trust, but it did lead to more cautious decision-making and a more appropriate level of trust, making participants more sensitive to the robot's performance.","The robot provided advice on which path to take in a maze, and the human participant decided whether to follow the robot's suggestion and selected the path.",ANOVA; Bonferroni correction; Robust regression; chi-square analysis; Poisson regression; Kruskal-Wallis; dunns post hoc comparisons,"The study used a variety of statistical tests to analyze the data. A one-way ANOVA was used to check the manipulation of the ToM condition. Follow-up simple effect analysis using the Bonferroni correction was used to compare the ToM group to the other groups. A robust one-way ANOVA was used to analyze the decision time data, which was positively skewed. Chi-square analyses were used to examine the proportion of participants who followed the robot's advice. Poisson regression was used to compare suggestion adherence at different trials. Kruskal-Wallis tests were used to analyze non-normally distributed data, such as confidence scores and trust measures. Dunns post hoc comparisons were used to compare groups after the Kruskal-Wallis test.",TRUE,Robot-verbal-communication-content; Task-complexity,Robot-verbal-communication-content,,"The study manipulated the robot's verbal communication content by varying the robot's utterances to mimic Theory of Mind (ToM), sensor-based reasoning, or a baseline. This is evident in the 'ToM Manipulation in the Maze Task' section, where it describes how the robot's advice varied per group, with the ToM group's advice considering the participant's perspective, the Sensor group's advice related to functional aspects of its hardware, and the Baseline group's advice simply stating which direction to take. The study also manipulated task complexity by introducing a trial where the robot made a mistake, which increased the ambiguity of the task and required participants to rely more on the robot's advice. This is described in the 'The Maze Task' section, where the task is divided into phases, with a 'Breaking of Trust' trial where the robot gives incorrect advice. The results show that the 'Robot-verbal-communication-content' manipulation impacted trust, as the ToM group showed more cautious decision-making and a more appropriate level of trust, being more sensitive to the robot's performance. The 'Task-complexity' manipulation did not directly impact trust, but it did influence the decision-making process, as participants in the ToM group took more time to decide and were less confident after the robot's error.",10.1109/RO-MAN53752.2022.9900550,https://ieeexplore.ieee.org/document/9900550/,"The ability to impute mental states to oneself or others, or Theory of Mind (ToM), has been intrinsically linked to trust between humans. However, less is known about how a robot mimicking ToM affects users’ trust and behaviour. We explore this through an online study, where we compare three robot personas in a cooperative maze navigation task: one neutral, one that explains its reasoning in technical terms, and one that mimics ToM. We show that ToM influences human decision-making behaviour and trust in a way that makes it more appropriate with respect to the competencies of the robot. This is key for human-robot collaboration and adoption of robotics moving forward."
"Romeo, Marta; Torre, Ilaria; Maguer, Sébastien Le; Cangelosi, Angelo; Leite, Iolanda",Putting Robots in Context: Challenging the Influence of Voice and Empathic Behaviour on Trust,2023,2,100,100,0,No participants were excluded,Online Crowdsourcing,mixed design,"Participants first provided demographic information and filled out the NARS questionnaire. Then, they watched 3 videos of a robot reading encyclopedia entries in one empathic condition and rated the robot on the MDMT scale. The same participants then watched 3 other videos in the other empathic condition and rated the robot again.",Participants watched videos of a robot reading encyclopedia entries and rated the robot's trustworthiness.,Pepper,Humanoid Robots; Expressive Robots,Research; Social,Evaluation,Rating,passive observation,Participants watched videos of the robot.,media,Participants watched videos of the robot.,physical,Participants watched videos of a physical robot.,pre-programmed (non-adaptive),The robot's actions were pre-programmed.,Multidimensional Measures; Questionnaires,Multi-Dimensional Measure of Trust (MDMT); Negative Attitude towards Robots Scale (NARS),Video Data,Trust was measured using the MDMT questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's voice and empathic behavior were manipulated to see how they affected trust.,No significant effect of voice or empathic behavior on trust was found in the general context.,"The empathic behavior manipulation was not entirely successful, as both affective and cognitive versions were rated similarly in terms of cognitive content.","In a general context, robot voice and empathic behavior did not significantly influence trust.","The robot read encyclopedia entries, and the human participant watched the videos and rated the robot's trustworthiness.",ANOVA,"A two-way ANOVA was conducted on the Moral Trust scores, using voice and empathic behavior as independent variables. The purpose was to determine if these factors had a significant effect on moral trust in a general context. The analysis excluded Capacity Trust data due to participants indicating it was not applicable.",TRUE,Robot-verbal-communication-style; Robot-emotional-display,,Robot-verbal-communication-style; Robot-emotional-display,"The study manipulated the robot's voice by using either the default Pepper voice or a custom TTS voice. This is categorized as 'Robot-verbal-communication-style' because it changes the way the robot's speech is delivered (tone, expressiveness). The study also manipulated the robot's empathic behavior by having it express either affective or cognitive empathy. This is categorized as 'Robot-emotional-display' because it changes the robot's expression of feelings. The results showed that neither of these manipulations had a significant impact on trust in this general context. Therefore, both 'Robot-verbal-communication-style' and 'Robot-emotional-display' are listed as factors that did not impact trust.",10.1109/RO-MAN57019.2023.10309631,https://ieeexplore.ieee.org/document/10309631/,"Trust is essential for social interactions, including those between humans and social artificial agents, such as robots. Several robot-related factors can contribute to the formation of trust. However, previous work has often treated trust as an absolute concept, whereas it is highly contextdependent, and it is possible that some robot-related features will influence trust in some contexts, but not in others. In this paper, we present the results of two video-based online studies aimed at investigating the role of robot voice and empathic behaviour on trust formation in a general context as well as in a task-specific context. We found that voice influences trust in the specific context, with no effect of voice or empathic behaviour in the general context. Thus, context mediated whether robotrelated features play a role in people’s trust formation towards robots."
"Romeo, Marta; Torre, Ilaria; Maguer, Sébastien Le; Cangelosi, Angelo; Leite, Iolanda",Putting Robots in Context: Challenging the Influence of Voice and Empathic Behaviour on Trust,2023,2,51,51,0,No participants were excluded,Online Crowdsourcing,mixed design,"Participants provided demographic information and filled out the NARS questionnaire. Then, they watched a video of a robot giving maze navigation recommendations in one empathic condition and rated the robot on the MDMT scale. The same participants then watched another video in the other empathic condition and rated the robot again.",Participants watched videos of a robot giving maze navigation recommendations and rated the robot's trustworthiness.,Pepper,Humanoid Robots; Expressive Robots,Research; Social,Evaluation,Rating,passive observation,Participants watched videos of the robot.,media,Participants watched videos of the robot.,physical,Participants watched videos of a physical robot.,pre-programmed (non-adaptive),The robot's actions were pre-programmed.,Multidimensional Measures; Questionnaires,Multi-Dimensional Measure of Trust (MDMT); Negative Attitude towards Robots Scale (NARS),Video Data,Trust was measured using the MDMT questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's voice and empathic behavior were manipulated to see how they affected trust in a specific context.,"Robot voice influenced moral-based trust, with the default voice being trusted more than the custom TTS voice. No effect of empathy was found.","People trusted the robot with the default Pepper voice more than the robot with the custom TTS voice in the specific context. Only moral-based trust was affected by the robot's voice, not capacity-based trust.","In a specific context, robot voice influenced moral-based trust, with the default voice being trusted more than the custom TTS voice.","The robot gave maze navigation recommendations, and the human participant watched the videos and rated the robot's trustworthiness.",ANOVA,"A two-way ANOVA was conducted on the full dataset, using voice and empathic behavior as independent variables. The purpose was to determine if these factors had a significant effect on both moral-based and capacity-based trust in a specific context (maze navigation). The analysis revealed a main effect of voice on moral-based trust, but no effect of empathy or any interactions. No significant effects were found for capacity-based trust.",TRUE,Robot-verbal-communication-style; Robot-emotional-display,Robot-verbal-communication-style,Robot-emotional-display,"Similar to Study 1, the robot's voice was manipulated using either the default Pepper voice or a custom TTS voice, which is categorized as 'Robot-verbal-communication-style'. The robot's empathic behavior was also manipulated by having it express either affective or cognitive empathy, categorized as 'Robot-emotional-display'. The results showed that the robot's voice ('Robot-verbal-communication-style') had a significant impact on moral-based trust, with the default voice being trusted more. However, the empathic behavior manipulation ('Robot-emotional-display') did not have a significant impact on trust. Therefore, 'Robot-verbal-communication-style' is listed as a factor that impacted trust, and 'Robot-emotional-display' is listed as a factor that did not impact trust.",10.1109/RO-MAN57019.2023.10309631,https://ieeexplore.ieee.org/document/10309631/,"Trust is essential for social interactions, including those between humans and social artificial agents, such as robots. Several robot-related factors can contribute to the formation of trust. However, previous work has often treated trust as an absolute concept, whereas it is highly contextdependent, and it is possible that some robot-related features will influence trust in some contexts, but not in others. In this paper, we present the results of two video-based online studies aimed at investigating the role of robot voice and empathic behaviour on trust formation in a general context as well as in a task-specific context. We found that voice influences trust in the specific context, with no effect of voice or empathic behaviour in the general context. Thus, context mediated whether robotrelated features play a role in people’s trust formation towards robots."
"Rosenthal, Stephanie; Carter, Elizabeth",Impact of Explanation on Trust of an Novel Mobile Robot,2020,1,60,60,0,No participants were excluded,Controlled Lab Environment,mixed design,Participants played a Snake game while monitoring a robot navigating a grid map. Some participants received an explanation of the robot's behavior. Participants completed questionnaires after each trial.,Participants were asked to play a Snake game while simultaneously monitoring a robot's navigation on a grid map and reporting errors.,Cozmo,Expressive Robots; Mobile Robots,Research,Supervision,Monitoring,minimal interaction,Participants observed the robot and pressed a button to report errors while playing a game.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical Cozmo robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed path without adapting to the environment or user.,Behavioral Measures; Performance-Based Measures; Questionnaires,Ten Item Personality Inventory (TIPI); Jian et al. Trust Scale; Muir's Trust Questionnaire,Performance Metrics; robot data,"Trust was assessed using questionnaires, game performance metrics, and robot error detection.",no modeling,The study did not use computational models of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"Participants were given an explanation of the robot's behavior or no explanation, influencing their expectations. The robot also made errors, influencing performance.","Explanations increased neglect tolerance in early trials, but this effect diminished over time. Robot errors decreased trust as measured by game performance and subjective ratings.",The dual-task experiment was able to measure subtle changes in trust that the survey results could not identify. The effect of the explanation diminished over time as participants became more familiar with the robot's behavior.,"Explanations of robot behavior can counteract the novelty effect of a new robot, increasing user trust and neglect tolerance in early trials, but this effect diminishes with experience.","The robot navigated a pre-programmed path on a grid map, and the human participant played a Snake game while monitoring the robot for errors, pressing a button when an error occurred.",ANOVA; Tukey HSD,"The study used ANOVA to analyze the effects of different conditions (explanation, error order, map, and error presence) on various dependent variables, including key count, key rate, death count in the Snake game, and questionnaire responses. Tukey HSD post-hoc tests were used for pairwise comparisons when significant main effects or interactions were found in the ANOVAs.",TRUE,Robot-verbal-communication-content; Robot-accuracy; Task-complexity,Robot-verbal-communication-content; Robot-accuracy,Task-complexity,"The study manipulated whether participants received an explanation of the robot's behavior, which falls under 'Robot-verbal-communication-content'. The robot's behavior was also manipulated by having it make errors (entering specific squares), which is categorized as 'Robot-accuracy' because it directly impacts task performance. The study also used a dual-task paradigm, where participants had to play a game while monitoring the robot, which is categorized as 'Task-complexity'. The results showed that the explanation and the robot's errors impacted trust, as evidenced by changes in game performance and subjective ratings. The dual-task paradigm was a constant across all conditions and did not have a differential impact on trust, therefore it is listed as a factor that did not impact trust. Specifically, the paper states: 'Participants who received an explanation of the robot's behavior were more likely to focus on their own task at the risk of neglecting their robot supervision task during the first trials of the robot's behavior compared to those who did not receive an explanation.' and 'Key press count and key press rate for the game were both slower when the robot made errors by entering target squares compared to when it did not, indicating that the participants spent more time monitoring the robot during periods when an error occurred.' These quotes demonstrate the impact of explanation and robot errors on trust. The paper also states: 'There were no significant main or interaction effects for ratings of ""I could not focus on the Snake task because the robot needed my attention.""' which demonstrates that the dual-task paradigm did not have a differential impact on trust.",,,"One challenge with introducing robots into novel environments is misalignment between supervisor expectations and reality, which can greatly affect a user’s trust and continued use of the robot. We performed an experiment to test whether the presence of an explanation of expected robot behavior affected a supervisor’s trust in an autonomous robot. We measured trust both subjectively through surveys and objectively through a dual-task experiment design to capture supervisors’ neglect tolerance (i.e., their willingness to perform their own task while the robot is acting autonomously). Our objective results show that explanations can help counteract the novelty effect of seeing a new robot perform in an unknown environment. Participants who received an explanation of the robot’s behavior were more likely to focus on their own task at the risk of neglecting their robot supervision task during the ﬁrst trials of the robot’s behavior compared to those who did not receive an explanation. However, this effect diminished after seeing multiple trials, and participants who received explanations were equally trusting of the robot’s behavior as those who did not receive explanations. Interestingly, participants were not able to identify their own changes in trust through their survey responses, demonstrating that the dual-task design measured subtler changes in a supervisor’s trust."
"Rossi, Alessandra",Evaluating People’s Perceptions of Trust in a Robot in a Repeated Interactions Study,2020,1,6,6,0,No participants were excluded,Real-World Environment,between-subjects,"Participants interacted with a robot in a house over three weeks, with errors introduced at the beginning or end of the interaction. They completed questionnaires before and after the study and were presented with an emergency scenario at the end.","Participants interacted with a robot in a home environment, performing various tasks, and were then presented with a simulated fire emergency to assess their trust in the robot's ability to handle the situation.",Care-O-bot 4,Humanoid Robots; Service and Assistive Robots,Care; Research; Social,Social,Social Guidance/Coaching,direct-contact interaction,Participants directly interacted with the robot in a real-world setting.,real-world,The interaction took place in a realistic home environment.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions.,Behavioral Measures; Questionnaires,Negative Attitude towards Robots Scale (NARS); Ten Item Personality Inventory (TIPI); Disposition to Trust Questionnaire,Video Data,Trust was measured using questionnaires and behavioral observations.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The timing of robot errors (beginning or end of interaction) was manipulated to see how it affected trust. The emergency task was framed as a critical situation.,Trust was lower when the robot made errors at the beginning of the interaction compared to the end.,"Some participants did not acknowledge that the robot made errors, possibly due to feeling inhibited to criticize the robot directly. There was no statistical difference between participant's choice of trust and their personalities or disposition of trust.",People's trust in a robot is more affected by errors made at the beginning of an interaction than at the end.,"The robot performed various tasks in a home environment, such as playing games, serving drinks, and controlling smart home devices. The human participant interacted with the robot, responded to its prompts, and made choices in an emergency scenario.",cronbach's alpha,Cronbach's Alpha was used to measure the internal consistency (or reliability) of the subscales of the Negative Attitudes towards Robots (NARS) questionnaire. The analysis was performed on the responses to the NARS questionnaire at the beginning and end of the interaction trials.,TRUE,Robot-accuracy; Task-environment,Robot-accuracy,,"The study manipulated 'Robot-accuracy' by introducing errors with severe consequences either at the beginning (condition C1) or at the end (condition C2) of the interaction. The paper states, 'Each condition included fourteen tasks performed by the robot, either correctly, or with errors with severe consequences on the first or last day of interaction.' This manipulation directly impacted the robot's performance and was designed to observe its effect on trust. The 'Task-environment' was also manipulated by having the interaction take place in a real house, which is described as 'a very realistic domestic environment suitable for live interactions.' This manipulation was not the primary focus of the study, but it was a factor that was intentionally set by the researchers. The study found that the timing of the robot's errors ('Robot-accuracy') significantly impacted trust, with errors at the beginning of the interaction leading to lower trust than errors at the end. The paper states, 'Moreover, this study showed that participants were affected more by robot errors made at the beginning of the interaction than at the end of the interaction.' There was no mention of the task environment impacting trust, so it is not included in the factors that impacted trust.",,,"Trust has been established to be a key factor in fostering human-robot interactions. However, trust can change overtime according to diﬀerent factors, including a breach of trust due to a robot’s error. In this exploratory study, we observed people’s interactions with a companion robot in a real house, adapted for human-robot interaction experimentation, over three weeks. The interactions happened in six scenarios in which a robot performed diﬀerent tasks under two diﬀerent conditions. Each condition included fourteen tasks performed by the robot, either correctly, or with errors with severe consequences on the ﬁrst or last day of interaction. At the end of each experimental condition, participants were presented with an emergency scenario to evaluate their trust in the robot. We evaluated participants’ trust in the robot by observing their decision to trust the robot during the emergency scenario, and by collecting their views through questionnaires. We concluded that there is a correlation between the timing of an error with severe consequences performed by the robot and the corresponding loss of trust of the human in the robot. In particular, people’s trust is subjected to the initial mental formation."
"Rossi, Alessandra; Dautenhahn, Kerstin; Koay, Kheng Lee; Walters, Michael L.",How the Timing and Magnitude of Robot Errors Influence Peoples’ Trust of Robots in an Emergency Scenario,2017,1,200,154,46,46 participants were excluded for giving more than one wrong answer to the verification questions,Online Crowdsourcing,between-subjects,"Participants interacted with a robot through an interactive storyboard, presented with 10 scenarios with varying robot errors, followed by an emergency scenario to assess trust.","Participants were asked to imagine living with a robot and interact with it through a storyboard, making choices in response to the robot's actions and finally choosing how to respond to an emergency.",Unspecified,Service and Assistive Robots,Care; Social,Evaluation,Text Evaluation,minimal interaction,"Participants interacted with the robot through a storyboard, making choices in response to the robot's actions.",simulation,"The interaction was presented through an interactive storyboard, simulating a home environment.",simulated,The robot was represented through a graphical interface in the storyboard.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user's behavior.,Behavioral Measures; Questionnaires,Ten Item Personality Inventory (TIPI); Interpersonal Trust Scale/Questionnaire,,Trust was assessed using questionnaires and by observing participants' choices in an emergency scenario.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the severity and timing of robot errors, and the task was framed as a home companion scenario.","Trust decreased when the robot made severe errors, especially at the beginning of the interaction. Participants were more inclined to trust in teamwork when the robot made small errors.","Participants' trust was more severely affected when the robot made severe errors, especially at the beginning of the interaction. There was no significant dependency between gender or age and trust choices.","The magnitude of errors made by a robot correlates with a decrease in human trust, with severe errors having a greater negative impact, especially when they occur early in the interaction.","The robot, Jace, performed various tasks in a home environment, sometimes making errors. The human participant read the scenarios and made choices in response to the robot's actions, including how to respond to an emergency.",Chi-squared,"A Chi-Square test was used to examine the association between the participants' choices in the emergency scenario and the experimental conditions. Additionally, a Chi-Square test was used to test the association between participants' emergency choices and their country of residence (specifically comparing India and USA). The strength of the relationship between the emergency choice and experimental conditions was measured using Cramer's V. Adjusted standardized residuals (Pearson residuals) were used to further analyze the differences between the results obtained.",TRUE,Robot-accuracy; Task-environment,Robot-accuracy,,"The study manipulated 'Robot-accuracy' by varying the severity (trivial or severe) and timing (beginning or end of interaction) of robot errors across different conditions. The paper states, 'In this study we used an interactive storyboard presenting ten different scenarios in which a robot performed different tasks under five different conditions. Each condition included the ten different tasks performed by the robot, either correctly, or with small or big errors.' This directly relates to the robot's performance on the tasks. The 'Task-environment' was also manipulated by presenting the scenarios in a home environment, as stated in the paper: 'Participants were asked to imagine that they lived with a robot as a companion in their home which helps them with everyday activities.' The study found that the severity and timing of robot errors ('Robot-accuracy') significantly impacted trust, as stated in the results: 'We can observe that participants' trust is affected more severely when the robot made errors with severe consequences.' The study did not find any significant impact of the task environment on trust, as the environment was consistent across all conditions and the focus was on the robot's actions within that environment.",,http://link.springer.com/10.1007/978-3-319-70022-9_5,"Trust is a key factor in human users’ acceptance of robots in a home or human oriented environment. Humans should be able to trust that they can safely interact with their robot. Robots will sometimes make errors, due to mechanical or functional failures. It is therefore important that a domestic robot should have acceptable interactive behaviours when exhibiting and recovering from an error situation. In order to deﬁne these behaviours, it is ﬁrstly necessary to consider that errors can have diﬀerent degrees of consequences. We hypothesise that the severity of the consequences and the timing of a robot’s diﬀerent types of erroneous behaviours during an interaction may have diﬀerent impacts on users’ attitudes towards a domestic robot. In this study we used an interactive storyboard presenting ten diﬀerent scenarios in which a robot performed diﬀerent tasks under ﬁve diﬀerent conditions. Each condition included the ten diﬀerent tasks performed by the robot, either correctly, or with small or big errors. The conditions with errors were complemented with four correct behaviours. At the end of each experimental condition, participants were presented with an emergency scenario to evaluate their current trust in the robot. We conclude that there is correlation between the magnitude of an error performed by the robot and the corresponding loss of trust of the human in the robot."
"Rossi, Alessandra; Dautenhahn, Kerstin; Koay, Kheng Lee; Saunders, Joe",Investigating Human Perceptions of Trust in Robots for Safe HRI in Home Environments,2017,1,50,50,0,No participants were excluded,Survey/Interview,within-subjects,Participants completed a questionnaire rating the magnitude of errors in different scenarios using a 7-point Likert scale.,Participants rated the magnitude of 20 different robot errors.,Unspecified,Service and Assistive Robots,Care,Evaluation,Rating,passive observation,Participants only read about the robot and interaction scenarios.,media,The interaction was based on text descriptions of scenarios.,hypothetical,The robot was only described in text.,not autonomous,"The robot's actions were described in text, without any real autonomy.",Questionnaires,,,Trust was assessed using a questionnaire with a Likert scale.,no modeling,No computational model of trust was used.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study assessed perceptions of error magnitude without manipulating any specific factors.,"The study did not directly measure trust, but rather the perceived magnitude of errors.",No significant differences were found between gender or age in rating the errors.,The study identified 3 big errors and 3 small errors based on participant ratings.,Participants read descriptions of robot errors and rated their magnitude.,,"The study did not explicitly mention any specific statistical tests. The analysis involved calculating the mean and standard deviation of error ratings and categorizing errors as 'small' or 'big' based on a threshold of 4 on a 7-point Likert scale. The study also reported that no significant differences were found between gender or age in rating the errors, but the specific statistical method used to determine this was not mentioned.",FALSE,,,,"The study did not manipulate any factors. Participants were asked to rate the magnitude of different robot errors, but there was no manipulation of the robot's behavior, communication, or any other factor. The study was designed to identify what people consider to be small or big errors, not to test the impact of any manipulated factor on trust. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust.",10.1145/3029798.3034822,https://dl.acm.org/doi/10.1145/3029798.3034822,"In an era in which robots take a part in our lives in daily living activities, humans have to trust robots in home environments. We aim to create guidelines that allow humans to trust robots to be able to look after their well-being by adopting human-like behaviours. We want to study a Human-Robot Interaction (HRI) to assess whether a certain degree of transparency in the robots actions, the use of social behaviours and natural communications can aﬀect humans’ sense of trust and companionship towards the robots. However, trust can change over time due to diﬀerent factors, e.g. due to perceiving erroneous robot behaviors. We believe that the magnitude and the timing of the error during an interaction may have diﬀerent impacts resulting in diﬀerent scales of loss of trust and of restoring lost trust."
"Rossi, Alessandra; Holthaus, Patrick; Dautenhahn, Kerstin; Koay, Kheng Lee; Walters, Michael L.",Getting to know Pepper: Effects of people's awareness of a robot's capabilities on their trust in the robot,2018,1,43,43,0,No participants were excluded,Educational Setting,within-subjects,"Participants were exposed to three different types of HRI: a video demonstration, a live interaction, and a programming task. After each session, they rated their trust in the robot using questionnaires.","Participants watched a video of the robot, interacted with the robot, and programmed the robot to perform a storytelling task.",Pepper,Humanoid Robots; Expressive Robots,Social; Educational; Research,Social,Emotional Expression,direct-contact interaction,Participants had direct physical interaction with the robot during the live and programming phases.,real-world,The study involved real-world interaction with a physical robot.,physical,Participants interacted with a physical robot.,shared control (fixed rules),"The robot's behavior was pre-programmed, but participants could influence it through programming.",Questionnaires; Custom Scales,,,Trust was measured using questionnaires with custom scales.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the interaction medium (video, live, programming) and the robot's behavior through programming, influencing participants' expectations and trust.","Trust increased after the live interaction compared to the video, and it was further affected by the programming phase, with some tasks showing a decrease in trust after programming.","Trust increased after the live interaction, but decreased for some tasks after the programming phase, suggesting that awareness of the robot's limitations can reduce trust in certain areas.","Participants' trust in the robot was positively affected by increased awareness of the robot's capabilities, but this effect was not uniform across all tasks.","The robot presented a story with emotional cues, and the human participants watched a video, interacted with the robot, and programmed the robot to perform the storytelling task.",ANOVA; t-test,"The study used ANOVA to examine the correlation between participants' trust ratings and the different types of interaction (video, live, programming). T-tests were used to analyze the differences in average ratings between paired samples of interactions, specifically comparing trust ratings after each interaction type.",TRUE,Robot-autonomy; Robot-emotional-display,Robot-autonomy; Robot-emotional-display,,"The study manipulated the level of control participants had over the robot, moving from passive observation (video) to direct interaction and finally to programming the robot's behavior. This change in control is categorized as 'Robot-autonomy'. The study also involved the robot displaying different emotions during the storytelling task, which is categorized as 'Robot-emotional-display'. The paper explicitly states that the participants' trust was affected by the different levels of interaction and the robot's emotional expressions, indicating that both 'Robot-autonomy' and 'Robot-emotional-display' impacted trust. The paper states that 'The study revealed that the pupils’ trust is positively affected across different domains after each session, indicating that human users trust a robot more the more awareness about the robot they have.' and 'The custom behaviours could be designed and implemented in the robot's graphical programming suite Choregraphe. To achieve this goal, the pupils could use predefined building blocks that allowed them to manipulate body movements, gesture, verbal and nonverbal cues individually per sentence and thus emotion.' This shows that the autonomy and emotional display were manipulated and impacted trust.",10.1145/3284432.3284464,https://dl.acm.org/doi/10.1145/3284432.3284464,"This work investigates how human awareness about a social robot’s capabilities is related to trusting this robot to handle different tasks. We present a user study that relates knowledge on different quality levels to participant’s ratings of trust. Secondary school pupils were asked to rate their trust in the robot after three types of exposures: a video demonstration, a live interaction, and a programming task. The study revealed that the pupils’ trust is positively affected across different domains after each session, indicating that human users trust a robot more the more awareness about the robot they have."
"Rossi, Alessandra; Moros, Silvia; Dautenhahn, Kerstin; Koay, Kheng Lee; Walters, Michael L.",Getting to know Kaspar : Effects of people’s awareness of a robot’s capabilities on their trust in the robot,2019,1,172,172,0,No participants were excluded,Educational Setting,within-subjects,"Participants were exposed to three different types of HRI: a video presentation, a live interaction, and a programming task. They completed a questionnaire after each interaction.","Participants watched a video of the robot, interacted with the robot in demo mode, and then programmed the robot to perform different emotions.",Kaspar,Humanoid Robots,Educational; Research; Social,Social,Emotional Expression,minimal interaction,"Participants interacted with the robot through video, live demo, and programming, but there was no physical touch.",real-world,"The study involved real-world interaction with a physical robot, including a live demo and programming.",physical,The robot was physically present during the live interaction and programming sessions.,wizard of oz (directly controlled),The robot was teleoperated by a researcher during the video and live demo interactions.,Questionnaires,,,Trust was measured using questionnaires with semantic differential scales.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the level of awareness of the robot's capabilities through different interaction types, which influenced the participants' expectations and perceptions of the robot.","The study found that trust in the robot did not significantly change across the different interaction types, but the likeability of the robot decreased after each interaction.","The study found that the likeability of the robot decreased after each interaction, which was unexpected. There was no significant difference in trust across the different interaction types.",The study's main finding is that increasing children's awareness of a robot's capabilities does not significantly affect their trust in the robot across different domains.,"The robot was teleoperated to interact with the children, and the children watched a video, observed a live demo, and then programmed the robot to express different emotions.",ANOVA; t-test,"The study used ANOVA to examine the correlation between the willingness of having the robot at home and the effects of the interaction type (video, live, programming). A t-test was then used to analyze the differences in participants' perceptions of the robot across the different interaction types, specifically comparing the average ratings after each interaction.",TRUE,Robot-autonomy; Task-complexity,,Robot-autonomy; Task-complexity,"The study manipulated the level of robot autonomy by having the robot teleoperated in the video and live demo conditions, and then allowing the participants to program the robot in the third condition. This change in control over the robot's actions is a manipulation of 'Robot-autonomy'. The study also manipulated the 'Task-complexity' by having participants first observe the robot, then interact with it in a demo mode, and finally program it. This progression from passive observation to active programming represents an increase in task complexity. The study found that these manipulations did not significantly impact the participants' trust in the robot, as stated in the paper: 'The study revealed that the pupils’ trust is not significantly affected across different domains after each session.' and 'We did not find any statistically significant differences in the results of this study concerning the pupils' perception of trust in the robot's capabilities.' Therefore, both 'Robot-autonomy' and 'Task-complexity' are listed as factors that did not impact trust.",10.1109/RO-MAN46459.2019.8956470,https://ieeexplore.ieee.org/document/8956470/,"In this work we investigate how humans’ awareness of a social robot’s capabilities affect their trust in the robot. We present a user study that relates knowledge on different quality levels to participants’ ratings of trust. Primary school pupils were asked to rate their trust in the robot after three types of interactions: a video demonstration, a live interaction, and a programming task. The study revealed that the pupils’ trust is not signiﬁcantly affected across different domains after each session. It did not appear to be signiﬁcant differences in trust tendencies for the different experiences either; however, our results suggest that human users trust a robot more the more awareness about the robot they have."
"Rossi, Alessandra; Garcia, Fernando; Maya, Arturo Cruz; Dautenhahn, Kerstin; Koay, Kheng Lee; Walters, Michael L.; Pandey, Amit K.",Investigating the Effects of Social Interactive Behaviours of a Robot on People’s Trust During a Navigation Task,2019,1,12,10,2,2 participants were excluded because they did not believe the robot was behaving autonomously,Controlled Lab Environment,within-subjects,"Participants followed the robot in three conditions: C1 (robot stops for obstacles), C2 (robot avoids obstacles), and C3 (robot uses social behaviors). Participants completed questionnaires after each condition and a final questionnaire at the end.",Participants were asked to follow a robot that guided them to an information center in a simulated shopping mall environment.,Pepper,Humanoid Robots; Expressive Robots,Research; Social,Navigation,Guiding,minimal interaction,Participants followed the robot and adapted their behavior to the robot's behavior.,real-world,The study was conducted in a real-world environment with a physical robot.,physical,The robot was physically present and interacted with the participants.,pre-programmed (non-adaptive),"The robot followed pre-programmed behaviors in each condition, without adapting to the user.",Questionnaires,Godspeed Questionnaire,,Trust was measured using questionnaires before and after each condition.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's behavior was manipulated across three conditions: stopping for obstacles, avoiding obstacles, and using social behaviors. This was intended to influence trust by changing the robot's perceived social competence.",Participants reported higher trust and comfort levels after interacting with the social robot (C3) compared to the other conditions. Initial trust in a generic robot was lower than trust in the robot after the interaction.,"Participants' initial trust in a generic robot was lower than their trust in the robot after the interaction, especially in the social condition. Participants also preferred a social robot that communicates using speech and maintains an appropriate distance from obstacles.",Participants felt more comfortable and trusted a social robot more than a non-social robot during a navigation task.,"The robot guided participants through a simulated shopping mall environment. The human participants followed the robot, adapting their behavior to the robot's actions. The robot either stopped for obstacles, avoided obstacles, or used social behaviors to navigate.",Friedman test; Wilcoxon rank sum; ANOVA; cronbach's alpha test,"The study used Friedman tests to analyze differences in participants' comfort and trust ratings across the three experimental conditions (C1, C2, and C3). Wilcoxon Signed-Ranks tests were used for post-hoc comparisons to determine which specific conditions differed significantly. An ANOVA test was used to assess the effects of the conditions on participants' ratings along the Godspeed questionnaire's dimensions. Cronbach's alpha test was used to test the internal consistency of the subscales of the Godspeed questionnaire.",TRUE,Robot-nonverbal-communication; Robot-verbal-communication-content; Robot-task-strategy,Robot-nonverbal-communication; Robot-verbal-communication-content,Robot-task-strategy,"The study manipulated several aspects of the robot's behavior across three conditions. In C1, the robot stopped for obstacles, which is a basic task strategy. In C2, the robot avoided obstacles without social behaviors, which is also a task strategy. In C3, the robot used social behaviors, including verbal communication ('Excuse me, I would like to pass', 'thank you') and non-verbal communication (changing the color of its shoulder LEDs, gesturing with its arm, and varying its velocity). The robot's task strategy (stopping or avoiding obstacles) did not significantly impact trust, as the main difference in trust was observed when the robot exhibited social behaviors. The verbal communication content (e.g., the robot speaking) and nonverbal communication (e.g., gestures, lights, velocity changes) were the factors that significantly impacted trust, as participants reported higher trust and comfort levels after interacting with the social robot (C3) compared to the other conditions. The paper states, 'Participants felt more comfortable (z = −2.4, p < 0.01) after condition C3 (mean rank = 4) then after condition C2 (mean rank = 0). Similarly, they felt more comfortable (z = −2.81, p < 0.01) after condition C3 (mean rank = 5) than after condition C1 (mean rank = 0).' and 'Participants trusted the robot to be able to guide them more (z = −1.86, p = 0.03) when tested with condition C3 (mean rank = 5) than their initial belief (mean rank = 4.43).'",,http://link.springer.com/10.1007/978-3-030-23807-0_29,"Identifying the roles and the speciﬁc social behaviours that evoke human trust towards robots is key for user acceptance. Specially, while performing tasks in the real world, such as navigation or guidance, the predictability of robot motion and predictions of user intentions facilitate interaction. We present a user study in which a humanoid-robot guided participants around a human populated environment, avoiding collisions while following a socially acceptable trajectory. We investigated which behaviours performed by a humanoid robot during a guidance task exhibited better social acceptance by people, and how robot behaviours inﬂuence their trust in a robot to safely complete a guiding task. We concluded that in general, people prefer and trust a robot that exhibits social behaviours such as talking and maintaining an appropriate safe distance from obstacles."
"Rossi, Alessandra; Dautenhahn, Kerstin; Lee Koay, Kheng; Walters, Michael L.",How Social Robots Influence People’s Trust in Critical Situations,2020,1,20,17,3,3 participants did not hear the robot's suggestion in the third task,Controlled Lab Environment,between-subjects,Participants were introduced to a robot in a room and engaged in three scenarios with increasing criticality. The robot either displayed social behaviors or did not. Participants' trust was measured through their decisions in each scenario and questionnaires.,"Participants interacted with a robot in three scenarios: choosing a song, handing over a package to a courier, and deciding whether to use gloves to pick up a cake from a microwave.",Pepper,Humanoid Robots; Expressive Robots,Social; Research,Social,Social Guidance/Coaching,minimal interaction,Participants received verbal instructions from the robot and interacted with it in a controlled setting.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,wizard of oz (directly controlled),The robot's actions were controlled by a human operator using a Wizard of Oz technique.,Behavioral Measures; Questionnaires,Ten Item Personality Inventory (TIPI); Robotic Social Attributes Scale (RoSAS),Video Data; Speech Data,Trust was assessed using questionnaires and behavioral observations of participants' choices.,no modeling,The study did not use computational models of trust.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's social behavior was manipulated by having it either use social cues or not, and the task criticality was increased across the three scenarios to influence trust.","Participants trusted the robot in low and medium criticality tasks, but trust decreased in the high criticality task, especially with the social robot.","Participants trusted the robot almost blindly in the first two scenarios, but not in the last scenario. The participants who did not trust the robot in the last scenario were tested with a social robot and perceived it as a friendly toy. Some participants expected a more conversational interaction, similar to Amazon Alexa.","Participants' trust in a robot was affected by the criticality of the task, with a decrease in trust observed in a high-stakes scenario, particularly when interacting with a social robot.","The robot greeted the participant and then asked them to choose a song, hand over a package to a courier, and decide whether to use gloves to pick up a cake. The human participant followed the robot's instructions and made decisions based on the robot's suggestions.",,"No statistical tests were explicitly mentioned in the paper. The analysis was primarily qualitative, focusing on coding and categorizing participants' responses to open-ended questions and observing their behavior during the tasks. The study did not report any statistical comparisons or significance testing.",TRUE,Robot-nonverbal-communication; Robot-verbal-communication-style; Task-complexity,Robot-nonverbal-communication; Task-complexity,Robot-verbal-communication-style,"The study manipulated the robot's nonverbal communication by having it either use social cues (body and head movements, gestures, appropriate distance) or not (no movements, neutral tone, 2 meters distance). This is explicitly stated in the 'Procedure' section: 'For the non-social robot, each robot's behaviours and responses during the interaction were scripted in a polite but neutral way and not to show any sentiment. The non-social Pepper did not move its body and head... In contrast, the social robot made slight movements while it was talking or listening to a participant...'. The robot's verbal communication style was also manipulated by having the robot use a polite but neutral tone in the non-social condition and a more engaging tone in the social condition. This is described in the 'Procedure' section: 'For the non-social robot, each robot's behaviours and responses during the interaction were scripted in a polite but neutral way and not to show any sentiment.' and 'In contrast, the social robot made slight movements while it was talking or listening to a participant...'. The task complexity was manipulated by increasing the criticality of the tasks across the three scenarios, from choosing a song to handing over a package to deciding whether to use gloves to pick up a cake. This is described in the 'Method' section: 'We chose to not randomise the scenarios, but to organise them according to an increasing level of criticality of the tasks.' The results showed that the robot's nonverbal communication and the task complexity impacted trust levels. Participants trusted the robot in low and medium criticality tasks, but trust decreased in the high criticality task, especially with the social robot. This is described in the 'Abstract' and 'Conclusion' sections. The verbal communication style did not have a significant impact on trust, as the study found no statistically significant difference between the social and non-social conditions in terms of trust, although the social robot was perceived as a friendly toy by some participants who did not trust it in the high criticality task. This is described in the 'Conclusion' section: 'We did not find any statistically significant difference comparing the participants' responses according the experimental conditions they were tested in.'",10.1109/RO-MAN47096.2020.9223471,https://ieeexplore.ieee.org/document/9223471/,"As we expect that the presence of autonomous robots in our everyday life will increase, we must consider that people will have not only to accept robots to be a fundamental part of their lives, but they will also have to trust them to reliably and securely engage them in collaborative tasks. Several studies showed that robots are more comfortable interacting with robots that respect social conventions. However, it is still not clear if a robot that expresses social conventions will gain more favourably people’s trust. In this study, we aimed to assess whether the use of social behaviours and natural communications can affect humans’ sense of trust and companionship towards the robots. We conducted a betweensubjects study where participants’ trust was tested in three scenarios with increasing trust criticality (low, medium, high) in which they interacted either with a social or a non-social robot. Our ﬁndings showed that participants trusted equally a social and non-social robot in the low and medium consequences scenario. On the contrary, we observed that participants’ choices of trusting the robot in a higher sensitive task was affected more by a robot that expressed social cues with a consequent decrease of their trust in the robot."
"Rossi, Alessandra; Dautenhahn, Kerstin; Koay, Kheng Lee",A matter of consequences: Understanding the effects of robot errors on people? trust in HRI,2024,2,200,154,46,"46 participants were excluded because they failed the attention checks, and some participants answered the question about the robot's secret with their own secret",Online Crowdsourcing,between-subjects,"Participants interacted with a virtual robot in planned scenarios using an interactive storyboard. They were assigned to one of five conditions where the robot performed 10 tasks, with varying error types and severities. Participants' trust was assessed through an emergency scenario and questionnaires.","Participants imagined living with a robot companion and observed the robot performing various tasks, some with errors, and then made a decision in an emergency scenario.",Jace,Humanoid Robots,Social; Research,Evaluation,Survey/Questionnaire Completion,minimal interaction,Participants interacted with a virtual robot through an interactive storyboard.,simulation,The study used an interactive storyboard with 3D objects and images to simulate a realistic environment.,simulated,"The robot was a 3D, fictional humanoid robot presented in a simulated environment.",pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user's behavior.,Behavioral Measures; Questionnaires,Ten Item Personality Inventory (TIPI); Disposition to Trust Questionnaire,,Trust was assessed using questionnaires and behavioral measures based on choices in an emergency scenario.,no modeling,No computational model of trust was used in this study.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers directly manipulated the robot's performance by introducing errors with varying consequences and framed the task as a home companion scenario.,Robot errors with severe consequences had a greater negative impact on trust than errors with minor consequences. Participants trusted the robot more when it showed flawless behavior.,"Participants' trust was more severely affected by errors with severe consequences, and they tended to trust the robot more when it had shown flawless behaviors. There was a tendency to attribute human characteristics to the robot.",Robot errors with severe consequences had a greater negative impact on trust than errors with minor consequences.,"The robot performed various domestic tasks, some with errors, and the human participant observed and made choices in an emergency scenario.",Chi-squared; ANOVA; Pearson correlation; Spearman correlation; Linear regression,"The study used a Chi-squared test to analyze the relationship between experimental conditions and participants' choices in the emergency scenario. ANOVA tests were used to examine the relationship between participants' personality traits, their propensity for trusting the robot, and their choices in the emergency scenario. Pearson correlation was used to determine the relationship between the participants' perception of a generic robot as a companion with both their experience with robots and participants' personality traits. Spearman's rank-order correlation was used to analyze the relationship between participants' desire of having Jace as home companion and their level of extroversion and trust in people's competencies, and also to analyze the relationship between participants' experience of robots and their wish of having a robot different from Jace as a companion. Multiple linear regression analysis was used to predict participants' previous experience from their perceptions of the robot and the different experimental conditions.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the robot's accuracy by having it perform tasks with varying levels of errors (flawless, small errors, and severe errors). The paper states, 'Depending on the experimental conditions participants were assigned to, they were either presented with scenarios where the robot executes its tasks flawlessly or with a mixture of flawless and erroneous behaviours.' The different conditions (C1-C5) varied the timing and severity of these errors. The results showed that the robot's accuracy, specifically the severity of the errors, directly impacted trust levels, as stated in the paper: 'participants' trust is affected more severely when the robot made errors with severe consequences... Participants trusted the robot more when it had shown flawless behaviours.' There were no other factors manipulated in this study.",,,
"Rossi, Alessandra; Dautenhahn, Kerstin; Koay, Kheng Lee",A matter of consequences: Understanding the effects of robot errors on people? trust in HRI,2024,2,6,6,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants interacted with a physical robot over three weeks, twice per week. They were assigned to one of two conditions: errors at the beginning or end of the interaction. Trust was assessed through an emergency scenario and questionnaires.","Participants interacted with a robot in a simulated home environment, performing various tasks, and then made a decision in an emergency scenario.",Care-O-bot 4,Service and Assistive Robots; Mobile Manipulators,Care; Social; Research,Social,Survey/Questionnaire Completion,direct-contact interaction,Participants had direct interaction with a physical robot in a simulated home environment.,real-world,"The study was conducted in a fully functional smart house, providing a realistic interaction environment.",physical,The robot was a physical Care-O-bot 4 robot.,wizard of oz (directly controlled),"The robot's speech was controlled by an experimenter, while other behaviors were autonomous.",Behavioral Measures; Questionnaires,,,Trust was assessed using questionnaires and behavioral measures based on choices in an emergency scenario.,no modeling,No computational model of trust was used in this study.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers directly manipulated the timing of robot errors, either at the beginning or end of the interaction, and framed the task as a home companion scenario.","Participants trusted the robot more when errors occurred at the end of the interaction, suggesting that trust can be recovered more easily after an established bond.","Participants trusted the robot more when errors occurred at the end of the interaction. Participants were reluctant to openly criticize the robot for its errors, possibly due to a bystander effect.","People's trust was affected the most when the robot made errors at the beginning of the interaction, suggesting that initial interactions have a strong impact on trust.","The robot performed various domestic tasks, and the human participant interacted with the robot and made choices in an emergency scenario.",ANOVA,The study used one-way ANOVA tests to examine the relationship between participants' choice to trust the robot and their personal traits and disposition.,TRUE,Robot-accuracy; Robot-social-timing,Robot-accuracy; Robot-social-timing,,"In this study, the researchers manipulated the robot's accuracy by introducing errors with severe consequences either at the beginning or the end of the interaction. The paper states, 'They participated in repeated interactions over three weeks, twice per week... the robot made big errors at the beginning of the interaction (i.e. on the first day of interaction); 2) the robot made big errors at the end of the interaction (i.e. on the last day of interaction).' This manipulation of error timing is a manipulation of 'Robot-social-timing' because it changes when the errors occur within the interaction. The study found that both the timing of the errors and the presence of errors impacted trust, as stated in the paper: 'Participants trusted the robot more when they were experiencing robot's behaviours with big errors at the end of the interaction... When tested in condition CP2, participants often did not trust the robot.' There were no other factors manipulated in this study.",,,
"Rovira, Ericka; Pak, Richard; McLaughlin, Anne",Effects of individual differences in working memory on performance and trust with various degrees of automation,2017,1,86,85,1,1 participant was excluded due to equipment failure,Controlled Lab Environment,within-subjects,Participants first completed a spatial working memory task followed by a simulated artillery sensor-to-shooter targeting task. They completed the targeting task under varying degrees of automation and task load. Trust and workload were measured after each block.,Participants completed a simulated artillery sensor-to-shooter targeting task with varying degrees of automation and task load.,Unspecified,Other,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated system through a computer interface.,simulation,The interaction was conducted in a simulated environment.,simulated,The robot was represented as a simulated system on a computer screen.,shared control (fixed rules),"The automation provided suggestions based on fixed rules, but participants could override them.",Questionnaires; Custom Scales,NASA Task Load Index (NASA-TLX); Jian et al. Trust Scale,Performance Metrics,"Trust was measured using questionnaires and a custom scale after each block, and performance metrics were collected.","parametric models (e.g., regression)","Multilevel linear models were used to analyze the relationship between working memory, automation, and performance.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the degree of automation, task load, and automation correctness to influence trust and performance.",Lower working memory was associated with more trust in automation. Trust decreased with higher task load in lower automation conditions.,"The study found that higher degrees of automation helped performance when correct, but worsened performance when incorrect, especially for those with lower working memory. The study also found that lower working memory was associated with more trust in automation.","Individual differences in working memory significantly impact performance and trust with varying degrees of automation, with lower working memory associated with more trust and greater performance decrements when automation fails.","The human participant was tasked with selecting the optimal enemy-friendly pairing in a simulated artillery targeting task, with varying degrees of automation support. The automation provided suggestions, and the human could either use the automation or make their own decisions. The robot (simulated) provided information, prioritized options, or narrowed down options for the human.",ANOVA; Multilevel Model; Pearson correlation,"The study used repeated measures ANOVAs to evaluate the effects of degrees of automation, task load, and automation correctness on decision accuracy, subjective mental workload, and trust. Multilevel linear models (MLMs) were used to measure the role of individual differences in cognitive ability on task performance under the various manipulations, examining the influence of working memory and decision accuracy. Correlation analysis was used to examine the relationship between working memory scores and trust measures.",TRUE,Robot-autonomy; Task-complexity; Robot-accuracy,Robot-autonomy; Task-complexity,,"The study manipulated the degree of automation support, which directly corresponds to 'Robot-autonomy'. The automation ranged from manual (no automation) to information automation (providing all options), low-decision automation (prioritized options), and medium-decision automation (top three options). This manipulation directly affected the level of decision authority given to the automation. The study also manipulated 'Task-complexity' by varying the number of friendly and enemy units, which increased the cognitive demands of the task. Finally, the study manipulated 'Robot-accuracy' by introducing automation failures, where the automation provided incorrect suggestions. The paper explicitly states that trust was impacted by the degree of automation and task load, as evidenced by the significant interaction between these factors on trust ratings (Figure 6). Specifically, trust decreased with higher task load in lower automation conditions. The study did not find any factors that did not impact trust.",10.1080/1463922X.2016.1252806,https://www.tandfonline.com/doi/full/10.1080/1463922X.2016.1252806,"Previous studies showed performance beneﬁts with correct automation, but performance costs when the automation was incorrect (i.e. provided an incorrect course of action), particularly as degrees of automation increased. Automation researchers have examined individual differences, but have not investigated the relationship between working memory and performance with various degrees of automation that is both correct and incorrect. In the current study, working memory ability interacted with automation reliability and degree of automation. Higher degrees of correct automation helped performance while higher degrees of incorrect automation worsened performance, especially for those with lower working memory. Lower working memory was also associated with more trust in automation. Results illustrate the interaction between degree of automation and individual differences in working memory on performance with automation that is correct and automation that fails."
"Rovira, Ericka; McLaughlin, Anne Collins; Pak, Richard; High, Luke","Looking for Age Differences in Self-Driving Vehicles: Examining the Effects of Automation Reliability, Driving Risk, and Physical Impairment on Trust",2019,1,138,138,0,No participants were excluded,Online Crowdsourcing,mixed design,"Participants were presented with 8 scenarios in random order, one at a time, and asked to assess their trust on a Likert scale after each scenario, followed by demographic surveys.",Participants read scenarios describing interactions with self-driving cars and rated their trust in the car for each scenario.,Unspecified,Autonomous Vehicles,Other,Evaluation,Text Evaluation,passive observation,Participants only read text-based scenarios.,media,The interaction was based solely on text descriptions.,hypothetical,The robot was only described in text.,fully autonomous (limited adaptation),"The self-driving car was described as operating autonomously, but with limited adaptation.",Questionnaires,,,Trust was measured using a single Likert scale question after each scenario.,"parametric models (e.g., regression)",Multilevel modeling was used to analyze the trust ratings.,Observational & Survey Studies,Quantitative Surveys,Direct Manipulation,"The study manipulated the reliability of the self-driving car (success or failure), the risk level of the driving scenario (high or low), and the physical impairment of the driver (yes or no) to influence trust.","Trust was higher when the car was reliable, lower when the risk was high, and higher when the driver was impaired. Younger adults adjusted their trust downward more than older adults when the car failed.","Older adults did not adjust their trust downward as much as younger adults when the car failed, which is inconsistent with some prior findings. There was also a significant three-way interaction between car reliability, travel risk, and driver impairment status.","Trust in self-driving cars is influenced by situational characteristics such as reliability, driver impairment, and risk level, and younger adults adjust their trust more than older adults when the car fails.",Participants read scenarios describing a person using a self-driving car and then rated their trust in the car.,t-test; Multilevel Model; ANOVA,"The study used t-tests to compare demographic variables between younger and older adults. Multilevel modeling was employed to analyze the trust ratings, accounting for the nested structure of the data (repeated measures within participants). ANOVA was used to examine the main effects and interactions of the manipulated factors (reliability, risk, impairment) on trust, as well as the interaction between age and reliability.",TRUE,Robot-accuracy; Task-environment; Task-constraints,Robot-accuracy; Task-environment; Task-constraints,,"The study manipulated three factors: reliability of the self-driving car (success or failure), which directly impacts the robot's accuracy in completing the task; the risk level of the driving scenario (high or low), which is a change in the task environment; and the physical impairment of the driver (yes or no), which is a change in the task constraints. The paper explicitly states that these factors were manipulated to examine their influence on trust. The results showed that all three factors significantly impacted trust levels. Specifically, trust was higher when the car was reliable (Robot-accuracy), lower when the risk was high (Task-environment), and higher when the driver was impaired (Task-constraints). The interaction between these factors also influenced trust. Therefore, all three manipulated factors impacted trust.",10.3389/fpsyg.2019.00800,https://www.frontiersin.org/article/10.3389/fpsyg.2019.00800/full,"Purpose: Self-driving cars are an extremely high level of autonomous technology and represent a promising technology that may help older adults safely maintain independence. However, human behavior with automation is complex and not straightforward (Parasuraman and Riley, 1997; Parasuraman, 2000; Rovira et al., 2007; Parasuraman and Wickens, 2008; Parasuraman and Manzey, 2010; Parasuraman et al., 2012). In addition, because no fully self-driving vehicles are yet available to the public, most research has been limited to subjective survey-based assessments that depend on the respondents’ limited knowledge based on second-hand reports and do not reﬂect the complex situational and dispositional factors known to affect trust and technology adoption. Methods: To address these issues, the current study examined the speciﬁc factors that affect younger and older adults’ trust in self-driving vehicles. Results: The results showed that trust in self-driving vehicles depended on multiple interacting variables, such as the age of the respondent, risk during travel, impairment level of the hypothesized driver, and whether the self-driving car was reliable. Conclusion: The primary contribution of this work is that, contrary to existing opinion surveys which suggest broad distrust in self-driving cars, the ratings of trust in selfdriving cars varied with situational characteristics (reliability, driver impairment, risk level). Speciﬁcally, individuals reported less trust in the self-driving car when there was a failure with the car technology; and more trust in the technology in a low risk driving situation with an unimpaired driver when the automation was unreliable."
"Roy, Sanjit K.; Singh, Gaganpreet; Sadeque, Saalem; Gruner, Richard L.",Customer experience quality with social robots: Does trust matter?,2024,1,326,326,0,No participants were excluded,Online Crowdsourcing,,Participants were shown a video of a prototypical humanoid robot and then completed a survey.,"Participants completed a survey about their perceptions of customer experience quality, trust in social robots, and trust in service providers.",Unspecified,Humanoid Robots,Social,Evaluation,Survey/Questionnaire Completion,passive observation,Participants watched a video of a robot.,media,Participants watched a video of a robot interaction.,simulated,Participants viewed a video of a robot.,not autonomous,The robot's actions were pre-recorded in a video.,Questionnaires,,,Trust was measured using questionnaires.,"parametric models (e.g., regression)",The study used PLS-SEM and ANN to model relationships between variables.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,"The study did not manipulate any factors directly related to trust, but measured trust based on the survey responses.","The study examined the relationships between customer equity drivers, trust in social robots, trust in service providers, and customer experience quality, but did not manipulate any factors to directly influence trust.","The study found that brand and relationship equity influence trust in social robots, while value equity does not. Trust in service providers also contributes to developing trust in social robots.","The study's key finding is that trust in service providers mediates the relationship between customer equity drivers and customer experience quality, and that trust in service providers also contributes to developing trust in social robots.","The robot was shown in a video, and the human participant completed a survey about their perceptions of the robot and the service provider.",Partial least squares; artificial neural network (ann),"The study employed a two-stage hybrid approach using Partial Least Squares Structural Equation Modeling (PLS-SEM) and Artificial Neural Network (ANN) analysis. PLS-SEM was used to examine linear relationships between customer equity drivers (value, brand, and relationship equity), trust in social robots, trust in service providers, and customer experience quality. Specifically, it tested the hypothesized direct and indirect effects. ANN was then used to capture non-linear relationships and assess the relative importance of predictor constructs, using the significant relationships identified by PLS-SEM as inputs. The ANN analysis used a sigmoid activation function and a 10-fold cross-validation technique to minimize overfitting. The study also used bootstrapping to generate t-values for the PLS-SEM analysis and examined common method bias using exploratory factor analysis and a marker variable method.",FALSE,,,,"The study did not manipulate any factors. Participants were shown a video of a robot and then completed a survey. The study examined the relationships between customer equity drivers, trust in social robots, trust in service providers, and customer experience quality, but did not manipulate any factors to directly influence trust. The study measured trust based on survey responses, and the analysis focused on the relationships between variables rather than the impact of manipulated factors.",10.1016/j.techfore.2023.123032,https://linkinghub.elsevier.com/retrieve/pii/S0040162523007175,"Although service providers increasingly adopt social robots, much remains to be learned about what influences customers’ experiences with robots. To address this issue, this study investigates the relationships among customer equity drivers (i.e., value equity, brand equity and relationship equity), trust in social robots, and trust in service providers. Specifically, we hypothesize that customer equity drivers influence trust in social robots and trust in service providers. We also propose that customer equity drivers influence customer experience quality in the context of social robots and that trust in social robots and trust in service providers mediate these re­ lationships. The study used a two-stage hybrid partial least squares structural equation modelling (PLS-SEM)artificial neural network (ANN) analysis to examine the proposed relationships. Findings show that while all the customer equity drivers influence trust in service providers, only brand and relationship equity influence trust in social robots. Results also suggest that trust in service providers mediates the relationship between customer equity drivers and customer experience quality. In addition, we find that consumers’ trust in service providers helps generate trust in social robots. Theoretical and managerial implications are discussed."
"Sadrfaridpour, Behzad; Saeidi, Hamed; Wang, Yue",An integrated framework for human-robot collaborative assembly in hybrid manufacturing cells,2016,1,5,5,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed an assembly task under three conditions: manual, pHRI, and integrated. The order of conditions was randomized. Workload and trust were measured after each condition.",Participants assembled parts brought by a robot.,Baxter,Humanoid Robots; Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically collaborated with the robot to complete an assembly task.,real-world,The study involved a real-world interaction with a physical robot.,physical,The study used a physical robot for the interaction.,shared control (adaptive),The robot adapted its speed based on human motion and trust levels.,Questionnaires,Human-Robot Trust Scale/Questionnaire (HRTS/HRITS); NASA Task Load Index (NASA-TLX),"Performance Metrics; robot data (sensor data, etc.)",Trust was measured using a questionnaire and performance metrics were used for modeling.,"parametric models (e.g., regression)",A parametric model was used to model trust based on robot and human performance.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's control scheme was manipulated to be either purely based on physical HRI or integrated with trust considerations, affecting robot speed and synchronization with the human.",The trust-based integrated scheme resulted in higher subjective trust compared to the pHRI-based method.,"The trust-based integrated scheme resulted in lower perceived task load and higher trust compared to the pHRI-based scheme, which in turn was better than the manual control method.",The trust-based integrated control scheme improved human experience and joint performance compared to the pHRI-based method.,"The robot brought the main part to the shared workspace, and the human brought two fitting parts and installed them on the main part.",,"No specific statistical tests are mentioned in the paper. The results are presented as mean values (µ) and standard deviations (σ) for the measured variables (workload, trust, and robot path velocity) across the different experimental conditions. However, no specific statistical tests are mentioned to compare these means.",TRUE,Robot-autonomy; Robot-task-strategy,Robot-autonomy,Robot-task-strategy,"The study manipulated the robot's control scheme, which directly impacts its autonomy. In the pHRI condition, the robot's motion was synchronized with the human's, while in the integrated condition, the robot's speed was also influenced by a trust model. This change in how the robot adapts its speed based on human motion and trust levels is a manipulation of 'Robot-autonomy'. The robot's task strategy was also implicitly manipulated, as the robot's speed and synchronization with the human changed between conditions, but this did not directly impact trust. The paper states that the trust-based integrated scheme resulted in higher subjective trust compared to the pHRI-based method, indicating that the manipulation of 'Robot-autonomy' impacted trust. The robot's task strategy was manipulated by changing the robot's speed and synchronization with the human, but this did not directly impact trust, as the trust model was the main driver of trust changes.",10.1109/COASE.2016.7743441,http://ieeexplore.ieee.org/document/7743441/,"Recently, lightweight and ﬂexible robots have been introduced for human-robot collaborative manufacturing. However, most of the research in this ﬁeld focuses on the physical aspects of the interaction only. Nevertheless, psychological and social impact of human-robot collaboration (HRC) on manufacturing needs to be addressed and embedded in the design as well such that the robot actions become acceptable and comfortable for the human. Motivated by this need, we propose an integrated physical and social HRC framework for assembly tasks in a hybrid manufacturing cell. To address human physical demands, the robot motion control will be developed to keep pace with human motion during the task. Furthermore, psychological human factors will also be taken into account into the robot motion control for better HRC. More speciﬁcally, we consider a computational model of a human worker’s trust in his/her robot partner and use the trust evaluation as a constraint in an optimal control problem. Finally, we run a pilot study to test our proposed framework."
"Saeidi, Hamed; Wagner, John R.; Wang, Yue",A Mixed-Initiative Haptic Teleoperation Strategy for Mobile Robotic Systems Based on Bidirectional Computational Trust Analysis,2017,1,32,30,2,2 participants were excluded since they did not follow the protocol correctly,Controlled Lab Environment,within-subjects,"Participants received a 20-min tutorial, 5 min of practice, and then completed four 2-min tests with different control strategies, followed by questionnaires.",Participants controlled a UAV to track a moving UGV at a desired altitude.,Unspecified,Unmanned Aerial Vehicles (UAVs); Unmanned Ground Vehicles (UGVs),Research,Navigation,Remote Navigation,minimal interaction,Participants controlled a UAV remotely with haptic feedback.,real-world,Participants interacted with real robots in a lab setting.,physical,The study used physical UAV and UGV robots.,shared control (adaptive),The robot's autonomy level adapted based on a computational trust model.,Questionnaires,,Performance Metrics,Trust was measured using post-test questionnaires and performance metrics.,"parametric models (e.g., regression)",A computational model of trust was used to dynamically scale control allocation.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the level of autonomy and haptic feedback based on a computational trust model, which was intended to improve task performance and operator satisfaction.",The trust-based mixed-initiative strategy was more trusted than the optimal control approach.,"The trust-based mixed-initiative strategy was more trusted than the optimal control approach, even though their tracking performances were similar. This was attributed to the trust-based approach following a more human-like decision-making pattern.","A trust-based mixed-initiative haptic teleoperation strategy resulted in higher task performance, operator satisfaction, and trust compared to manual teleoperation and an optimal control approach.","The human operator controlled a UAV to track a moving UGV, receiving haptic feedback. The robot's autonomous controller also contributed to the UAV's control, with the balance between human and robot control dynamically adjusted based on a computational trust model.",ANOVA,"A one-way repeated measures ANOVA with a Bonferroni correction was conducted on the dependent variables (DVs) to compare the four control strategies (IV): manual teleoperation (M), manually adjusted mixed-initiative (MMI), optimal mixed-initiative (OMI), and trust-based mixed-initiative (TMIG). The DVs included objective measures like average tracking error and applied force feedback, and subjective measures like perceived task load (NASA TLX), satisfaction, and trust.",TRUE,Robot-autonomy,Robot-autonomy,,"The study manipulated the level of autonomy provided to the robot. The control strategies included manual teleoperation (M), manually adjusted mixed-initiative (MMI), optimal mixed-initiative (OMI), and trust-based mixed-initiative (TMIG). The key difference between these conditions was how the control of the UAV was shared between the human operator and the autonomous controller. In M, the human had full control. In MMI, the human could manually adjust the level of autonomy. In OMI, the level of autonomy was adjusted based on an optimization algorithm. In TMIG, the level of autonomy was adjusted based on a computational trust model. This manipulation of the level of autonomy directly relates to the 'Robot-autonomy' category, as it changes the decision authority between the human and the robot. The paper explicitly states that the trust-based mixed-initiative strategy (TMIG) was more trusted than the optimal control approach (OMI), even though their tracking performances were similar. This indicates that the manipulation of autonomy impacted trust. The other conditions were also compared in terms of trust, indicating that the manipulation of autonomy was the key factor influencing trust. There were no other factors that were manipulated that were found to impact trust.",10.1109/TRO.2017.2718549,http://ieeexplore.ieee.org/document/7973034/,"The goal of this paper is to improve the performance of joint human–robot systems while balancing human experience through computational trust analysis in telerobotics applications. A mixed-initiative approach is enabled by scaling the manual and autonomous controls using a function of computational human-to-robot trust. A dynamic haptic force feedback scaling strategy is performed via a function of computational robot-to-human trust to adjust the assistive force and, hence, physical workload of the operator. Passivity-based methods are used to guarantee the stability of the overall framework. Moreover, guidelines are provided to adjust the transparency of force feedback and velocity signals while guaranteeing passivity. An experimental study with 30 human subjects demonstrates a 12.8% increase in task performance and a 10.7% decrease in operator workload when the proposed strategy is used as compared to a manual autonomy allocation approach. The results also indicate that the proposed method yields higher operator satisfaction compared to manual and optimal autonomy allocation methods and is 10.1% more trusted by the operators than the optimal allocation method."
"Saeidi, Hamed; Wang, Yue",Incorporating Trust and Self-Confidence Analysis in the Guidance and Control of (Semi)Autonomous Mobile Robotic Systems,2019,1,20,18,2,The experimental data corresponding to participants 4 and 5 were dropped since they did not follow the protocol.,Controlled Lab Environment,within-subjects,"Participants received a 15-minute tutorial, followed by a 10-minute identification test to determine parameters for the models. Participants then completed two 6-minute trials under different control conditions and completed post-test questionnaires.","Participants controlled a UAV to follow a UGV, switching between manual and autonomous control.",Unspecified,Unmanned Aerial Vehicles (UAVs); Unmanned Ground Vehicles (UGVs),Research,Navigation,Remote Navigation,minimal interaction,Participants interacted with the robot through a joystick and visual feedback from the UAV camera view.,simulation,The interaction took place in a simulated environment using a Gazebo robot simulator.,simulated,The robot was a simulated UAV controlled through a joystick.,shared control (fixed rules),The robot switched between manual and autonomous control based on a fixed rule using the T SC model.,Questionnaires,,Performance Metrics,Trust was measured using questionnaires and performance metrics.,"parametric models (e.g., regression)",The study used a performance-centric computational model of trust and self-confidence based on objective measures of human and robot performance.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the control allocation strategy, switching between manual, T SC-based, and performance maximization strategies, which influenced the level of autonomy and task difficulty.","The T SC-based strategy resulted in higher trust and satisfaction compared to the performance maximization strategy, which caused rapid control switches and confusion.",The T SC-based control allocation strategy was more likely to be followed by participants compared to the performance maximization strategy. The study also found that participants preferred to receive good suggestions and follow them to avoid higher cognitive load.,"The T SC-based autonomy allocation strategy can capture human autonomy allocation patterns and improve robot performance while reducing operator workload, and is more trusted and preferred by participants compared to a performance maximization strategy.","The robot (UAV) autonomously followed a UGV or was manually controlled by the human using a joystick. The human's task was to control the UAV to follow the UGV as precisely as possible, switching between manual and autonomous control.",ANOVA; t-test,"A one-way repeated measures ANOVA was used to analyze the differences in robot performance, human utilization, human performance, perceived workload, satisfaction, and trust across the five different control strategies (manual, T SC-based with and without forced compliance, and performance maximization with and without forced compliance). A dependent t-test was used to compare the percentage of followed suggestions between the T SC-based and performance maximization conditions.",TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,,"The study manipulated the control allocation strategy, which directly changes the level of autonomy the robot has. This is described in section IV-B, where the study compares manual control (C1) with T SC-based automated control (C2 and C3) and performance maximization strategies (C4 and C5). The T SC-based strategy switches between manual and autonomous control based on a trust and self-confidence model, while the performance maximization strategy switches based on robot performance. This manipulation of control allocation directly influences the robot's autonomy, making 'Robot-autonomy' the most appropriate category. The study also manipulated the task complexity by changing the control strategy, which influenced the cognitive load on the participants. The T SC-based strategy aimed to reduce workload, while the performance maximization strategy caused rapid control switches and confusion, thus increasing the cognitive load. This is described in the results section (IV-C-1) where the study reports on perceived workload. The study found that the T SC-based strategy resulted in higher trust and satisfaction compared to the performance maximization strategy, which caused rapid control switches and confusion. This indicates that the manipulation of 'Robot-autonomy' impacted trust levels. The study did not find any factors that did not impact trust.",10.1109/LRA.2018.2886406,https://ieeexplore.ieee.org/document/8573827/,"We propose a trust and self-conﬁdence-based autonomy allocation strategy to automatically choose between manual and autonomous control of (semi)autonomous mobile robots in guidance and navigation tasks. We utilize a performance-centric, computational trust and self-conﬁdence model and automated autonomy allocation strategy, developed in our earlier work (H. Saeidi and Y. Wang, “Trust and self-conﬁdence based autonomy allocation for robotic systems,” in Proc. 54th IEEE Conf. Decis. Control, 2015, pp. 6052–6057.), based on objective and unbiased performance measures for the human and the robot. A set of robot simulations with a human-in-the-loop is conducted for a teleoperated unmanned aerial vehicle tracking task. The results demonstrate that our allocation strategy can capture human autonomy allocation pattern with an accuracy of 64.05%. We also show that the strategy can improve the overall robot performance by 11.76% and reduce operator’s workload by 10.07% compared to a manual allocation. Moreover, compared to a performance maximization strategy, our strategy is 23.42% more likely to be accepted and generally preferred and trusted by the participants. Furthermore, we design a decision pattern correction algorithm based on nonlinear model predictive control to help a human operator gradually adapt to a modiﬁed allocation pattern for improved overall performance."
"Salcedo, J. N.; Ortiz, E. C.; Lackey, S. J.; Hudson, I.; Taylor, A. H.",Effects of Autonomous vs. Remotely-Operated Unmanned Weapon Systems on Human-Robot Teamwork and Trust,2011,1,48,48,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were randomly assigned to teams and completed a training scenario. They then completed two experimental scenarios with either an autonomous or remotely-operated weapon system, followed by questionnaires after each scenario.","Participants engaged targets as a human-robot team with a weapon system, either autonomous or remotely-operated.",Unspecified,Unmanned Ground Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated robot in a virtual environment.,simulation,The interaction took place in a virtual environment using a large projector screen.,simulated,The robot was a simulated representation in the virtual environment.,pre-programmed (non-adaptive),"The robot followed a consistent pattern of movement, firing, and target engagement for all scenarios.",Questionnaires,,,Trust was assessed using a self-report questionnaire.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The weapon system was either fully autonomous or remotely-operated, which was intended to influence trust perceptions.",No significant differences in trust were found between the autonomous and remotely-operated conditions.,"A significant difference in cognitive load was found, with the autonomous condition resulting in higher cognitive load, particularly for participants in Lane 4. This suggests that the perceived isolation from the team may have influenced cognitive load and potentially trust.",The study found that whether a robotic weapon system is autonomous or remotely-operated does not significantly influence overall teamwork quality and trust.,The robot moved and fired at targets following a pre-programmed pattern. The human participants engaged targets using simulated M16 rifles as a team with the robot.,t-test,"Paired sample t-tests were used to compare performance, Team Experience and Quality Questionnaire (TEQ), Cognitive Load Questionnaire (CLQ), and RWS Perception Questionnaire (RPQ) data between the autonomous and remotely-operated conditions. Specifically, the tests examined differences in these measures between groups and individuals teaming with a computer or human operated RWS. Significant differences were found for CLQ measures, indicating higher cognitive load in the autonomous condition, particularly for participants in Lane 4.",TRUE,Robot-autonomy,,Robot-autonomy,"The study explicitly manipulated the level of autonomy of the weapon system, which was either fully autonomous or remotely-operated. This is a direct manipulation of the robot's decision authority, fitting the 'Robot-autonomy' category. The results indicated that this manipulation did not significantly impact trust, as stated in the paper: 'Results from this study suggest that whether a robotic weapon system is autonomous or remotely-operated does not significantly influence overall teamwork quality and trust.' Therefore, 'Robot-autonomy' is listed as a factor that did not impact trust. There were no other factors manipulated in the study.",10.1177/1071181311551130,http://pro.sagepub.com/lookup/doi/10.1177/1071181311551130,"In the United States Military, 2011 marks the third year of a 25 year plan to increase the number of unmanned systems across the air, ground, and maritime domains. These systems perform as members of human-robot teams either autonomously or by remote-operation. The success of employing unmanned systems in coordination with human team members depends on system capabilities which support teamwork and trust. Weaponization of these systems introduces new concerns in teamwork and trust research. This paper presents research comparing the effects of autonomous and remotely-operated unmanned weapon systems on human-robot teamwork and trust. The results will contribute to the development of recommended roles and automation levels for future weaponized robotic systems."
"Saleh, Jamil Abou; Karray, Fakhreddine; Morckos, Michael",A qualitative evaluation criterion for human-robot interaction system in achieving collective tasks,2012,1,9,9,0,No participants were excluded,Controlled Lab Environment,within-subjects,Nine users interacted with two Peoplebot robots in a series of five to six scenarios with varying task complexity and success. User feedback was collected and used to tune the fuzzy knowledge base. First and second-order perceptions were marked at different time units and compared to the model's inferences.,"Participants instructed the robot to perform simple tasks such as moving, turning, and controlling its gripper, as well as more complex tasks such as picking up and placing objects while navigating the environment.",PeopleBot,Mobile Robots; Mobile Manipulators,Research,Manipulation,Object Passing,minimal interaction,Participants verbally instructed the robot and observed its actions.,real-world,Participants interacted with physical robots in a real-world setting.,physical,Participants interacted with physical robots.,shared control (fixed rules),The robot followed instructions from the user but had some autonomous navigation capabilities.,Behavioral Measures; Real-time Trust Measures,,Performance Metrics,Trust was assessed through user feedback and performance metrics.,no modeling,"Trust was not modeled computationally, but a fuzzy logic model was used to infer trust.",Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's task success and complexity were varied to influence trust.,"User trust was influenced by the robot's performance and task complexity, with trust decreasing when the robot performed poorly.","Users tended to build trust slower than they lost it, and the build-up process was even slower when trust was already very low. The study also found that users tend to give more weight to task completion than task complexity when evaluating productivity.","The study validated a two-level fuzzy model for estimating human trust in automation, showing that user feedback can be used to tune the model's knowledge base and improve its accuracy.","The robot moved, turned, and used its gripper to pick up and place objects. The human instructed the robot using natural language and monitored its performance.",,No statistical tests were explicitly mentioned in the paper. The study focused on qualitative analysis and tuning of a fuzzy logic model based on user feedback. The analysis involved comparing user perceptions with the model's inferences and adjusting the model's rules to improve accuracy. The study did not employ any inferential statistical tests.,TRUE,Robot-accuracy; Task-complexity,Robot-accuracy; Task-complexity,,"The study explicitly manipulated the robot's performance (success/failure) and the complexity of the tasks to observe their impact on user trust. The paper states, 'each was exposed to a set of five to six scenarios where the robot attempts to complete a set of different tasks, with varying levels of success... The tasks vary from simple to more complex.' This indicates a direct manipulation of 'Robot-accuracy' through varying levels of success in task completion and 'Task-complexity' through the use of simple and complex tasks. The paper also mentions that 'Users tended to build trust slower than they lost it, and the build-up process was even slower when trust was already very low,' which implies that the robot's accuracy (success/failure) directly impacted trust. Additionally, the paper states that 'Some rules belonging to the productivity knowledge base were tuned when feedback showed that users tend to give more weight toward task completion than task complexity and sophistication,' indicating that both task complexity and robot accuracy influenced trust, with task completion (a component of robot accuracy) having a stronger influence. Therefore, both 'Robot-accuracy' and 'Task-complexity' are chosen as factors that impacted trust. There were no factors that were explicitly manipulated that did not impact trust.",10.1109/FUZZ-IEEE.2012.6251313,http://ieeexplore.ieee.org/document/6251313/,"This work intends to identify common performance metrics for task-oriented human-robot interaction. We present a methodology to assess the system performance of a human-robot team in achievement of collective tasks. We propose a systematic approach that addresses the performance of both the human user and the robotic agent as a team. Toward this end, we attempt to determine the true time that an operator has to dedicate to a robot in action. We deﬁne the robot attention demand (RAD) as a function of both direct interaction time (DIT) and indirect interaction time (IIT), where the IIT is a direct consequence of the human trust in automation. We propose a two-level fuzzy temporal model to evaluate the human trust in automation while interacting with robots. Another fuzzy temporal model is presented to evaluate the human reliability during interaction time. The model is then generalized to accommodate multi-robot scenarios. Sequential and parallel robot cooperation schemes with varying levels of task dependency are considered. The fuzzy knowledge bases are further updated by implementing an application robotic platform where robots and users interact naturally to complete tasks with varying levels of complexity. User feedback is noted and used to tune the knowledge base rules where needed, to better represent a human expert’s knowledge."
"Saleh, Jamil Abou; Karray, Fakhreddine; Morckos, Michael",Modelling of robot attention demand in human-robot interaction using finite fuzzy state automata,2012,1,9,9,0,No participants were excluded,Controlled Lab Environment,within-subjects,Nine users interacted with two Peoplebot robots in a series of five to six scenarios with varying task complexity. User feedback was collected and used to tune the fuzzy knowledge base.,"Participants instructed the robot to perform tasks of varying complexity, including moving, turning, controlling the gripper, picking up and placing objects, and locating and following colored objects.",PeopleBot,Mobile Robots; Mobile Manipulators,Research,Manipulation,Object Passing,direct-contact interaction,Participants directly interacted with the robot by giving it instructions and observing its actions.,real-world,The study involved real-world interaction with a physical robot.,physical,The study used a physical robot for the interaction.,shared control (fixed rules),The robot operated based on pre-defined rules and responded to user instructions.,Behavioral Measures; Questionnaires,,Performance Metrics; Speech Data,Trust was assessed through user feedback and performance metrics.,"parametric models (e.g., regression)",A fuzzy temporal model was used to estimate trust based on user feedback and robot performance.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's task success and complexity were varied to influence user trust.,Trust increased with successful task completion and decreased with poor performance.,"Inexperienced users were more easily impressed by successful task completion and more frustrated by failures, requiring adjustments to the fuzzy model.","The study demonstrated that human trust in automation is influenced by robot performance and task complexity, and that a fuzzy temporal model can be used to estimate trust.","The robot moved, turned, controlled its gripper, picked up and placed objects, and located and followed colored objects. The human instructed the robot using natural language and provided feedback on its performance.",,"No specific statistical tests were explicitly mentioned in the paper. The study focused on developing and tuning a fuzzy temporal model for trust estimation based on user feedback and robot performance. The analysis involved comparing the model's output with user perceptions, and tuning the fuzzy rules to minimize the error between the model's predictions and the user's feedback. The paper mentions the use of a weighted average method for aggregating the qualified crisp output of each rule, but this is not a statistical test.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy; Task-complexity,,"The study manipulated the robot's performance by varying the success of task completion, which directly relates to 'Robot-accuracy'. The tasks also varied in complexity, which is categorized as 'Task-complexity'. The paper states, 'The tasks vary from simple to more complex. In some scenarios, the robot is instructed to perform a series of simple tasks of moving a certain distance forward or backward, turning left or right at a certain angle, and/or controlling its gripper. More complex tasks require the robot to pick an object from a certain location and place it at a goal location. Other scenarios require the robot to locate, grab, or follow a predefined coloured object in a room, etc.' The paper also states, 'The scenarios emphasize how the human trust in automation varies with time, according to the system's success fulfilling the required tasks.' This indicates that both robot accuracy and task complexity were manipulated and impacted trust. There is no indication that any other factors were manipulated or impacted trust.",10.1109/FUZZ-IEEE.2012.6250792,http://ieeexplore.ieee.org/document/6250792/,"Many systems have been implemented towards achieving effective human-machine interaction, but run the risk of being ignored if appropriate performance metrics are not in place. As a result, our goal becomes that of providing a foundation upon which we can assess how well the human and the robot perform as a team. Toward the efﬁcient modelling of such metrics, we attempt to determine the true amount of time that an operator has to dedicate to the robot. Therefore, we deﬁne the robot attention demand (RAD) as a function of both direct interaction time (DIT) and indirect interaction time (IIT), where the IIT is a direct consequence of the human trust in automation. We propose a two-level fuzzy temporal model to evaluate the human trust in automation while collaborating with robots to complete some tasks. The model combines the advantages of fuzzy logic and ﬁnite state machines to best model this phenomenon, and reduces the system complexity and the size of the knowledge base by grouping perceptions into ﬁrst- and second-order perceptions. The fuzzy knowledge base is further updated by implementing an application robotic platform where robots and users interact via natural language to complete tasks with varying levels of complexity. User feedback is noted and used to tune the knowledge base where needed."
"Salem, Maha; Lakatos, Gabriella; Amirabdollahian, Farshid; Dautenhahn, Kerstin","Would You Trust a (Faulty) Robot? Effects of Error, Task Type and Personality on Human-Robot Cooperation and Trust",2015,1,40,40,0,No participants were excluded,Real-World Environment,between-subjects,"Participants interacted with a robot in a home environment, completing tasks and unusual requests. The robot's behavior was manipulated to be either correct or faulty. Participants then completed a questionnaire and interview.","Participants interacted with a robot in a simulated home environment, completing tasks such as setting a table and responding to unusual requests.",Sunflower Robot,Mobile Robots,Social; Research,Social,Persuasion,direct-contact interaction,Participants directly interacted with the robot in a real-world setting.,real-world,The interaction took place in a realistic domestic environment.,physical,Participants interacted with a physical robot.,shared control (fixed rules),"The robot operated autonomously based on pre-programmed behaviors, with some aspects controlled using a Wizard-of-Oz technique.",Behavioral Measures; Questionnaires; Custom Scales,Godspeed Questionnaire; Ten Item Personality Inventory (TIPI); Propensity to Trust Scales,Video Data; Performance Metrics,"Trust was assessed using questionnaires, behavioral measures of compliance, and video recordings.",no modeling,No computational model of trust was used; the study used statistical analysis.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's behavior was manipulated to be either correct or faulty, influencing participants' perceptions of its reliability and trustworthiness. The task was also manipulated by including unusual requests.","The robot's faulty behavior decreased subjective ratings of trustworthiness and reliability, but did not significantly affect participants' willingness to comply with its requests. The type of task had a significant impact on compliance.","Participants' subjective ratings of the robot's trustworthiness were affected by its faulty behavior, but their willingness to comply with its requests was not. The type of task significantly influenced compliance, with participants being less likely to comply with requests that involved irrevocable actions. Extroversion and emotional stability were correlated with anthropomorphism and psychological closeness, but not with trust.","Errors in a domestic robot's behavior affect humans' perception of its reliability and trustworthiness, but may not influence their willingness to comply with its instructions, especially if the actions are not harmful or irrevocable.","The robot navigated in a home environment, presented messages on a tablet, and made requests. Participants followed the robot, completed tasks such as setting a table, and responded to unusual requests, such as throwing away letters or pouring juice on a plant.",Mann-Whitney U; Fisher's exact test; χ 2 test; Spearman correlation,"The study used non-parametric statistical tests due to non-normal distribution of data. Mann-Whitney U-tests were used to compare two independent samples, specifically to examine the effects of the experimental manipulation (correct vs. faulty robot behavior) on participants' subjective perceptions of the interaction (questionnaire scales). Fisher's exact test and chi-squared (χ 2) tests were used to analyze whether ratios differed among groups, for example, to test the effects of condition or type of request on the participants' performance during the interaction (compliance with unusual requests). Spearman correlation was used to analyze the effect of personality traits (extroversion and emotional stability) on the participants' subjective assessment of the robot.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy,,"The study manipulated 'Robot-accuracy' by having the robot perform correctly in one condition and make errors in another, such as navigating erratically and playing the wrong music. This directly impacted the robot's perceived reliability and trustworthiness, as stated in the results: 'Participants in C rated the robot as more trustworthy (U=129.5; p<0.05) and gave significantly higher scores on the ""Reliability"" scale (U=127; p< 0.05)'. The study also manipulated 'Task-complexity' by including unusual requests that varied in their nature (revocable vs. irrevocable actions, privacy breaches, information disclosure). This is described in the method section: 'Consequently, our interaction design involved tasks requiring revocable action (throwing away letters) as well as irrevocable action (pouring orange juice over the plant), in addition to requested breaches of privacy (take laptop and use password), and finally, a request to disclose personal information.' While the task type influenced compliance, the manipulation of task complexity itself was not directly measured for its impact on trust, but rather on compliance. The study found that the robot's faulty behavior affected its perceived reliability and trustworthiness, but this did not impact participants' willingness to comply with its instructions. Therefore, 'Robot-accuracy' is listed as a factor that impacted trust, and no factors are listed as not impacting trust.",,,"How do mistakes made by a robot affect its trustworthiness and acceptance in human-robot collaboration? We investigate how the perception of erroneous robot behavior may influence human interaction choices and the willingness to cooperate with the robot by following a number of itsunusual requests. For this purpose, we conducted an experiment in which participants interacted with a home companion robot in one of two experimental conditions: (1) the correct modeor (2) the faulty mode. Our findings reveal that, while significantly affecting subjective perceptions of the robot and assessments of its reliability and trustworthiness, the robot's performance does not seem to substantially influence participants' decisions to (not) comply with its requests. However, our results further suggest that the nature of the task requested by the robot, e.g. whether its effects are revocable as opposed to irrevocable, has a significant impact on participants' willingness to follow its instructions."
"Salomons, Nicole; van der Linden, Michael; Strohkorb Sebo, Sarah; Scassellati, Brian","Humans Conform to Robots: Disambiguating Trust, Truth, and Conformity",2018,1,30,30,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants played a modified card game with three robots. In the experimental condition, participants saw the robots' preliminary answers before making their final choice. In the control condition, they did not. Participants completed a questionnaire after the game.","Participants chose a card that best represented a given word, with the goal of matching the robots' choices.",MyKeepon,Expressive Robots,Research; Social,Game,Cooperative Game,minimal interaction,"Participants interacted with the robots through a game, with verbal instructions and visual feedback.",real-world,Participants interacted with physical robots in a real-world setting.,physical,The study used physical MyKeepon robots.,pre-programmed (non-adaptive),The robots' actions were pre-programmed and did not adapt to the participant's behavior.,Behavioral Measures; Questionnaires,Godspeed Questionnaire,,Trust was assessed using a post-experiment questionnaire and by measuring conformity in critical rounds.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers manipulated the visibility of the robots' preliminary answers, influencing the information available to participants and thus their conformity.","Participants in the experimental condition conformed more to the robots' answers, but this conformity decreased when the robots made incorrect choices, indicating a loss of trust.","Participants initially conformed to the robots, but stopped conforming after observing the robots make mistakes. There was a correlation between reporting pressure to change and critical round changes.","People conform to groups of robots when there is no objective correct answer, but this conformity decreases when the robots make incorrect choices, indicating that trust plays a role in conformity.","The robots chose cards to match a given word, and the human participant also chose cards, with the goal of matching the robots' choices. The participant could change their answer after seeing the robots' choices in the experimental condition.",ANOVA; Pearson correlation,A mixed model Analysis of Variance (ANOVA) test was used to determine if there was a statistically significant difference in conformity rates between the experimental and control groups. Pearson Correlation was used to assess the relationship between participants' reported pressure to change their answers and the number of times they changed their answers in critical rounds.,TRUE,Robot-accuracy; Robot-interface-design,Robot-accuracy,Robot-interface-design,"The study manipulated the visibility of the robots' preliminary answers (Robot-interface-design) by showing or not showing the number of robots that chose each card on a shared screen. This manipulation influenced the information available to participants, which in turn affected their conformity. The study also manipulated the accuracy of the robots' choices (Robot-accuracy) by having them choose incorrect answers in critical rounds. The results showed that participants initially conformed to the robots, but stopped conforming after observing the robots make mistakes, indicating that the robot's accuracy impacted trust. The interface design, while manipulated, did not directly impact trust levels, but rather served as a medium for the manipulation of information about the robots' choices.",10.1145/3171221.3171282,https://dl.acm.org/doi/10.1145/3171221.3171282,"Asch’s [2] conformity experiment has shown that people are prone to adjusting their view to match those of group members even when they believe the answer of the group to be wrong. Previous studies have attempted to replicate Asch’s experiment with a group of robots but have failed to observe conformity [7, 25]. One explanation can be made using Hodges and Geyers work [17], in which they propose that people consider distinct criteria (truth, trust, and social solidarity) when deciding whether to conform to others. In order to study how trust and truth affect conformity, we propose an experiment in which participants play a game with three robots, in which there are no objective answers. We measured how many times participants changed their preliminary answers to match the group of robots’ in their final answer. We conducted a betweensubjects study (N = 30) in which there were two conditions: one in which participants saw the group of robots’ preliminary answer before deciding their final answer, and a control condition in which they did not know the robots’ preliminary answer. Participants in the experimental condition conformed significantly more (29%) than participants in the control condition (6%). Therefore we have shown that groups of robots can cause people to conform to them. Additionally trust plays a role in conformity: initially, participants conformed to robots at a similar rate to Asch’s participants, however, many participants stop conforming later in the game when trust is lost due to the robots choosing an incorrect answer."
"Sanbonmatsu, David M.; Strayer, David L.; Yu, Zhenghui; Biondi, Francesco; Cooper, Joel M.",Cognitive underpinnings of beliefs and confidence in beliefs about fully automated vehicles,2018,1,114,114,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants completed an online survey with questions about their beliefs and confidence in their beliefs about fully automated vehicles, as well as measures of general self-confidence and trust in technology.","Participants answered questions about their beliefs, evaluations, intentions, and policy beliefs regarding fully autonomous vehicles, as well as their perceived and actual knowledge of the technology.",Unspecified,Autonomous Vehicles,Other,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the autonomous vehicles and answered questions.,media,The interaction was based on text descriptions of autonomous vehicles.,hypothetical,"The robot was only described in text, with no visual or physical representation.",not autonomous,"The robot's actions were hypothetical and described in text, without any real autonomy.",Questionnaires; Custom Scales,Propensity to Trust Scales,,Trust was measured using a questionnaire and a custom scale.,"parametric models (e.g., regression)","The study used correlational and regression analyses to examine the relationships between trust, knowledge, and confidence.",Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study did not manipulate any factors; it assessed existing beliefs and confidence levels.,,"Participants with the least knowledge of self-driving cars held the most negative views, yet they were also highly confident in their opinions. Confidence was more strongly correlated with perceived knowledge and general confidence than with actual knowledge.","Consumers' confidence in their opinions about fully automated vehicles is driven by factors largely irrelevant to their judgments, such as perceived knowledge and general self-confidence, rather than actual knowledge.",Participants completed a survey about their beliefs and confidence in their beliefs about fully automated vehicles. The human's role was to answer questions about their opinions and knowledge of the technology.,t-test; Pearson correlation; Linear regression; Mediation analysis,"The study used t-tests to compare the mean assessment of self-driving cars to the midpoint of the scale. Correlation analyses were used to examine the relationships between knowledge, perceived knowledge, trust in technology, beliefs about autonomous vehicles, and confidence. Regression analysis was used to examine the relationship between the favorableness of beliefs and confidence. Finally, a mediation analysis was conducted to test whether the relation between general self-confidence and judgmental confidence is mediated by perceived knowledge.",FALSE,,,,"The study did not manipulate any factors. It was an observational study that measured existing beliefs, knowledge, and confidence levels regarding autonomous vehicles. Therefore, no factors were intentionally manipulated by the researchers. The study assessed the relationship between these pre-existing factors and trust, but did not actively change any of them. The study did not manipulate any of the factors listed in the prompt.",10.1016/j.trf.2018.02.029,https://linkinghub.elsevier.com/retrieve/pii/S1369847817307118,"A study investigated the cognitive underpinnings of consumers’ beliefs and conﬁdence in their beliefs about fully automated vehicles. Following previous research, opinions about self-driving cars tended to be mixed. The most negative views were held by consumers who had the least knowledge of self-driving cars. Low trust in technology was also associated with more negative views. Although consumers were generally conﬁdent in their views of self-driving cars, many were uninformed about them. Consumers’ conﬁdence in their beliefs was more strongly correlated with perceived knowledge and general conﬁdence than real expertise. Thus, consumers’ conﬁdence in their opinions about fully automated vehicles appears to be driven by cognitions that are largely superﬂuous. A mediation analysis suggests that general self-conﬁdence inﬂuences judgmental conﬁdence by affecting perceived judgment relevant knowledge. Participants’ conﬁdence in negative beliefs about fully automated vehicles suggests their opinions will not be easily inﬂuenced via persuasion."
"Sanders, Tracy L.; Volante, William; Stowers, Kimberly; Kessler, Theresa; Gabracht, Katharina; Harpold, Brandon; Oppold, Paul; Hancock, Peter A.",The Influence of Robot Form on Trust,2015,1,8,8,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed demographics and pre-experimental surveys, then viewed each of the four robots, completed pre-interaction trust surveys, observed the robot perform a simple task, and completed post-interaction trust surveys. The order of robot trials was randomized.",Participants rated their trust in four different robots before and after observing each robot perform a simple action.,Lego Mindstorm EV3; Sphero 2.0; Keepon; Unspecified,Mobile Robots; Mobile Robots; Expressive Robots; Other,Research; Research; Social; Research,Evaluation,Rating,passive observation,Participants observed the robots performing simple actions.,real-world,Participants observed the physical robots in person.,physical,The study used physical robots.,wizard of oz (directly controlled),The Ozzy robot was controlled by a human operator.,Questionnaires; Custom Scales,Human-Robot Trust Scale/Questionnaire (HRTS/HRITS); Trust in Automation Scale (TAS),,Trust was measured using two questionnaires before and after a simple robot action.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The study manipulated the robot's appearance by using four different robot designs and manipulated the robot's behavior by having each robot perform a simple action.,Trust ratings for Sphero and Lego were consistently higher than Keepon and Ozzy. Trust scores generally increased after observing the robot performing a simple action.,"The two trust scales used in the study were not correlated, suggesting they may be measuring different aspects of trust. The Sphero robot showed a significant pre/post increase on both trust measures, while the other robots did not show significant pre/post differences on both scales.","Robot form influences initial trust, with Sphero and Lego receiving higher trust ratings than Keepon and Ozzy. Trust generally increased after observing the robot performing a simple action.","The robot performed a simple action, and the human participant observed the robot and rated their trust in the robot before and after the action.",t-test; Pearson correlation,The study used paired sample t-tests to compare pre- and post-interaction trust scores for each robot on two different trust scales (HRTS and TAS). Correlation analysis was used to examine the relationship between the two trust scales (HRTS and TAS) for each robot.,TRUE,Robot-aesthetics; Robot-task-strategy,Robot-aesthetics; Robot-task-strategy,,"The study manipulated the robot's appearance by using four different robot designs (Lego Mindstorm, Sphero 2.0, Keepon, and Ozzy), which falls under 'Robot-aesthetics'. The study also manipulated the robot's behavior by having each robot perform a simple action, which is classified as 'Robot-task-strategy' because the action itself was not complex or designed to influence task performance metrics, but rather to provide a basic demonstration of the robot's capabilities. The paper states that 'Sphero and Lego received consistently higher trust ratings than Keepon and Ozzy' and 'Pre-post measures reveal a difference between the initial measure of trust based on form, and the second measure of trust based on the observation of robot function.' This indicates that both the robot's form ('Robot-aesthetics') and the simple action ('Robot-task-strategy') impacted trust. The initial metric that measured participant responses only to the robot form showed no significant differences between the different robots, but the pre-post measures did show differences, indicating that the simple action did impact trust. The study did not find any factors that did not impact trust, as both the form and the simple action had an impact on trust ratings.",10.1177/1541931215591327,http://journals.sagepub.com/doi/10.1177/1541931215591327,"Assistive robotics is a rapidly progressing field of study that contains facets yet to be fully understood. Here we look at the effect of robot form on user’s level of trust placed on the robot. Form-based trust was evaluated in this study by comparing participant trust ratings based on four robot designs: Lego Mindstorm, Keepon, Sphero and Ozzy. The first view of the robot and the interactions with the robots were examined with pre and post measurements of trust. Sphero and Lego received consistently higher trust ratings than Keepon and Ozzy. Pre-post measures reveal a difference between the initial measure of trust based on form, and the second measure of trust based on the observation of robot function."
"Sanders, Tracy L.; Schafer, Kathryn E.; Volante, William; Reardon, Ashley; Hancock, Peter A.",Implicit Attitudes Toward Robots,2016,1,23,23,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants completed a demographic questionnaire, followed by an Implicit Association Test (IAT), then the Interpersonal Trust Questionnaire (ITQ) and Negative Attitude toward Robots Scale (NARS).","Participants completed an IAT to measure implicit attitudes towards humans and robots, followed by explicit trust and attitude questionnaires.",Unspecified,Other,Research,Evaluation,Survey/Questionnaire Completion,passive observation,"Participants did not interact with a physical robot, only with images and text.",media,The study used images of humans and robots in the IAT.,simulated,The robots were represented through images.,not autonomous,The robot did not perform any autonomous actions.,Questionnaires; Custom Scales,Interpersonal Trust Scale/Questionnaire; Negative Attitude towards Robots Scale (NARS),,Trust was assessed using two questionnaires.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,"The study did not manipulate any factors related to trust, but measured implicit attitudes and explicit trust.","The study found that participants had more positive implicit associations towards humans than robots, but there was no significant correlation between implicit attitudes and explicit trust measures.","The study found no significant correlation between implicit and explicit measures of trust, which is a notable trend that contrasts with some previous findings.","Participants showed more positive implicit associations towards humans than robots, but these implicit attitudes did not correlate with explicit trust measures.","Participants categorized images of humans and robots with positive and negative words in an IAT, and then completed trust and attitude questionnaires.",paired samples t-test,"A paired samples t-test was used to compare participants' implicit attitudes towards humans with their implicit attitudes towards robots, specifically comparing the mean reaction times from blocks 3 & 4 of the IAT (representing positive attitudes toward humans) with blocks 6 & 7 of the IAT (representing positive attitudes toward robots). The study also examined correlations between IAT scores and explicit measures of trust, but these correlations were not statistically significant.",FALSE,,,,"The study did not manipulate any factors. The study measured implicit attitudes towards humans and robots using an IAT and then measured explicit trust using questionnaires. There was no manipulation of any kind, so no factors are listed in factors_manipulated, factors_that_impacted_trust, or factors_that_did_not_impact_trust.",10.1177/1541931213601400,http://journals.sagepub.com/doi/10.1177/1541931213601400,"This study explores employing a measurement of implicit attitudes to better understand attitudes and trust levels towards robots. This work builds upon an existing implicit measure (Implicit Associations Test) to compare attitudes toward humans with attitudes toward robots. Results are compared with explicit self-report measures, and future directions for this work are discussed."
"Sanders, Tracy L.; MacArthur, Keith; Volante, William; Hancock, Gabriella; MacGillivray, Thomas; Shugars, William; Hancock, P. A.",Trust and Prior Experience in Human-Robot Interaction,2017,1,525,525,0,No participants were excluded,Online Crowdsourcing,mixed design,"Participants completed demographic questions and the NARS scale, then observed videos and descriptions of human and robot task performers, and completed the TAS after each trial.","Participants observed videos and descriptions of human and robot task performers completing one of four tasks (Nurse, Assembly Technician, IED Technician, and Warehouse Technician) and then rated them on trust scales.",Unspecified,Unspecified,Other: Task performance,Evaluation,Rating,passive observation,Participants passively observed videos and descriptions of task performers.,media,Participants viewed videos of task performance.,simulated,The robot was presented through images and videos.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires,Trust in Automation Scale (TAS); Negative Attitude towards Robots Scale (NARS),,Trust was measured using the Trust in Automation Scale and the Negative Attitudes Toward Robots Scale.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the task performer (human or robot) and the participants' prior experience with robots, influencing expectations and perceptions of performance.",Participants with prior robotics experience reported higher trust in robots and smaller differences in trust scores between human and robot task performers. They also reported more positive attitudes toward robots.,"The Warehouse Robot task performer did not reach the significance criterion, but approached it. The difference in trust scores between human and robot performers was not significant for experienced users in the Nurse task.",Prior experience with robots positively influences trust ratings and general attitudes toward robots.,"Participants observed videos and descriptions of either a human or a robot performing one of four tasks (Nurse, Assembly Technician, IED Technician, and Warehouse Technician). Participants then rated the task performer on trust scales.",t-test,One-tailed independent samples t-tests were used to compare trust ratings and NARS subscale scores between participants with and without prior robotics experience. The t-tests were conducted separately for each of the four task categories and for each of the three NARS subscales. The purpose was to determine if prior experience with robots influenced trust ratings and attitudes toward robots.,TRUE,Robot-accuracy; Task-complexity,Robot-accuracy,Task-complexity,"The study manipulated the task performer (human or robot), which directly impacts the perceived accuracy of the task performance. The paper states, 'Participants observed both robotic and human task performers completing the same job tasks, then rated them on trust scales.' This indicates a manipulation of the task performer's accuracy, as humans and robots are perceived to have different levels of accuracy. The study also implicitly manipulated task complexity by using four different task categories (Nurse, Assembly Technician, IED Technician, and Warehouse Technician). While the study did not explicitly manipulate the complexity of each task, the different tasks inherently vary in complexity. The results showed that the task performer (human or robot) significantly impacted trust ratings, with experienced users showing less difference in trust between human and robot performers. The study did not find that the different task categories had a significant impact on trust, as the main effect was the difference between human and robot performers, and the impact of experience on that difference. Therefore, 'Robot-accuracy' is listed as a factor that impacted trust, and 'Task-complexity' is listed as a factor that did not impact trust.",10.1177/1541931213601934,http://journals.sagepub.com/doi/10.1177/1541931213601934,"This experiment explored the influence of users’ experience (prior interaction) with robots on their attitudes and trust toward robotic agents. Specifically, we hypothesized that prior experience would lead to 1) higher trust scores after viewing a robot complete a task, 2) smaller differences in trust scores when comparing a human and a robot completing the same task, and 3) more positive general attitudes towards robots. These hypotheses were supported although not all results achieved significant levels of differentiation. These findings confirm that prior experience plays an important role in both user trust and general attitude in human-robot interactions."
"Sanders, Tracy; Kaplan, Alexandra; Koch, Ryan; Schwartz, Michael; Hancock, P. A.",The Relationship Between Trust and Use Choice in Human-Robot Interaction,2019,1,364,360,52,"48 participants were removed due to studentized residual values above 2.5 for the IED task, 4 data points were removed due to studentized residual values above 2.5 for the warehouse task",Online Crowdsourcing,within-subjects,"Participants completed pre-experimental surveys, then viewed videos of human and robot agents performing IED and warehouse tasks, rated their trust in each agent, and chose which agent should complete the task, providing reasons for their choice.",Participants rated their trust in human and robot agents performing IED and warehouse tasks and then chose which agent should complete the task.,Unspecified,Unmanned Ground Vehicles,Industrial; Research,Supervision,Monitoring,passive observation,Participants observed videos of the robot and human agents.,media,Participants viewed videos of the agents performing tasks.,physical,Participants viewed videos of physical robots.,pre-programmed (non-adaptive),The robot performed pre-set actions in the videos.,Questionnaires,Trust in Automation Scale (TAS),,Trust was measured using the Trust in Automation Scale.,"parametric models (e.g., regression)",Binomial logistic regression was used to predict use choice based on trust scores.,Empirical HRI Studies,Virtual/Simulated Studies,Indirect Manipulation,"The task was framed as either a dangerous IED task or a mundane warehouse task, influencing the perceived risk and thus the choice of agent.","Trust in the robot was higher for the IED task, and participants chose the robot more often for the IED task, while trust in the robot was lower for the warehouse task, and participants chose the human more often for the warehouse task.","Participants chose the robot more often for the dangerous IED task, prioritizing human safety, and chose the human more often for the mundane warehouse task, citing financial reasons. Trust was rarely mentioned as a reason for choosing the robot.","While trust predicted use choice, task type and perceived risk to human welfare were more influential factors in the decision to choose a robot or human agent.","The robot performed either an IED technician task or a warehouse technician task, and the human participant observed the robot and human performing the same tasks and then chose which agent should complete the task.",Logistic regression,"Binomial logistic regression was used to predict the influence of trust scores on the dichotomous dependent variable of use choice (choosing either the human or robot agent). This was done separately for the IED technician task and the warehouse technician task. The Box-Tidwell procedure was used to assess the linearity of the continuous independent variable (trust) in terms of the logit of the dependent variable. The model's significance, variance explained, classification accuracy, sensitivity, specificity, positive predictive value, and negative predictive value were reported. The odds ratio was also calculated to show the increase in the odds of choosing the robot for each additional point on the trust scale.",TRUE,Task-complexity,Task-complexity,,"The study manipulated the perceived risk associated with the task by framing it as either a dangerous IED task or a mundane warehouse task. This manipulation is best categorized as 'Task-complexity' because the IED task is inherently more complex and dangerous, requiring more careful consideration and potentially higher cognitive load due to the risk involved, compared to the warehouse task. The paper states, 'The IED technician task involves searching for explosive devices,' indicating a higher level of complexity and risk. The results show that trust in the robot was higher for the IED task, and participants chose the robot more often for the IED task, while trust in the robot was lower for the warehouse task, and participants chose the human more often for the warehouse task. This indicates that the manipulation of task complexity impacted trust levels. The paper explicitly states, 'Results indicated that while trust leads to use, use is also heavily influenced by the specific task at hand. Users more often chose a robot for a dangerous task where loss of life is likely, citing safety as their primary concern. Conversely, users chose humans for the mundane warehouse task, mainly citing financial reasons, specifically fear of job and income loss for the human worker.' This further supports the choice of 'Task-complexity' as the manipulated factor that impacted trust.",10.1177/0018720818816838,http://journals.sagepub.com/doi/10.1177/0018720818816838,"Objective: To understand the influence of trust on use choice in human-robot interaction via experimental investigation. Background: The general assumption that trusting a robot leads to using that robot has been previously identified, often by asking participants to choose between manually completing a task or using an automated aid. Our work further evaluates the relationship between trust and use choice and examines factors impacting choice. Method: An experiment was conducted wherein participants rated a robot on a trust scale, then made decisions about whether to use that robotic agent or a human agent to complete a task. Participants provided explicit reasoning for their choices. Results: While we found statistical support for the “trust leads to use” relationship, qualitative results indicate other factors are important as well. Conclusion: Results indicated that while trust leads to use, use is also heavily influenced by the specific task at hand. Users more often chose a robot for a dangerous task where loss of life is likely, citing safety as their primary concern. Conversely, users chose humans for the mundane warehouse task, mainly citing financial reasons, specifically fear of job and income loss for the human worker."
"Sarkar, Satragni; Araiza-Illan, Dejanira; Eder, Kerstin","Effects of Faults, Experience, and Personality on Trust in a Robot Co-Worker",2017,1,20,18,2,2 participants were excluded due to malfunction of the video recording,Controlled Lab Environment,between-subjects,"Participants completed a pre-study questionnaire, then assembled and disassembled a toy car with a robot, and then completed a post-study questionnaire and interview.",Participants assembled and disassembled a toy race car with the help of a robot.,Baxter,Humanoid Robots; Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically interacted with the robot to assemble and disassemble a toy car.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The robot was a physical robot that interacted with the participants.,wizard of oz (directly controlled),The robot's actions were directly controlled by a human operator using a Wizard of Oz setup.,Behavioral Measures; Questionnaires; Custom Scales,Negative Attitude towards Robots Scale (NARS); Ten Item Personality Inventory (TIPI); Godspeed Questionnaire,Video Data; Performance Metrics,"Trust was assessed using questionnaires, behavioral measures, and video recordings of the interaction.",no modeling,No computational model of trust was used; the study used statistical analysis.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's behavior was manipulated to be either faulty or non-faulty, with different types of faults introduced, and time pressure was added to the task.","The presence of faults did not have a significant effect on the participants' perception of the robot, but previous experience and personality did have a small effect.","The study found that the presence of faults did not significantly impact the participants' perception of the robot, which contradicts findings from other studies in more social settings. Extroversion was negatively correlated with the perception of human nature, which is also contrary to other studies. There was a conflict between the results of this study and the results of other studies in terms of the effect of faults and personality on trust.","The presence of faults in the robot's behavior did not significantly affect participants' perception of the robot's trustworthiness, competence, or likeability in a collaborative manufacturing task.","The robot picked up and presented parts of a toy car to the human participant, who then assembled the car following instructions displayed on the robot's screen. The human then disassembled the car.",Shapiro-Wilk; independent t-tests; Mann-Whitney U; Kruskal-Wallis; t-test; Wilcoxon signed-rank test; Pearson correlation; Spearman correlation; Fisher's exact test,"The study used a variety of statistical tests to analyze the data. The Shapiro-Wilk test was used to check for normality of data distribution. Parametric independent t-tests were used to compare means between two groups for normally distributed data, while non-parametric Mann-Whitney U tests were used for non-normal data. Kruskal-Wallis tests were used for comparing more than two groups with non-normal data. Paired t-tests and Wilcoxon signed-rank tests were used to compare pre- and post-interaction measures. Pearson's and Spearman's correlation were used to analyze the relationship between personality traits and subjective perceptions. Fisher's exact test was used to analyze the effect of condition, experience, and personality on task completion.",TRUE,Robot-accuracy; Task-constraints,,Robot-accuracy,"The study manipulated 'Robot-accuracy' by introducing cognitive faults in the robot's behavior, such as missing a component or giving wrong instructions during the assembly task. This is explicitly stated in the 'Experimental Design' section: 'Cognitive faults were introduced in the robot's behaviour, i.e. errors in its reasoning such as choosing an inadequate action.' The study also manipulated 'Task-constraints' by adding time pressure to the task, as mentioned in the 'Experimental Design' section: 'Additionally, time pressures were introduced in the overall task, to emulate high performance manufacturing industrial settings. Further pressure was added to the interaction by asking the participants to complete the task as fast as possible as part of the instructions.' The results section indicates that the manipulation of 'Robot-accuracy' did not have a significant impact on trust: 'The condition (i.e. a faulty or non-faulty robot) did not have an effect on the participants' subjective perception of Baxter'. Therefore, 'Robot-accuracy' is listed as a factor that did not impact trust. The study did not find any factors that impacted trust from the manipulated factors.",,http://arxiv.org/abs/1703.02335,"To design trustworthy robots, we need to understand the impact factors of trust: people’s attitudes, experience, and characteristics; the robot’s physical design, reliability, and performance; a task’s speciﬁcation and the circumstances under which it is to be performed, e.g. at leisure or under time pressure. As robots are used for a wide variety of tasks and applications, robot designers ought to be provided with evidence and guidance, to inform their decisions to achieve safe, trustworthy and eﬃcient human-robot interactions. In this work, the impact factors of trust in a collaborative manufacturing scenario are studied by conducting an experiment with a real robot and participants where a physical object was assembled and then disassembled. Objective and subjective measures were employed to evaluate the development of trust, under faulty and non-faulty robot conditions, and the eﬀect of previous experience with robots, and personality traits. Our ﬁndings highlight diﬀerences when compared to other, more social, scenarios with robotic assistants (such as a home care assistant), in that the condition (faulty or not) does not have a signiﬁcant impact on the human’s perception of the robot in terms of humanlikeliness, likeability, trustworthiness, and even competence. However, personality and previous experience do have an eﬀect on how the robot is perceived by participants, even though that is relatively small."
"Savery, Richard; Rose, Ryan; Weinberg, Gil",Establishing Human-Robot Trust through Music-Driven Robotic Emotion Prosody and Gesture,2019,1,24,22,2,2 participants were excluded due to a problem with the testing interface,Controlled Lab Environment,between-subjects,"Participants were randomly assigned to one of two groups: one group heard Shimi's voice, and the other heard text-to-speech. Both groups saw the same gestures. Participants first identified the emotion displayed by Shimi, then completed a trust survey.",Participants were asked to identify the emotion displayed by the robot and then complete a trust survey.,Shimi,Expressive Robots,Research; Social,Social,Emotion Recognition,passive observation,Participants observed the robot's gestures and listened to its audio.,media,Participants watched videos of the robot performing gestures and audio.,physical,"The robot was a physical robot, but the interaction was through video.",pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires,Trust Perception Scale - HRI,,Trust was measured using a questionnaire.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The study manipulated the robot's audio (musical vs. text-to-speech) and gestures to influence the perception of emotion and trust.,The study found that the robot's musical audio led to higher levels of trust compared to text-to-speech.,"The study found that audio without gesture led to the clearest display of emotion, which was unexpected. The robot's musical voice also unexpectedly received higher ratings for its perception as being conscious.",The study's main finding was that a robot using non-linguistic audio generation achieved a significantly higher mean of average trust than using a state-of-the-art text-to-speech system.,"The robot displayed emotional expressions through music-driven prosody and gestures, while the human participant identified the emotion and completed a trust survey.",t-test; t-test,The study used a two-sided t-test to compare the accuracy of emotion recognition between Shimi's voice with generated gestures and Shimi's voice with stochastic gestures. A t-test was also used to compare the mean trust scores between the group that heard Shimi's voice and the group that heard text-to-speech.,TRUE,Robot-nonverbal-communication; Robot-verbal-communication-style,Robot-verbal-communication-style,Robot-nonverbal-communication,"The study manipulated the robot's audio by using either a musical voice or text-to-speech, which is categorized as 'Robot-verbal-communication-style' because it changes the way the content (emotional expression) is communicated, not the content itself. The study also manipulated the robot's gestures, which falls under 'Robot-nonverbal-communication'. The results showed that the musical voice led to higher trust compared to text-to-speech, indicating that 'Robot-verbal-communication-style' impacted trust. While the study did manipulate gestures, the results did not show a statistically significant impact on trust, therefore 'Robot-nonverbal-communication' is listed as a factor that did not impact trust. The paper states, 'Our second hypothesis was that we will see higher levels of trust from the Shimi using non-speech' and 'The average score variation between speech and Shimi audio Fig. 6. Participants Trust Mean showed a significant result (p=0.047), proving the hypothesis.' This shows that the audio manipulation impacted trust. The paper also states, 'While the confusion matrices show a clear prediction improvement in using generated gestures over stochastic, the results are not statistically significant. A two-sided T-test provides a p-value of 0.089, which does not reject the null hypothesis at α = 0.05.' This shows that the gesture manipulation did not have a statistically significant impact on trust.",10.1109/RO-MAN46459.2019.8956386,https://ieeexplore.ieee.org/document/8956386/,"As human-robot collaboration opportunities continue to expand, trust becomes ever more important for full engagement and utilization of robots. Affective trust, built on emotional relationship and interpersonal bonds is particularly critical as it is more resilient to mistakes and increases the willingness to collaborate. In this paper we present a novel model built on music-driven emotional prosody and gestures that encourages the perception of a robotic identity, designed to avoid uncanny valley. Symbolic musical phrases were generated and tagged with emotional information by human musicians. These phrases controlled a synthesis engine playing back prerendered audio samples generated through interpolation of phonemes and electronic instruments. Gestures were also driven by the symbolic phrases, encoding the emotion from the musical phrase to low degree-of-freedom movements. Through a user study we showed that our system was able to accurately portray a range of emotions to the user. We also showed with a signiﬁcant result that our non-linguistic audio generation achieved an 8% higher mean of average trust than using a state-of-the-art text-to-speech system."
"Schaefer, Kristin E.; Sanders, Tracy L.; Yordon, Ryan E.; Billings, Deborah R.; Hancock, P.A.",Classification of Robot Form: Factors Predicting Perceived Trustworthiness,2012,1,200,161,39,39 were excluded from the analysis due to incomplete data or unsuccessful completion of the control questions,Online Crowdsourcing,within-subjects,"Participants viewed and rated 49 robot images on a 7-point Likert scale for robot classification, trustworthiness, likelihood to use, perceived level of automation, and perceived intelligence, followed by Mini-IPIP, NARS, and a demographics questionnaire.","Participants rated images of robots on several scales, including perceived trustworthiness and likelihood to use.",Unspecified,Industrial Robot Arms; Mobile Robots; Service and Assistive Robots; Social Robots; Medical Robots; Military Robots; Entertainment Robots; Therapy Robots,Industrial; Care; Social; Entertainment; Research,Evaluation,Rating,passive observation,Participants observed static images of robots.,media,The study used static 2D images of robots.,simulated,The robots were represented by static images.,not autonomous,"The robots were presented as static images, with no autonomous behavior.",Questionnaires; Custom Scales,Negative Attitude towards Robots Scale (NARS); Big Five Inventory Scale,,Trust was measured using questionnaires and custom scales.,"parametric models (e.g., regression)",Multiple regression analysis was used to predict trustworthiness.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,"The study did not directly manipulate any factors, but the robot's appearance and the participants' expectations influenced perceived trustworthiness.","Perceived intelligence and robot classification were the strongest predictors of trustworthiness, with social influence also playing a role. Higher trustworthiness was associated with a higher likelihood to use or interact with the robot.",Perceived intelligence was the strongest predictor of trustworthiness across all robot domains. Social influence was also a significant factor in predicting trust.,"A robot's physical form significantly impacts perceived trustworthiness, with perceived intelligence and robot classification being the primary predictors.","Participants viewed static images of robots and rated them on several scales, including perceived trustworthiness and likelihood to use. The robot did not perform any actions.",Linear regression; ANOVA,"The study used multiple regression analysis with stepwise entry of variables to determine the factors that predicted trustworthiness from perceived robot form, including demographic variables, personality traits, negative attitudes towards robots, perceived intelligence, perceived degree of automation, and robot classification. Separate regression analyses were conducted for the general category of robot images and for each of the seven robot domains. A one-way ANOVA was used to determine the impact of perceived trustworthiness on intended use across all robot images and for each robotic domain.",FALSE,Robot-aesthetics,Robot-aesthetics,,"The study did not explicitly manipulate any factors. However, the study design implicitly manipulated the robot's appearance (Robot-aesthetics) by presenting different robot images. The paper states, 'This study is the first in a series that aims to extend the research on human-robot trust by demonstrating the importance of the robot's physical form on trust development.' and 'Results show that the physical form does matter when predicting initial trustworthiness of a robot, primarily through the perceived intelligence and classification of the robot.' This indicates that the different robot images, which vary in their aesthetic qualities, were the primary factor influencing perceived trustworthiness. The study found that perceived intelligence and robot classification, which are influenced by the robot's appearance, were the strongest predictors of trustworthiness. Therefore, Robot-aesthetics is the factor that impacted trust. No other factors were explicitly manipulated or found to impact trust.",10.1177/1071181312561308,http://journals.sagepub.com/doi/10.1177/1071181312561308,"Many factors influence perceived usability of robots, including attributes of the human user, the environment, and the robot itself. Traditionally, the primary focus of research has been on performancebased characteristics of the robot for the purposes of classification, design, and understanding human-robot trust. In this work, we examine the human perceptions of the aesthetic dimensions of a variety of robot domains to gain insight into the impact of physical form on perceived trustworthiness that occurs prior to human-robot interaction. Results show that the physical form does matter when predicting initial trustworthiness of a robot, primarily through the perceived intelligence and classification of the robot."
"Schaefer, Kristin E.",Measuring Trust in Human Robot Interactions: Development of the “Trust Perception Scale-HRI”,2016,3,161,161,0,No participants were excluded,Online Crowdsourcing,,"Participants rated 63 images of robots on perceived machine, robot, object, intelligence, level of automation, trustworthiness, and likelihood of use.",Participants rated images of robots on various scales.,Unspecified,Industrial Robot Arms; Mobile Robots; Service and Assistive Robots; Other,Other,Evaluation,Rating,passive observation,Participants observed images of robots.,media,The study used static images of robots.,physical,The study used images of physical robots.,not autonomous,The robots were not acting autonomously in the study.,Questionnaires,,,Trust was measured using a questionnaire.,"parametric models (e.g., regression)",Multiple regression analysis was used to predict trustworthiness.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,The study assessed the relationship between robot form and perceived trustworthiness without direct manipulation of robot behavior or task.,"Perceived intelligence, robot classification, and negative social influence predicted trustworthiness.",Preconceived ideas about robot intelligence are form-dependent and assessed prior to interaction.,Physical form is important to the trust that develops prior to HRI.,Participants viewed and rated images of various robots.,multiple regression correlation analysis,"A multiple regression analysis with stepwise entry of variables was conducted to determine the factors that predicted trustworthiness from perceived robot form. The analysis regressed trustworthiness onto human-related factors, personality traits, negative attitudes toward robots, and self-report items of robot form.",FALSE,,,,"This study did not manipulate any factors. Participants rated images of robots, and the study focused on identifying correlations between perceived robot attributes and trustworthiness. There was no intentional manipulation of any factor to observe its impact on trust.",,http://link.springer.com/10.1007/978-1-4899-7668-0_10,
"Schaefer, Kristin E.",Measuring Trust in Human Robot Interactions: Development of the “Trust Perception Scale-HRI”,2016,3,200,200,0,No participants were excluded,Online Crowdsourcing,,Participants rated a subset of robot images from the previous study using the Godspeed questionnaire.,Participants rated robot images using the Godspeed questionnaire.,Unspecified,Industrial Robot Arms; Mobile Robots; Service and Assistive Robots; Other,Other,Evaluation,Rating,passive observation,Participants observed images of robots.,media,The study used static images of robots.,physical,The study used images of physical robots.,not autonomous,The robots were not acting autonomously in the study.,Questionnaires,Godspeed Questionnaire,,Trust was measured using the Godspeed questionnaire.,no modeling,The study did not use a computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,The study assessed the relationship between robot attributes and perceived trustworthiness without direct manipulation of robot behavior or task.,There was a significant relationship between robot classification and perceived trustworthiness.,"The higher the rating of a robot to actually be classified as a robot, the more likely it was to be rated as trustworthy.",Specific robot attributes impact trustworthiness ratings.,Participants viewed and rated images of various robots.,Pearson correlation,A correlation analysis was used to determine the relationship between how individuals rated the robot image on the robot classification scale and their perceived level of trustworthiness in the robot.,FALSE,,,,"This study did not manipulate any factors. Participants rated images of robots using the Godspeed questionnaire. The study aimed to identify which perceived robot attributes could impact trustworthiness ratings, but no factors were intentionally manipulated to observe their impact on trust.",,http://link.springer.com/10.1007/978-1-4899-7668-0_10,
"Schaefer, Kristin E.",Measuring Trust in Human Robot Interactions: Development of the “Trust Perception Scale-HRI”,2016,3,81,81,0,No participants were excluded,Educational Setting,within-subjects,Participants completed the 42-item Trust Scale before and after watching videos of a robot performing a task with varying reliability.,Participants monitored a video of a robot performing a task.,Talon,Unmanned Ground Vehicles,Research,Supervision,Monitoring,passive observation,Participants passively observed a video of a robot.,media,The study used videos of a simulated robot interaction.,simulated,The robot was represented in a video simulation.,pre-programmed (non-adaptive),The robot followed a pre-programmed path and actions.,Questionnaires; Custom Scales,Trust Perception Scale - HRI; Negative Attitude towards Robots Scale (NARS); Interpersonal Trust Scale/Questionnaire,,Trust was measured using a custom scale and questionnaires.,no modeling,The study did not use a computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The reliability of the robot's feedback was manipulated to influence trust.,Trust increased after experiencing a 100% reliable interaction and decreased after experiencing a 25% reliable interaction.,The 40-item trust scale provided a finer level of granularity and thus a more accurate trust rating than the 14-item scale.,Trust changes over time with respect to changes in robot reliability.,"Participants monitored a video of a robot navigating and detecting targets, with the robot providing feedback on target detection.",ANOVA; Tukey HSD,"A one-way within-subjects repeated measures analysis of variance was conducted for each of the 42 items of the Trust Scale to determine if each item changed over time. Post-hoc analysis was used to identify significant differences between time points. Additionally, a repeated measures ANOVA was used to assess the impact of time on trust development using the total trust score, and a 2 Scale x 3 Time repeated measures ANOVA was used to compare the 40-item and 14-item scales.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the reliability of the robot's feedback on target detection, which directly impacts the robot's accuracy in the task. The robot provided 100% reliable feedback in one condition and 25% reliable feedback in another. This manipulation of robot accuracy was found to significantly impact trust levels, with trust increasing after the 100% reliable interaction and decreasing after the 25% reliable interaction. The manipulation of the robot's feedback directly influenced the robot's performance on the task, thus it is classified as 'Robot-accuracy'.",,http://link.springer.com/10.1007/978-1-4899-7668-0_10,
"Schaefer, Kristin E.",Measuring Trust in Human Robot Interactions: Development of the “Trust Perception Scale-HRI”,2016,3,21,21,0,No participants were excluded,Controlled Lab Environment,within-subjects,Participants completed two simulated tasks with varying robot navigation errors and completed trust scales before and after each task.,Participants assisted a robot in navigating to a rendezvous point in a simulated environment.,Talon,Unmanned Ground Vehicles,Research,Navigation,Path Following,minimal interaction,Participants interacted with a robot in a simulated environment.,simulation,The study used a computer-based simulation of a robot interaction.,simulated,The robot was represented in a computer simulation.,shared control (fixed rules),The robot navigated autonomously but required human assistance when stuck.,Questionnaires; Custom Scales,Trust Perception Scale - HRI; Checklist for Trust between People and Automation; Interpersonal Trust Scale/Questionnaire; Big Five Inventory Scale,,Trust was measured using custom scales and questionnaires.,no modeling,The study did not use a computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's navigation performance was manipulated by varying the number of obstacles it could autonomously navigate around.,"The 40-item and 14-item scales showed a significant change in trust from pre-interaction to post-interaction, while the Checklist for Trust did not.",The 40-item trust scale showed more sensitivity to changes in trust than the Checklist for Trust between People and Automation.,The developed trust scale measures the construct of trust and is more sensitive to changes in trust than previously used scales.,"Participants controlled a soldier avatar to assist a robot in navigating to a rendezvous point, moving obstacles out of the way when needed.",Pearson correlation; ANOVA; paired samples t-tests,"Pearson correlations were used to identify the relationships between the three trust scales. A within-subjects repeated measures analysis of variance was conducted to determine if there was a significant mean difference between the post-interaction change scores for the three scales. Paired samples t-tests were conducted to assess the change in trust pre-post interaction for each scale. Additionally, a 3 Trust Scales x 3 Conditions repeated measures ANOVA was conducted to explore scale differences across conditions.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the robot's navigation performance by varying the number of obstacles it could autonomously navigate around. This directly impacts the robot's accuracy in completing the navigation task. In one condition, the robot navigated around four out of five obstacles, and in the other, it navigated around only one. This manipulation of the robot's ability to perform the task was found to impact trust levels. The manipulation of the robot's navigation performance directly influenced the robot's performance on the task, thus it is classified as 'Robot-accuracy'.",,http://link.springer.com/10.1007/978-1-4899-7668-0_10,
"Schaefer, Kristin E.; Brewer, Ralph W.; Putney, Joe; Mottern, Edward; Barghout, Jeffrey; Straub, Edward R.",Relinquishing Manual Control: Collaboration Requires the Capability to Understand Robot Intent,2016,1,20,20,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed pre-interaction questionnaires, a working memory task, and a practice session. They then completed two 20-minute trials as passengers in a simulated driverless vehicle, with different control interfaces. Post-interaction questionnaires were completed after each trial.",Participants were instructed to act as passengers in a simulated driverless vehicle and were able to intervene in the autonomous control of the vehicle if they felt it was necessary.,Unspecified,Autonomous Vehicles,Research,Navigation,Path Following,minimal interaction,Participants interacted with a simulated vehicle and could intervene in its autonomous control.,simulation,The interaction took place in a computer-based simulation of a driverless vehicle.,simulated,The robot was represented as a simulated vehicle in a virtual environment.,shared control (fixed rules),The robot operated autonomously but participants could intervene using fixed control interfaces.,Behavioral Measures; Questionnaires,,Performance Metrics,Trust was assessed using behavioral measures of intervention frequency and post-interaction questionnaires.,no modeling,Trust was not modeled computationally; descriptive statistics were used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the type of control interface available to the user, either traditional controls or a button system, to influence reliance on the autonomous system.","Participants intervened more frequently and for longer durations when traditional controls were available, suggesting that the type of interface mediates trust.","Participants intervened more during the first leg of the trip with the traditional controls, and the highest amount of interventions for the button system occurred in the last leg of the trip. This suggests that the type of interface and the timing of the interaction influence user behavior.","The availability of traditional controls (steering wheel and pedals) led to more frequent and longer interventions by users, even when the autonomous system was functioning correctly, suggesting that interface design significantly impacts reliance on autonomous systems.","The robot, a simulated autonomous vehicle, navigated a predefined route, picking up and dropping off virtual passengers. The human participant acted as a passenger and could intervene in the vehicle's autonomous control using either traditional controls or a button system.",ANOVA,"A one-way ANOVA was used to determine if there was a statistically significant difference between the two vehicle conditions (traditional controls vs. button controls) for the number of interventions during the first leg of the trial. Confidence interval analysis was also used to assess the difference in time between those who intervened in the autonomous capabilities of the vehicle and those who did not, for each vehicle across all three legs of the run.",TRUE,Robot-interface-design,Robot-interface-design,,"The study manipulated the type of control interface available to the user. Participants interacted with two simulated driverless vehicles. Vehicle 1 had a traditional wheel and pedal control interface, as well as two buttons to pause and resume the autonomous capabilities of the system. Vehicle 2 only had the button system available. This manipulation of the interface directly influenced how often and for how long participants intervened with the autonomous system, indicating that the interface design impacted trust. The paper states, 'Results showed that when the option to take control is available and familiar (i.e. steering wheel and pedals), people will intervene more often and for longer periods than when only offered the option to stop. This suggests that the amount of trust a human places in the robot is mediated by the type of interface available to the human.' This clearly indicates that the interface design was the manipulated factor and that it impacted trust. There were no other factors manipulated in the study.",10.1109/CTS.2016.0071,http://ieeexplore.ieee.org/document/7871009/,"Collaboration between robots and humans means different things to different people in different applications. Collaboration could range from a robot and a human simply operating in the same area, to operations that require complex, interdependent decisions based on joint goals. Despite the level of coordination, all effective collaborations require understanding the control allocation processes, and human engagement or reengagement strategies. This is especially true as humans begin to relinquish manual control, and the robot becomes a team member rather than just a tool. The importance behind this paper is understanding the implications of relinquishing direct control and allowing it to make decisions that could affect the safety of users or bystanders. Also important is understanding when and how to facilitate appropriate engagement strategies. To build appropriate reliance and to calibrate trust, the robot should have a means to convey its reasoning processes or intent. Our research begins to show how user displays can facilitate the development of a shared situation awareness (SA) of the mission space. Shared SA can enhance the teaming effort which engenders and calibrates trust in the robotic system."
"Scharowski, Nicolas; Perrig, Sebastian A. C.; Aeschbach, Lena Fanya; von Felten, Nick; Opwis, Klaus; Wintersberger, Philipp; Brühlmann, Florian",To Trust or Distrust Trust Measures: Validating Questionnaires for Trust in AI,2024,1,1500,1485,15,15 participants were excluded due to unspecified reasons,Online Crowdsourcing,mixed design,"Participants watched two videos, one showing an interaction with an autonomous vehicle (AV) and the other with a chatbot, each displaying either trustworthy or untrustworthy behavior. After each video, participants completed the TPA and TAI questionnaires, along with other related survey scales.",Participants watched videos of AI interactions and then completed questionnaires.,Unspecified,Autonomous Vehicles; Other,Other; Other,Evaluation,Survey/Questionnaire Completion,passive observation,Participants passively observed videos of AI interactions.,media,Participants watched pre-recorded videos of AI interactions.,simulated,The robots were presented through videos.,pre-programmed (non-adaptive),The AI systems in the videos followed pre-set behaviors.,Questionnaires,Jian et al. Trust Scale; N/A,,Trust was measured using the TPA and TAI questionnaires.,no modeling,No computational model of trust was developed.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers manipulated the AI's performance and behavior by showing either a high-performing trustworthy AI or a low-performing untrustworthy AI in the videos.,"Trust scores were significantly higher in the trustworthy condition, and distrust scores were significantly higher in the untrustworthy condition.","The study found that the TPA scale is better used as a two-factor model, separating trust and distrust, rather than a single-factor model. The TAI was found to be a reliable single-factor measure of trust.",The study validated the TAI as a reliable single-dimensional measure of trust in AI and found that the TPA is better used as a two-factor model measuring both trust and distrust.,Participants watched videos of either an autonomous vehicle driving or a chatbot answering questions. The human's role was to observe the AI's behavior and then complete questionnaires.,ANOVA; Wilcoxon rank sum; Multilevel Model; Factor analysis,"The study used ANOVA to examine the effects of application area (AV vs. chatbot) and condition (trustworthy vs. untrustworthy) on risk ratings, TPA trust scores, TPA distrust scores, and TAI scores. Wilcoxon rank sum tests were also used to confirm the ANOVA results. Confirmatory factor analysis (CFA) was used to assess the fit of the proposed models for the TPA and TAI scales. Exploratory factor analysis (EFA) was used to explore alternative models for the TPA due to sub-optimal model fit from the CFA.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the AI's performance by showing either a high-performing trustworthy AI or a low-performing untrustworthy AI in the videos. This directly impacts the accuracy of the AI's actions, making 'Robot-accuracy' the most appropriate category. The paper states, 'In one condition, participants were presented with a high-performing trustworthy AI, without failures, eliciting trust. Conversely, in the other condition, participants were exposed to low-performing untrustworthy AI, with failures, intended to evoke distrust.' This manipulation of performance directly influenced trust levels, as evidenced by the significant differences in trust scores between the trustworthy and untrustworthy conditions. The paper explicitly states, 'Trust scores were significantly higher in the trustworthy condition, and distrust scores were significantly higher in the untrustworthy condition.' Therefore, 'Robot-accuracy' is the factor that impacted trust. There were no other factors manipulated, so there are no factors that did not impact trust.",,http://arxiv.org/abs/2403.00582,"Despite the importance of trust in human-AI interactions, researchers must adopt questionnaires from other disciplines that lack validation in the AI context. Motivated by the need for reliable and valid measures, we investigated the psychometric quality of two trust questionnaires, the Trust between People and Automation scale (TPA) by Jian et al. (2000) and the Trust Scale for the AI Context (TAI) by Hoffman et al. (2023). In a pre-registered online experiment (N = 1485), participants observed interactions with trustworthy and untrustworthy AI (autonomous vehicle and chatbot). Results support the psychometric quality of the TAI while revealing opportunities to improve the TPA, which we outline in our recommendations for using the two questionnaires. Furthermore, our findings provide additional empirical evidence of trust and distrust as two distinct constructs that may coexist independently. Building on our findings, we highlight the opportunities and added value of measuring both trust and distrust in human-AI research and advocate for further work on both constructs."
"Schellen, Elef; Bossi, Francesco; Wykowska, Agnieszka",Robot Gaze Behavior Affects Honesty in Human-Robot Interaction,2021,1,33,30,3,"One based on their many noresponse trials (80%), two for a lack of deceitful trials (0 and 0.02%, respectively)",Controlled Lab Environment,within-subjects,"Participants were told they would interact with a robot via videoconference and complete a memory task. They provided feedback to the robot, choosing to be truthful or deceitful. The robot would then look at the participant or look away. Participants completed a questionnaire after the task.","Participants provided feedback to a robot on its performance in a memory task, choosing to be truthful or deceitful.",iCub,Humanoid Robots,Research,Game,Economic Game,minimal interaction,"Participants interacted with a robot on a screen, providing feedback.",media,Participants viewed a video of the robot on a screen.,simulated,The robot was presented as a video on a screen.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the participant's behavior.,Behavioral Measures; Physiological Measures; Questionnaires,,Physiological Signals,"Trust was assessed using behavioral measures, physiological data, and a post-task questionnaire.",no modeling,No computational model of trust was used in this study.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's gaze behavior was manipulated to look at or away from the participant after their response. The task was framed as a decision-making experiment with potential rewards and punishments. Feedback was given on a block-by-block basis.,"Participants were more honest after the robot established eye contact with them, but only if this was in response to deceptive behavior. The robot's gaze did not influence behavior if the participant was already being honest.","Participants were more likely to lie when the robot made a mistake, suggesting a prosocial lying effect. The physiological data did not show significant differences between truthful and deceitful trials.","Direct gaze from a robot increases honesty, but only after a participant has been dishonest.","The robot attempted to reproduce a sequence of colored circles. The human participant provided feedback on the robot's performance, choosing to be truthful or deceitful. The robot would then look at the participant or look away.",t-test; Wilcoxon rank sum; multiway frequency analysis; ANOVA; matched paired t-tests,"The study used several statistical tests. A t-test and a Wilcoxon signed rank test were used to compare the frequency of incorrect feedback when the robot made a mistake. A multiway frequency analysis was used to assess the influence of robot gaze on honesty, including the effect of previous participant response and robot looking behavior. Specifically, model comparisons were used to test the effect of robot looking behavior on subsequent participant lying behavior, the effect of participant response during the previous trial on participant response during the current trials, and an interaction effect between robot looking behavior and participant response during the previous trial. A Wilcoxon signed rank test was used to test the hypothesis that the effect of being looked at by a robot is contingent on the participant's previous behavior. A repeated-measures ANOVA was used to analyze differences in heart rate between deceitful and truthful trials. Finally, matched paired t-tests were used to analyze differences in skin conductance between truthful and deceitful trials.",TRUE,Robot-nonverbal-communication; Robot-morality,Robot-nonverbal-communication,,"The study manipulated the robot's gaze behavior (looking at or away from the participant) after the participant provided feedback, which falls under 'Robot-nonverbal-communication'. The study also implicitly manipulated the participant's ability to be honest or deceitful, which falls under 'Robot-morality' as it involves the participant's choice to lie to the robot, which is a moral violation outside of the game's rules. The robot's gaze behavior was found to impact trust, as participants were more honest after the robot made eye contact, but only after the participant had been dishonest. The study did not find any factors that did not impact trust.",10.3389/frai.2021.663190,https://www.frontiersin.org/articles/10.3389/frai.2021.663190/full,"As the use of humanoid robots proliferates, an increasing amount of people may find themselves face-to-“face” with a robot in everyday life. Although there is a plethora of information available on facial social cues and how we interpret them in the field of human-human social interaction, we cannot assume that these findings flawlessly transfer to human-robot interaction. Therefore, more research on facial cues in human-robot interaction is required. This study investigated deception in human-robot interaction context, focusing on the effect that eye contact with a robot has on honesty toward this robot. In an iterative task, participants could assist a humanoid robot by providing it with correct information, or potentially secure a reward for themselves by providing it with incorrect information. Results show that participants are increasingly honest after the robot establishes eye contact with them, but only if this is in response to deceptive behavior. Behavior is not influenced by the establishment of eye contact if the participant is actively engaging in honest behavior. These findings support the notion that humanoid robots can be perceived as, and treated like, social agents, since the herein described effect mirrors one present in human-human social interaction."
"Scherf, Lisa; Turan, Cigdem; Koert, Dorothea",Learning from Unreliable Human Action Advice in Interactive Reinforcement Learning,2022,1,28,28,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were asked to help a robot sort objects into boxes based on weight. The experiment was divided into three parts (Ex.1A, Ex.1B, Ex.2) with varying levels of feedback and information about the sorting criterion. In Ex.1A, participants received no feedback; in Ex.1B, they received feedback after each episode; and in Ex.2, they were briefed on the sorting criterion but had to sort both known and unknown objects.",Participants provided action advice to a robot to sort objects into boxes based on weight.,Unspecified,Mobile Manipulators,Research,Manipulation,Sorting/Arranging,minimal interaction,Participants provided action advice to the robot via a tablet interface.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,shared control (adaptive),The robot used human advice but could also reject it based on learned trust parameters.,Behavioral Measures; Performance-Based Measures,,Video Data; Performance Metrics,"Trust was assessed using behavioral cues (response time, facial dynamics) and performance metrics.",no modeling,"Trust was not modeled computationally, but used to influence the robot's behavior.",Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers manipulated the feedback given to participants and the information about the sorting criterion, influencing their expectations and the difficulty of the task.","The study found that response times and facial dynamics varied based on the certainty of the advice, suggesting that these behavioral cues could be used to detect unreliable advice. The study did not directly measure trust, but used the trust indicators to influence the robot's behavior.","Response times were significantly higher for sorting actions on unknown objects, indicating uncertainty. Facial dynamics showed a trend of higher activity for sorting advice, but the results were not as clear as response times. The study also found that the correctness of advice was state-dependent.",The study demonstrated that response times can be used as a behavioral cue to detect uncertain and potentially unreliable human action advice in human-robot interaction.,"The robot sorted objects into boxes based on weight. The human provided action advice to the robot via a tablet interface, guiding the robot's actions.",Kruskal-Wallis; posthoc conover's; Wilcoxon signed-rank test,"The study used a Kruskal Wallis test followed by posthoc Conover's tests to compare the performance of different learning algorithms in the gridworld scenario. Specifically, it compared LUNAA with different indicator combinations against a state-independent linear increase of self-confidence (T-SC) and a policy without human advice. In the robotic sorting tasks, Wilcoxon signed-rank tests were used to compare response times and facial dynamics between different conditions (e.g., sorting vs. non-sorting actions, known vs. unknown objects) to analyze the relationship between behavioral cues and uncertainty in human advice.",TRUE,Robot-verbal-communication-content; Task-complexity; Robot-adaptability,Robot-verbal-communication-content; Task-complexity,Robot-adaptability,"The study manipulated the information provided to participants about the sorting criterion (Robot-verbal-communication-content). In Ex.1A, participants received no information about the sorting criterion, leading to a false prior based on object color. In Ex.1B, they received feedback after each episode, which could lead them to question their initial assumptions. In Ex.2, they were briefed on the sorting criterion (weight) but had to sort both known and unknown objects, increasing the Task-complexity. The robot's ability to adapt to the human's advice was also a factor, as the robot could reject advice based on learned trust parameters (Robot-adaptability), but this was not directly manipulated as a factor to measure its impact on trust. The study found that the correctness of advice was state-dependent, and response times and facial dynamics varied based on the certainty of the advice, indicating that the manipulation of information about the sorting criterion and the introduction of unknown objects (Task-complexity) impacted trust. The robot's adaptability was a core part of the algorithm, but not a manipulated factor to measure its impact on trust.",10.1109/Humanoids53995.2022.10000078,https://ieeexplore.ieee.org/document/10000078/,"Interactive Reinforcement Learning (IRL) uses human input to improve learning speed and enable learning in more complex environments. Human action advice is here one of the input channels preferred by human users. However, many existing IRL approaches do not explicitly consider the possibility of inaccurate human action advice. Moreover, most approaches that account for inaccurate advice compute trust in human action advice independent of a state. This can lead to problems in practical cases, where human input might be inaccurate only in some states while it is still useful in others. To this end, we propose a novel algorithm that can handle statedependent unreliable human action advice in IRL. Here, we combine three potential indicator signals for unreliable advice, i.e. consistency of advice, retrospective optimality of advice, and behavioral cues that hint at human uncertainty. We evaluate our method in a simulated gridworld and in robotic sorting tasks with 28 subjects. We show that our method outperforms a stateindependent baseline and analyze occurrences of behavioral cues related to unreliable advice."
"Schnieders, Thomas M; Wang, Zhonglun; Stone, Richard T; Backous, Gary; Danford-Klein, Erik","The effect of human-robot interaction on trust, situational awareness, and performance in drone clearing operations",2019,1,14,14,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed both control and experimental conditions in a randomized order. In the control condition, participants performed room clearing alone. In the experimental condition, a drone operator provided information to the participant who then performed the room clearing. Participants completed post-study surveys, SART-10D, NASA-TLX, and a trust in human-robotic interaction survey.","Participants performed a room clearing operation in a simulated building, either alone or with the assistance of a drone.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,Participants received verbal information from the drone operator and then performed the room clearing task.,real-world,The study was conducted in a real-world setting with a physical drone.,physical,A physical drone was used in the study.,wizard of oz (directly controlled),The drone was directly controlled by a human operator.,Questionnaires,Human-Robot Trust Scale/Questionnaire (HRTS/HRITS),,Trust was measured using a post-study questionnaire.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The presence of the drone and the information provided by the drone operator were manipulated to influence the task performance and difficulty.,"The study found a moderate level of trust in the drone, with participants believing it was not deceptive or underhanded, and that it offered security.","The study found that using a drone increased the time taken to complete the task, but also increased the accuracy and safety of the clearing operation. There was a significant increase in information quality and a decrease in temporal demand when using the drone.","The use of a drone during room clearing operations increased the information quality and efficiency of the task, while also showing a moderate level of trust in the drone.","The drone operator provided information about the rooms to the police officer, who then entered and cleared the rooms. The police officer's task was to clear the rooms based on the information provided by the drone.",paired-samples t-test; paired-samples t-test; paired-samples t-test,"The study used paired-samples t-tests to compare the means of different conditions. Specifically, t-tests were used to compare the time taken to clear a floor with and without drone assistance, to compare the results of each metric of the SART-10D rating sheet, and to compare the results of each metric of the NASA-TLX survey. The purpose was to determine if there were statistically significant differences between the control and experimental conditions for these measures.",TRUE,Robot-verbal-communication-content; Robot-autonomy; Task-complexity; Task-constraints,Robot-verbal-communication-content,Robot-autonomy; Task-complexity; Task-constraints,"The study manipulated several factors. 'Robot-verbal-communication-content' was manipulated because the drone operator provided information about the rooms to the police officer, which directly influenced the content of the communication. 'Robot-autonomy' was manipulated because the drone was directly controlled by a human operator (wizard of oz), which is a low level of autonomy. 'Task-complexity' was manipulated because the presence of the drone and the information provided by the drone operator changed the cognitive demands of the task. 'Task-constraints' was manipulated because the presence of the drone increased the time taken to complete the task, thus changing the temporal constraints. The study found that the content of the communication from the drone operator impacted trust, as evidenced by the trust questionnaire results showing that participants believed the drone was not deceptive or underhanded and offered security. The study did not find that the level of autonomy, task complexity, or task constraints directly impacted trust, as the trust questionnaire results were not directly correlated with these factors. The study found a moderate level of trust in the drone, but this was not directly linked to the level of autonomy, task complexity, or task constraints.",,,"With advances in microcomputers, microprocessors, and battery form factor, small drones are seeing a growing trend of deployment. Building clearing operations, especially in active shooter scenarios, can be high risk when officers need to clear a building on their own. This study analysed the use of a small drone in a building clearing operation with a County Sheriff’s Department to help mitigate the danger of single officer clearing operations."
"Schniter, E.; Shields, T.W.; Sznycer, D.",Trust in humans and robots: Economically similar but emotionally different,2020,1,387,229,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were randomly assigned to the roles of investor or trustee. Investors were given $10 and could transfer any portion to the trustee, which was tripled upon receipt. The trustee then chose to return a portion of the amount received. In the robot conditions, the robot's behavior was based on previously observed human behavior.","Participants played a one-shot trust game where they decided how much of their endowment to entrust to a trustee (human or robot), and the trustee decided how much to reciprocate.",Unspecified,Other,Research,Game,Economic Game,minimal interaction,Participants interacted with a computer interface to make decisions in a trust game.,simulation,"The interaction was conducted through a computer interface, simulating a trust game.",simulated,"The robot was a virtual agent, not a physical robot.",pre-programmed (non-adaptive),"The robot's behavior was pre-programmed to mimic previously observed human behavior, without adapting to the current interaction.",Behavioral Measures; Questionnaires,,,Trust was assessed through investment decisions in the trust game and self-reported emotions using a questionnaire.,"parametric models (e.g., regression)","Regression analysis was used to examine the relationship between investment, earnings, and reported emotions.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The type of trustee (human, robot, or robot affecting a human) was manipulated to examine its effect on investment and emotions.","Initial trust, measured by investment amounts, did not differ across conditions, but social emotions varied depending on whether the trustee was a human or a robot.","The study found that while initial investment behavior was similar across human and robot trustees, social emotions were significantly different, suggesting that people react differently to robots than to humans on an emotional level. There was no difference in investment across conditions, which was not predicted by some theories.","People invest similarly in humans and robots in a trust game, but experience different social emotions depending on whether the trustee is a human or a robot.",The human investor decided how much of their endowment to send to the trustee. The trustee (human or robot) then decided how much to return to the investor. The robot's behavior was based on previously observed human behavior.,Kruskal-Wallis; Fisher's exact test; Levene's test; Linear regression,"The study used Kruskal-Wallis tests to compare earnings and reciprocity across conditions, and Fisher's exact test to confirm the robustness of these findings. Levene's test was used to compare variances in investment across conditions. Regression analysis was employed to examine the relationship between emotions (contentment, frustration, pride, guilt, gratitude, anger) and variables such as earnings, investment, and reciprocation, while also considering the different experimental conditions (Human, Robot1, Robot2). The regression models included interaction terms to assess how these relationships varied across conditions.",TRUE,Teaming,Teaming,,"The study manipulated the type of trustee the participant interacted with. Participants were either paired with a human trustee (Human condition), a robot trustee (Robot1 condition), or a robot trustee whose payoffs went to another human (Robot2 condition). This manipulation directly impacts the social context of the interaction, which is best captured by the 'Teaming' category. The study found that while initial investment behavior was similar across conditions, social emotions varied depending on whether the trustee was a human or a robot, indicating that the 'Teaming' factor impacted trust-related emotions. The study explicitly states that 'Trust-based interactions with fellow humans and with robots affecting fellow humans elicited more intense social emotions compared to trust-based interactions with robots acting alone.' This shows that the type of trustee (human or robot) influenced the emotional response, which is a key aspect of trust. The study found no difference in initial investment across conditions, indicating that the manipulation did not impact initial trust behavior, but it did impact the emotional response to the interaction. Therefore, the 'Teaming' factor is the most appropriate category to describe the manipulation and its impact on trust-related emotions.",10.1016/j.joep.2020.102253,https://linkinghub.elsevier.com/retrieve/pii/S0167487020300106,"Trust-based interactions with robots are increasingly common in the marketplace, workplace, on the road, and in the home. However, a valid concern is that people may not trust robots as they do humans. While trust in fellow humans has been studied extensively, little is known about how people extend trust to robots. Here we compare trust-based investments and self-reported emotions from across three nearly identical economic games: human-human trust games, humanrobot trust games, and human-robot trust games where the robot decision impacts another human. Robots in our experiment mimic humans: they are programmed to make reciprocity decisions based on previously observed behaviors by humans in analogous situations. We find that people invest similarly in humans and robots. By contrast the social emotions (i.e., gratitude, anger, pride, guilt) elicited by the interactions (but not the non-social emotions) differed across human and robot trust games. Emotional reactions depended on the trust game interaction, and how another person was affected."
"Schule, Mareike; Kraus, Johannes Maria; Babel, Franziska; Reisner, Nadine","Patients' Trust in Hospital Transport Robots: Evaluation of the Role of User Dispositions, Anxiety, and Robot Characteristics",2022,1,96,92,4,"1 participant was excluded because of a completion time below 40% of the median duration, 1 participant was excluded for answering an attention check item with low agreement, 2 participants were excluded for repeatedly not showing variance in their responses",Online Crowdsourcing,mixed design,Participants completed an online questionnaire with multiple measurement points. They were presented with a robot description and two videos. The second video had three versions manipulating predictability and controllability.,Participants were asked to evaluate a transport robot based on written and video-based material.,Unspecified,Mobile Robots; Service and Assistive Robots,Care; Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants passively observed the robot through videos and images.,media,Participants watched videos of the robot interacting in a hospital setting.,simulated,The robot was presented as a simulated entity in videos and images.,pre-programmed (non-adaptive),The robot's behavior was pre-programmed and did not adapt to the user.,Questionnaires; Custom Scales,Negative Attitude towards Robots Scale (NARS); Affinity for Technological Interaction (ATI) Scale; Propensity to Trust Scales,,Trust was measured using questionnaires and custom scales.,"parametric models (e.g., regression)",The study used correlation and mediation analysis to model trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the predictability and controllability of the robot's behavior in videos, and the task was framed as a transport scenario.","The manipulation of predictability was partially successful, but the manipulation of controllability was not. No significant effects of the manipulations on trust were found, but perceived predictability and control were correlated with trust.","The manipulation of robot controllability was not successful. The study found that state anxiety was negatively associated with trust and intention to use, but only after participants were introduced to the robot. The study also found that trust mediated the effect of state anxiety on intention to use.","The study found that technology-related user dispositions and state anxiety are associated with trust in a transport robot, and that trust mediates the effect of state anxiety on intention to use.",The robot was described as a transport robot for patients in a hospital. The human participant watched videos of the robot and answered questionnaires about their trust and intention to use the robot.,Pearson correlation; Bootstrapping; Path analysis; ANOVA; Kruskal-Wallis; dunn's tests,"The study used Pearson correlations to examine associations between variables, with bootstrapped confidence intervals to address non-normal distributions. A path analysis was conducted to test the mediation effect of trust on the relationship between state anxiety and intention to use. One-way ANOVAs and Kruskal-Wallis tests were used to investigate group differences in the experimental conditions, with Dunn's tests for post-hoc comparisons.",TRUE,Robot-verbal-communication-content; Robot-autonomy,Robot-verbal-communication-content,Robot-autonomy,"The study manipulated the predictability of the robot's behavior by having the robot communicate its intentions (e.g., changing direction, using an elevator) through a display and subtitles. This falls under 'Robot-verbal-communication-content' because it involves changes to what information is being communicated to the user. The study also manipulated the controllability of the robot by allowing participants to make choices about the route and elevator in one condition, which is a change in the robot's decision authority and thus falls under 'Robot-autonomy'. The manipulation of predictability was partially successful, as perceived predictability was significantly higher in the predictable condition compared to the unpredictable condition. However, the manipulation of controllability was not successful, as there were no significant differences in perceived control between the conditions. The study found that perceived predictability was correlated with trust, but the manipulation of predictability did not significantly impact trust. The manipulation of controllability was not successful, and therefore did not impact trust.",10.1109/HRI53351.2022.9889635,https://ieeexplore.ieee.org/document/9889635/,"For designing the interaction with robots in healthcare scenarios, understanding how trust develops in such situations characterized by vulnerability and uncertainty is important. The goal of this study was to investigate how technology-related user dispositions, anxiety, and robot characteristics influence trust. A second goal was to substantiate the association between hospital patients’ trust and their intention to use a transport robot. In an online study, patients, who were currently treated in hospitals, were introduced to the concept of a transport robot with both written and video-based material. Participants evaluated the robot several times. Technology-related user dispositions were found to be essentially associated with trust and the intention to use. Furthermore, hospital patients’ anxiety was negatively associated with the intention to use. This relationship was mediated by trust. Moreover, no effects of the manipulated robot characteristics were found. In conclusion, for a successful implementation of robots in hospital settings patients’ individual prior learning history – e.g., in terms of existing robot attitudes – and anxiety levels should be considered during the introduction and implementation phase."
"Sebo, Sarah Strohkorb; Krishnamurthi, Priyanka; Scassellati, Brian",“I Don't Believe You”: Investigating the Effects of Robot Trust Violation and Repair,2019,1,82,82,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants played a space shooting game against a robot. The robot made a promise not to use a specific power-up, then broke that promise. The robot then attempted to repair trust using either an apology or denial, framed as either a competence or integrity violation. Participants then completed a post-experiment questionnaire.",Participants played a competitive space shooting game against a robot.,Nao,Humanoid Robots; Expressive Robots,Research; Social,Game,Competitive Game,minimal interaction,"Participants interacted with the robot through a game, with verbal communication and no physical touch.",real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the participant's behavior.,Questionnaires; Behavioral Measures,Dyadic Trust Scale; Robotic Social Attributes Scale (RoSAS),Performance Metrics,Trust was assessed using questionnaires and behavioral measures.,"parametric models (e.g., regression)",The study used ANOVA and logistic regression to analyze the data.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's behavior was manipulated by varying the framing of the trust violation (competence or integrity) and the trust repair strategy (apology or denial). The robot's promise also influenced human expectations.,Participants who experienced an integrity trust violation and a denial were more likely to retaliate. Participants who received an apology viewed the robot as more warm. Participants in the competence-apology condition had higher trust ratings than those in the competence-denial condition.,"Participants in the integrity-denial condition showed a higher rate of retaliation but not significantly lower trust ratings, suggesting a disconnect between immediate behavior and overall trust perception. There was also a strong influence of participant promises on trust-related behavior and perceptions of the robot.","The study found an interaction effect between trust violation framing and trust repair strategy, where an apology was more effective for competence violations and a denial was more effective for integrity violations, similar to human-human interactions.","The robot played a space shooting game, making a promise not to use a specific power-up, then breaking that promise. The human participant played the same game, and their power-up choices were measured as a response to the robot's actions.",Logistic regression; chi-squared tests of independence; Multilevel Model; ANOVA; t-test; Pearson correlation; Linear regression,"The study used logistic regression to analyze the influence of trust violation framing and repair strategy on participants' first power-up choice, and multilevel mixed-effects logistic regression to analyze power-up choices over time. Chi-squared tests were used for pairwise comparisons of power-up choices between conditions. ANOVA was used to examine the effects of trust violation framing and repair strategy on the robot's perceived warmth, and on the Dyadic Trust Scale (DTS) measure. Independent t-tests were used for comparisons between conditions on the DTS measure. Pearson correlation was used to assess the relationship between first power-up choice and DTS ratings, and between the perception of the robot lying and DTS ratings. Linear regression was used to analyze the influence of participant promises on DTS ratings.",TRUE,Robot-verbal-communication-content; Robot-morality,Robot-verbal-communication-content; Robot-morality,,"The study manipulated the robot's verbal communication content by varying the framing of the trust violation (competence or integrity) and the trust repair strategy (apology or denial). This is explicitly stated in the 'Experimental Conditions' section: 'The robot's response to this trust violation varied between conditions: competence-apology, competence-denial, integrity-apology, integrity-denial'. The robot's initial utterance after breaking the promise ('Oh no! I hit the wrong button!' or 'Yes! You're immobilized!') and the subsequent repair utterance ('I'm sorry for immobilizing you' or 'I didn't push the button to immobilize you') directly altered the content of the robot's communication. The study also manipulated the robot's morality by having it break a promise not to use a specific power-up, which is a violation of a moral code within the context of the game. This is described in the 'Experimental Conditions' section: 'In the Space Shooting game, the robot made a promise not to use the immobilization power-up. The trust violation occurred when the robot used the immobilization power-up against the participant, breaking its promise.' Both of these manipulations were found to impact trust, as evidenced by the results showing that participants reacted differently to the different framings and repair strategies, and that participants who believed the robot lied had lower trust ratings. There were no factors that were manipulated that did not impact trust.",10.1109/HRI.2019.8673169,https://ieeexplore.ieee.org/document/8673169/,"When a robot breaks a person’s trust by making a mistake or failing, continued interaction will depend heavily on how the robot repairs the trust that was broken. Prior work in psychology has demonstrated that both the trust violation framing and the trust repair strategy inﬂuence how effectively trust can be restored. We investigate trust repair between a human and a robot in the context of a competitive game, where a robot tries to restore a human’s trust after a broken promise, using either a competence or integrity trust violation framing and either an apology or denial trust repair strategy. Results from a 2x2 between-subjects study (n = 82) show that participants interacting with a robot employing the integrity trust violation framing and the denial trust repair strategy are signiﬁcantly more likely to exhibit behavioral retaliation toward the robot. In the Dyadic Trust Scale survey, an interaction between trust violation framing and trust repair strategy was observed. Our results demonstrate the importance of considering both trust violation framing and trust repair strategy choice when designing robots to repair trust. We also discuss the inﬂuence of humanto-robot promises and ethical considerations when framing and repairing trust between a human and robot."
"Seet, M.; Harvy, J.; Bose, R.; Dragomir, A.; Bezerianos, A.; Thakor, N.",Differential Impact of Autonomous Vehicle Malfunctions on Human Trust,2020,1,26,26,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a driving simulation with manual, conditional, and full automation modes. Malfunctions were introduced in the conditional and full automation modes. EEG data and trust ratings were collected.","Participants drove a simulated vehicle in different automation modes, responding to traffic scenarios and malfunctions.",Unspecified,Autonomous Vehicles,Research,Navigation,Path Following,minimal interaction,Participants interacted with the simulated vehicle through a driving simulator.,simulation,Participants experienced the interaction through a driving simulation.,simulated,The robot was a simulated autonomous vehicle.,shared control (fixed rules),The vehicle operated autonomously but allowed for human takeover in the conditional automation mode.,Questionnaires; Behavioral Measures; Physiological Measures,,Physiological Signals,"Trust was assessed using questionnaires, takeover decisions, and EEG data.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers manipulated the automation level and introduced malfunctions to influence trust.,"Trust decreased in the full automation mode after malfunctions, but not in the conditional automation mode.","The study found that trust was not reduced by malfunctions when participants could take over control, suggesting that the ability to intervene is crucial for maintaining trust in automated systems. The study also found that frontal alpha power is a neural correlate of trust-in-automation.","Trust was reduced by AV malfunctions during full automation, but not during conditional automation where human drivers could take over vehicle control.","The robot (simulated AV) drove through a simulated urban environment, and the human participant monitored the road and took over control when needed in the conditional automation mode.",t-test; bayes factor test; ANOVA; Pearson correlation,"The study used t-tests to compare means of trust ratings, takeover decisions, and user preferences between different conditions (e.g., normal vs. malfunction trials, CAD vs. FAD). Bayes Factor tests were used to provide evidence for null effects and to confirm the absence of modulating effects of baseline trust and gender. Repeated-measures ANOVAs were employed to analyze the effects of hemisphere and AV functionality on EEG power spectral density (PSD) and functional connectivity graph metrics. Correlation analysis was used to examine the relationship between the number of crashes experienced and average trust during FAD malfunction trials.",TRUE,Robot-autonomy; Robot-accuracy,Robot-accuracy,Robot-autonomy,"The study manipulated 'Robot-autonomy' by having participants drive in three modes: manual (SAE L0), conditional automation (SAE L3), and full automation (SAE L5). In the conditional automation mode, participants could take over control, while in the full automation mode, they could not. This is a manipulation of the level of decision authority given to the robot. The study also manipulated 'Robot-accuracy' by introducing malfunctions in the conditional and full automation modes, where the AV failed to decelerate at junctions. The malfunctions directly impacted the robot's performance on the driving task. The results showed that 'Robot-accuracy' (malfunctions) impacted trust in the full automation mode, but not in the conditional automation mode. This is because in the conditional automation mode, the human could take over control, mitigating the negative impact of the malfunction. The level of autonomy itself did not impact trust, as trust was similar in normal trials between CAD and FAD. The key finding was that the *inability* to intervene during malfunctions (in FAD) led to trust loss, not the malfunctions themselves. Therefore, the manipulation of 'Robot-accuracy' was the factor that impacted trust, while the manipulation of 'Robot-autonomy' did not directly impact trust, but rather modulated the impact of the accuracy manipulation.",10.1109/TITS.2020.3013278,,"Trust in autonomous vehicles (AV) is of critical importance and demands comprehensive interdisciplinary research. While most studies utilize subjective measures, we employ electroencephalography (EEG) to study in a more objective manner the cognitive states associated with trust during AV driving. Subjects drove a simulated AV in Conditional Automation Driving (SAE L3) and Full Automation Driving (SAE L5) modes. In the experimental design, malfunctions were induced at both automation levels. Self-reported trust in the AV was reduced immediately after Full Automation malfunctions, but not after Conditional Automation malfunctions when subjects were able to take over vehicle control to avoid danger. EEG analyses reveal that during Full Automation malfunctions, there was a significant enhancement in approach motivation (i.e. desire to re-engage) and a disruption of right frontal functional clustering that supports executive cognition (i.e. planning and decision-making). No neurocognitive disruptions were observed during Conditional Automation malfunctions. Our results demonstrate that it is not automation malfunctions per se (e.g. failure to decelerate) that deteriorate trust, but rather the inability for human drivers to adaptively mitigate the risk of negative outcomes (e.g. risk of crashing) resulting from those malfunctions. This is reflected in changes in brain activity associated with motivational state and action planning. Keeping the human driver on-the-loop protects against trust loss. Frontal alpha EEG is a neural correlate of trust-in-automation, with potential for trust monitoring using wearable technology to support driver-vehicle adaptivity."
"Sengupta, Sailik; Ai, Amazon; Zahedi, Zahra; Kambhampati, Subbarao",Game-theoretic Model of Trust to Infer Human’s Observation Strategy of Robot Behavior,2021,1,32,32,0,No participants were excluded,Educational Setting,within-subjects,"Participants were asked to monitor a robot for an hour while also having the option to grade exam papers for pay. They completed five trials, receiving feedback on their utility after each trial. The robot's behavior was not adaptive to the human's strategy.","Participants were asked to allocate their time between monitoring a robot and grading exam papers, with the goal of maximizing their overall utility.",Unspecified,Mobile Robots,Other,Supervision,Monitoring,minimal interaction,"Participants interacted with a simulated robot through a user interface, making decisions about monitoring time.",simulation,The interaction was presented through a user interface simulating a robot delivery scenario.,simulated,The robot was represented through a simulation in the user interface.,pre-programmed (non-adaptive),The robot followed a pre-programmed plan and did not adapt to the human's monitoring strategy.,Questionnaires,,,Trust was assessed through subjective questions about the participants' willingness to use a software that provides an optimal monitoring strategy.,no modeling,Trust was not modeled computationally; the study focused on human behavior and preferences.,Empirical HRI Studies,Virtual/Simulated Studies,Indirect Manipulation,"The task was framed as a supervision scenario where participants had to balance monitoring a robot with another task, and they received feedback on their performance after each trial.","The study did not directly measure trust, but the results showed that participants initially adopted risk-averse or risk-taking strategies, and gradually moved towards more optimal strategies with feedback.","Participants initially adopted extreme strategies (either monitoring all the time or not at all), but with feedback, they converged towards more optimal strategies. Some participants were willing to take risks to gain more utility, which may not be the case in real-world settings.","Humans struggle to find optimal monitoring strategies without assistance, and they tend to deviate to more split-time strategies, highlighting the need for a system that provides optimal monitoring strategies.","The robot was simulated to perform a delivery task, and the human participant was tasked with monitoring the robot and deciding how much time to spend on this task versus grading papers.",,"No specific statistical tests were mentioned in the paper. The analysis focused on observing trends in participant behavior across trials, such as the shift from extreme monitoring strategies to more optimal ones, and the distribution of participant utilities and variances. The study also included subjective evaluations through questionnaires.",TRUE,Task-constraints; Task-complexity,,,"The study manipulated 'Task-constraints' by introducing a time constraint (one hour) and a resource limitation (time allocation between monitoring and grading papers). This is evident in the description of the experimental procedure where participants had to balance monitoring a robot with grading papers for pay, thus creating a trade-off. The study also implicitly manipulated 'Task-complexity' by requiring participants to understand the utility values associated with different monitoring strategies and the robot's potential actions, and to make decisions that maximize their overall utility. This is described in the experimental task where participants had to allocate their time to maximize their utility, which requires cognitive effort. The study did not explicitly manipulate any factors that directly impacted trust, but the study design influenced the trust outcomes by creating a scenario where participants had to balance the cost of monitoring with the risk of robot failure. The study did not explicitly manipulate any factors that did not impact trust, as the study was not designed to test the impact of different factors on trust, but rather to observe how humans behave in a supervision scenario.",,,"In scenarios where a robot generates and executes a plan, there may be instances where this generated plan is less costly for the robot to execute but incomprehensible to the human. When the human acts as a supervisor and is held accountable for the robot’s plan, the human may be at a higher risk if the incomprehensible behavior is deemed to be infeasible or unsafe. In such cases, the robot, who may be unaware of the human’s exact expectations, may choose to execute (1) the most constrained plan (i.e. one preferred by all possible supervisors) incurring the added cost of executing highly sub-optimal behavior when the human is monitoring it and (2) deviate to a more optimal plan when the human looks away. While robots do not have human-like ulterior motives (such as being lazy), such behavior may occur because the robot has to cater to the needs of different human supervisors. In such settings, the robot, being a rational agent, may take any chance it gets to deviate to a lower cost plan. On the other hand, continuous monitoring of the robot’s behavior is often difﬁcult for humans because it costs them valuable resources (e.g., time, cognitive overload, etc.). Thus, to optimize the cost for monitoring while ensuring the robots follow the safe behavior and to assist the human to deal with the possible unsafe robots, we model this problem in the game-theoretic framework of trust. In settings where the human does not initially trust the robot, pure-strategy Nash Equilibrium provides a useful policy for the human."
"Seppelt, Bobbie D.; Lee, John D.",Keeping the driver in the loop: Dynamic feedback to support appropriate use of imperfect vehicle control automation,2019,1,48,48,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants drove a simulated vehicle using Adaptive Cruise Control (ACC) with different feedback displays (discrete warnings, continuous visual, continuous auditory, and combined visual-auditory). They completed a secondary detection task and mental model questionnaires before, during, and after the drives. The driving scenarios included situations where ACC approached or exceeded its limits.",Participants were asked to maintain a 1.5 s time headway to a lead vehicle using ACC while also detecting dice images on billboards.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated vehicle using ACC and received feedback through visual and auditory displays.,simulation,Participants drove in a simulated environment with a 50-degree field of view and full instrumentation.,simulated,The robot was a simulated vehicle with ACC capabilities.,shared control (fixed rules),"The ACC system operated autonomously within fixed rules, maintaining a set speed or headway.",Questionnaires; Behavioral Measures; Performance-Based Measures,,Performance Metrics,"Trust was assessed using questionnaires, reaction time, time-to-collision, and mental model accuracy.",no modeling,Trust was not modeled computationally; statistical analysis was used to compare conditions.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The type of feedback (discrete vs. continuous, visual vs. auditory) was manipulated to influence driver understanding and reliance on ACC, and the driving scenarios were designed to vary the difficulty of the task.","Continuous feedback improved driver understanding and reliance on ACC, leading to faster responses and safer time-to-collision values. The combined visual-auditory display was most effective.","The study found that continuous feedback, especially in a combined visual-auditory format, led to more proactive responses and better understanding of the ACC system. The interaction between modality and display for the measure of trust qualified with self-confidence was also notable.","Continuous feedback, particularly in a combined visual-auditory format, improves driver understanding and reliance on Adaptive Cruise Control (ACC), leading to more proactive responses and safer driving behavior.","The robot (simulated vehicle with ACC) maintained a set speed or headway, while the human participant monitored the system and responded to events, also completing a secondary detection task.",ANOVA; t-test; χ² test,"The study used mixed model analysis of variance (ANOVA) to analyze the effects of display type (discrete vs. continuous), modality (visual vs. auditory), event type, and block order on various dependent variables such as reaction time (RT), time-to-collision (TTC), mental model accuracy, subjective trust, and detection task performance. Post-hoc comparisons were evaluated using Bonferroni t-tests. Chi-squared tests (χ²) were used to compare the frequency of responses between conditions.",TRUE,Robot-interface-design; Task-environment,Robot-interface-design,,"The study manipulated the type of feedback provided to the driver regarding the Adaptive Cruise Control (ACC) system. This is categorized as 'Robot-interface-design' because the manipulation involved changes to the visual and auditory displays that provided information about the ACC's state and limits. Specifically, the study compared discrete warnings to continuous information displays, and also varied the modality of the continuous information (visual, auditory, and combined visual-auditory). The study also manipulated the 'Task-environment' by introducing different driving conditions (e.g., braking limits, sensor limits, detection range limits, and setting limits) that caused the ACC to approach or exceed its operational limits. These conditions were designed to vary the difficulty of the task and the demands on the ACC system. The 'Robot-interface-design' was found to impact trust, as continuous feedback improved driver understanding and reliance on ACC, leading to faster responses and safer time-to-collision values. The combined visual-auditory display was most effective. The 'Task-environment' was not found to directly impact trust, but rather served as a context for evaluating the effectiveness of the different interface designs.",10.1016/j.ijhcs.2018.12.009,https://linkinghub.elsevier.com/retrieve/pii/S1071581918301277,"Objective: This study evaluates the benefits and costs associated with providing drivers continuous feedback on the limits and behavior of imperfect vehicle control automation. Background: In-vehicle automated systems remove drivers from active vehicle control, often at the expense of timely interventions when failures occur. Discrete warnings, as a type of feedback to inform drivers about automated system behavior, fail to keep drivers aware of its proximity to operating limits. Method: In a fixed-based simulator, 48 drivers drove using Adaptive Cruise Control (ACC)—a form of control automation that maintains a set speed, or a set headway if the vehicle encounters a slower moving vehicle. A first experiment compared ACC with discrete warnings to ACC with continuous information, which indicated moment-to-moment ACC state relative to its operating limits. Three display conditions, designed to provide nonobtrusive, ecologically-valid information, were evaluated in a second experiment: 1) a visual interface; 2) an auditory interface; and 3) a combined visual-auditory interface. Results: Drivers provided with continuous feedback relied more appropriately on ACC than did those with discrete warnings. Continuous feedback increased the frequency of proactive responses to automation failures and improved system understanding. Of the three displays, the combined visual-auditory interface performed the best. Conclusion: Continuous feedback helped communicate to drivers the evolving relationship between system performance and operating limits. Application: Displays for increasingly automated vehicles should inform about the automation's situation-specific behavior rather than simply alert drivers to failures and/or the need to resume vehicle control in order to promote appropriate understanding and trust."
"Shahrdar, Shervin; Park, Corey; Nojoumian, Mehrdad",Human Trust Measurement Using an Immersive Virtual Reality Autonomous Vehicle Simulator,2019,1,50,50,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were randomly assigned to one of two scenarios, each consisting of five driving segments. After each segment, participants rated their trust in the self-driving car using a Likert scale.",Participants experienced different simulated self-driving car driving scenarios and rated their trust after each segment.,Unspecified,Autonomous Vehicles,Research,Evaluation,Rating,minimal interaction,Participants experienced a simulated driving scenario and provided trust ratings.,simulation,Participants used a VR headset and motion simulator to experience the driving scenarios.,simulated,The self-driving car was simulated in a virtual reality environment.,fully autonomous (limited adaptation),"The self-driving car operated autonomously, following pre-programmed driving scenarios.",Questionnaires; Real-time Trust Measures,,,Trust was measured using a Likert scale after each driving segment.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the driving behavior of the simulated self-driving car to include smooth, erratic, and rule-breaking actions, which was intended to influence trust levels.","Trust levels decreased after exposure to erratic driving and increased after smooth driving, with the lowest trust levels reported after negative trust mutation segments. Participants were able to rebuild trust after trust-damaging incidents, but not to the initial levels.","Participants in both scenarios showed a significant decrease in trust after negative trust mutation segments. The negative trust mutation segment in Scenario-2 was milder than in Scenario-1, resulting in less of a trust decrease. Participants did not trust the SDC around pedestrians.","Human trust levels in self-driving cars are significantly influenced by the car's driving style, with aggressive driving diminishing trust and defensive driving increasing it. Participants were able to rebuild trust after trust-damaging incidents, but not to the initial levels.","The simulated self-driving car drove through various scenarios, including smooth driving, erratic driving, and rule-breaking actions. The human participant observed the driving scenarios and rated their trust in the self-driving car after each segment.",Wilcoxon rank sum,"The Wilcoxon Rank Sum Test was used to assess whether the observed changes in trust scores across different driving segments were statistically significant. This test was applied within each scenario (Scenario-1 and Scenario-2) to compare trust levels between segments, and also to compare trust levels between the two scenarios for each segment.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the driving behavior of the simulated self-driving car to include smooth, erratic, and rule-breaking actions. This directly impacts the robot's accuracy in adhering to traffic laws and driving safely, which is a key factor influencing task performance. The paper states, 'An initial trust/trust escalation segment involved the SDC moving slowly and predictably while adhering to the rules of the road. A trust reduction segment involved the SDC along with Human-Driving Cars (HDC) moving erratically and unpredictably, breaking rules of the road including speeding, tailgating, and sudden lane changes.' This clearly shows that the researchers manipulated the robot's driving behavior, which directly relates to its accuracy in performing the driving task. The results section also confirms that 'Participants scored the initial trust and rebuild trust segments with high levels of trust, the trust reduction segments with lower levels of trust, and the negative trust mutation segment with the lowest level of trust,' indicating that the manipulation of driving behavior (accuracy) directly impacted trust.",10.1145/3306618.3314264,https://dl.acm.org/doi/10.1145/3306618.3314264,"Recent studies indicate that people are negatively predisposed toward utilizing autonomous systems. These findings highlight the necessity of conducting research to better understand the evolution of trust between humans and growing autonomous technologies such as self-driving cars (SDC). This research presents a new approach for real-time trust measurement between passengers and SDCs. We utilized a new structured data collection approach along with a virtual reality SDC simulator to understand how various autonomous driving scenarios can increase or decrease human trust and how trust can be re-built in the case of incidental failures. To verify our methodology, we designed and conducted an empirical experiment on 50 human subjects. The results of this experiment indicated that most subjects could rebuild trust during a reasonable time frame after the system demonstrated faulty behavior. Our analysis showed that this approach is highly effective for collecting real-time data from human subjects and lays the foundation for more-involved future research in the domain of human trust and autonomous driving."
"Sharp, William H.; Jackson, Kenneth M.; Shaw, Tyler H.",The frequency of positive and negative interactions influences relationship equity and trust in automation,2023,1,145,144,1,1 participant was excluded due to a computer error,Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of four conditions, completed a demographics questionnaire, the Propensity to Trust assessment, and the Ten Item Personality Inventory. They then completed the Jian et al. Trust Survey, followed by either 120 or 40 trials of a pattern recognition task. Participants self-reported their trust in the computer agent throughout the task. Upon completion of the pattern recognition task, participants gave a final subjective trust rating and then completed the Desert Survival Task. Finally, participants completed the Jian et al. Trust Survey again and were debriefed.",Participants completed a pattern recognition task where they received recommendations from a computer agent and then completed the Desert Survival Task with the same agent.,Unspecified,Other,Research,Evaluation,Ranking,minimal interaction,Participants interacted with a computer agent through a screen.,simulation,The interaction was conducted through a simulated task environment.,simulated,The robot was a simulated computer agent.,pre-programmed (non-adaptive),The computer agent provided recommendations based on pre-programmed reliability.,Behavioral Measures; Custom Scales; Questionnaires; Real-time Trust Measures,Jian et al. Trust Scale; Propensity to Trust Scales; Ten Item Personality Inventory (TIPI),Performance Metrics,"Trust was assessed using questionnaires, real-time subjective ratings, and behavioral measures such as compliance and performance.","parametric models (e.g., regression)","A discontinuous growth model was used to examine self-reported trust over time, and ANOVAs were used to analyze the effects of positivity and frequency on trust.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the computer agent's recommendations was manipulated to be either positive (85% correct) or negative (45% correct), and the frequency of interactions was manipulated to be either high (120 trials) or low (40 trials). This was intended to influence the participants' trust in the agent.","Positive interactions led to higher trust ratings and compliance, while negative interactions led to lower trust ratings and compliance. The frequency of interaction had a less significant impact on trust.","The frequency of interaction did not have a significant impact on trust, suggesting that participants may have quickly calibrated their trust based on the agent's reliability. There was a marginal non-significant trend of an interaction between test time and positivity, such that in the negative conditions there was a greater difference between pre and post task trust scores.","Relationship equity, defined as the combination of agent positivity and frequency of interactions, has an impact on user trust in an automated teammate, and high relationship equity may have a significant impact on user trust resilience after a major error occurs.","The computer agent provided recommendations on a pattern recognition task, and participants decided whether to comply with the agent's recommendations. Participants also ranked items in the Desert Survival Task based on the agent's recommendations.",ANOVA; t-test; Mixed-effects model; discontinuous growth model; Intraclass correlation coefficient; log likelihood difference test,"The study used a variety of statistical tests to analyze the data. ANOVAs were used to compare the effects of positivity and frequency on pattern recognition task performance, compliance, and Desert Survival Task (DST) performance and compliance. T-tests were used for planned comparisons between conditions. A Mixed Model ANOVA was used to examine the effects of positivity, frequency, and time on the Jian Trust Scale score. A discontinuous growth model was used to examine self-reported trust over time, and an intraclass correlation coefficient (ICC(1)) was calculated to justify a multilevel approach. Log likelihood difference tests were used to compare different model fits.",TRUE,Robot-accuracy; Task-constraints,Robot-accuracy,Task-constraints,"The study manipulated the reliability of the computer agent's recommendations, which directly impacts the accuracy of the agent's performance on the pattern recognition task. This is categorized as 'Robot-accuracy' because it directly influences the success rate of the agent's recommendations. The study also manipulated the frequency of interactions by varying the number of trials (120 or 40) in the pattern recognition task. This is categorized as 'Task-constraints' because it changes the amount of time and number of interactions participants had with the agent, which is a constraint on the task. The results showed that the reliability of the agent (Robot-accuracy) significantly impacted trust, with higher reliability leading to higher trust ratings and compliance. However, the frequency of interactions (Task-constraints) did not have a significant impact on trust, suggesting that participants quickly calibrated their trust based on the agent's reliability rather than the number of interactions. The paper states, 'Results demonstrated that frequency of positive and negative interactions did have an impact on user trust and trust resilience after a major error.' However, the statistical analysis shows that the main effect of frequency was not significant, and the impact of frequency was only seen in the planned comparisons of the positive-high condition. Therefore, the main effect of frequency is not considered to have impacted trust.",10.1016/j.apergo.2022.103961,https://linkinghub.elsevier.com/retrieve/pii/S0003687022002848,"The purpose of this study was to 1) examine whether frequency of positive and negative interactions (manip­ ulated via reliability) with a computer agent had an impact on an individual’s trust resilience after a major error occurs and 2) empirically test the notion of relationship equity, which encompasses the total accumulation of positive and negative interactions and experiences between two actors, on user trust on a separate transfer task. Participants were randomized into one of four groups, differing in agent positivity and frequency of interaction, and completed both a pattern recognition task and transfer task with the aid of the same computer agent. Subjective trust ratings, performance data, compliance, and agreement were collected and analyzed. Results demonstrated that frequency of positive and negative interactions did have an impact on user trust and trust resilience after a major error. Additionally, it was shown that relationship equity has an impact on user trust and trust resilience. This is the first empirical demonstration of relationship equity’s impact on user trust in an automated teammate."
"Shayesteh, Shayan; Ojha, Amit; Jebelli, Houtan",Workers’ Trust in Collaborative Construction Robots: EEG-Based Trust Recognition in an Immersive Environment,2022,1,7,7,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a masonry task in a virtual environment with a virtual robot in two scenarios: one with an autonomous robot and one with a semi-autonomous robot. EEG data was collected during the task, and trust was measured using a questionnaire after each scenario.",Participants performed a collaborative masonry task with a virtual robot.,Unspecified,Industrial Robot Arms,Industrial; Research,Manipulation,Object Passing,minimal interaction,Participants interacted with a virtual robot in a simulated environment.,simulation,The interaction took place in a virtual environment using a VR headset.,simulated,The robot was a virtual representation in the simulation.,shared control (fixed rules),"The robot was either autonomous or semi-autonomous, with the semi-autonomous version having some human control.",Questionnaires; Physiological Measures,Trust Perception Scale - HRI,Physiological Signals,Trust was measured using a questionnaire and EEG signals.,"parametric models (e.g., regression)",Supervised learning algorithms were used to classify EEG signals based on trust levels.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's autonomy level was manipulated by having the robot be either fully autonomous or semi-autonomous, with the human having some control over the semi-autonomous version.",Trust was higher when participants had some control over the robot's movements.,The study found that participants had higher trust in the semi-autonomous robot compared to the autonomous robot.,EEG signals can be used to differentiate levels of trust in collaborative construction robots.,"The virtual robot lifted and moved materials, while the human participant collaborated with the robot to complete a masonry task.",Kruskal-Wallis,The Kruskal-Wallis test was performed on the average trust scores from the questionnaire to determine if there were statistically significant differences in trust levels between the two robot autonomy conditions (autonomous vs. semi-autonomous). The test confirmed a significant difference between trust levels in collaborative robots between different scenarios.,TRUE,Robot-autonomy,Robot-autonomy,,"The study explicitly manipulated the level of robot autonomy. Participants interacted with the robot in two scenarios: one with an autonomous robot and one with a semi-autonomous robot where the human had some control over its movements. This manipulation is described in the 'Experimental Procedure' section: 'Two scenarios of a masonry task were designed to induce different levels of trust in collaborative robots. The first scenario was to work along with an autonomous version of the material lifting robot... The second scenario was to work along with a semi-autonomous version of the robot, of which humans had control over most of its movements.' The results section also confirms that the manipulation of robot autonomy impacted trust levels: 'The results indicated that working alongside an autonomous robot would lead to lower levels of trust, while working alongside a semi-autonomous robot with a certain control over it would lead to higher trust levels among humans.' Therefore, 'Robot-autonomy' is the appropriate category for both manipulated and impacted factors. There were no other factors manipulated, and no factors that were manipulated that did not impact trust.",,https://link.springer.com/10.1007/978-3-030-77163-8_10,"Today, advances in robotics and autonomous systems enable construction workers to collaboratively work with robots, assigning physically demanding tasks to them. However, working alongside an industrial robot is a novel experience that may take a heavy toll on workers’ bodies and minds, particularly in dynamic and uncertain environments, such as construction sites. Given that trust is identified as a critical element for successful cooperation between humans and robots, effective measurement of workers’ trust in collaborative robots can lead to practical evaluation of human-robot partnership. In this context, most studies of trust have relied on self-reports. Nevertheless, questionnaires are unable to determine the trust promptly, impeding early preventive interventions. Furthermore, they are invasive, interfering with workers’ daily operations. To address these issues, this study proposes a procedure to non-invasively and continuously recognize workers’ trust in collaborative construction robots using electroencephalogram (EEG) signals. To that end, an experiment was conducted in which human workers performed a collaborative construction task (i.e., handling materials) with a virtual robot in an immersive environment. Meanwhile, workers’ EEG signals were recorded using a wearable sensor. Subsequently, the level of trust of the workers in collaborative robots was measured using the Trust Perception Scale-HRI. By analyzing acquired signals and applying different machine learning algorithms, it was found that EEG signals can be implemented to differentiate levels of trust of construction workers in their robotic counterparts. These findings suggest the feasibility of using workers’ EEG signals as a reliable, real-time indicator of trust in collaborative construction robots, which can be regarded as a practical approach for evaluating human-robot collaboration."
"Sheng, Shili; Pakdamanian, Erfan; Han, Kyungtae; Kim, BaekGyu; Tiwari, Prashant; Kim, Inki; Feng, Lu",A Case Study of Trust on Autonomous Driving,2019,1,19,19,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a demographic questionnaire, a baseline experiment, and a training trial before completing 16 driving trials in a simulator. They adjusted their trust level using buttons on the steering wheel during the trials.","Participants drove a simulated vehicle in autonomous and semi-autonomous modes, responding to hazardous events and adjusting their trust level in real-time.",Unspecified,Autonomous Vehicles,Research,Navigation,Path Following,minimal interaction,Participants interacted with a driving simulator and adjusted trust levels using buttons.,simulation,Participants used a driving simulator to experience the autonomous driving scenarios.,simulated,The robot was a simulated autonomous vehicle within the driving simulator.,shared control (fixed rules),The vehicle operated autonomously but allowed participants to switch to manual driving and adjust trust levels.,Real-time Trust Measures; Physiological Measures,,Physiological Signals; Eye-tracking Data,Trust was measured using a real-time Likert scale and physiological data.,"parametric models (e.g., regression)",ANOVA was used to analyze the influence of different factors on trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated alarm types, weather conditions, and driving mode to influence trust. The alarm types included missing and false alarms, and the driving mode was either fully or semi-autonomous.","Missing alarms significantly decreased trust, while weather conditions and driving mode had a weak effect. False alarms also decreased trust.",The study found that missing alarms had a greater negative impact on trust than false alarms. There was also individual variability in reaction time and trust change.,"Missing alarms significantly decreased trust in autonomous driving, while weather conditions and driving mode had a weak effect.","The simulated vehicle drove autonomously, and the human participant monitored the driving and adjusted their trust level using buttons on the steering wheel. The human also had the option to switch to manual driving in the semi-autonomous mode.",ANOVA; Tukey HSD,"The study used repeated-measures ANOVA to examine the effects of alarm type, weather conditions, and driving mode on trust. Post-hoc Tukey HSD tests were used for pairwise comparisons of alarm types. ANOVA was also used to analyze the effects of weather conditions and gender on physiological measures such as pupil size, GSR peaks, and HRV. The overall purpose was to determine the influence of these factors on trust and physiological responses.",TRUE,Robot-accuracy; Task-environment; Robot-autonomy,Robot-accuracy,Task-environment; Robot-autonomy,"The researchers manipulated the alarm type, which directly relates to the accuracy of the robot's (autonomous vehicle's) hazard detection and alerting system. Specifically, the alarm types included all alarms activated (AAAA), all alarms missing (MMMM), an early false alarm (FAAAA), and a late false alarm (AAAFA). This manipulation directly impacts the robot's accuracy in correctly identifying and reporting hazards, thus influencing trust. The weather conditions (sunny and rainy) were also manipulated, which is a change to the task environment. The driving mode (fully autonomous and semi-autonomous) was also manipulated, which is a change to the robot's autonomy level. The study found that the alarm type (robot accuracy) significantly impacted trust, with missing alarms leading to a significant decrease in trust. However, the weather conditions (task environment) and driving mode (robot autonomy) had a weak effect on trust, indicating that these manipulations did not significantly impact trust levels.",,http://arxiv.org/abs/1904.11007,"As autonomous vehicles have beneﬁted the society, understanding the dynamic change of humans’ trust during human-autonomous vehicle interaction can help to improve the safety and performance of autonomous driving. We designed and conducted a human subjects study involving 19 participants. Each participant was asked to enter their trust level in a Likert scale in real-time during experiments on a driving simulator. We also collected physiological data (e.g., heart rate, pupil size) of participants as complementary indicators of trust. We used analysis of variance (ANOVA) and Signal Temporal Logic (STL) to analyze the experimental data. Our results show the inﬂuence of different factors (e.g., automation alarms, weather conditions) on trust, and the individual variability in human reaction time and trust change."
"Sheng, Shili; Pakdamanian, Erfan; Han, Kyungtae; Wang, Ziran; Lenneman, John; Feng, Lu",Trust-Based Route Planning for Automated Vehicles,2021,2,100,100,0,No participants were excluded,Online Crowdsourcing,,Participants watched driving videos and answered questionnaires about their trust in the automated vehicle and their takeover decisions.,Participants watched videos of an automated vehicle approaching different road incidents and indicated their trust level and whether they would take over control.,Unspecified,Autonomous Vehicles,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants passively observed videos of the automated vehicle.,media,Participants watched videos of the automated vehicle driving.,simulated,The robot was represented through simulated driving videos.,pre-programmed (non-adaptive),The automated vehicle followed a pre-programmed route and behavior.,Questionnaires,Muir's Trust Questionnaire,Video Data,Trust was measured using questionnaires after participants watched videos.,"parametric models (e.g., regression)",Trust dynamics were modeled using a linear Gaussian system.,Empirical HRI Studies,Virtual/Simulated Studies,Indirect Manipulation,"The type of road incident (pedestrian, obstacle, truck) influenced the perceived difficulty and the participants' expectations of the automated vehicle's capability.",Trust decreased when participants chose to take over control and tended to increase when they did not take over and the vehicle handled the incident successfully.,"Trust values were more likely to increase when human decides to not take over, while trust values tend to be constant or decrease when there is a takeover decision.","Trust dynamics were modeled as a linear Gaussian system, and takeover decisions were influenced by both the incident type and the level of trust.","The robot (simulated automated vehicle) drove through different road incidents, and the human participant watched the videos and indicated their trust level and takeover decisions.",,No statistical tests were explicitly mentioned for this study. The study focused on collecting data to build models of trust dynamics and takeover decisions.,TRUE,Task-complexity,Task-complexity,,"The study manipulated the type of road incident (pedestrian, obstacle, truck), which directly influenced the perceived difficulty and complexity of the driving task. This is described in the paper as influencing the participants' expectations of the automated vehicle's capability. The paper states, 'To concretize the problem, we consider a motivating example where the automated vehicle may encounter three types of typical road incidents (i.e., pedestrian, obstacle, and oncoming truck).' and 'Trust is therefore affected by human's takeover decision and the vehicle's capability of handling an incident.' The different incident types represent varying levels of task complexity for the automated vehicle, thus influencing trust. The results show that participants were more likely to take over control with riskier incidents, indicating that the task complexity impacted trust.",,http://arxiv.org/abs/2101.03267,"Several recent works consider the personalized route planning based on user profiles, none of which accounts for human trust. We argue that human trust is an important factor to consider when planning routes for automated vehicles. This paper presents the first trust-based route planning approach for automated vehicles. We formalize the human-vehicle interaction as a partially observable Markov decision process (POMDP) and model trust as a partially observable state variable of the POMDP, representing human’s hidden mental state. We designed and conducted an online user study with 100 participants on the Amazon Mechanical Turk platform to collect data of users’ trust in automated vehicles. We build datadriven models of trust dynamics and takeover decisions, which are incorporated in the POMDP framework. We compute optimal routes for automated vehicles by solving optimal policies in the POMDP planning. We evaluated the resulting routes via human subject experiments with 22 participants on a driving simulator. The experimental results show that participants taking the trust-based route generally resulted in higher cumulative POMDP rewards and reported more positive responses in the after-driving survey than those taking the baseline trust-free route."
"Sheng, Shili; Pakdamanian, Erfan; Han, Kyungtae; Wang, Ziran; Lenneman, John; Feng, Lu",Trust-Based Route Planning for Automated Vehicles,2021,2,22,22,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants drove a simulated automated vehicle on a driving simulator, following either a trust-based or trust-free route, and provided trust ratings and takeover decisions.",Participants drove a simulated automated vehicle and made takeover decisions based on their trust in the vehicle's ability to handle road incidents.,Unspecified,Autonomous Vehicles,Research,Navigation,Path Following,minimal interaction,Participants interacted with the simulated vehicle through a driving simulator.,simulation,Participants experienced a simulated driving environment.,simulated,The robot was represented as a simulated vehicle in a driving simulator.,shared control (fixed rules),"The automated vehicle followed a pre-programmed route, but participants could take over control.",Questionnaires; Real-time Trust Measures,,Performance Metrics,Trust was measured using a post-driving survey and real-time user input via buttons on the steering wheel.,POMDP,Trust was modeled as a partially observable state variable in a POMDP framework.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The route (trust-based vs. trust-free) was manipulated, which changed the order of road incidents and influenced the participants' expectations and the vehicle's behavior.",Participants on the trust-based route had higher trust and were less likely to take over control compared to those on the trust-free route.,"Participants were more likely to take over control when approaching pedestrian incidents, and the average trust of participants taking the trust-based route was generally higher than those taking the trust-free route.",Participants taking the trust-based route generally resulted in higher cumulative POMDP rewards and reported more positive responses in the after-driving survey than those taking the trust-free route.,"The robot (simulated automated vehicle) drove along a pre-defined route, and the human participant could take over control and provide trust ratings using buttons on the steering wheel.",ANOVA,A one-way analysis of variance (ANOVA) was used to compare the cumulative POMDP rewards between the trust-based and trust-free route groups. The ANOVA was used to evaluate the hypothesis that participants taking the trust-based route would achieve higher cumulative rewards.,TRUE,Robot-autonomy,Robot-autonomy,,"The study manipulated the route that the automated vehicle followed (trust-based vs. trust-free), which directly influenced the order of road incidents and the vehicle's behavior. This manipulation is described in the paper as 'We manipulate a single factor: the route that the autopilot controller follows. As stated in Section 4.5, the two conditions are: trust-based route and trust-free route.' The route manipulation directly impacts the level of autonomy the robot exhibits, as the trust-based route is designed to optimize for trust, while the trust-free route is not. The paper states, 'We applied the proposed trust-based route planning approach to the motivating example and obtained two routes: a trust-based route where human makes takeover decisions based on trust dynamics and incidents, and a trust-free route (as a baseline for comparison) where human's takeover decisions only depend on incidents.' The results show that participants on the trust-based route had higher trust and were less likely to take over control, indicating that the manipulation of the route, and thus the robot's autonomy, impacted trust.",,http://arxiv.org/abs/2101.03267,"Several recent works consider the personalized route planning based on user profiles, none of which accounts for human trust. We argue that human trust is an important factor to consider when planning routes for automated vehicles. This paper presents the first trust-based route planning approach for automated vehicles. We formalize the human-vehicle interaction as a partially observable Markov decision process (POMDP) and model trust as a partially observable state variable of the POMDP, representing human’s hidden mental state. We designed and conducted an online user study with 100 participants on the Amazon Mechanical Turk platform to collect data of users’ trust in automated vehicles. We build datadriven models of trust dynamics and takeover decisions, which are incorporated in the POMDP framework. We compute optimal routes for automated vehicles by solving optimal policies in the POMDP planning. We evaluated the resulting routes via human subject experiments with 22 participants on a driving simulator. The experimental results show that participants taking the trust-based route generally resulted in higher cumulative POMDP rewards and reported more positive responses in the after-driving survey than those taking the baseline trust-free route."
"Shidujaman, Mohammad; Mi, Haipeng; Jamal, Lafifa","""I trust you more"": A Behavioral Greeting Gesture Study on Social Robots for Recommendation Tasks",2020,1,20,20,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants answered questionnaires and could ask the NAO robot for recommendations. The robot provided recommendations using the Wizard-of-Oz method, with either a normal greeting or a close-to-native-custom greeting. The order of the conditions was counterbalanced.",Participants answered multiple-choice questions and could request recommendations from the robot.,Nao,Humanoid Robots; Expressive Robots,Research; Social,Social,Social Guidance/Coaching,minimal interaction,Participants interacted with the robot verbally and received recommendations.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical NAO robot.,wizard of oz (directly controlled),The robot's recommendations were controlled by a human operator using the Wizard-of-Oz method.,Behavioral Measures,,,Trust was measured by the acceptance rate of the robot's recommendations.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's greeting gesture and language were manipulated to be either a standard greeting or a greeting close to the participant's native culture, which was intended to increase trust.","The close-to-native-custom greeting significantly increased the acceptance rate of the robot's recommendations, indicating higher trust.",Participants showed more enthusiasm and interest in interacting with the robot in the close-to-native-custom greeting condition. Some participants were confused by greeting gestures from other cultures.,"A robot with a close-to-native-custom greeting design improved the user's acceptance rate of the robot's recommendations, indicating higher trust.",The robot performed a greeting gesture and provided recommendations for multiple-choice questions. The human participant answered the questions and could request recommendations from the robot.,1-way-anova,"A one-way ANOVA was conducted to compare the acceptance rates of the robot's recommendations under two conditions: Normal Greetings (NG) and Close-to-Native-Custom Greetings (CG). The analysis aimed to determine if there was a statistically significant difference in acceptance rates between the two conditions, with the hypothesis that the CG condition would lead to higher acceptance rates.",TRUE,Robot-nonverbal-communication; Robot-verbal-communication-content,Robot-nonverbal-communication; Robot-verbal-communication-content,,"The study manipulated the robot's greeting gesture (Robot-nonverbal-communication) by having it perform either a standard greeting or a greeting close to the participant's native culture. This is explicitly stated in the paper: 'In the CG condition, at the beginning the NAO robot will perform a greeting gesture that is close to the participant's native custom.' The robot's verbal communication was also manipulated (Robot-verbal-communication-content) by having it speak in either English or the participant's native language when greeting and recommending an answer. This is stated in the paper: 'In addition, the NAO will speak in English while greeting and recommending an answer' for the NG condition and 'The NAO will also speak in the participants' native language while greeting and recommending an answer' for the CG condition. The results showed that the close-to-native-custom greeting and language significantly increased the acceptance rate of the robot's recommendations, indicating higher trust. Therefore, both Robot-nonverbal-communication and Robot-verbal-communication-content impacted trust. There were no factors that were manipulated that did not impact trust.",10.1109/ICIP48927.2020.9367364,https://ieeexplore.ieee.org/document/9367364/,"In this paper, we present our effort to understand how an agent’s recommendation can affect human decision making by establishing trust and familiarity in a cross-cultural context. We designed four different greeting gestures for the NAO robot based on human’s greeting custom and conducted a user experiment in which participants from different countries interact with the robot for a designated recommendation task. In the experiment, we investigated how cultural familiarity affects acceptance of an agent’s recommendation. 20 participants from four different countries participated in the experiment. The result of the experiment suggests a cross-cultural design method can increase the likelihood of acceptance of a humanoid social robot’s recommendation task when interacting with users with different cultural backgrounds."
"Shill, Ponkoj Chandra; Hakim, Md Azizul; Khan, Muhammad Jahanzeb; Anima, Bashira Akter",Human Reactions to Incorrect Answers from Robots,2024,1,31,31,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a pre-interaction survey, interacted with a robot that answered questions with some errors and apologies, and then completed two post-interaction surveys.",Participants asked a robot 10 general knowledge questions.,Nao,Humanoid Robots; Expressive Robots,Research; Social,Social,Conversation,minimal interaction,Participants verbally interacted with the robot.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),"The robot followed a pre-programmed sequence of responses, including errors and apologies.",Questionnaires,,,Trust was measured using pre and post-interaction questionnaires.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot was programmed to give incorrect answers and then apologize, which was intended to influence trust.",Trust increased when the robot acknowledged and apologized for errors.,Participants were more forgiving of errors from humans and internet search engines than from robots. The majority of participants experienced a decrease in trust when errors were not acknowledged.,Participants' trust in robotic technologies increased significantly when robots acknowledged their errors or limitations.,"The robot answered general knowledge questions, sometimes incorrectly, and then apologized. The human asked the robot questions from a predefined list.",ANOVA; Chi-squared,A one-way ANOVA test was conducted to analyze the change in participants' trust levels before and after the experiment. A chi-square test was also conducted on the contingency table to analyze the association between trust levels before and after the experiment.,TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the robot's verbal communication content by programming it to sometimes give incorrect answers and then apologize. This manipulation was explicitly designed to observe its impact on trust. The paper states, 'To investigate the impact of robot errors on human trust and perception, our experimental protocol deliberately included a mechanism for generating incorrect responses... Upon issuing a mistaken response, our system was designed to automatically detect the anomaly and initiate an apology sequence.' This clearly indicates a manipulation of the content of the robot's verbal communication. The results show that participants' trust increased when the robot acknowledged and apologized for errors, indicating that the manipulated factor, 'Robot-verbal-communication-content', impacted trust. There were no other factors manipulated in the study.",,http://arxiv.org/abs/2403.14293,"As robots grow more and more integrated into numerous industries, it is critical to comprehend how humans respond to their failures. This paper systematically studies how trust dynamics and system design are affected by human responses to robot failures. The three-stage survey used in the study provides a thorough understanding of human-robot interactions. While the second stage concentrates on interaction details, such as robot precision and error acknowledgment, the first stage collects demographic data and initial levels of trust. In the last phase, participants' perceptions are examined after the encounter, and trust dynamics, forgiveness, and propensity to suggest robotic technologies are evaluated. Results show that participants' trust in robotic technologies increased significantly when robots acknowledged their errors or limitations to participants and their willingness to suggest robots for activities in the future points to a favorable change in perception, emphasizing the role that direct engagement has in influencing trust dynamics. By providing useful advice for creating more sympathetic, responsive, and reliable robotic systems, the study advances the science of human-robot interaction and promotes a wider adoption of robotic technologies."
"Shu, Pan; Min, Chen; Bodala, Indu; Nikolaidis, Stefanos; Hsu, David; Soh, Harold",Human Trust in Robot Capabilities across Tasks,2018,1,32,32,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants grouped tasks, indicated initial trust, observed robot performance, and then re-indicated trust.",Participants evaluated their trust in a robot performing household tasks.,Fetch Robot,Mobile Manipulators,Research,Manipulation,Object Passing,passive observation,Participants observed the robot performing tasks.,real-world,Participants observed a real robot performing tasks in a real-world setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot performed pre-programmed success and failure demonstrations.,Questionnaires; Custom Scales,,,Trust was measured using a 7-point Likert scale questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's performance (success/failure) and task difficulty were manipulated to observe their effect on trust generalization.,"Trust transferred more easily to simpler tasks than to more difficult tasks, and was influenced by robot performance.","Trust generalized across tasks, but more strongly within the same task category and to easier tasks. The effect of failure was not statistically significant for the difficulty manipulation.","Human trust transfers across tasks, with similar tasks sharing similar levels of trust, and trust transfers more easily to simpler tasks.","The robot performed pre-programmed household tasks, such as picking and placing objects and navigation, while participants observed and rated their trust in the robot's ability to perform these tasks.",t-test; Wilcoxon signed-rank test,"The study used t-tests to compare means of trust distances and trust changes between different conditions (e.g., same category vs. different category tasks, easier vs. more difficult tasks). Wilcoxon signed-rank tests were also used to assess statistical significance, but only t-statistics were reported due to space constraints. The tests were used to evaluate the hypotheses related to trust generalization across tasks, the impact of robot performance on trust change, and the influence of task difficulty on trust transfer.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy; Task-complexity,,"The study manipulated 'Robot-accuracy' by having the robot perform pre-programmed success and failure demonstrations of household tasks, as stated in the 'Experimental Design' section: 'We developed pre-programmed success and failure demonstrations of robot performance'. This manipulation directly influenced the participants' trust in the robot's capabilities. The study also manipulated 'Task-complexity' by using easy and difficult tasks, as mentioned in the 'Experimental Design' section: 'Each independent variable consisted of two levels: two task categories, easy/difficult tasks, and robot success/failure'. The results section shows that both robot performance (success/failure) and task difficulty impacted trust generalization, as stated in the 'RESULTS' section: 'The trust distance was significantly less for tasks perceived to be easier than the observed task... For the failure condition, the results were not statistically significant, but suggest that the effect was reversed; belief in robot inability would transfer more to difficult tasks compared to simpler tasks.' Therefore, both 'Robot-accuracy' and 'Task-complexity' are listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1145/3173386.3177034,https://dl.acm.org/doi/10.1145/3173386.3177034,"In this work, we study how humans transfer or generalize trust in robot capabilities across tasks, even with limited observations. We present results from a human-subjects experiment using a realworld Fetch robot performing household tasks. In summary, we find that human trust generalization is influenced by perceived task similarity, difficulty, and robot performance."
"Siebert, Felix Wilhelm; Pickl, Johannes; Klein, Jacobe; Rötting, Matthias; Roesler, Eileen",Let’s Not Get Too Personal – Distance Regulation for Follow Me Robots,2020,1,24,24,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants walked through a hallway while a robot followed them at varying distances, carrying a personal item. Each distance condition was presented twice, and after each condition, participants completed a questionnaire.",The robot transported a personal item while the participant walked through a hallway.,Unspecified,Mobile Robots,Research,Navigation,Path Following,minimal interaction,"Participants walked while the robot followed, with no physical touch.",real-world,The study was conducted in a real-world hallway with a physical robot.,physical,A physical robot was used in the study.,shared control (fixed rules),The robot followed a pre-programmed path and adjusted its speed to maintain a set distance.,Questionnaires,,,Trust was measured using a single item on a questionnaire.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's following distance was directly manipulated to assess its impact on user experience and trust.,Trust was significantly higher at a 150 cm distance compared to a 50 cm distance.,"The study found that a direct transfer of human-human interaction conventions to human-robot interaction may not be feasible, as the 50 cm distance was perceived as too close, even when the robot carried a personal item. The 200 cm distance was perceived as further away than human-human following.",Participants showed significantly higher trust in the robot when it followed at a distance of 150 cm compared to 50 cm.,The robot followed the participant while carrying a personal item. The participant walked along a predefined path in a hallway.,ANOVA; t-test,"The study used a one-factorial repeated-measures ANOVA to analyze the questionnaire data, examining the effect of different following distances on perceived comfort, expectancy conformity, safety, unobtrusiveness, trust, and likeability. Post hoc comparisons with Bonferroni correction were conducted for significant ANOVA results. One-sample t-tests were used to analyze the overall evaluation of the distance and the comparability to a human's walking distance, comparing the means against a scale mean of 3.",TRUE,Robot-nonverbal-communication,Robot-nonverbal-communication,,"The study manipulated the robot's following distance, which is a form of nonverbal communication through proxemics. The paper states, 'In a within-subject design, human-robot distance was varied within the personal space (0.5 and 1.0 m) and social space (1.5 and 2.0 m).' This manipulation of distance directly impacted trust, as the results showed that 'Only the 150 cm condition showed significantly higher trust scores than the 50 cm condition.' The other factors were not manipulated in the study.",,http://link.springer.com/10.1007/978-3-030-60700-5_58,"The spatial behavior of robots working alongside humans critically inﬂuences the experience of comfort and personal space of users. The spatial behavior of service robots is especially important, as they move in close proximity to their users. To identify acceptable spatial behavior of Follow Me robots, we conducted an experimental study with 24 participants. In a within-subject design, human-robot distance was varied within the personal space (0.5 and 1.0 m) and social space (1.5 and 2.0 m). In all conditions, the robot carried a personal item of the participants. After each condition, the subjective experience of users in their interaction with the robot was assessed on the dimensions of trust, likeability, human likeness, comfort, expectation conformity, safety, and unobtrusiveness. The results show that the subjective experience of participants during the interaction with the Follow Me robot was generally more positive in the social distance conditions (1.5 and 2.0 m) than in the personal distance conditions (0.5 and 1 m). Interestingly, the following behavior was not perceived as comparable to humanhuman following behavior in the 0.5 and 2.0 m conditions, which were rated as either closer than human following or further away. This result, in combination with the more positive user experience in the social space conditions, illustrates that an exact transfer of interaction conventions from human-human interaction to human-robot interaction may not be feasible. And while users generally rate the interaction with Follow Me robots as positive, the following-distance of robots will need to be considered to optimize robot-behavior for user acceptance."
"Siegel, Mikey; Breazeal, Cynthia; Norton, Michael I.",Persuasive Robotics: The influence of robot gender on human behavior,2009,1,134,134,0,No participants were excluded,Real-World Environment,between-subjects,"Participants were recruited at a museum, given $5, interacted with a robot that gave an educational presentation and a donation request, and then completed a questionnaire.",Participants were asked to listen to a robot's presentation and then decide whether to donate some of the money they were given.,Unspecified,Humanoid Robots; Mobile Manipulators; Expressive Robots,Research; Social; Educational,Social,Persuasion,minimal interaction,Participants interacted with the robot through a presentation and a donation request.,real-world,The interaction took place in a real-world museum setting with a physical robot.,physical,The robot was a physical entity present in the interaction.,pre-programmed (non-adaptive),The robot followed a pre-programmed script during the interaction.,Questionnaires; Behavioral Measures,,,Trust was measured using a questionnaire and a behavioral measure of donation.,no modeling,No computational model of trust was used in this study.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's gender was manipulated using a male or female voice, and the social context was manipulated by whether the participant was alone or accompanied.","Men found the female robot more trustworthy, while women showed no preference. The presence of other visitors influenced women's donation behavior.","Men showed a strong preference for the female robot in terms of trust, credibility, and engagement, while women showed little preference or a slight preference for the male robot. Women's donation behavior changed based on whether they were alone or accompanied.","Men were more likely to donate money to the female robot, and they also rated the female robot as more credible, trustworthy, and engaging.",The robot gave a presentation about its capabilities and then asked for a donation. The human listened to the presentation and then decided whether to donate some of the money they were given.,chi-square analysis; ANOVA; t-test,"The study used Chi-Square analysis to examine the relationship between robot gender and donation behavior, treating donation as a binary variable (gave nothing vs. gave something). ANOVA was used to analyze the effects of robot gender and subject gender on credibility, trust, and engagement, which were measured using Likert scales. T-tests were used to further investigate the interaction effects of robot gender and subject gender on trust and engagement by comparing the means of the groups.",TRUE,Robot-verbal-communication-style; Task-environment,Robot-verbal-communication-style,Task-environment,"The study manipulated the robot's gender by using a pre-recorded male or female voice, which is a manipulation of the style of verbal communication, thus 'Robot-verbal-communication-style'. The study also considered whether the participant was alone or accompanied by other museum visitors, which is a manipulation of the 'Task-environment'. The results showed that the robot's gender (voice) significantly impacted trust, with men finding the female robot more trustworthy, while women showed no preference. The presence of other visitors influenced women's donation behavior, but not their reported trust, thus 'Task-environment' did not impact trust.",10.1109/IROS.2009.5354116,http://ieeexplore.ieee.org/document/5354116/,"Persuasive Robotics is the study of persuasion as it applies to human-robot interaction (HRI). Persuasion can be generally deﬁned as an attempt to change another’s beliefs or behavior. The act of inﬂuencing others is fundamental to nearly every type of social interaction. Any agent desiring to seamlessly operate in a social manner will need to incorporate this type of core human behavior. As in human interaction, myriad aspects of a humanoid robot’s appearance and behavior can signiﬁcantly alter its persuasiveness – this work will focus on one particular factor: gender. In the current study, run at the Museum of Science in Boston, subjects interacted with a humanoid robot whose gender was varied. After a short interaction and persuasive appeal, subjects responded to a donation request made by the robot, and subsequently completed a post-study questionnaire. Findings showed that men were more likely to donate money to the female robot, while women showed little preference. Subjects also tended to rate the robot of the opposite sex as more credible, trustworthy, and engaging. In the case of trust and engagement the effect was much stronger between male subjects and the female robot. These results demonstrate the importance of considering robot and human gender in the design of HRI."
"Simon, Loïck; Guérin, Clément; Rauffet, Philippe; Chauvin, Christine; Martin, Éric",How Humans Comply With a (Potentially) Faulty Robot: Effects of Multidimensional Transparency,2023,1,53,53,0,No participants were excluded,Controlled Lab Environment,within-subjects,Participants supervised a simulated order preparation line where a cobot requested resources. Participants decided whether to comply with the cobot's request based on different transparency conditions. They also completed a secondary task.,"Participants decided whether to comply with a cobot's request to transfer resources, while also completing a secondary visual pattern matching task.",Unspecified,Collaborative Robots (Cobots),Industrial; Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated robot through a computer interface.,simulation,The interaction was conducted in a simulated environment.,simulated,The robot was represented through a software interface.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions without adapting to the user.,Questionnaires; Behavioral Measures,Jian et al. Trust Scale,Performance Metrics,Trust was assessed using a questionnaire and behavioral measures of compliance.,"parametric models (e.g., regression)","The study used logistic regression to analyze the effects of transparency on compliance, trust, and risk perception.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the robot's transparency about its limitations (rTOh) and the impact of its actions on a teammate (rOFh), influencing how participants perceived the robot's reliability and the risk of compliance.","Transparency about the robot's limitations (rTOh L3) decreased compliance, but did not significantly impact subjective trust. Transparency about the impact on the teammate (rOFh) influenced both compliance and perceived risk, with negative feedback decreasing compliance and increasing perceived risk. Positive feedback increased trust.","The study found that rTOh transparency (revealing the robot's myopia) decreased compliance, while rOFh transparency (showing the impact on a teammate) significantly moderated compliance and risk perception. The study also found that subjective trust was not affected by rTOh transparency, but was affected by rOFh transparency.","Transparency about a robot's limitations decreases compliance, while transparency about the impact of the robot's actions on others significantly moderates compliance and risk perception.","The robot requested resources from the human participant. The human participant decided whether to comply with the robot's request, while also completing a secondary visual pattern matching task.",Logistic regression; Logistic regression; linear mixed effects analyses; Mann-Whitney U,"The study used logistic regression to analyze the effects of rTOh and rOFh transparency on compliance. Ordinal logistic regression was used to analyze the effects of transparency on trust and risk perception. Linear mixed effects analyses were used to study the effects on continuous variables such as completion time and task completion. Mann-Whitney analyses were used to examine the relationship between compliance, trust, and perceived risk.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the robot's transparency about its limitations (rTOh) and the impact of its actions on a teammate (rOFh). Specifically, rTOh was manipulated at two levels: L2 where the robot provides a request and its rationale but is not transparent about its myopia, and L3 where the robot specifies that it has not taken the warehouse stock into account. rOFh was manipulated at three levels: L0 with no rOFh transparency, L1+ with positive feedback on the teammate's capacity factor, and L1- with negative feedback on the teammate's capacity factor. These manipulations directly altered the information communicated by the robot to the human operator, which falls under the category of 'Robot-verbal-communication-content'. The study found that rOFh transparency significantly impacted trust, with positive feedback increasing trust and negative feedback decreasing trust. rTOh transparency did not significantly impact subjective trust, but did impact compliance. Therefore, 'Robot-verbal-communication-content' is listed as a factor that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1109/THMS.2023.3273773,https://ieeexplore.ieee.org/document/10136126/,"This article deals with how a human operator follows a request made by a robot in an industrial context. This robot is potentially myopic, i.e., its request could be based on partial and limited information. Therefore, it could possibly be faulty. This exploratory study aims to analyze whether multidimensional agent transparency may have an effect on two drivers of compliance identiﬁed in the literature: trust in signal and risk perception. In this experiment, we manipulated different agent transparency levels combined with two dimensions of agent transparency: robot-TOhuman transparency (rTOh) and robot-OF-human transparency (rOFh). Results mainly show that adding rOFh to rTOh transparency changes human compliance with the agent and moderates both trust and risk perception. Moreover, task performance and completion time are shown to vary according to the different transparency levels. Our results show that transparency has no effect on mental workload. From a methodological perspective, this article shows the importance of distinguishing the different types of information about which a robot can be transparent, especially the combination of rTOh and rOFh transparencies. From a practical point of view, the article shows that the agent transparency framework needs to be considered carefully when designing human-robot teaming in the context of Industry 4.0 in the case of robots that may be unreliable due to their myopia."
"Tyshka, Alexander; Louie, Wing-Yue Geoffrey",Interactive Task Learning for Social Robots: A Pilot Study,2023,1,34,34,0,No participants were excluded,Survey/Interview,,"Interviews were conducted with 34 stakeholders, including robotics researchers, nursing scientists, and healthcare practitioners, to gather their perspectives on social, ethical, and legal concerns related to the use of GARMI and other healthcare robots. The interviews were analyzed using the Grounded Theory framework.","Participants were asked about their social, ethical, and legal concerns regarding GARMI and healthcare robotics.",GARMI,Humanoid Robots; Service and Assistive Robots,Care; Research,Evaluation,Survey/Questionnaire Completion,passive observation,"Participants were interviewed about their perceptions of the robot, without direct interaction.",media,The study involved text-based descriptions of the robot and its use.,hypothetical,"The robot was described in the interviews, but no physical robot was present.",not autonomous,The robot's actions were not directly observed or controlled in the study.,Questionnaires,,,"Trust was assessed through qualitative interviews, without specific trust measures.",no modeling,No computational model of trust was developed in this study.,Observational & Survey Studies,Qualitative Interviews,No Manipulation,The study did not manipulate any factors related to trust; it explored stakeholder perceptions.,"The study did not directly measure the impact of manipulations on trust, but rather explored stakeholder concerns about accountability and transparency.","The interviews revealed that accountability was a major concern for stakeholders, highlighting a potential 'accountability gap' in healthcare robotics.","Stakeholders expressed significant concerns about accountability and transparency in healthcare robotics, emphasizing the need for data recorders to enhance these aspects.","The robot, GARMI, is a humanoid service robot intended to assist in healthcare. The human participants were interviewed about their perceptions of the robot and its use in healthcare settings.",,"The study used a qualitative approach, analyzing interview transcripts and field notes using the Grounded Theory framework. No statistical tests were applied.",FALSE,,,,"The study did not manipulate any factors. The interviews were conducted to gather stakeholder perspectives on social, ethical, and legal concerns related to the use of GARMI and other healthcare robots. The focus was on understanding their perceptions and concerns, not on manipulating any variables to observe their impact on trust. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust in the context of experimental manipulation.",10.1109/IROS55552.2023.10341713,https://ieeexplore.ieee.org/document/10341713/,"For socially assistive robots to achieve widespread adoption, the ability to learn new tasks in the wild is critical. Learning from Demonstration (LfD) approaches are a popular method for learning in the wild, but current methods require significant amounts of data and can be difficult to interpret. Interactive Task Learning (ITL) is an emerging learning paradigm that aims to teach tasks in a structured manner, minimizing the need for data and increasing transparency. However, to date ITL has only been explored for physical robotics applications. Additionally, minimal research has explored how usable existing ITL systems are for non-expert users. In this work, we propose a novel approach to learn social tasks via ITL. This system utilizes recent advances in Natural Language Understanding (NLU) to learn from natural dialogue. We conducted a pilot study to compare the ITL system against an LfD approach to investigate differences in teaching performance as well as teachers’ perceptions of trust and workload towards these systems. Additionally, we analyzed the teaching behavior of participants to identify successful and unsuccessful teaching strategies. Our findings suggest ITL could provide more transparency to users and improve performance by correcting speech recognition errors. However, participants generally preferred LfD and found it an easier teaching method. From the observed teaching behavior, we identify existing challenges in ITL for non-experts to teach social tasks. Using this, we propose areas of improvement toward future ITL learning paradigms that are intuitive, transparent, and performant."
"Smakman, Matthijs H. J.; Vanegas, Daniel F. Preciado; Smit, Koen; Leewis, Sam; Okkerse, Youri; Obbes, Jesper; Uffing, Thom; Soliman, Marina; Van Der Krogt, Tony; Tönjes, Lucas",A Trustworthy Robot Buddy for Primary School Children,2022,2,55,55,0,No participants were excluded,Educational Setting,between-subjects,"Children were introduced to the robot, completed a pre-interaction questionnaire, interacted with the robot, and then completed a post-interaction trust questionnaire.",Children interacted with a robot that had one of five different dialogue options.,SAMBuddy Storytelling Cuddle,Expressive Robots,Social; Research,Social,Conversation,minimal interaction,Children had a verbal interaction with the robot.,real-world,Children interacted with a physical robot in a real-world setting.,physical,The robot was a physical plush toy with basic hardware components.,pre-programmed (non-adaptive),The robot followed a fixed dialogue script.,Questionnaires,,,Trust was measured using a questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's voice intonation, gender, and humor were manipulated to see how they affected trust.","Overall, children showed high levels of trust regardless of the robot's features, but there was a trend that girls trusted the humorous robot less than boys.","There was a trend that girls trusted the humorous robot less than boys, and children with prior robot experience showed a trend of higher trust after the interaction with the humorous and monotonous female robot.","Children showed high levels of trust in the robot regardless of the robot's voice intonation, gender, or humor, suggesting that social trust is less affected by these features.","The robot introduced itself and engaged in small talk with the child, while the child answered the robot's questions and pressed a button to continue the dialogue.",Multilevel Model; cronbach's alpha; unianova,"A factor analysis was conducted to check the reliability of the trust scale, followed by an internal consistency test using Cronbach's alpha. The initial reliability test resulted in an unacceptable Cronbach's alpha, so two questions were removed to raise the reliability. Finally, a UNIANOVA test was performed to compare gender and robot type to check for differences in trust.",TRUE,Robot-verbal-communication-style; Robot-emotional-display,Robot-verbal-communication-style,Robot-emotional-display,"The study manipulated the robot's voice intonation (monotone vs. spontaneous) and the presence of humor in the dialogue. These are classified as 'Robot-verbal-communication-style' because they relate to how the robot communicates verbally (tone, style). The study also manipulated the gender of the robot's voice, which is classified as 'Robot-emotional-display' because it is a social cue that can influence how the robot is perceived. The results showed that the humor aspect of the robot's dialogue (a component of 'Robot-verbal-communication-style') had a trend of negatively impacting trust, especially for girls, while the gender of the voice ('Robot-emotional-display') did not significantly impact trust.",10.3390/mti6040029,https://www.mdpi.com/2414-4088/6/4/29,"Social robots hold potential for supporting children’s well-being in classrooms. However, it is unclear which robot features add to a trustworthy relationship between a child and a robot and whether social robots are just as able to reduce stress as traditional interventions, such as listening to classical music. We set up two experiments wherein children interacted with a robot in a real-life school environment. Our main results show that regardless of the robotic features tested (intonation, male/female voice, and humor) most children tend to trust a robot during their ﬁrst interaction. Adding humor to the robots’ dialogue seems to have a negative impact on children’s trust, especially for girls and children without prior experience with robots. In comparing a classical music session with a social robot interaction, we found no signiﬁcant differences. Both interventions were able to lower the stress levels of children, however, not signiﬁcantly. Our results show the potential for robots to build trustworthy interactions with children and to lower children’s stress levels. Considering these results, we believe that social robots provide a new tool for children to make their feelings explicit, thereby enabling children to share negative experiences (such as bullying) which would otherwise stay unnoticed."
"Smakman, Matthijs H. J.; Vanegas, Daniel F. Preciado; Smit, Koen; Leewis, Sam; Okkerse, Youri; Obbes, Jesper; Uffing, Thom; Soliman, Marina; Van Der Krogt, Tony; Tönjes, Lucas",A Trustworthy Robot Buddy for Primary School Children,2022,2,60,60,0,No participants were excluded,Educational Setting,between-subjects,"Children were introduced to the robot or music, completed a pre-interaction questionnaire, interacted with the robot or listened to music, and then completed a post-interaction questionnaire.",Children interacted with a robot or listened to classical music to assess stress reduction.,SAMBuddy Storytelling Cuddle,Expressive Robots,Social; Research,Social,Conversation,minimal interaction,Children had a verbal interaction with the robot.,real-world,Children interacted with a physical robot in a real-world setting.,physical,The robot was a physical plush toy with basic hardware components.,pre-programmed (non-adaptive),The robot followed a fixed dialogue script.,,,,Trust was not directly measured in this experiment.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The study compared a robot interaction to a classical music intervention to see which reduced stress more.,"Trust was not directly measured, but the robot was found to be as effective as classical music in reducing stress.","Girls showed more signs of discomfort compared to boys during the robot interaction, and children smiled more during the robot interaction compared to the music session.","The robot was as effective as classical music in reducing children's stress levels, suggesting that simple robots can be a viable option for stress reduction.","The robot introduced itself and asked the child about their likes and hobbies, while the child answered the robot's questions and pressed a button to continue the dialogue.",ANOVA; unianova,"A General Linear Model (GLM) was used to explore the differences in children's comfort levels before and after the music and robot intervention, also considering age and gender. A GLM test was also used to explore the differences in observations for gender, age, and intervention type. A UNIANOVA test was conducted to explore how children experienced the robot in relation to the changes in stress levels before and after the robot, taking into account gender and age.",TRUE,Task-environment,,,"This study compared a robot interaction to a classical music intervention. The manipulation here is the 'Task-environment' because the children were either in a robot interaction environment or a music listening environment. The study did not directly measure trust, but rather focused on stress reduction. Therefore, no factors were found to impact or not impact trust.",10.3390/mti6040029,https://www.mdpi.com/2414-4088/6/4/29,"Social robots hold potential for supporting children’s well-being in classrooms. However, it is unclear which robot features add to a trustworthy relationship between a child and a robot and whether social robots are just as able to reduce stress as traditional interventions, such as listening to classical music. We set up two experiments wherein children interacted with a robot in a real-life school environment. Our main results show that regardless of the robotic features tested (intonation, male/female voice, and humor) most children tend to trust a robot during their ﬁrst interaction. Adding humor to the robots’ dialogue seems to have a negative impact on children’s trust, especially for girls and children without prior experience with robots. In comparing a classical music session with a social robot interaction, we found no signiﬁcant differences. Both interventions were able to lower the stress levels of children, however, not signiﬁcantly. Our results show the potential for robots to build trustworthy interactions with children and to lower children’s stress levels. Considering these results, we believe that social robots provide a new tool for children to make their feelings explicit, thereby enabling children to share negative experiences (such as bullying) which would otherwise stay unnoticed."
"Soh, Harold; Pan, Shu; Min, Chen; Hsu, David",The Transfer of Human Trust in Robot Capabilities across Tasks,2018,1,32,31,1,1 participant's results were removed due to a failure to pass attention/consistency check questions,Controlled Lab Environment,within-subjects,"Participants grouped tasks, indicated pre-observation trust, observed robot demonstrations, and indicated post-observation trust.",Participants observed a robot performing household tasks or an autonomous vehicle performing driving tasks and rated their trust in the robot's ability to perform these tasks.,Fetch Robot,Mobile Manipulators; Autonomous Vehicles,Research,Manipulation,Object Passing,passive observation,Participants observed the robot performing tasks.,simulation,Participants observed a virtual simulation of an autonomous vehicle.,physical,Participants observed a physical Fetch robot and a simulated autonomous vehicle.,pre-programmed (non-adaptive),The robot performed pre-programmed actions.,Questionnaires; Custom Scales,Muir's Trust Questionnaire,,Trust was measured using a 7-point Likert scale and Muir's questionnaire.,"deep learning (e.g., neural networks, reinforcement learning)",Trust was modeled using a recurrent neural network and a Bayesian Gaussian process.,Empirical HRI Studies,Mixed Reality Studies,Direct Manipulation,"The researchers manipulated robot performance (success/failure), task difficulty (easy/difficult), and task category (household/driving) to observe their effect on trust transfer.","Trust was higher for similar tasks, and trust change was greater for similar tasks. Positive trust transferred more easily to simpler tasks.","Trust transfer occurred even between different task categories, although to a lesser extent. The effect of difficulty on trust transfer was reversed for failure conditions.","Human trust transfers across tasks, with similar tasks sharing similar levels of trust, and observations of robot performance changing trust in both observed and similar tasks.","The robot performed either pick and place or navigation tasks in a household setting, or driving and parking maneuvers in a virtual driving environment. Participants observed the robot and rated their trust in its ability to perform the tasks.",t-test,"The study used t-tests to compare trust distances between tasks in the same category and tasks in different categories, as well as to compare the change in trust due to performance observations between similar and dissimilar tasks. T-tests were also used to determine if the trust change for different task categories was significantly different from zero. The study also analyzed the relationship between perceived difficulty and trust transfer, but did not report specific statistical tests for this analysis.",TRUE,Robot-accuracy; Task-complexity; Task-environment,Robot-accuracy; Task-complexity,,"The researchers manipulated 'Robot-accuracy' by showing the robot either successfully completing tasks or failing at them. This is explicitly stated in the paper: 'In both domains, we developed pre-programmed success and failure demonstrations of robot performance.' The paper also manipulated 'Task-complexity' by having easy and difficult versions of the tasks, as described in the experimental design section: 'We explored three factors as independent variables: task category, task difficulty, and robot performance. Each independent variable consisted of two levels: two task categories, easy/difficult tasks, and robot success/failure.' The 'Task-environment' was also manipulated by using two different environments: a household setting with a physical robot and a virtual driving environment. This is described in the paper: 'We used tasks in two domains, each with an appropriate robot...Household, which included two common categories of household tasks...Driving, where we used a Virtual Reality (VR) environment to simulate an autonomous vehicle (AV)'. The results showed that 'Robot-accuracy' (success/failure) and 'Task-complexity' (easy/difficult) impacted trust levels, as described in the results section: 'The trust change is significantly greater for SG than DG...We analyzed the relationship between perceived difficulty and trust transfer (H3) by first splitting the data into two conditions: participants who received successful demonstrations, and those that observed failures'. There was no mention of any factor that did not impact trust.",10.15607/RSS.2018.XIV.033,http://www.roboticsproceedings.org/rss14/p33.pdf,"Trust is crucial in shaping human interactions with one another and with robots. This work investigates how human trust in robot capabilities transfers across tasks. We present a human-subjects study of two distinct task domains: a Fetch robot performing household tasks and a virtual reality simulation of an autonomous vehicle performing driving and parking maneuvers. Our ﬁndings lead to a functional view of trust and two novel predictive models—a recurrent neural network architecture and a Bayesian Gaussian process—that capture trust evolution and transfer via latent task representations. Experiments show that the two proposed models outperform existing approaches when predicting trust across unseen tasks and participants. These results indicate that (i) a task-dependent functional trust model captures human trust in robot capabilities more accurately, and (ii) trust transfer across tasks can be inferred to a good degree. The latter enables trust-based robot decision-making for ﬂuent human-robot interaction. In particular, our models can be used to derive robot policies that mitigate under-trust or over-trust by human teammates in collaborative multi-task settings."
"Soh, Harold; Pan, Shu; Chen, Min; Hsu, David",Trust Dynamics and Transfer across Human-Robot Interaction Tasks: Bayesian and Neural Computational Models,2019,1,32,31,1,1 participant's results were removed due to a failure to pass attention/consistency check questions,Controlled Lab Environment,within-subjects,"Participants first indicated their trust on three 'tested tasks'. Then, they observed two tasks from a specific category and difficulty, and indicated their trust in the robot for those tasks. Finally, they re-indicated their trust on the three tested tasks.","Participants observed a Fetch robot performing household tasks and a simulated autonomous vehicle performing driving maneuvers, and rated their trust in the robot's ability to perform these tasks.",Fetch Robot,Mobile Manipulators; Autonomous Vehicles,Research,Manipulation,Object Passing,passive observation,Participants observed the robot performing tasks.,simulation,Participants interacted with a simulated autonomous vehicle via a VR headset.,physical,A physical Fetch robot was used for the household tasks.,fully autonomous (limited adaptation),"The robot performed tasks autonomously, but with limited adaptation.",Custom Scales,,Performance Metrics,Trust was measured using subjective ratings of the robot's capability to perform specific tasks.,"deep learning (e.g., neural networks, reinforcement learning)",Trust was modeled using a recurrent neural network and a Bayesian Gaussian process model.,Empirical HRI Studies,Mixed Reality Studies,Direct Manipulation,The researchers manipulated robot performance (success/failure) and task difficulty (easy/difficult) to observe their effect on trust.,"Trust transferred more between similar tasks, and was influenced by both task difficulty and robot performance. Trust decreased more for difficult tasks after observing a failure.","Trust changes were greater for similar tasks compared to dissimilar tasks, and trust transferred between task categories, but to a lesser extent. The effect of failure on trust was greater for difficult tasks.","Human trust transfers across tasks, and similar tasks are more likely to share a similar level of trust.","The Fetch robot performed household tasks such as picking and placing objects and navigating, while the simulated autonomous vehicle performed driving and parking maneuvers. Participants observed these tasks and rated their trust in the robot's ability to perform them.",t-test; one-sample t-test,"The study used t-tests to compare trust distances between similar and dissimilar task categories in both household and driving domains. One-sample t-tests were used to determine if trust changes for dissimilar tasks were significantly different from zero. Additionally, t-tests were used to compare trust distances for easy and difficult tasks under success and failure conditions.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy; Task-complexity,,"The study explicitly manipulated robot performance (success/failure), which directly impacts the robot's accuracy in completing the task, thus 'Robot-accuracy' is selected. The study also manipulated task difficulty (easy/difficult), which directly relates to the cognitive demands of the task, thus 'Task-complexity' is selected. The paper states that 'observations of robot performance have a greater affect on the trust dynamics over similar tasks compared to dissimilar tasks' and 'The trust change for SG is significantly greater than DG in both domains' which indicates that both robot accuracy and task complexity impacted trust. The paper also states 'For the success condition, the trust distance among tasks was significantly less for tasks perceived to be easier than the observed task in both the household domain and driving domain' and 'For the failure condition, the results were not statistically significant (at the α = 1% level), but suggest that belief in robot inability would transfer more to difficult tasks compared to simpler tasks' which further supports that both robot accuracy and task complexity impacted trust. There were no factors that were manipulated that did not impact trust.",10.24963/ijcai.2019/868,https://www.ijcai.org/proceedings/2019/868,"This work contributes both experimental ﬁndings and novel computational human-robot trust models for multi-task settings. We describe Bayesian non-parametric and neural models, and compare their performance on data collected from realworld human-subjects study. Our study spans two distinct task domains: household tasks performed by a Fetch robot, and a virtual reality driving simulation of an autonomous vehicle performing a variety of maneuvers. We ﬁnd that human trust changes and transfers across tasks in a structured manner based on perceived task characteristics. Our results suggest that task-dependent functional trust models capture human trust in robot capabilities more accurately, and trust transfer across tasks can be inferred to a good degree. We believe these models are key for enabling trust-based robot decisionmaking for natural human-robot interaction."
"Soh, Harold; Xie, Yaqi; Chen, Min; Hsu, David",Multi-task trust transfer for human–robot interaction,2020,1,32,31,1,1 participant's results were removed owing to a failure to pass attention/consistency check questions,Controlled Lab Environment,within-subjects,"Participants were introduced to the robot, completed a pre-observation questionnaire, observed robot performance, and completed a post-observation questionnaire and debrief.",Participants observed a robot performing household tasks or an autonomous vehicle performing driving tasks and rated their trust in the robot's ability to perform specific tasks.,Fetch Robot,Mobile Manipulators; Autonomous Vehicles,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants observed the robot or a simulation of the robot performing tasks.,simulation,Participants observed a real robot or a virtual reality simulation of a robot.,physical,Participants observed a physical robot or a simulation of a robot.,pre-programmed (non-adaptive),The robot performed pre-programmed success and failure demonstrations.,Questionnaires; Custom Scales,Muir's Trust Questionnaire,Other Data,Trust was measured using a 7-point Likert scale and Muir's questionnaire.,"deep learning (e.g., neural networks, reinforcement learning)","Trust was modeled using a Bayesian Gaussian process model, a recurrent neural network model, and a hybrid model.",Empirical HRI Studies,Mixed Reality Studies,Direct Manipulation,The researchers manipulated robot performance (success/failure) and task difficulty (easy/difficult) to observe their effect on trust transfer across tasks.,"Trust was higher for similar tasks, easier tasks, and successful robot performance; distrust transferred more readily to difficult tasks.","The study found that trust transfers across tasks, but the degree of transfer depends on task similarity and difficulty. The effect of failure on trust transfer was not statistically significant at the a = 1% level, but suggested that the effect was reversed; belief in robot inability would transfer more to difficult tasks compared with simpler tasks.","Human trust transfers across tasks, with similar tasks sharing similar levels of trust, and trust transfer being asymmetric, with positive trust transferring more easily to simpler tasks.","The robot performed household tasks (picking and placing objects, navigation) or driving tasks (lane merging, parking), while participants observed and rated their trust in the robot's ability to perform these tasks.",ANOVA; t-test; one-sample t-test; Pearson correlation; polynomial contrast model,The study used ANOVA to analyze the effects of task category and difficulty on trust distance and trust change. T-tests were used to compare trust distances and changes between different task groups (same vs. different category). One-sample t-tests were used to determine if trust change for different groups was non-zero. Pearson correlation was used to assess the relationship between task-specific and general trust. A polynomial contrast model was used to analyze the effect of participant characteristics on initial trust and trust change.,TRUE,Robot-accuracy; Task-complexity,Robot-accuracy; Task-complexity,,"The study explicitly manipulated robot performance (success/failure) and task difficulty (easy/difficult) to observe their effect on trust transfer across tasks. 'Robot-accuracy' was chosen because the manipulation directly affected the robot's success or failure in completing the task. 'Task-complexity' was chosen because the study manipulated the perceived difficulty of the tasks, which directly relates to the cognitive demands placed on the robot. The paper states, 'We explored three factors as independent variables: task category, task difficulty, and robot performance. Each independent variable consisted of two levels: two task categories, easy/difficult tasks, and robot success/failure.' The results section also confirms that both robot performance and task difficulty had a significant impact on trust, as stated: 'We also found a significant effect of category on trust change F(1, 89) = 24:89, p\10 À6 , and of success/failure outcomes on trust change F(1, 89) = 10:04, p = 0:002.' and 'For the success condition, the trust distance among the household tasks was significantly less for tasks perceived to be easier than the observed task (M = 2:0, SE = 0:27), compared with tasks that were perceived to be more difficult (M = 0:5, SE = 0:27), t(14) = 4:58, p\10 À3 .'",10.1177/0278364919866905,http://journals.sagepub.com/doi/10.1177/0278364919866905,"Trust is essential in shaping human interactions with one another and with robots. In this article we investigate how human trust in robot capabilities transfers across multiple tasks. We present a human-subject study of two distinct task domains: a Fetch robot performing household tasks and a virtual reality simulation of an autonomous vehicle performing driving and parking maneuvers. The findings expand our understanding of trust and provide new predictive models of trust evolution and transfer via latent task representations: a rational Bayes model, a data-driven neural network model, and a hybrid model that combines the two. Experiments show that the proposed models outperform prevailing models when predicting trust over unseen tasks and users. These results suggest that (i) task-dependent functional trust models capture human trust in robot capabilities more accurately and (ii) trust transfer across tasks can be inferred to a good degree. The latter enables trust-mediated robot decision-making for fluent human–robot interaction in multi-task settings."
"Song, Yao; Luximon, Yan",The face of trust: The effect of robot face ratio on consumer preference,2021,1,240,240,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were informed about the study, provided demographic information, and were randomly assigned to one of six scenarios. They were then exposed to stimuli and completed a questionnaire. Finally, they were informed that they had completed the task.",Participants viewed images of robots with varying facial width-to-height ratios (fWHR) and face shapes and then completed a questionnaire assessing perceived trustworthiness and purchase intentions.,Unspecified,Humanoid Robots,Research,Evaluation,Rating,passive observation,Participants observed images of robots without direct interaction.,media,Participants viewed static images of robots.,simulated,The robots were presented as images.,not autonomous,The robots were presented as static images and did not perform any actions.,Questionnaires,Mayer and Davis' Trust/Trustworthiness Scales (1999),,Trust was measured using a questionnaire based on the dimensions of trustworthiness.,"parametric models (e.g., regression)","The study used regression analysis to examine the relationship between fWHR, perceived trustworthiness, and purchase intentions.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers directly manipulated the facial width-to-height ratio (fWHR) and face shape of the robot images to examine their effect on perceived trustworthiness and purchase intentions.,Robots with high fWHR were perceived as more trustworthy than those with low fWHR. Face shape did not significantly impact trust.,"The study found a counterintuitive effect of fWHR on robot trustworthiness, where high fWHR increased perceived trustworthiness, unlike in human facial evaluations. There was no significant effect of face shape on trustworthiness or purchase intentions.","The facial width-to-height ratio (fWHR) of a robot significantly influences perceived trustworthiness and purchase intentions, with higher fWHR leading to increased trust and purchase intent, which is mediated by perceived trustworthiness.",Participants viewed images of robots with different facial features and rated their trustworthiness and purchase intentions. The robot did not perform any actions.,ANOVA; Linear regression,"The study used one-way ANOVA to check the manipulation of fWHR and face shape. A two-way ANOVA was performed to analyze the effect of fWHR and face shape on perceived trustworthiness and purchase intentions. Regression analysis was used to examine the mediating role of perceived trustworthiness in the relationship between fWHR and purchase intentions. Additionally, two-way ANOVAs were conducted to examine the effect of fWHR and face shape on the three dimensions of trustworthiness: ability, benevolence, and integrity.",TRUE,Robot-aesthetics,Robot-aesthetics,,"The study manipulated the facial width-to-height ratio (fWHR) and face shape of the robot images. These manipulations fall under the category of 'Robot-aesthetics' because they directly alter the visual appearance and design of the robot's face. The paper states, 'Our study conducted a 2 (face shape) * 3 (fWHR) full factorial between-subject experiment...' and 'We recruited a designer to make all of the experiment stimuli (see Fig. 2). During the design process, we instructed the designer to control for potential confounding factors, such as robot facial expression, facial features, body height and width, color tone, and background.' The results showed that fWHR significantly impacted perceived trustworthiness, as stated in the paper: 'The results of the two-way ANOVA showed that the main effect of fWHR on trustworthiness evaluation was significant (F (2, 234) = 6.01, p < 0.01)... Specifically, post hoc tests revealed that robots with high fWHR and medium fWHR showed significantly higher trustworthiness perceptions than those with low fWHR.' Therefore, 'Robot-aesthetics' is listed as a factor that impacted trust. The study found that face shape did not significantly impact trust: '...while the effect of face shape (F (1, 234) = 0.28, p = 0.60) and the interaction effect were not significant (F (2, 234) = 0.12, p = 0.89).' Therefore, face shape, which is also part of 'Robot-aesthetics', is not listed as a factor that did not impact trust, because the manipulation was not on face shape alone, but on the combination of face shape and fWHR.",10.1016/j.chb.2020.106620,https://linkinghub.elsevier.com/retrieve/pii/S0747563220303678,"As one of the latest applications in the era of artificial intelligence, a social robot is playing a more and more important role in our daily life. However, few attempts to date have been made to explore how robot facial appearance, particularly the facial width-to-height ratio (fWHR) and face shape of a robot, influence people’s evaluation, such as perceived trustworthiness and purchase intention. Our study conducted a 2 (face shape) * 3 (fWHR) full factorial between-subject experiment to fill this research gap. 240 participants were recruited and randomly assigned to six scenarios in the study. After exposure to the stimuli, perceived trustworthiness and purchase intention were measured accordingly. Results suggested a counterintuitive phenomenon: unlike the effect of fWHR on human trustworthiness evaluation, high fWHR worked as a significant factor to improve robot trustworthiness and purchase intention. However, the effect of face shape and its interaction with fWHR did not significantly improve robot trustworthiness and purchase intention. In addition, the effect of fWHR on purchase intention was mediated by the robot trustworthiness. Theoretical and practical contributions are also discussed in this paper."
"Song, Yao; Luximon, Ameersing; Luximon, Yan",Facial Anthropomorphic Trustworthiness Scale for Social Robots: A Hybrid Approach,2023,1,200,125,75,incomplete responses or responses with only consecutive or extreme values (1-9),Online Crowdsourcing,,"Participants viewed a set of robot faces and answered questions about their trustworthiness. The study involved multiple phases: item generation, refinement, reduction, and validation using EFA and CFA.",Participants rated the trustworthiness of robot faces using a 9-point Likert scale.,Unspecified,Humanoid Robots,Research,Evaluation,Rating,passive observation,Participants observed robot faces through images.,media,Participants viewed static images of robot faces.,simulated,The robots were represented through images.,not autonomous,The robots did not perform any actions; they were static images.,Questionnaires; Custom Scales,,,Trust was measured using a custom scale developed in the study.,no modeling,The study did not model trust computationally; it used factor analysis.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,"The study did not manipulate any factors directly, but the appearance of the robot faces was the main focus of the study.","The study developed a scale to measure facial anthropomorphic trustworthiness, but did not directly test the impact of manipulations on trust.","The study identified four dimensions of facial anthropomorphic trustworthiness: ethics concern, capability, positive affect, and anthropomorphism.",The study developed a reliable and valid 17-item scale (FATSR-17) to measure facial anthropomorphic trustworthiness in social robots.,Participants viewed images of robot faces and rated their trustworthiness using a questionnaire. The robots did not perform any actions.,kaiser-meyer-olkin (kmo); bartlett's test of sphericity; Factor analysis; Multilevel Model; Structural equation modeling,The study used several statistical tests to develop and validate a scale for measuring facial anthropomorphic trustworthiness. The Kaiser-Meyer-Olkin (KMO) measure and Bartlett's test of sphericity were used to assess the suitability of the data for factor analysis. Exploratory Factor Analysis (EFA) was employed to explore the underlying structure of the items and identify the latent factors. Confirmatory Factor Analysis (CFA) was used to validate the factor structure derived from EFA. Structural Equation Modeling (SEM) was used to examine the fit of the proposed model with the sample data. These tests were used to ensure the reliability and validity of the developed scale.,FALSE,,,,"The study did not manipulate any factors. The study focused on developing a scale to measure facial anthropomorphic trustworthiness. Participants viewed images of robot faces and rated their trustworthiness. There was no intentional manipulation of any factor to observe its impact on trust. The study's focus was on identifying the dimensions of facial trustworthiness and developing a scale, not on manipulating variables to observe their effect on trust. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust due to manipulation.",10.3390/biomimetics8040335,https://www.mdpi.com/2313-7673/8/4/335,"Social robots serve as autonomous systems for performing social behaviors and assuming social roles. However, there is a lack of research focusing on the speciﬁc measurement of facial trustworthiness toward anthropomorphic robots, particularly during initial interactions. To address this research gap, a hybrid deep convolution approach was employed in this study, involving a crowdsourcing platform for data collection and deep convolution and factor analysis for data processing. The goal was to develop a scale, called Facial Anthropomorphic Trustworthiness towards Social Robots (FATSR-17), to measure the trustworthiness of a robot’s facial appearance. The ﬁnal measurement scale comprised four dimensions, “ethics concern”, “capability”, “positive affect”, and “anthropomorphism”, consisting of 17 items. An iterative examination and a reﬁnement process were conducted to ensure the scale’s reliability and validity. The study contributes to the ﬁeld of robot design by providing designers with a structured toolkit to create robots that appear trustworthy to users."
"Song, Yao; Tao, Da; Luximon, Yan",In robot we trust? The effect of emotional expressions and contextual cues on anthropomorphic trustworthiness,2023,2,35,35,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants viewed robot faces on a screen while eye-tracking data was recorded, then completed a questionnaire to evaluate perceived trustworthiness.",Participants were asked to evaluate the trustworthiness of a robot displaying different facial expressions.,Unspecified,Expressive Robots,Research,Evaluation,Trustworthiness Rating,passive observation,Participants observed the robot's facial expressions on a screen.,media,The interaction was limited to viewing static images of the robot's face.,simulated,The robot was presented as a simulated face on a screen.,pre-programmed (non-adaptive),The robot's facial expressions were pre-programmed and did not adapt to the user.,Questionnaires; Physiological Measures,,Eye-tracking Data,Trust was assessed using subjective ratings and eye-tracking measures.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's facial expressions (positive vs. negative, active vs. inactive) were directly manipulated to see how they affected trustworthiness.",Positive facial expressions led to higher perceived trustworthiness and visual attention.,"The study found that positive facial expressions led to higher trustworthiness and visual attention, while arousal did not have a significant effect. Participants focused more on the mouth region for positive expressions and the eye/brow region for negative expressions.",Positive facial expressions in robots lead to higher perceived trustworthiness and visual attention compared to negative expressions.,"The robot displayed different facial expressions on a screen, and the human participant observed these expressions and then rated the robot's trustworthiness.",ANOVA; within-subjects anova; pearson test,"A two-way ANOVA was used to analyze the effects of valence and arousal on trustworthiness, fixation duration, and fixation count. A within-subjects ANOVA was used to compare fixation durations between the eye/brow and mouth regions for different emotional expressions. A Pearson test was used to check for correlations between demographic information (age and gender) and trustworthiness.",TRUE,Robot-emotional-display,Robot-emotional-display,,"The study manipulated the robot's facial expressions, specifically the valence (positive vs. negative) and arousal (active vs. inactive) of the expressions. The paper states, 'For instance, positive expressions are created with an upturned and enlarged mouth, while negative expressions are created with a downturned and shrunk mouth with a frown. In addition, we also discussed the animation presentation of facial arousals, such as enlarged pupils.' This clearly indicates a manipulation of the robot's emotional display. The results showed that the valence of the emotional display significantly impacted trustworthiness, with positive expressions leading to higher perceived trustworthiness. Arousal did not have a significant impact on trust. Therefore, 'Robot-emotional-display' is the appropriate category for the manipulated factor and the factor that impacted trust. Since arousal did not impact trust, it is not included in the 'factors_that_impacted_trust' list.",10.1016/j.apergo.2023.103967,https://linkinghub.elsevier.com/retrieve/pii/S0003687023000054,"Following the evolution of technology and its application in various daily contexts, social robots work as an advanced artificial intelligence (AI) system to interact with humans. However, limited research has been done to discuss the role of emotional expressions and contextual cues in influencing anthropomorphic trustworthiness, especially from the design perspective. To address this research gap, the current study designed a specific robot prototype and conducted two lab experiments to explore the effect of emotional expressions and contextual cues on trustworthiness via a combination of subjective ratings and physiological measures. Results showed that: 1) positive (vs. negative) emotional expressions enjoyed a higher level of anthropomorphic trustworthiness and visual attention; 2) regulatory fit was expanded in parasocial interaction and worked as a prime to activate anthropomorphic trustworthiness for social robots. Theoretical contributions and design implications were also discussed in this study."
"Song, Yao; Tao, Da; Luximon, Yan",In robot we trust? The effect of emotional expressions and contextual cues on anthropomorphic trustworthiness,2023,2,35,35,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants interacted with a robot that delivered messages with different regulatory focuses and facial expressions, while EDA was recorded, and then they completed a questionnaire to evaluate perceived trustworthiness.",Participants were asked to evaluate the trustworthiness of a robot delivering messages in different contexts.,Unspecified,Expressive Robots,Research; Social,Social,Persuasion,minimal interaction,Participants interacted with a physical robot that delivered messages.,real-world,The interaction involved a physical robot in a lab setting.,physical,The robot was physically present during the interaction.,pre-programmed (non-adaptive),The robot's actions and messages were pre-programmed and did not adapt to the user.,Questionnaires; Physiological Measures,,Physiological Signals,Trust was assessed using subjective ratings and electrodermal activity (EDA) measurements.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's facial expressions and the regulatory focus of the message (promotion vs. prevention) were manipulated to see how they affected trustworthiness.,Regulatory fit (positive expressions in promotion-focused contexts and negative expressions in prevention-focused contexts) increased perceived trustworthiness and reduced EDA.,"The study found that regulatory fit significantly influenced trustworthiness and EDA, with a higher level of perceived trustworthiness and a lower level of EDA when there was a fit between the emotional expression and the regulatory focus. Arousal did not have a significant effect.",Regulatory fit between emotional expressions and contextual cues increases perceived trustworthiness and reduces stress (lower EDA) in human-robot interaction.,"The robot delivered messages with different regulatory focuses and facial expressions, and the human participant listened to the messages and then rated the robot's trustworthiness.",ANOVA,"A three-way ANOVA was used to analyze the effects of valence, arousal, and regulatory focus on trustworthiness and EDA. The analysis examined main effects and interaction effects between these variables.",TRUE,Robot-emotional-display; Robot-verbal-communication-content,Robot-emotional-display; Robot-verbal-communication-content,,"In study 2, the researchers manipulated both the robot's facial expressions (valence and arousal) and the content of the messages delivered by the robot. The paper states, 'Two scenarios focused on promotionfocused contexts and prevention-focused contexts were developed accordingly. For promotion-focused context, the message is framed as ""Hello! I have prepared water for you! Water is good for your health! Have some water!"". For the prevention-focused context, the message is framed as ""Hello! You had too much Cola today! Too much Cola is bad for your health! Stop drinking!"".' This shows a clear manipulation of the robot's emotional display and the content of the robot's verbal communication. The results showed that the interaction between the valence of the emotional display and the regulatory focus of the message (content) significantly impacted trustworthiness and EDA. Specifically, regulatory fit (positive expressions in promotion-focused contexts and negative expressions in prevention-focused contexts) increased perceived trustworthiness and reduced EDA. Therefore, both 'Robot-emotional-display' and 'Robot-verbal-communication-content' are appropriate categories for the manipulated factors and the factors that impacted trust. Arousal did not have a significant impact on trust, so it is not included in the 'factors_that_impacted_trust' list.",10.1016/j.apergo.2023.103967,https://linkinghub.elsevier.com/retrieve/pii/S0003687023000054,"Following the evolution of technology and its application in various daily contexts, social robots work as an advanced artificial intelligence (AI) system to interact with humans. However, limited research has been done to discuss the role of emotional expressions and contextual cues in influencing anthropomorphic trustworthiness, especially from the design perspective. To address this research gap, the current study designed a specific robot prototype and conducted two lab experiments to explore the effect of emotional expressions and contextual cues on trustworthiness via a combination of subjective ratings and physiological measures. Results showed that: 1) positive (vs. negative) emotional expressions enjoyed a higher level of anthropomorphic trustworthiness and visual attention; 2) regulatory fit was expanded in parasocial interaction and worked as a prime to activate anthropomorphic trustworthiness for social robots. Theoretical contributions and design implications were also discussed in this study."
"Song, Yao; Luximon, Yan",When Trustworthiness Meets Face: Facial Design for Social Robots,2024,1,211,211,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of six conditions, each with a different combination of eye shape (round or narrow) and mouth shape (upturned, neutral, or downturned). They were then asked to complete a questionnaire.",Participants viewed an image of a robot with varying facial features and completed a questionnaire assessing trustworthiness and attitude towards the robot.,Unspecified,Expressive Robots,Research,Social,Trustworthiness Rating,passive observation,Participants observed images of robots without direct interaction.,media,Participants viewed static images of the robot.,simulated,The robot was presented as a static image.,not autonomous,The robot's actions were simulated and did not involve any autonomous behavior.,Questionnaires,,,Trust was measured using a five-item questionnaire.,"parametric models (e.g., regression)","The study used ANCOVA and regression analysis to model the relationship between facial features, trustworthiness, and robot attitude.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers directly manipulated the robot's facial features (eye shape and mouth shape) to influence perceived trustworthiness and attitude.,Round eyes and upturned or neutral mouths increased perceived trustworthiness and positive attitude towards the robot.,"The study found that age was a significant covariate, with older people tending to have a higher level of perceived trustworthiness. There was no interaction effect between eye and mouth shape on trustworthiness evaluation.","Round eyes and upturned or neutral mouths on a robot significantly improve perceived trustworthiness and attitude, and trustworthiness mediates the effect of facial features on robot attitude.",Participants viewed images of a robot with different eye and mouth shapes and then completed a questionnaire to assess their perception of the robot's trustworthiness and their attitude towards it.,ANCOVA; Linear regression,"The study used a two-way ANCOVA to examine the main effects of eye shape and mouth shape on perceived trustworthiness and robot attitude, with age as a covariate. Regression analysis, specifically using the PROCESS macro in SPSS, was then used to test the mediating role of trustworthiness in the relationship between facial features (eye and mouth shape) and robot attitude. This involved regressing robot attitude on each facial feature separately, with age as a covariate, and assessing the direct and indirect effects of the facial features on robot attitude through trustworthiness.",TRUE,Robot-aesthetics,Robot-aesthetics,,"The study manipulated the robot's facial features, specifically eye shape (round vs. narrow) and mouth shape (upturned, neutral, or downturned). These manipulations directly alter the visual appearance of the robot, which falls under the category of 'Robot-aesthetics'. The results showed that both eye shape and mouth shape significantly impacted perceived trustworthiness, thus 'Robot-aesthetics' is also listed as a factor that impacted trust. The paper explicitly states 'The results showed that the main effect of mouth and eye design on trustworthiness evaluation was significant, while the interaction effect was not significant'. This indicates that the interaction between eye and mouth shape did not impact trust, but the individual manipulations did. Therefore, there are no factors that did not impact trust.",10.3390/s24134215,https://www.mdpi.com/1424-8220/24/13/4215,"As a technical application in artificial intelligence, a social robot is one of the branches of robotic studies that emphasizes socially communicating and interacting with human beings. Although both robot and behavior research have realized the significance of social robot design for its market success and related emotional benefit to users, the specific design of the eye and mouth shape of a social robot in eliciting trustworthiness has received only limited attention. In order to address this research gap, our study conducted a 2 (eye shape) × 3 (mouth shape) full factorial between-subject experiment. A total of 211 participants were recruited and randomly assigned to the six scenarios in the study. After exposure to the stimuli, perceived trustworthiness and robot attitude were measured accordingly. The results showed that round eyes (vs. narrow eyes) and an upturnedshape mouth or neutral mouth (vs. downturned-shape mouth) for social robots could significantly improve people’s trustworthiness and attitude towards social robots. The effect of eye and mouth shape on robot attitude are all mediated by the perceived trustworthiness. Trustworthy human facial features could be applied to the robot’s face, eliciting a similar trustworthiness perception and attitude. In addition to empirical contributions to HRI, this finding could shed light on the design practice for a trustworthy-looking social robot."
"Sonoda, Kohei; Wada, Takahiro",Driver's trust in automated driving when sharing of spatial awareness,2016,1,10,10,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were briefed on the driving scenario and task requirements, completed practice drives, and then performed experimental trials with automated driving and vibrotactile feedback. They then rated their trust in the system.","Participants experienced automated driving scenarios where the vehicle passed a motorbike with different driving methods and vibrotactile feedback, and then rated their trust in the system.",Unspecified,Autonomous Vehicles,Research,Navigation,Path Following,minimal interaction,Participants observed the automated driving in a simulator and did not physically interact with the system.,simulation,The study used a driving simulator to create an interactive virtual environment.,simulated,The automated vehicle was a simulated entity within the driving simulator.,pre-programmed (non-adaptive),The automated vehicle followed pre-programmed driving methods without adapting to the user.,Questionnaires,,,Trust was measured using subjective ratings on an 11-point scale.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the automated vehicle's driving method (straight, middle, over) and the type of vibrotactile feedback (directed, non-directed, none) to influence trust.",The middle driving method and directed vibrotactile feedback resulted in higher trust ratings compared to other conditions.,The middle driving method consistently resulted in higher trust ratings across all trust measures. The directed vibrotactile feedback also significantly increased trust compared to non-directed or no feedback.,Sharing spatial awareness through directed vibrotactile feedback and using an appropriate driving method (middle) significantly increases driver trust in automated driving systems.,The automated vehicle drove along a two-lane road and passed a motorbike using different methods. The human participant observed the driving and provided trust ratings after each trial.,ANOVA; LSD post-hoc,A two-way ANOVA with repeated measures was used to analyze the effects of driving methods and spatial information on subjective trust ratings. Post hoc tests were conducted using the LSD method to determine specific differences between conditions.,TRUE,Robot-task-strategy; Robot-interface-design,Robot-task-strategy; Robot-interface-design,,"The study manipulated the automated vehicle's driving method (straight, middle, over) which is a change in the task completion strategy of the robot, thus 'Robot-task-strategy'. The study also manipulated the type of vibrotactile feedback (directed, non-directed, none), which is a change in the interactive elements of the system, thus 'Robot-interface-design'. The results showed that both the driving method and the vibrotactile feedback significantly impacted trust ratings, therefore both are included in 'factors_that_impacted_trust'. There were no factors that were manipulated that did not impact trust.",10.1109/SMC.2016.7844618,http://ieeexplore.ieee.org/document/7844618/,Vibrotactile display has been investigated to support driver’s monitoring of traffic situation when auto driving. A vibrotactile display was assumed to contribute to driver trust in automation since driver can know the spatial awareness of an automated system to predict or understand the action selection for driving. The display provided the spatial information of close traffic objects with haptic stimulus. The present study considered driving scenes of passing a motorbike when vehicles are approaching from behind. A driving simulator study was conducted to investigate effects of the spatial information and driving behavior of automated system on driver trust. The results showed that the trust was affected by the information and behavior of the system.
"Stanton, Christopher; Stevens, Catherine J.",Robot Pressure: The Impact of Robot Eye Gaze and Lifelike Bodily Movements upon Decision-Making and Trust,2014,1,59,59,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants completed a shell game task with a robot assistant, where the robot's gaze, breathing, and eye-tracking were manipulated. Participants could ask the robot for help, and the robot would sometimes offer advice. The robot was controlled using a Wizard of Oz setup.","Participants played a computerized shell game and were asked to identify the cup hiding an object, with the robot acting as an assistant.",Nao,Humanoid Robots; Expressive Robots,Research; Social,Game,Cooperative Game,minimal interaction,"Participants interacted with a robot in a controlled setting, with limited verbal interaction.",real-world,Participants interacted with a physical robot in a lab setting.,physical,A physical robot was present during the experiment.,wizard of oz (directly controlled),The robot's actions were directly controlled by a human operator.,Behavioral Measures; Performance-Based Measures,,Performance Metrics,"Trust was assessed through behavioral measures such as the frequency of accepting the robot's advice and asking for help, and performance metrics.",no modeling,Trust was not modeled computationally; descriptive statistics were used.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's eye gaze, breathing, and eye-tracking were manipulated, and the task difficulty was varied to influence trust.",Robot gaze had a positive impact on trust for difficult decisions and a negative impact for easier decisions. Lifelike bodily movements had no significant effect on trust.,"The study found that robot gaze had a positive impact on trust for difficult decisions and a negative impact for easier decisions, which was an unexpected result. The robot's gaze also improved performance on easy trials but hindered performance on difficult trials.","Robot gaze had a positive impact on trust for difficult decisions and a negative impact for easier decisions, suggesting that robot gaze can have either a positive or negative impact upon trust and compliance, depending upon the nature of the robot's request or suggestion.","The robot would ask the participant for their answer to the shell game, and the participant would respond. The robot would sometimes offer advice or help, and the participant could also ask the robot for help.",ANOVA; ANOVA,"The study used mixed repeated measures analyses of variance (ANOVA) to examine the effects of robot eye gaze, task difficulty, breathing, and eye tracking on participant behavior. Specifically, ANOVAs were used to analyze the frequency with which participants changed their answers to the robot's, the frequency with which they asked for the robot's opinion, task accuracy, and response time. The first ANOVA used cup movement speed as a measure of task difficulty, while a second ANOVA used accuracy means to group trials into quartiles to measure task difficulty. The ANOVAs tested for main effects and interactions between the manipulated variables.",TRUE,Robot-nonverbal-communication; Task-complexity,Robot-nonverbal-communication; Task-complexity,,"The study manipulated robot eye gaze, which is a form of nonverbal communication, by having the robot look directly at the participant or at the monitor. This falls under 'Robot-nonverbal-communication'. The study also manipulated the speed of the cup movement in the shell game, which directly impacts the difficulty of the task and thus falls under 'Task-complexity'. The results showed that both robot gaze and task difficulty had a significant impact on trust, as participants were more likely to trust the robot's opinion when it gazed at them on the hardest trials, but less likely to trust the robot on easier trials. The study also found that participants were more likely to ask for the robot's opinion as the speed of cup movement increased. The robot's breathing and eye tracking movements were also manipulated, but these did not have a significant impact on trust, so they are not included in the factors that impacted trust. Therefore, only 'Robot-nonverbal-communication' and 'Task-complexity' are included in 'factors_that_impacted_trust'. The study did not find any factors that were manipulated that did not impact trust, so 'factors_that_did_not_impact_trust' is empty.",,http://link.springer.com/10.1007/978-3-319-11973-1_34,"Between people, eye gaze and other forms of nonverbal communication can inﬂuence trust. We hypothesised similar eﬀects would occur during human-robot interaction, predicting a humanoid robot’s eye gaze and lifelike bodily movements (eye tracking movements and simulated “breathing”) would increase participants’ likelihood of seeking and trusting the robot’s opinion in a cooperative visual tracking task. However, we instead found signiﬁcant interactions between robot gaze and task diﬃculty, indicating that robot gaze had a positive impact upon trust for diﬃcult decisions and a negative impact for easier decisions. Furthermore, a signiﬁcant eﬀect of robot gaze was found on task performance, with gaze improving participants’ performance on easy trials but hindering performance on diﬃcult trials. Participants also responded signiﬁcantly faster when the robot looked at them. Results suggest that robot gaze exerts “pressure” upon participants, causing audience eﬀects similar to social facilitation and inhibition. Lifelike bodily movements had no signiﬁcant eﬀect upon participant behaviour."
"Stanton, Christopher John; Stevens, Catherine J.",Don’t Stare at Me: The Impact of a Humanoid Robot’s Gaze upon Trust During a Cooperative Human–Robot Visual Task,2017,1,52,52,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were told the study was to test the robot's vision system. They played a shell game with the robot, and the robot would sometimes disagree with the participant's answer. Participants were asked to give a final answer after the robot's disagreement. The robot's gaze was manipulated between averted, constant, and situational.","Participants played a computerized shell game with a robot partner, and were asked to identify the location of a hidden object.",Nao,Humanoid Robots,Research; Social,Game,Cooperative Game,minimal interaction,Participants interacted with the robot through a computer game and verbal communication.,real-world,Participants interacted with a physical robot in a lab setting.,physical,The robot was physically present during the experiment.,wizard of oz (directly controlled),The robot's actions were controlled by a human operator using a Wizard-of-Oz setup.,Behavioral Measures,,,Trust was measured by the frequency with which participants changed their answer to the robot's suggested answer.,no modeling,No computational model of trust was used in this study.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's gaze was manipulated to be averted, constant, or situational, and the task difficulty was varied by changing the speed and number of cup shuffles.","Task difficulty had a significant impact on trust, with participants more likely to trust the robot on harder trials. For female participants, constant gaze decreased trust, especially when they were initially correct.","Unexpected gender effects were found, with females being less likely to trust a robot with constant gaze, while males showed the opposite trend. The effect of gaze was only significant when female participants were initially correct.","Participants were more likely to trust the robot when they were uncertain of their own judgment, and constant robot gaze had a negative impact on trust for female participants when they were confident in their initial answer.","The robot moved its head to face either the monitor or the participant, and verbally disagreed with the participant's answer on some trials. The human participant played a shell game and provided their answer to the robot.",ANOVA,"A mixed analysis of variance (ANOVA) was conducted to examine the effects of Gaze (Averted, Constant, and Situational) and Task Difficulty (Hard vs Medium) on the frequency with which participants changed their initial answer to the robot's suggested answer. Further ANOVAs were conducted to examine the effect of gaze on female participants when they were initially correct.",TRUE,Robot-nonverbal-communication; Task-complexity,Task-complexity,Robot-nonverbal-communication,"The study manipulated the robot's gaze (averted, constant, situational), which is a form of nonverbal communication, thus 'Robot-nonverbal-communication' was selected. The task difficulty was also manipulated by changing the speed and number of cup shuffles, which directly impacts the cognitive demands of the task, thus 'Task-complexity' was selected. The results showed that task difficulty had a significant impact on trust, with participants more likely to trust the robot on harder trials, thus 'Task-complexity' was selected as a factor that impacted trust. While the study hypothesized that gaze would impact trust, the results showed that gaze did not have a significant impact on trust overall, thus 'Robot-nonverbal-communication' was selected as a factor that did not impact trust. However, it is important to note that gaze did have an impact on trust for female participants when they were initially correct.",10.1007/s12369-017-0422-y,http://link.springer.com/10.1007/s12369-017-0422-y,"Gaze is an important tool for social communication. Gaze can inﬂuence trust, likability, and compliance. However, excessive gaze in some contexts can signal threat, dominance and aggression, and hence complex social rules govern the appropriate use of gaze. Using a between-subjects design we investigated the impact of three levels of robot gaze (averted, constant and “situational”) upon participants’ likelihood of trusting a humanoid robot’s opinion in a cooperative visual tracking task. The robot, acting as a confederate, would disagree with participants’ responses on certain trials, and suggest a different answer. As constant, staring gaze between strangers is associated with dominance and threat, and averted gaze is associated with lying, we predicted participants would be most likely to be persuaded by a robot which only gazed during disagreements (“situational gaze”). However, gender effects were found, with females least likely to trust a robot which stared at them, and no signiﬁcant differences between averted gaze and situational gaze. Implications and future work are discussed."
"Stephenson, Alice C.; Willis, Rachel; Alford, Chris","Using in-seat electrical potential sensors for non-contact monitoring of heart rate, heart rate variability, and heart rate recovery",2021,1,30,29,1,1 participant was excluded due to poor signal from the Plessey sensor,Controlled Lab Environment,within-subjects,"Participants completed a 5-minute baseline recording, followed by three counterbalanced tasks: rural autonomous driving, city autonomous driving, and the Harvard Step Test. Heart rate and rMSSD were measured using contact and non-contact sensors.",Participants experienced simulated autonomous driving in rural and city environments and performed the Harvard Step Test.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants sat in a simulated autonomous vehicle and experienced different driving scenarios.,simulation,Participants experienced simulated autonomous driving scenarios using a driving simulator.,physical,Participants sat in a physical vehicle seat with integrated sensors in a driving simulator.,fully autonomous (limited adaptation),"The vehicle was fully autonomous during the driving scenarios, with no direct control from the participant.",Physiological Measures,,Physiological Signals,Trust was indirectly assessed through physiological measures of heart rate and heart rate variability.,no modeling,No computational model of trust was developed; the study focused on comparing sensor measurements.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,The study did not manipulate any factors related to trust; it focused on comparing the accuracy of two different sensors.,,"The non-contact sensor slightly underestimated heart rate during the Harvard Step Test, possibly due to movement artifacts. There was also an outlier in the HRV data, suggesting the non-contact sensor may overestimate extreme rMSSD values.",The in-seat non-contact Plessey EPIC sensor showed a high level of agreement with the contact Biopac sensor for measuring heart rate and rMSSD during simulated autonomous driving and following the Harvard Step Test.,"The robot (simulated autonomous vehicle) drove through different environments, while the human participant sat in the vehicle and monitored the driving scenario. The human also performed the Harvard Step Test.",ANOVA; lin concordance coefficients; Bland-Altman analysis,Repeated measures ANOVAs were used to investigate if the non-contact sensor was sensitive to changes in heart rate and rMSSD during simulated driving and heart rate recovery. Lin's concordance correlation coefficients were calculated to assess the agreement between the contact and non-contact sensors for heart rate and rMSSD. Bland-Altman analysis was used to assess the agreement between the two sensors by studying the mean difference and constructing limits of agreement.,FALSE,,,,"The study did not manipulate any factors related to trust. The primary goal was to compare the accuracy of a non-contact sensor against a contact sensor for measuring heart rate and heart rate variability during simulated autonomous driving and following the Harvard Step Test. There were no intentional manipulations of any factors that would directly influence trust. The study focused on the sensor's ability to accurately capture physiological data, not on how different conditions might affect trust in the autonomous system. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust.",10.1016/j.ijpsycho.2021.08.005,https://linkinghub.elsevier.com/retrieve/pii/S0167876021008734,"Detecting transient changes in heart rate and heart rate variability during experimental simulated autonomous driving scenarios can indicate participant arousal and cognitive load, providing valuable insights into the rela­ tionship between human and vehicle autonomy. Successfully detecting such parameters unobtrusively may assist these experimental situations as well as naturalistic driver monitoring systems within an autonomous vehicle. However, non-contact sensors must collect reliable and accurate signals. This study aims to compare the in-seat, non-contact Plessey EPIC sensor to the gold standard, contact Biopac sensor. Thirty participants took part in fiveminute simulated autonomous vehicle journeys in a city environment and a rural environment, and a five-minute resting condition. To ensure the seat sensor was sensitive to elevated heart rate values, heart rate was also collected following the energetic Harvard Step Test. Lin concordance coefficients and Bland-Altman analyses were employed to assess the level of agreement between the non-contact Plessey EPIC sensor and the contact Biopac sensor for heart rate and heart rate variability. Analyses revealed a high level of agreement (rc > 0.96) between both sensors for one-minute averaged heart rate and five-minute averaged heart rate variability during simulated autonomous driving and rest, and one-minute averaged heart rate following the Harvard Step Test. In addition, the non-contact sensor was sensitive to significant differences during tasks. This proof of principle study demonstrates the feasibility of using the non-contact Plessey EPIC sensor to accurately detect heart rate and heart rate variability during simulated autonomous driving environments."
"Stiber, Maia; Taylor, Russell; Huang, Chien-Ming",Modeling Human Response to Robot Errors for Timely Error Detection,2022,2,23,19,4,"2 participants exhibited strong robot novelty effects, 2 exhibited no visible reaction to the robot errors",Controlled Lab Environment,within-subjects,"Participants completed a practice task, then programmed a robot to unpack boxes, during which a pre-programmed error occurred. Participants then completed a questionnaire and were debriefed.","Participants programmed a robot to pick and place pasta boxes, and observed a pre-programmed error during the task.",Kinova Gen 3,Industrial Robot Arms,Research,Manipulation,Object Passing,direct-contact interaction,Participants physically interacted with the robot to program it.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,wizard of oz (directly controlled),The robot's actions were pre-programmed and triggered by the experimenter.,Questionnaires; Behavioral Measures,,Video Data,Trust was assessed using a post-task questionnaire and behavioral measures based on facial expressions.,"deep learning (e.g., neural networks, reinforcement learning)",A neural network was used to classify facial action units for error detection.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's performance was manipulated by introducing a pre-programmed error (dropping the box).,"The study did not directly measure trust, but the error was intended to elicit a negative response.",Participants exhibited social responses to robot errors even in a non-social context. The algorithm was able to detect errors using facial cues.,"Facial action units can be used to detect robot errors in real-time, even in non-social contexts.","The robot performed a pick-and-place task, and the human participant programmed the robot using kinesthetic teaching.",welch's t-tests,"Welch's t-tests were used to compare the intensities of 17 action units (AUs) during error and no-error instances. This was done to determine if there was a statistically significant difference in AU intensities between these two conditions, indicating the discriminative potential of AUs for error detection. The test was used because the data was imbalanced and there was potential unequal variance in the corresponding intensities.",TRUE,Robot-accuracy,,,"The study manipulated the robot's accuracy by introducing a pre-programmed error where the robot dropped the box. This is a direct manipulation of the robot's performance on the task, which is why 'Robot-accuracy' is the most appropriate category. The paper states, 'During the execution of the second box's pick-and-place, we inserted a pre-programmed error-the robot dropping the box before it reached its goal-for the participant to observe and react to.' The study did not directly measure trust, but the error was intended to elicit a negative response, which is why no factors are listed as impacting or not impacting trust.",10.1109/IROS47612.2022.9981726,https://ieeexplore.ieee.org/document/9981726/,"In human-robot collaboration, robot errors are inevitable—damaging user trust, willingness to work together, and task performance. Prior work has shown that people naturally respond to robot errors socially and that in social interactions it is possible to use human responses to detect errors. However, there is little exploration in the domain of nonsocial, physical human-robot collaboration such as assembly and tool retrieval. In this work, we investigate how people’s organic, social responses to robot errors may be used to enable timely automatic detection of errors in physical human-robot interactions. We conducted a data collection study to obtain facial responses to train a real-time detection algorithm and a case study to explore the generalizability of our method with different task settings and errors. Our results show that natural social responses are effective signals for timely detection and localization of robot errors even in non-social contexts and that our method is robust across a variety of task contexts, robot errors, and user responses. This work contributes to robust error detection without detailed task specifications."
"Stiber, Maia; Taylor, Russell; Huang, Chien-Ming",Modeling Human Response to Robot Errors for Timely Error Detection,2022,2,5,5,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a practice task, then programmed a robot to sort objects, experiencing three different errors in three rounds. Participants then completed a questionnaire.","Participants programmed a robot to sort PVC pipes and joints into bins, and observed different types of pre-programmed errors.",Kinova Gen 3,Industrial Robot Arms,Research,Manipulation,Sorting/Arranging,direct-contact interaction,Participants physically interacted with the robot to program it.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,wizard of oz (directly controlled),The robot's actions were pre-programmed and triggered by the experimenter.,Questionnaires; Behavioral Measures,,Video Data,Trust was assessed using a post-task questionnaire and behavioral measures based on facial expressions.,"deep learning (e.g., neural networks, reinforcement learning)",A neural network was used to classify facial action units for error detection.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's performance was manipulated by introducing different types of pre-programmed errors (physical, concept, and generalization).","The study did not directly measure trust, but the errors were intended to elicit a negative response.","Participants reacted to concept errors before the perceived error start. The algorithm was able to detect errors, but with a longer delay for generalization errors.","The error detection algorithm trained on one dataset generalized to a different task and error types, but with some performance differences.","The robot performed a sorting task, and the human participant programmed the robot using kinesthetic teaching.",,"No specific statistical tests were mentioned for this study. The analysis focused on descriptive statistics such as means and standard deviations of reaction times, error severity, and algorithmic detection delays.",TRUE,Robot-accuracy,,,"The study manipulated the robot's accuracy by introducing different types of pre-programmed errors (physical, concept, and generalization). The paper states, 'For the main task, each participant experienced three different errors in three interaction rounds, where each round consisted of sorting three different objects into bins. In each round, the participant programmed the robot with only one of the objects, and the robot ""generalized"" to the other two objects when executed.' These errors directly impact the robot's performance on the task, making 'Robot-accuracy' the most appropriate category. The study did not directly measure trust, but the errors were intended to elicit a negative response, which is why no factors are listed as impacting or not impacting trust.",10.1109/IROS47612.2022.9981726,https://ieeexplore.ieee.org/document/9981726/,"In human-robot collaboration, robot errors are inevitable—damaging user trust, willingness to work together, and task performance. Prior work has shown that people naturally respond to robot errors socially and that in social interactions it is possible to use human responses to detect errors. However, there is little exploration in the domain of nonsocial, physical human-robot collaboration such as assembly and tool retrieval. In this work, we investigate how people’s organic, social responses to robot errors may be used to enable timely automatic detection of errors in physical human-robot interactions. We conducted a data collection study to obtain facial responses to train a real-time detection algorithm and a case study to explore the generalizability of our method with different task settings and errors. Our results show that natural social responses are effective signals for timely detection and localization of robot errors even in non-social contexts and that our method is robust across a variety of task contexts, robot errors, and user responses. This work contributes to robust error detection without detailed task specifications."
"Stock, Ruth; Merkle, Moritz; Eidens, Dietmar; Hannig, Martin; Heineck, Paul; Nguyen, Mai Anh; Vã, Johannes",When Robots Enter Our Workplace: Understanding Employee Trust in Assistive Robots,2019,1,25,25,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants received a vignette, summarized the task, and then interacted with the android robot connected to a chatbot to get HR consultation. Afterwards, they completed a questionnaire.",Participants had to interact with a robot HR expert to get consultation about training and work-life balance topics.,Elenoide; Pepper,Android Robots; Humanoid Robots,Social; Research,Social,Conversation,minimal interaction,Participants interacted with the robot verbally to complete a task.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot used a pre-programmed chatbot to respond to participants.,Questionnaires,,Video Data; Physiological Signals; Speech Data,Trust was measured using questionnaires and physiological data was collected.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the robot's appearance (humanoid vs. android), task complexity, and the requirement for self-disclosure to examine their effects on trust.","The study hypothesized that trust would be lower for android robots compared to humanoid robots and humans, and that higher task complexity and self-disclosure would increase trust. The pilot study did not test these hypotheses, but the main study will.","The pilot study showed that participants found the robot's verbal expressions natural and the robot friendly and understandable. The main study will test the hypotheses about the effects of robot appearance, task complexity, and self-disclosure on trust.","The pilot study showed that participants were able to complete the task and receive the required information from the android robot, and that the vignette was considered comprehensible and realistic.","The robot acted as an HR expert, providing consultation on training and work-life balance topics. The human participant's role was to ask questions and receive advice from the robot.",t-test,The study used a t-test to compare the means of the complex and simple vignettes in terms of resource and structural complexity. The purpose was to validate that the complex vignette was significantly more complex than the simple vignette.,TRUE,Robot-aesthetics; Task-complexity; Robot-verbal-communication-content,Robot-aesthetics; Task-complexity; Robot-verbal-communication-content,,"The study manipulated the robot's appearance (humanoid vs. android), which falls under 'Robot-aesthetics' as it is a change to the visual appeal of the robot. The study also manipulated task complexity, which is a direct manipulation of the cognitive demands of the task. Finally, the study manipulated the requirement for self-disclosure, which is a change to what is communicated by the robot, thus falling under 'Robot-verbal-communication-content'. The paper explicitly states that the study will examine the effects of these manipulations on trust, therefore all three factors are included in 'factors_that_impacted_trust'. The paper does not state that any of these factors did not impact trust, so 'factors_that_did_not_impact_trust' is empty.",,,"This study is about assistive robots as internal service provider within the company Merck KGaA and examines how the physical appearance of a service representative (humanoid robot, android robot, human) affects employees’ trust. Based on the uncanny valley paradigm, we argue that employees’ trust is the lowest for the android robot and the highest for the human."
"Story, Matthew; Webb, Phil; Fletcher, Sarah R.; Tang, Gilbert; Jaksic, Cyril; Carberry, Jon",Do Speed and Proximity Affect Human-Robot Collaboration with an Industrial Robot Arm?,2022,1,83,83,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a collaborative assembly task with a robot arm under varying speed and proximity conditions, completing surveys after each run.","Participants collaborated with a robot arm to assemble pipes into foam blocks, positioning and securing the pipes after the robot presented them.",UR5,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically interacted with the robot to complete the assembly task.,real-world,The study was conducted in a real-world setting with a physical robot.,physical,A physical robot arm was used in the study.,shared control (fixed rules),The robot operated autonomously but with fixed rules for avoidance and task execution.,Questionnaires; Custom Scales,Trust in Human Robot Collaboration Scale,,Trust was measured using a custom questionnaire.,no modeling,Trust data was collected but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's speed and proximity were directly manipulated to assess their impact on workload and trust.,Trust was not significantly affected by changes in speed or proximity.,"The study found a significant positive relationship between robot speed and workload, but no significant relationship between proximity and workload or between speed/proximity and trust. The lack of significant interaction with trust was attributed to the robot's relatively small size and high success rate.","The speed of an industrial robot arm significantly influences human workload during human-robot collaboration, even within safety guidelines.","The robot arm picked up pipes and presented them to the human, who then positioned and secured the pipes. The robot used an avoidance algorithm to avoid collisions with the human.",linear mixed-effects model (lmm); t-test,"A linear mixed-effects model (LMM) analysis was used to examine the relationships between location, speed, and proximity (fixed factors), and participant (random factor) on workload and trust (dependent variables). Paired t-tests with Bonferroni correction were used to determine the differences between the speed variables on workload and its subscales.",TRUE,Robot-task-strategy,,Robot-task-strategy,"The study manipulated the robot's speed and proximity during the collaborative task. These manipulations are classified as 'Robot-task-strategy' because they directly altered how the robot performed its task (picking and placing pipes) without changing the task's success rate. The paper states, 'There were 3 speed settings chosen for the UR5 robot to operate at during the different conditions... There were 2 proximity settings chosen for the UR5 robot to operate at during the separate tasks'. The results section indicates that neither speed nor proximity significantly impacted trust, stating, 'There was no significant relationship between trust and speed... nor with proximity setting'. Therefore, 'Robot-task-strategy' is the only factor manipulated, and it did not impact trust.",10.1007/s12369-021-00853-y,https://link.springer.com/10.1007/s12369-021-00853-y,"Current guidelines for Human-Robot Collaboration (HRC) allow a person to be within the working area of an industrial robot arm whilst maintaining their physical safety. However, research into increasing automation and social robotics have shown that attributes in the robot, such as speed and proximity setting, can inﬂuence a person’s workload and trust. Despite this, studies into how an industrial robot arm’s attributes affect a person during HRC are limited and require further development. Therefore, a study was proposed to assess the impact of robot’s speed and proximity setting on a person’s workload and trust during an HRC task. Eighty-three participants from Cranﬁeld University and the ASK Centre, BAE Systems Samlesbury, completed a task in collaboration with a UR5 industrial robot arm running at different speeds and proximity settings, workload and trust were measured after each run. Workload was found to be positively related to speed but not signiﬁcantly related to proximity setting. Signiﬁcant interaction was not found for trust with speed or proximity setting. This study showed that even when operating within current safety guidelines, an industrial robot can affect a person’s workload. The lack of signiﬁcant interaction with trust was attributed to the robot’s relatively small size and high success rate, and therefore may have an inﬂuence in larger industrial robots. As workload and trust can have a signiﬁcant impact on a person’s performance and satisfaction, it is key to understand this relationship early in the development and design of collaborative work cells to ensure safe and high productivity."
"Stower, Rebecca; Kappas, Arvid","""Oh no, my instructions were wrong!"" An Exploratory Pilot Towards Children’s Trust in Social Robots",2020,1,33,29,4,4 children's data had to be excluded due to technical issues or non-compliance,Real-World Environment,between-subjects,"Children interacted with a NAO robot that gave either correct or incorrect instructions for a learning task. Afterwards, both parents and children completed questionnaires related to social and competency trust towards the robot. A preference test was also embedded in the interaction.","Children completed a computational thinking learning task using Cubetto, navigating it around a map based on instructions from the NAO robot.",Nao,Humanoid Robots; Expressive Robots,Educational; Research; Social,Game,Cooperative Game,minimal interaction,Children interacted with the robot through verbal instructions and a physical task.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The robot was physically present during the interaction.,wizard of oz (directly controlled),The robot's speech was controlled by a human operator.,Behavioral Measures; Questionnaires,,Video Data,"Trust was measured using questionnaires and a behavioral preference test, and video data was collected.",no modeling,No computational model of trust was used.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's instructions were manipulated to be either correct or incorrect, influencing the robot's performance on the task.","The study did not find a significant effect of the robot's reliability on children's trust, but this could be due to low sample size.","The study found that parents perceived a difference between social and competency trust, but this distinction did not emerge in the child questionnaire. The study also found no significant difference in how likely the child was to pick the robots suggestion, regardless of condition.","The study found tentative support for the idea that social and competency trust can be evaluated separately in robots, but this was only evident in the parent questionnaire and not the child questionnaire.","The robot provided instructions for a computational thinking task, and the child programmed a robot to move around a map based on those instructions. The robot also expressed a preference for one of two destinations, and the child chose a destination.",ANOVA; Chi-squared,"A one-way MANOVA was used to compare whether children's responses on social trust, competency trust, liking, and agency differed depending on whether they interacted with a reliable or unreliable robot. A chi-square contingency table was used to compare the likelihood that the child followed the recommendation of the robot between the reliable and unreliable robot conditions.",TRUE,Robot-accuracy,,Robot-accuracy,"The study manipulated the accuracy of the robot's instructions, with the robot providing either correct or incorrect instructions for the Cubetto navigation task. This directly impacts the robot's performance on the task, making 'Robot-accuracy' the most appropriate category. The study found no significant effect of the robot's reliability on children's trust, therefore 'Robot-accuracy' is listed under 'factors_that_did_not_impact_trust'. There were no factors that impacted trust.",10.1109/RO-MAN47096.2020.9223495,https://ieeexplore.ieee.org/document/9223495/,"Whilst there has been growing interest in the use of social robots in educational settings, the majority of this research focuses on learning outcomes, with less emphasis on the social processes surrounding these interactions. One such understudied factor is children’s trust in the robot as a teacher. Trust is a relevant domain in that if and how children trust a robot could inﬂuence their subsequent learning outcomes. The extent to which the robot’s behaviour (including making errors) inﬂuences trust is yet to be fully explored. Consequently, the goal of this research is to determine the role of trust in children’s learning from social robots. We report a pilot study investigating the conceptualisation and measurement of children’s trust in robots. 33 children aged between 4-9 completed a computational thinking learning task with a NAO robot at a Science Festival. Observations of the interactions in terms of developing tasks and measurements for child robot interaction are discussed. The ﬁndings tentatively suggest children’s trust in the robot can be divided into two parts: social afﬁliation towards the robot, and perceived competence/reliability of the robot."
"Strauch, Christoph; Mühl, Kristin; Patro, Katarzyna; Grabmaier, Christoph; Reithinger, Susanne; Baumann, Martin; Huckauf, Anke",Real autonomous driving from a passenger’s perspective: Two experimental investigations using gaze behaviour and trust ratings in field and simulator,2019,2,11,8,3,3 participants were excluded due to technical problems with gaze data collection,Real-World Environment,within-subjects,"Participants experienced two drives, one manual and one autonomous, in a real car on public roads. Gaze data and trust ratings were collected before and after each drive.",Participants were passengers in a car driven either manually or autonomously.,Unspecified,Autonomous Vehicles,Research,Navigation,Path Following,passive observation,Participants were passengers and observed the driving without direct interaction.,real-world,The study was conducted in a real car on public roads.,physical,Participants were in a real car with a physical autonomous driving system.,fully autonomous (limited adaptation),The car drove autonomously with limited adaptation to the environment.,Questionnaires,Jian et al. Trust Scale,Eye-tracking Data,Trust was measured using a questionnaire and eye-tracking data.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The driving mode was manipulated by having the car driven either manually or autonomously, which directly influenced the level of autonomy.","Trust ratings were descriptively higher for manual driving, but this effect was not statistically significant.","Gaze behavior showed more safety-relevant fixations in autonomous mode, but trust ratings did not significantly differ between manual and autonomous driving.",Passengers fixated more on safety-relevant areas when driven autonomously in a real-world setting.,"The robot drove the car either manually or autonomously, while the human was a passenger and observed the driving.",Wilcoxon rank sum,"Two non-parametric Wilcoxon tests were performed. The first test compared fixations on safety-relevant areas between autonomous and manual driving modes for the entire route. The second test compared fixations on safety-relevant areas during a safety-critical situation (joining the expressway) versus the rest of the route, averaging across driving modes. Additionally, a Wilcoxon test was used to compare trust ratings before and after the drives, and another Wilcoxon test was used to compare trust ratings between manual and autonomous driving modes.",TRUE,Robot-autonomy; Task-environment,,,"The study manipulated 'Robot-autonomy' by having the car driven either manually or autonomously, directly changing the level of automation. The 'Task-environment' was also manipulated by conducting the experiment in a real-world setting with public traffic, which is different from a controlled lab environment. While the paper states that trust ratings were descriptively higher for manual driving, this effect was not statistically significant, so no factors impacted trust. The paper also states that gaze behavior was affected by the driving mode, but this is not a factor that directly impacts trust.",10.1016/j.trf.2019.08.013,https://linkinghub.elsevier.com/retrieve/pii/S1369847818307265,"Trusting autonomous vehicles is seen as crucial for their dissemination. However, research on autonomous driving so far is restricted by using closed training courses or simulators and by comparing behaviour and evaluation while driving oneself (a manual car) with being driven (by an autonomous car). In the current study, we investigated passengers’ eye movements, categorized as safety-relevant or not safety-relevant, and trust ratings while being driven, once manually and once by an autonomous car, in real trafﬁc as well as in a simulator. As some of the effects observed in the ﬁeld experiment might have been caused by driving style, driving style was additionally varied in the simulator. Fixations in safety-relevant regions (e.g., on the road and steering wheel) were observed more frequently during safety critical driving situations than during regular driving. More safetyrelevant ﬁxations for the autonomous compared to the manual driving mode were observed particularly in the ﬁeld. Trust ratings were affected by driving mode mainly in the simulator: Here, being driven autonomously led to a lower reported trust than believing to be driven by a human driver. Driving style showed to affect trust ratings, but not gaze behaviour in the simulator experiment. Correlations between gazing into safety relevant regions and trust ratings were of smaller descriptive size than in recent investigations on drivers, suggesting that gazing into safety-relevant regions as objective alternative to trust ratings may not be as exhaustive for passengers as for drivers."
"Strauch, Christoph; Mühl, Kristin; Patro, Katarzyna; Grabmaier, Christoph; Reithinger, Susanne; Baumann, Martin; Huckauf, Anke",Real autonomous driving from a passenger’s perspective: Two experimental investigations using gaze behaviour and trust ratings in field and simulator,2019,2,24,24,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants experienced four drives in a simulator, with driving mode (manual/autonomous) and driving style (safe/risky) manipulated. Gaze data and trust ratings were collected after each drive.",Participants were passengers in a simulated car driven either manually or autonomously with varying driving styles.,Unspecified,Autonomous Vehicles,Research,Navigation,Path Following,minimal interaction,Participants were passengers in a simulator and observed the driving without direct interaction.,simulation,The study was conducted in a driving simulator.,simulated,Participants interacted with a simulated autonomous vehicle.,pre-programmed (non-adaptive),"The car was pre-programmed to drive autonomously, with no adaptation to the user.",Questionnaires,Jian et al. Trust Scale,Eye-tracking Data,Trust was measured using a questionnaire and eye-tracking data.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The driving mode (manual/autonomous) and driving style (safe/risky) were manipulated, influencing the perceived autonomy and behavior of the vehicle.",Trust ratings were significantly higher for manual driving and for the safe driving style. The difference in trust between autonomous and manual driving was more pronounced for the safe driving style.,"Trust ratings were significantly lower for autonomous driving and risky driving styles, and the difference between manual and autonomous driving was more pronounced for the safe driving style. Gaze behavior was not affected by driving mode or style.",Passengers reported lower trust in autonomous driving and risky driving styles in a simulator environment.,"The robot drove the car either manually or autonomously, with varying driving styles, while the human was a passenger and observed the driving.",ANOVA,"A repeated measures ANOVA was used to analyze the proportion of fixations on safety-relevant areas, with driving mode (manual vs. autonomous), safety criticality of the driving situation (rest of the route vs. overtaking), and driving style (80m vs. 150m safety distance) as within-subject factors. A repeated measures ANOVA was also used to analyze trust ratings, with driving mode and driving style as within-subject factors. The interaction between driving mode and driving style on trust ratings was also examined using ANOVA.",TRUE,Robot-autonomy; Robot-task-strategy,Robot-autonomy; Robot-task-strategy,,"The study manipulated 'Robot-autonomy' by having the car driven either manually or autonomously in the simulator, although the car was always driven autonomously. 'Robot-task-strategy' was manipulated by varying the driving style (safe vs. risky) through different safety distances before overtaking. The paper explicitly states that trust ratings were significantly higher for manual driving and for the safe driving style, and that the difference in trust between autonomous and manual driving was more pronounced for the safe driving style. Therefore, both 'Robot-autonomy' and 'Robot-task-strategy' impacted trust. The paper also states that gaze behavior was not affected by driving mode or style, but this is not a factor that directly impacts trust.",10.1016/j.trf.2019.08.013,https://linkinghub.elsevier.com/retrieve/pii/S1369847818307265,"Trusting autonomous vehicles is seen as crucial for their dissemination. However, research on autonomous driving so far is restricted by using closed training courses or simulators and by comparing behaviour and evaluation while driving oneself (a manual car) with being driven (by an autonomous car). In the current study, we investigated passengers’ eye movements, categorized as safety-relevant or not safety-relevant, and trust ratings while being driven, once manually and once by an autonomous car, in real trafﬁc as well as in a simulator. As some of the effects observed in the ﬁeld experiment might have been caused by driving style, driving style was additionally varied in the simulator. Fixations in safety-relevant regions (e.g., on the road and steering wheel) were observed more frequently during safety critical driving situations than during regular driving. More safetyrelevant ﬁxations for the autonomous compared to the manual driving mode were observed particularly in the ﬁeld. Trust ratings were affected by driving mode mainly in the simulator: Here, being driven autonomously led to a lower reported trust than believing to be driven by a human driver. Driving style showed to affect trust ratings, but not gaze behaviour in the simulator experiment. Correlations between gazing into safety relevant regions and trust ratings were of smaller descriptive size than in recent investigations on drivers, suggesting that gazing into safety-relevant regions as objective alternative to trust ratings may not be as exhaustive for passengers as for drivers."
"Strohkorb Sebo, Sarah; Traeger, Margaret; Jung, Malte; Scassellati, Brian",The Ripple Effects of Vulnerability: The Effects of a Robot's Vulnerable Behavior on Trust in Human-Robot Teams,2018,1,132,105,27,"video data recording failure, participant non-compliance, substantial hardware / software failures",Controlled Lab Environment,between-subjects,"Participants were placed in groups of three and played a collaborative game with a robot. The robot made either vulnerable or neutral statements after each round. Participants completed pre- and post-experiment surveys, and their behavior was video recorded.",Participants played a collaborative tablet-based railroad route construction game with a robot.,Nao,Humanoid Robots; Expressive Robots,Social; Research,Game,Cooperative Game,minimal interaction,Participants interacted with the robot through verbal communication and observation during a collaborative game.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot's actions and utterances were pre-programmed and did not adapt to the participants' behavior.,Behavioral Measures; Questionnaires,Robotic Social Attributes Scale (RoSAS),Video Data; Speech Data,"Trust was assessed using questionnaires and behavioral measures, including video analysis of participant interactions.",no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's end-of-round utterances were manipulated to be either vulnerable or neutral, which was intended to influence trust-related behaviors.",The robot's vulnerable statements increased engagement with the robot and trust-related behaviors among human teammates.,"Participants in the experimental condition were more likely to explain their mistakes, console teammates, and laugh together, suggesting a 'ripple effect' of the robot's vulnerability. There was no significant difference in the psychological safety survey measure between conditions, which may be due to the questionnaire being designed for established teams.","A robot's vulnerable behavior can positively influence trust-related behaviors among human teammates, leading to increased engagement with the robot and more supportive interactions among humans.","The robot made pre-scripted utterances at the beginning, middle, and end of each round of the game. The human participants played a collaborative railroad route construction game on tablets, attempting to build the most efficient path.",Multilevel Model; Multilevel Model,"The study used multilevel mixed-effects generalized linear models to evaluate continuous dependent variables and multilevel mixed-effects logistic regression to evaluate binary dependent variables. These models were used to analyze the data because each participant's data cannot be treated as wholly independent from the other participants within their group and the data has repeated measures. The experimental condition and mistake round number were treated as fixed effects, and the groups of participants belonged to were evaluated as a random effect. Covariates such as age, gender, familiarity, and extraversion were treated as fixed effects. The models produced a coefficient to linearly or logistically map the predictor variables with the dependent variable and a p-value to indicate the significance of this relationship. The coefficient is presented in odds ratios, the odds of the dependent variable occurring in the experimental group over the control group.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the content of the robot's verbal communication at the end of each round. In the experimental condition, the robot made vulnerable statements, including self-disclosures, personal stories, and humor, while in the control condition, the robot made neutral comments. This manipulation was intended to influence trust-related behaviors. The paper explicitly states that the robot made either vulnerable or neutral statements after each round, which directly changes the content of the robot's verbal communication. The results showed that the vulnerable statements increased engagement with the robot and trust-related behaviors among human teammates, indicating that the manipulated factor, 'Robot-verbal-communication-content', impacted trust. There were no other factors manipulated that were found to impact trust.",10.1145/3171221.3171275,https://dl.acm.org/doi/10.1145/3171221.3171275,"Successful teams are characterized by high levels of trust between team members, allowing the team to learn from mistakes, take risks, and entertain diverse ideas. We investigated a robot’s potential to shape trust within a team through the robot’s expressions of vulnerability. We conducted a between-subjects experiment (N = 35 teams, 105 participants) comparing the behavior of three human teammates collaborating with either a social robot making vulnerable statements or with a social robot making neutral statements. We found that, in a group with a robot making vulnerable statements, participants responded more to the robot’s comments and directed more of their gaze to the robot, displaying a higher level of engagement with the robot. Additionally, we discovered that during times of tension, human teammates in a group with a robot making vulnerable statements were more likely to explain their failure to the group, console team members who had made mistakes, and laugh together, all actions that reduce the amount of tension experienced by the team. These results suggest that a robot’s vulnerable behavior can have “ripple effects” on their human team members’ expressions of trust-related behavior."
"Stuck, Rachel E.; Rogers, Wendy A.",Understanding Older Adult's Perceptions of Factors that Support Trust in Human and Robot Care Providers,2017,1,32,32,0,No participants were excluded,Survey/Interview,,"Participants completed questionnaires and a semi-structured interview about trust in care providers for various tasks, with the interview divided into sections for human and robot care providers.","Participants were asked about their perceptions of trust in human and robot care providers for four care tasks: bathing, medication assistance, transfer, and household tasks.",Unspecified,Service and Assistive Robots,Care,Evaluation,Survey/Questionnaire Completion,passive observation,Participants were asked about hypothetical interactions with robots.,media,The study used text-based descriptions of robot interactions.,hypothetical,The study involved hypothetical robots described in text.,not autonomous,The robot's actions were hypothetical and not autonomous.,Questionnaires,,,Trust was assessed through questionnaires and interviews.,no modeling,No computational modeling of trust was performed.,Observational & Survey Studies,Qualitative Interviews,No Manipulation,"The study did not manipulate any factors related to trust, but rather explored perceptions of trust.",,Preliminary results suggest that personal qualities of human caregivers and empathy of robots are important for trust.,"The study aims to identify factors influencing trust in human and robot care providers, and to compare these factors between human-human and human-robot contexts.",The human participants were interviewed about their perceptions of trust in human and robot care providers for various care tasks. The robot's actions were hypothetical and described in text.,descriptive statistics,The questionnaires were planned to be analyzed using descriptive statistics. The study also planned to use thematic analysis for the qualitative interviews.,FALSE,,,,"The study did not manipulate any factors. The study was designed to explore perceptions of trust in human and robot care providers through questionnaires and interviews. The participants were asked about their perceptions of trust in hypothetical scenarios, but there was no manipulation of any factors related to the robot or the task. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust due to manipulation.",10.1145/3056540.3076186,https://dl.acm.org/doi/10.1145/3056540.3076186,"As the population of older adults increase so will the need for care providers, both human and robot. Trust is a key aspect to establish and maintain a successful older adultcare provider relationship. However, due to trust volatility it is essential to understand it within specific contexts. This proposed mixed methods study will explore what dimensions of trust emerge as important within the humanhuman and human-robot dyads in older adults and care providers. First, this study will help identify key qualities that support trust in a care provider relationship. By understanding what older adults perceive as needing to trust humans and robots for various care tasks, we can begin to provide recommendations based on user expectations for design to support trust."
"Stuck, Rachel E.; Rogers, Wendy A.",Older Adults’ Perceptions of Supporting Factors of Trust in a Robot Care Provider,2018,1,24,24,0,No participants were excluded,Survey/Interview,,Participants completed questionnaires and a semi-structured interview about their perceptions of trust in robot care providers for four home-care tasks.,"Participants discussed their perceptions of trust in a robot care provider for bathing, transferring, medication assistance, and household tasks.",Unspecified,Service and Assistive Robots,Care,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only discussed their perceptions of a hypothetical robot care provider.,media,The study used text-based descriptions of robot interactions.,hypothetical,The robot was only described hypothetically without any visual representation.,not autonomous,The robot's actions were hypothetical and not physically present or acting autonomously.,Questionnaires,,,Trust was assessed through qualitative interviews and questionnaires.,no modeling,Trust was not modeled computationally.,Observational & Survey Studies,Qualitative Interviews,No Manipulation,"The study did not manipulate any factors related to trust, but rather explored perceptions of trust.",,"Participants identified gentleness, material/texture, and companionability as unique factors influencing trust in robot care providers, which were not previously highlighted in the literature.","Older adults identified professional skills, personal traits, and communication as key dimensions supporting trust in robot care providers, with specific factors like gentleness and material texture emerging as important.","The robot was described as a care provider performing tasks such as bathing, transferring, medication assistance, and household tasks, while the human participant provided their perceptions of trust in the robot for these tasks.",Pearson correlation,"A correlation analysis was used to examine the relationship between participants' self-efficacy in operating a robot and their preference for trusting a robot versus a human for home-care tasks. Specifically, a positive correlation was found between self-efficacy and trust preference.",FALSE,,,,"The study did not manipulate any factors related to trust. The participants were asked about their perceptions of trust in a hypothetical robot care provider for various tasks. There were no intentional changes to any robot characteristics, task parameters, or communication styles. The study was observational and focused on identifying factors that older adults perceive as important for trust, rather than manipulating those factors to measure their impact. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust due to manipulation.",10.1155/2018/6519713,https://www.hindawi.com/journals/jr/2018/6519713/,"The older adult population is increasing worldwide, leading to an increased need for care providers. An insufficient number of professional caregivers will lead to a demand for robot care providers to mitigate this need. Trust is an essential element for older adults and robot care providers to work effectively. Trust is context dependent. Therefore, we need to understand what older adults would need to trust robot care providers, in this specific home-care context. This mixed methods study explored what older adults, who currently receive assistance from caregivers, perceive as supporting trust in robot care providers within four common home-care tasks: bathing, transferring, medication assistance, and household tasks. Older adults reported three main dimensions that support trust: professional skills, personal traits, and communication. Each of these had subthemes including those identified in prior human-robot trust literature such as ability, reliability, and safety. In addition, new dimensions perceived to impact trust emerged such as the robot’s benevolence, the material of the robot, and the companionability of the robot. The results from this study demonstrate that the older adult-robot care provider context has unique dimensions related to trust that should be considered when designing robots for home-care tasks."
"Sukkerd, Roykrong; Simmons, Reid; Garlan, David",Tradeoff-Focused Contrastive Explanation for MDP Planning,2020,1,106,99,7,7 participants who gave invalid answers were excluded,Online Crowdsourcing,between-subjects,Participants were randomly assigned to either a control or treatment group. Both groups were given a series of navigation planning scenarios and a cost profile. The treatment group received contrastive explanations in addition to the information given to the control group. Participants were asked to determine if the robot's plan was the best option and rate their confidence.,Participants were asked to determine whether a robot's proposed navigation plan was the best option according to a given cost profile and rate their confidence in their answer.,Unspecified,Mobile Robots,Research,Evaluation,Text Evaluation,minimal interaction,"Participants interacted with the robot through a screen, receiving information about the robot's plan and explanations.",media,Participants were presented with static images of the robot's navigation plan on a map.,simulated,The robot was represented through a visual depiction of its navigation plan on a map.,pre-programmed (non-adaptive),The robot's navigation plan was pre-computed and did not adapt to user input.,Questionnaires,,,Trust was measured using a single question about the participant's confidence in their assessment of the robot's plan.,no modeling,Trust was not modeled computationally; the study focused on analyzing the correctness and confidence of participants' responses.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the presence of contrastive explanations and the alignment of the robot's objective function with the user's preference, which influenced the user's ability to assess the robot's decisions.",The contrastive explanations significantly increased the users' ability to correctly assess the robot's decisions and their confidence in their assessment. Misalignment of the robot's objective function with the user's preference decreased the users' ability to correctly assess the robot's decisions.,The study found that users were significantly less likely to correctly assess the robot's decisions when the robot's objective function was misaligned with their preference. The explanations significantly improved the users' ability to correctly assess the robot's decisions and their confidence in their assessment.,Providing quality-attribute-based contrastive explanations significantly improves users' ability and confidence in understanding the tradeoff rationale of a planning agent and in determining the appropriateness of the agent's decisions.,The robot generated a navigation plan based on multiple objectives. The human participant evaluated the robot's plan based on a given cost profile and determined if the plan was the best option.,Multilevel Model; Linear regression,"The study used mixed-effects logistic regression to analyze the binary outcome of participants' correctness in assessing the robot's plan, and linear mixed-effects regression to analyze the participants' confidence levels and reliable confidence scores. Both analyses accounted for random effects from individual participants and different question items. The predictor variables were the presence of explanations and whether the scenario was preference-misaligned.",TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-verbal-communication-content; Robot-accuracy,,"The study manipulated the presence of contrastive explanations, which falls under 'Robot-verbal-communication-content' because it changes what information is communicated to the user about the robot's decision-making process. The study also manipulated whether the robot's objective function was aligned with the user's preference, which directly impacts the correctness of the robot's plan and thus falls under 'Robot-accuracy'. The results showed that both the presence of explanations and the alignment of the robot's objective function significantly impacted the users' ability to correctly assess the robot's decisions and their confidence, indicating that both factors influenced trust. There were no other factors manipulated in the study.",10.1109/RO-MAN47096.2020.9223614,https://ieeexplore.ieee.org/document/9223614/,"End-users’ trust in automated agents is important as automated decision-making and planning is increasingly used in many aspects of people’s lives. In real-world applications of planning, multiple optimization objectives are often involved. Thus, planning agents’ decisions can involve complex tradeoffs among competing objectives. It can be difﬁcult for the end-users to understand why an agent decides on a particular planning solution on the basis of its objective values. As a result, the users may not know whether the agent is making the right decisions, and may lack trust in it. In this work, we contribute an approach, based on contrastive explanation, that enables a multi-objective MDP planning agent to explain its decisions in a way that communicates its tradeoff rationale in terms of the domain-level concepts. We conduct a human subjects experiment to evaluate the effectiveness of our explanation approach in a mobile robot navigation domain. The results show that our approach signiﬁcantly improves the users’ understanding, and conﬁdence in their understanding, of the tradeoff rationale of the planning agent."
"Sun, Xu; Li, Jingpeng; Tang, Pinyan; Zhou, Siyuan; Peng, Xiangjun; Li, Hao Nan; Wang, Qingfeng",Exploring Personalised Autonomous Vehicles to Influence User Trust,2020,1,36,36,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a driving simulation in three conditions: manual driving, standard AV, and personalized AV, with questionnaires after each condition and a post-interview.","Participants drove a simulated vehicle in three different conditions, navigating through three typical traffic scenarios.",Unspecified,Autonomous Vehicles,Research,Navigation,Path Following,minimal interaction,"Participants interacted with a driving simulator, controlling the vehicle in manual mode and observing the automated driving in the AV modes.",simulation,The study used a driving simulator with a 360° field of view projected onto screens.,simulated,The autonomous vehicle was simulated within the driving simulator environment.,shared control (adaptive),The personalized AV adapted its driving style based on the participant's manual driving behavior.,Questionnaires,Automation Trust Scale (ATS); Situational Awareness Rating Technique (SART),Performance Metrics,"Trust was measured using the Automation Trust Scale, and situational awareness was measured using the SART.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the driving style of the AV, comparing a standard AV with a personalized AV that adapted to the driver's style, and also included a manual driving condition.",The personalized AV condition resulted in higher levels of trust compared to both the standard AV and manual driving conditions.,"The personalized AV was perceived as more intelligent and reliable, leading to higher trust. There was no significant difference in situational awareness between the three conditions, but comfort was rated higher in both AV conditions compared to manual driving. There was a strong correlation in driving speed and acceleration between manual driving and adaptive driving, but not between manual and standard driving, or between personal and standard driving.","Personalized autonomous vehicles, which adapt to a driver's driving style, can significantly increase user trust compared to standard autonomous vehicles and manual driving.","The robot (simulated AV) drove along a predefined path, adapting its speed and acceleration based on the driving mode. The human participant drove the vehicle in manual mode and observed the automated driving in the AV modes, completing questionnaires after each condition.",Pearson correlation; Friedman test,"Person correlations were used to analyze the relationships between driving speed and acceleration across the three driving modes (manual, standard AV, and personalized AV), as well as the relationships between age and driving performance and cognitive factors. Friedman tests were used to analyze the differences in comfort, situational awareness, and user trust across the three driving conditions. Multiple dependent samples paired comparisons were conducted to further analyze significant differences found by the Friedman tests.",TRUE,Robot-autonomy; Robot-adaptability,Robot-adaptability,Robot-autonomy,"The study manipulated the level of autonomy by having three conditions: manual driving, standard AV, and personalized AV. The personalized AV condition also introduced adaptability, as it adjusted its driving style based on the participant's manual driving behavior. The paper states, 'The personalised AV system is able to identify the driving behaviours of users and thus adapt the driving style of the AV accordingly.' This clearly indicates a manipulation of the robot's adaptability. The study found that the personalized AV, which adapted to the driver's style, resulted in higher trust compared to the standard AV and manual driving. This indicates that the adaptability of the robot impacted trust. While the level of autonomy was manipulated (manual vs. automated), the results showed no significant difference in trust between the manual and standard AV conditions, indicating that the change in autonomy alone did not impact trust. The key finding was that the *adaptability* of the AV to the user's driving style was the factor that significantly increased trust, not just the presence of automation itself. Therefore, 'Robot-autonomy' is listed as a manipulated factor but not as a factor that impacted trust, while 'Robot-adaptability' is listed as both a manipulated factor and a factor that impacted trust.",10.1007/s12559-020-09757-x,http://link.springer.com/10.1007/s12559-020-09757-x,"Trust is a major determinant of acceptance of an autonomous vehicle (AV), and a lack of appropriate trust could prevent drivers and society in general from taking advantage of such technology. This paper makes a new attempt to explore the effects of personalised AVs as a novel approach to the cognitive underpinnings of drivers’ trust in AVs. The personalised AV system is able to identify the driving behaviours of users and thus adapt the driving style of the AV accordingly. A prototype of a personalised AV was designed and evaluated in a lab-based experimental study of 36 human drivers, which investigated the impact of the personalised AV on user trust when compared with manual human driving and non-personalised AVs. The findings show that a personalised AV appears to be significantly more reliable through accepting and understanding each driver’s behaviour, which could thereby increase a user’s willingness to trust the system. Furthermore, a personalised AV brings a sense of familiarity by making the system more recognisable and easier for users to estimate the quality of the automated system. Personalisation parameters were also explored and discussed to support the design of AV systems to be more socially acceptable and trustworthy."
"Surendran, Vidullan; Wagner, Alan R.",Your Robot is Watching: Using Surface Cues to Evaluate the Trustworthiness of Human Actions,2019,1,10,10,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants played a card game against a computer, a human, and a robot opponent in a counterbalanced order. The time taken to play each round, the phrasing of the opponent's questions, and the movement speeds were matched as closely as possible. Participants were told that the opponent would analyze visual data to detect trustworthiness, but in reality, all opponents played randomly. Video data was collected and hand-coded by two coders.",Participants played a card game where they discarded a card face down and stated the suit. The opponent then decided if they thought the participant was telling the truth or lying.,Baxter,Humanoid Robots; Collaborative Robots,Research,Game,Economic Game,minimal interaction,"Participants interacted with the robot by discarding cards and stating the suit, with the robot responding verbally.",real-world,Participants interacted with a physical robot in a real-world setting.,physical,The robot was a physical Baxter robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions and did not adapt to the participant's behavior.,Behavioral Measures,,Video Data,Trust was assessed by analyzing video data of participants' cues and comparing the model's predictions to human coders' judgments.,"parametric models (e.g., regression)",A Bayesian model was used to predict trustworthiness based on observed sequences of cues.,Empirical HRI Studies,Physical Robot Studies,No Manipulation,The study did not manipulate any specific factors related to trust; it assessed trust based on observed cues during the game.,"The study did not manipulate trust, but it found that the model's accuracy was comparable to human coders in predicting truth-telling versus lying.","The model's performance was comparable to human coders, and the model performed better for participants who displayed more cues. Participants were more animated when playing against the robot, and less animated when playing against the computer.",A computational model for evaluating trustworthiness based on surface cues was developed and shown to perform comparably to human coders in a card game setting.,"The robot picked up and discarded cards, and verbally stated if it thought the human was lying or telling the truth. The human discarded a card and stated the suit.",t-test,An unpaired t-test was conducted to compare the model's prediction accuracy against a random guessing model. The t-test was used to determine if the model's accuracy was statistically better than chance at predicting the outcome class (lie or truth).,FALSE,,,,"The study did not manipulate any factors related to trust. The study focused on developing a model to predict trustworthiness based on observed cues during a card game. The participants played against a computer, a human, and a robot, but these were not manipulated as independent variables. The opponents all played randomly, and the study's focus was on the model's ability to learn from the participant's cues, not on the effect of different opponents on trust. Therefore, no factors were intentionally manipulated to impact trust.",10.1109/RO-MAN46459.2019.8956343,https://ieeexplore.ieee.org/document/8956343/,"A number of important human-robot applications demand trust. Although a great deal of research has examined how and why people trust robots, less work has explored how robots might decide whether to trust humans. Surface cues are perceptual clues that provide hints as to a person's intent and are predictive of behavior. This paper proposes and evaluates a model for recognizing trust surface cues by a robot and predicting if a person's behavior is deceitful in the context of a trust game. The model was tested in simulation and on a physical robot that plays an interactive card game. A human study was conducted where subjects played the game against a simulation, the robot, and a human opponent. Video data was hand coded by two coders with an inter-rater reliability of 0.41 based on Levenshtein distance. It was found that the model outperformed/matched the human coders on 50% of the subjects. Overall, this paper contributes a method that may begin to allow robots to evaluate the surface cues generated by a person to determine whether or not it should trust them."
"Syrdal, Dag Sverre; Dautenhahn, Kerstin; Koay, Kheng Lee; Walters, Michael L",The Negative Attitudes towards Robots Scale and Reactions to Robot Behaviour in a Live Human-Robot Interaction Study,2009,1,28,28,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants interacted with a robot in two sessions with different behaviors (Socially Ignorant and Socially Interactive). They completed a task involving moving in a shared space and receiving a pen from the robot. After each session, participants evaluated the robot's behavior and rated its personality.",Participants moved in a shared space with the robot and received a pen from the robot.,Unspecified,Mobile Robots,Research,Manipulation,Object Passing,direct-contact interaction,Participants interacted directly with the robot in a shared space.,real-world,The interaction took place in a simulated living room with a physical robot.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot's behavior was pre-programmed and did not adapt to the user.,Questionnaires,Negative Attitude towards Robots Scale (NARS),,Trust was assessed using the Negative Attitudes towards Robots Scale (NARS).,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's behavior was manipulated to be either Socially Ignorant or Socially Interactive, influencing how it interacted with the participant.",Participants with higher NARS scores found the Socially Interactive robot behavior less comfortable.,"Participants with higher scores on the NARS and its 'Actual Interactions' subscale had more negative evaluations of the Socially Interactive robot behavior, which was intended to be more socially appropriate. This suggests that some participants may be more wary of robots displaying a higher degree of sophistication.","The Negative Attitudes towards Robots Scale (NARS) can differentiate between participant responses to different robot behavior styles, with higher NARS scores associated with more negative evaluations of a socially interactive robot.",The robot moved in a shared space with the participant and brought a pen to the seated participant. The participant moved in the same space as the robot and received the pen.,cronbach's alpha; Principal component analysis; t-test; Pearson correlation,"The study used Cronbach's Alpha to assess the internal reliability of the NARS scale, removing three items to improve the alpha. Principal Components Analysis (PCA) with Varimax rotation was used to explore the factor structure of the NARS, resulting in three subscales. Paired t-tests were used to compare the evaluations of the two robot behavior styles. Finally, correlations were used to examine the relationships between NARS scores (overall and subscales) and participant evaluations of the robot's behavior and attributed personality traits.",TRUE,Robot-social-attitude,Robot-social-attitude,,"The study manipulated the robot's behavior to be either 'Socially Ignorant' or 'Socially Interactive'. This manipulation directly relates to the robot's social approach and how it interacts with the participant, fitting the 'Robot-social-attitude' category. The paper states, 'The two behaviour styles were defined by the research team in terms of how much the robot adjusted its behaviour to the participant, rather than treating her as any other obstacle in the environment.' The results showed that participants with higher NARS scores found the 'Socially Interactive' robot behavior less comfortable, indicating that the manipulation of the robot's social attitude impacted trust. There were no other factors manipulated in the study.",,,"This paper describes the use of the Negative Attitudes Towards Robots Scale (NARS) to explain participants' evaluations of robot behaviour styles in a Human-Robot Interaction (HRI) study. Twentyeight participants interacted with a robot in two experimental conditions in which the robot’s behaviour was varied. Reliability analysis and a PCA was performed on the NARS items, creating three new subscales. Correlations between the subscales and other evaluations of the robot's behaviour found meaningful results, supporting the use of the NARS in English speaking samples."
"Szabó, Balázs; Őrsi, Balázs; Csukonyi, Csilla",Robots for surgeons? Surgeons for robots? Exploring the acceptance of robotic surgery in the light of attitudes and trust in robots,2024,1,197,197,0,No participants were excluded,Online Crowdsourcing,,"Participants completed an online questionnaire that included demographic questions, a surgical robot attitude questionnaire, and the Multi-dimensional Robot Attitude Scale (MdRAS).",Participants answered questions about their attitudes towards surgical robots and their willingness to participate in robotic surgery.,Unspecified,Other,Care,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the robot in the context of the questionnaire.,media,The interaction was based on text descriptions of surgical robots.,hypothetical,"The robot was only described in text, without any visual representation.",not autonomous,"The robot's actions were not part of the study, and the robot was only described in the questionnaire.",Questionnaires; Custom Scales,Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using a custom questionnaire and the Multi-dimensional Robot Attitude Scale (MdRAS).,"parametric models (e.g., regression)",The study used statistical correlations to analyze the relationship between questionnaire scores and willingness to participate in robotic surgery.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study did not manipulate any factors; it measured existing attitudes and willingness to participate.,,"Men showed more positive attitudes and a higher willingness to participate in robotic surgery than women. Healthcare workers had more positive attitudes towards surgical robots than non-healthcare workers, but there was no difference in willingness to participate between the two groups. Age did not correlate with attitudes or willingness to participate.","Attitudes towards surgical robots, as measured by the developed questionnaire, showed a strong positive correlation with the willingness to participate in robotic surgery.","Participants completed an online questionnaire about their attitudes towards surgical robots and their willingness to participate in robotic surgery. The robot was not physically present, and the interaction was limited to reading and responding to questions.",cronbach's alpha; Spearman correlation; Shapiro-Wilk; Mann-Whitney U; t-test,"The study used Cronbach's alpha to assess the reliability of the developed questionnaire. Spearman correlation was used to examine the relationships between questionnaire scores, MdRAS subscales, and willingness to participate in robotic surgery. The Shapiro-Wilk test was used to check for normality of data distribution. Mann-Whitney U tests and independent sample t-tests were used to compare differences in scores between groups (healthcare vs. non-healthcare, men vs. women, healthcare workers vs. healthcare students).",FALSE,,,,"The study did not manipulate any factors. It was an observational study that used a questionnaire to measure attitudes and willingness to participate in robotic surgery. The study did not involve any experimental manipulation of robot characteristics, task parameters, or user-related factors. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust due to manipulation.",10.1186/s40359-024-01529-8,https://bmcpsychology.biomedcentral.com/articles/10.1186/s40359-024-01529-8,"Background  Over the last century, technological progress has been tremendous, and technological advancement is reflected in the development of medicine. This research assessed attitudes towards surgical robots and identified correlations with willingness to participate in robotic surgery based on factors influencing trust in automated systems. Method  Using data from a survey, which included the Multi-dimensional Robot Attitude Scale (MdRAS) and a questionnaire consisting of attitude statements regarding the factors affecting trust in automated systems, the experiment assessed the attitudes of healthcare workers and potential patients towards surgery robots, and attempted to find a correlation between these attitudes, age, and gender. Results and Conclusion  Statistical evaluation of the responses (N = 197) showed that positive attitude towards surgical robots showed a high correlation with the willingness to participate in robotic surgery and gave the strongest correlations with the MdRAS utility and negative attitude towards robots subscales. For the assessment of willingness, the MdRAS subscales alone did not provide a strong enough correlation. All factors examined showed a significant correlation with participation. Having faith in the surgery robot, the propensity to trust technology, the designer’s reputation, the ease of work that a surgical robot provides, positive experience with robots, and believing the surgeon is competent at operating the machine seemed to have been the most important positive correlations, while fear of errors gave the highest negative correlation. The healthcare workers and potential patients showed significant differences in the subscales of the questionnaire perceived risk and knowledge but no significant difference in the characteristics of the surgical robot. There was no difference in willingness to participate between the samples. Age did not show a significant correlation with the score achieved and willingness in any of the samples. Significant differences were found between male and female respondents, with men having more positive attitudes and being more likely to participate in surgeries using surgery robots than women. As a result, the research potentially sheds light on the factors that need to be considered when building trust in robotic surgery."
"Ta+D568y, Benedict; Jung, Younbo; Park, Taezoon",When stereotypes meet robots: The double-edge sword of robot gender and personality in human–robot interaction,2014,1,164,164,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were randomly assigned to one of eight conditions, interacted with a robot in a healthcare or security scenario, and then completed a survey.","Participants interacted with a robot in either a healthcare or security scenario, where the robot performed tasks related to each role.",Unspecified,Humanoid Robots,Social; Care; Research,Social,Social Perception,minimal interaction,Participants interacted with the robot in a controlled lab setting with verbal instructions.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical humanoid robot.,wizard of oz (directly controlled),The robot was controlled by an experimenter behind a one-way mirror.,Questionnaires,,,Trust was measured using a questionnaire.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's gender (voice and name) and personality (non-verbal cues and color) were manipulated to match or violate occupational role stereotypes, and the task was framed as either healthcare or security.",Trust was higher when the robot's gender and personality matched the occupational role stereotypes.,"Personality-occupational role stereotypes had a stronger effect on user responses than gender-occupational role stereotypes. The study found that gender-occupational role conforming, vs. violation, influences participants' affective evaluations significantly, but not their cognitive evaluations.","Matching gender-occupational role and personality-occupational role stereotypes resulted in positive user responses, including increased trust and acceptance.","The robot performed tasks related to either healthcare (measuring vitals, giving advice) or security (monitoring surveillance, locking doors). Participants responded to the robot's prompts and completed a survey.",ANOVA; multiple three-way anova; Linear regression,"The study used a one-way ANOVA for manipulation checks to ensure the robot's gender and personality were perceived as intended. Multiple three-way ANOVAs were conducted to test the hypotheses (H1 and H2) by examining the effects of role, gender, and personality on user responses (affective attitudes, cognitive evaluations, perceived behavioral control, perceived trust, and acceptance). Finally, multiple linear regression was used to determine the antecedents of user acceptance of social robots, specifically examining the impact of attitudes, subjective norms, perceived behavioral control, and perceived trust.",TRUE,Robot-verbal-communication-style; Robot-nonverbal-communication; Robot-aesthetics,Robot-verbal-communication-style; Robot-nonverbal-communication,,"The study manipulated the robot's gender through voice (male/female) and name (John/Joan), which is categorized as 'Robot-verbal-communication-style' because it changes how the robot communicates. The robot's personality (extrovert/introvert) was manipulated through non-verbal cues such as speech speed, volume, pitch, body movements, and poses, which is categorized as 'Robot-nonverbal-communication'. Additionally, the robot's color (red for extrovert, grey for introvert) was manipulated, which is categorized as 'Robot-aesthetics'. The study found that matching the robot's gender and personality to the occupational role stereotypes (healthcare/security) positively impacted trust. Specifically, the study found that the robot's gender and personality, as conveyed through verbal style and nonverbal cues, influenced trust. The aesthetic manipulation (color) was used to reinforce the personality manipulation, but the paper does not explicitly state that the color alone impacted trust, so it is not included in the 'factors_that_impacted_trust' list. The paper does not explicitly state that any of the manipulated factors did not impact trust, so the 'factors_that_did_not_impact_trust' list is empty.",10.1016/j.chb.2014.05.014,https://linkinghub.elsevier.com/retrieve/pii/S0747563214002921,"With the emerging application of social and psychological concepts to human–robot interaction, we investigated the effects of occupational roles (security vs. healthcare), gender (male vs. female), and personality (extrovert vs. introvert) on user acceptance of a social robot. In a laboratory experiment, a robot performed two different roles of a healthcare and security to address the potential usage of social robots at home. During the task, the robot manifested different genders and personalities via nonverbal cues. The results showed that participants (n = 164) preferred the robot with matching gender-occupational role and personality-occupational role stereotypes. This ﬁnding implies that the gender and personality of social robots do not monotonically inﬂuence user responses; instead, they interact with corresponding role stereotypes to affect user acceptance of social robots. In addition, personality-occupational role stereotypes showed a stronger effect on users’ responses than gender-occupational role stereotypes. The overall results lay a foundation for designers to reduce the wide design spaces of social robots by grouping the various parameters under the big umbrella of social role stereotypes."
"Tenhundfeld, Nathan L.; de Visser, Ewart J.; Haring, Kerstin S.; Ries, Anthony J.; Finomore, Victor S.; Tossell, Chad C.",Calibrating Trust in Automation Through Familiarity With the Autoparking Feature of a Tesla Model X,2019,1,43,43,0,No participants were excluded,Real-World Environment,mixed design,"Participants were assigned to either an information or demonstration condition. In the information condition, participants were verbally instructed on how to use the autopark feature. In the demonstration condition, participants were shown a demonstration of the autopark feature and also given the same verbal instructions. Participants then completed as many trials as possible in a 30-minute session, attempting to use the autopark feature.",Participants were asked to use the autopark feature of a Tesla Model X to park the car between two sets of trashcans.,Tesla Model X,Autonomous Vehicles,Other: Automated parking,Navigation,Path Following,minimal interaction,Participants interacted with the car by initiating the autopark feature and monitoring its performance.,real-world,Participants interacted with the car in a real-world parking lot.,physical,Participants interacted with a physical Tesla Model X.,shared control (fixed rules),"The car autonomously parks itself, but the driver can intervene at any time.",Behavioral Measures; Physiological Measures,,Eye-tracking Data; Performance Metrics,"Trust was assessed using intervention rates, intervention distances, and eye-tracking data.",no modeling,No computational model of trust was used in this study.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"Participants were either shown a demonstration of the autopark feature or only given verbal instructions, which influenced their expectations and understanding of the robot's behavior.","Participants who were shown a demonstration of the autopark feature had lower initial intervention rates, indicating higher initial trust. With repeated exposures, intervention rates decreased for both groups.","Initial intervention rates were significantly higher in the information condition, but these rates decreased with repeated exposure. The distance of interventions from the parking spot decreased with repeated trials, suggesting a development of trust. There was a shift in eye gaze from the center display to the dashboard over time.","Familiarity with the autopark feature, gained through demonstration, significantly reduced initial driver intervention rates, suggesting that demonstration is more effective than verbal instruction for building initial trust.",The robot (Tesla Model X) autonomously parks itself after the human initiates the autopark feature. The human monitors the car's performance and can intervene by hitting the brake.,ANOVA; Mann-Whitney U; wilcoxon signed-ranks tests; one-sample binomial test,"The study used a one-way ANOVA to compare the rates of the Tesla stopping itself and human intervention rates between the information and demonstration conditions. A Mann-Whitney U test was used to compare the success rates between the first and last park for the information condition, and between the information and demonstration conditions for both the first and last park. Wilcoxon signed-ranks tests were used to compare fixation counts and durations on the center display and dashboard between the first and last trials for the eyetracking data. A one-sample binomial test was used to determine if the proportion of second interventions being closer to the trashcan than the first intervention was statistically significant.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated how participants were introduced to the autopark feature. In the 'information' condition, participants were only verbally instructed on how to use the feature. In the 'demonstration' condition, participants were shown a demonstration of the feature in addition to receiving the same verbal instructions. This manipulation directly altered the content of the information provided to participants about the robot's capabilities and operation, which is why 'Robot-verbal-communication-content' is the most appropriate category. The results showed that the demonstration condition led to lower initial intervention rates, indicating that the way the information was presented impacted trust. The study did not manipulate any other factors from the provided list. The difference in intervention rates between the two conditions was statistically significant, indicating that the manipulation of the information content impacted trust. There were no factors that were manipulated that did not impact trust.",10.1177/1555343419869083,http://journals.sagepub.com/doi/10.1177/1555343419869083,"Because one of the largest influences on trust in automation is the familiarity with the system, we sought to examine the effects of familiarity on driver interventions while using the autoparking feature of a Tesla Model X. Participants were either told or shown how the autoparking feature worked. Results showed a significantly higher initial driver intervention rate when the participants were only told how to employ the autoparking feature, than when shown. However, the intervention rate quickly leveled off, and differences between conditions disappeared. The number of interventions and the distances from the parking anchoring point (a trashcan) were used to create a new measure of distrust in autonomy. Eyetracking measures revealed that participants disengaged from monitoring the center display as the experiment progressed, which could be a further indication of a lowering of distrust in the system. Combined, these results have important implications for development and design of explainable artificial intelligence and autonomous systems. Finally, we detail the substantial hurdles encountered while trying to evaluate “autonomy in the wild.” Our research highlights the need to re-evaluate trust concepts in highrisk, high-consequence environments."
"Tenhundfeld, Nathan L.; de Visser, Ewart J.; Ries, Anthony J.; Finomore, Victor S.; Tossell, Chad C.",Trust and Distrust of Automated Parking in a Tesla Model X,2020,1,23,23,0,No participants were excluded,Real-World Environment,within-subjects,"Participants were asked to use the Tesla's Autopark feature repeatedly, and their interventions, risk-taking preferences, and self-reported measures of trust were recorded.",Participants were asked to repeatedly use the Autopark feature of a Tesla Model X to park the car between two sets of trash cans.,Tesla Model X,Autonomous Vehicles,Other: Automated parking,Navigation,Path Following,minimal interaction,"Participants were in the car while the robot performed the parking task, and could intervene.",real-world,The study was conducted in a real-world parking lot using a real car.,physical,The study used a physical Tesla Model X.,shared control (fixed rules),"The robot performed the parking task autonomously, but the human could intervene.",Behavioral Measures; Questionnaires,,Performance Metrics,Trust was measured using a post-experiment questionnaire and behavioral measures of driver intervention.,"parametric models (e.g., regression)",Linear regression was used to model the relationship between trust and other variables.,Empirical HRI Studies,Physical Robot Studies,No Manipulation,"The study did not directly manipulate any factors, but the repeated exposure to the Autopark feature influenced the participants' expectations and perceptions of the robot's performance.","Initial distrust was high, but trust increased with repeated exposure to the Autopark feature.","Initial intervention rates were high, indicating initial distrust, but decreased rapidly with repeated trials. This is contrary to the typical finding of overtrust in automation. The study also found that self-confidence in one's own parking ability was a predictor of preference for self-parking over automated parking.","Initial distrust of the automated parking system was high, but trust increased with repeated exposure, and self-reported trust was predicted by risk-taking behaviors, self-confidence, self-reported errors, and intervention rates.",The robot (Tesla Model X) autonomously parks itself between two sets of trash cans. The human participant sits in the driver's seat and can intervene by braking or steering.,Chi-squared; ANOVA; t-test; Linear regression,"The study used chi-square tests to compare intervention rates between different trials. One-way ANOVAs were used to compare intervention rates and self-confidence scores between groups based on their preference for self-parking or automated parking. A paired-samples t-test was used to compare self-reported trust in the Autopark feature with self-reported confidence in one's own parking ability. Linear regression was used to model the relationship between risk-taking, self-confidence, self-reported errors, intervention rates and self-reported trust in the autoparking feature, as well as to model the relationship between risk-taking, self-confidence and driver intervention.",FALSE,Robot-accuracy; Task-environment,Robot-accuracy,,"The study did not explicitly manipulate any factors. However, the robot's performance (Robot-accuracy) varied across trials, with the car sometimes failing to engage the Autopark, stopping mid-parking, or requiring multiple maneuvers. This variation in performance influenced trust, as participants' trust increased with repeated successful trials. The Task-environment was also a factor, as the parking spot was on a slope and flanked by trash cans, which could have influenced the robot's performance and, consequently, the participants' trust. The study did not manipulate the task environment, but it was a factor that influenced the robot's performance. The study found that self-reported trust was predicted by risk-taking behaviors, self-confidence, self-reported errors, and intervention rates, which are all related to the robot's accuracy. The study did not find any factors that did not impact trust.",10.1177/0018720819865412,https://doi.org/10.1177/0018720819865412,"ObjectiveThe present study aims to evaluate driver intervention behaviors during a partially automated parking task.BackgroundCars with partially automated parking features are becoming widely available. Although recent research explores the use of automation features in partially automated cars, none have focused on partially automated parking. Recent incidents and research have demonstrated that drivers sometimes use partially automated features in unexpected, inefficient, and harmful ways.MethodParticipants completed a series of partially automated parking trials with a Tesla Model X and their behavioral interventions were recorded. Participants also completed a risk-taking behavior test and a post-experiment questionnaire that included questions about trust in the system, likelihood of using the Autopark feature, and preference for either the partially automated parking feature or self-parking.ResultsInitial intervention rates were over 50%, but declined steeply in later trials. Responses to open-ended questions revealed that once participants understood what the system was doing, they were much more likely to trust it. Trust in the partially automated parking feature was predicted by a model including risk-taking behaviors, self-confidence, self-reported number of errors committed by the Tesla, and the proportion of trials in which the driver intervened.ConclusionUsing partially automated parking with little knowledge of its workings can lead to high degree of initial distrust. Repeated exposure of partially automated features to drivers can greatly increase their use.ApplicationShort tutorials and brief explanations of the workings of partially automated features may greatly improve trust in the system when drivers are first introduced to partially automated systems."
"Tennent, Hamish; Moore, Dylan; Jung, Malte; Ju, Wendy",Good vibrations: How consequential sounds affect perception of robotic arms,2017,1,320,320,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants watched one of six videos of a robot arm moving a block, with variations in sound (high-end, low-end, or no sound) and context (functional or social). They then answered questions about their perception of the robot.","Participants watched a video of a robot arm moving a Jenga block and rated the robot on competence, trust, aesthetics, and human-likeness.",KUKA youBot; OWI Robotic Arm Edge,Industrial Robot Arms,Research,Evaluation,Rating,passive observation,Participants passively observed a video of a robot arm.,media,Participants watched a video of the robot arm performing a task.,physical,The robot was a physical robot arm shown in a video.,pre-programmed (non-adaptive),The robot arm performed a pre-programmed task without adapting to the user.,Questionnaires,,,Trust was measured using a questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the sound of the robot arm (high-end, low-end, or no sound) and the context of the task (functional or social) to see how these factors influenced trust.","The presence of any sound reduced trust, but the social context marginally increased trust overall. Low-end sound reduced trust more than high-end sound, but the difference was not significant.","The study found that the presence of any sound negatively impacted trust, aesthetics, and human-likeness, but high-end sound increased perceived competence in a social context. The OWI sound was rated as more precise, but also more annoying than the KUKA sound.","The presence of robot motor sounds negatively affects visual perception of interactions, reducing trust, aesthetics, and human-likeness, but high-end sound can improve perceived competence in a social context.","The robot arm moved a Jenga block from a hopper to a specified location, either onto a tower of blocks (functional context) or into a person's hand (social context). The human participant watched the video and rated the robot.",ANOVA; Levene's test; cronbach's alpha; t-test; Pearson correlation; χ 2 tests,"The study used a general linear model (lm()) to calculate regressions of compound measures (competence, trust, aesthetics) and human-likeness from the context and sound conditions. Levene's test was used to check for homogeneity of variance, and data transformations were applied when necessary. Cronbach's alpha was used to assess the consistency of the compound measures. T-tests were used to compare means between different conditions, and correlation analysis was used to assess the relationship between trust and competency scores. Chi-squared tests were used in the pilot study to compare the count with each measure and adjusted p values for multiple comparisons using Benjamini and Hochberg's method.",TRUE,Robot-aesthetics; Task-environment,Robot-aesthetics; Task-environment,,"The researchers manipulated the sound of the robot arm (high-end, low-end, or no sound), which directly impacts the robot's aesthetics, as sound is a key component of how a robot is perceived aesthetically. This is supported by the paper's discussion of product sound design and how sound influences perception. The study also manipulated the context of the task (functional or social), which changes the task environment by introducing a human into the interaction. The paper explicitly states that the social context increased perceived competence, trust, aesthetic and human-likeness of the robot. The presence of any sound reduced trust, but the social context marginally increased trust overall. Low-end sound reduced trust more than high-end sound, but the difference was not significant. Therefore, both Robot-aesthetics and Task-environment impacted trust. There were no factors that were manipulated that did not impact trust.",10.1109/ROMAN.2017.8172414,http://ieeexplore.ieee.org/document/8172414/,"How does a robot’s sound shape our perception of it? We overlaid sound from high-end and low-end robot arms on videos of the high-end KUKA youBot desktop robotic arm moving a small block in functional (working in isolation) and social (interacting with a human) contexts. The low-end audio was sourced from an inexpensive OWI arm. Crowdsourced participants watched one video each and rated the robot along dimensions of competence, trust, aesthetic, and human-likeness. We found that the presence and quality of sound shapes subjective perception of the KUKA arm. The presence of any sound reduced human-likeness and aesthetic ratings, however the high-end sound rated better in the competence evaluation in the social context measures when compared to no sound. Overall, the social context increased the perceived competence, trust, aesthetic and human-likeness of the robot. Based on motor sound’s signiﬁcant mixed impact on visual perception of robots, we discuss implications for sound design of interactive systems."
"Trainer, Thomas; Taylor, John R.; Stanton, Christopher J.",Choosing the Best Robot for the Job: Affinity Bias in Human-Robot Interaction,2020,1,61,61,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants sorted avatar resumés, then completed a shell game task with two avatars (one preferred, one assigned), and then rated the avatars.",Participants sorted avatar resumés and then played a computerized shell game with two avatar teammates.,Unspecified,Humanoid Robots,Research,Game,Cooperative Game,minimal interaction,Participants interacted with avatars through a computer interface.,simulation,The interaction was conducted in a simulated environment using avatars.,simulated,The robots were represented by avatars on a screen.,wizard of oz (directly controlled),The avatars' actions were controlled by a human operator.,Behavioral Measures; Questionnaires,,Performance Metrics,Trust was measured by the number of times participants changed their answers based on the avatar's opinion and through post-task questionnaires.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The study manipulated avatar appearance, perceived competence, and the selection process to influence trust.","Participants showed higher trust towards their preferred avatar, which was also rated as more self-similar and attractive.","Although participants reported selecting avatars based on competence, they also showed an unconscious bias towards self-similar and attractive avatars. There was a conflict between the reported selection criteria and the actual selection based on self-similarity and attractiveness.","Affinity bias, based on perceived self-similarity, enhances perceived trustworthiness, even when participants chose avatars based on competence.","The human participant sorted avatar resumés and then played a shell game, while the avatar provided opinions on the correct answer.",Chi-squared; paired-sample t-tests; Friedman test,"The study used chi-square tests to examine the association between competency and sorting of avatar resumés, as well as to compare the reported selection strategy with the actual selection. Paired-sample t-tests were used to compare mean ratings of self-similarity and attractiveness of avatars in the top-8 and bottom-8 categories. A two-way analysis of variance was performed to investigate trust towards the two avatars in the shell game, examining the effect of avatar type (chosen vs. assigned) on trust scores.",TRUE,Robot-aesthetics; Task-complexity; Teaming,Robot-aesthetics; Teaming,Task-complexity,"The study manipulated several factors. 'Robot-aesthetics' was manipulated by varying the avatars' appearance (gender and skin tone) on the resumés, which influenced participants' initial preferences and subsequent trust. This is evident in the paper where it states, 'In this study, we asked 61 participants to order the resumés of 24 different avatars that varied in gender, skin tone, and competency'. 'Task-complexity' was manipulated by varying the difficulty of the shell game (easy, medium, hard) through cup speed and shuffle count, as described in the 'Shell Game Task' section: 'The game had three difficulty levels, which were determined by cup speed and the number of times the cups shuffled'. However, the paper states that trust scores were higher in the medium difficulty trials than in the hard trials, but this might be attributable to the difficult trials occurring later in our experiment, after participants had become more familiar with the process and the avatars. This suggests that the task complexity did not have a direct impact on trust. 'Teaming' was manipulated by having participants work with two avatars, one chosen and one assigned, which influenced trust levels, as the paper states, 'Participants were then assigned two avatar teammates for a cooperative visual-tracking task designed to measure trust, with one avatar being their handpicked favorite, and the other being a randomly assigned avatar from the ""bottom 8""'. The paper also states, 'mean trust ratings were higher for the most preferred avatars than the least preferred avatars'. The manipulation of teaming is further supported by the fact that the avatars were presented as teammates in a cooperative game.",,http://link.springer.com/10.1007/978-3-030-62056-1_41,"Humans subconsciously judge others as being either similar or dissimilar to themselves, manifesting as an unconscious preference, or afﬁnity bias, for those who are perceived to be similar. In human-to-human interaction, afﬁnity bias can signiﬁcantly inﬂuence trust formation and lead to discrimination, for example, in decisions related to recruitment and team selection. We investigate whether afﬁnity bias is observed in human-robot interaction during team formation with social agents that differ in gender and skin tone. In this study, we asked 61 participants to order the resumés of 24 different avatars that varied in gender, skin tone, and competency under the pretext of choosing the “best” avatars to be the participant’s teammate. Then, using a wizard-of-oz style experiment, participants performed a task with two avatar teammates (one most preferred and one least preferred) to measure trust. Results showed that while avatars were predominantly chosen based upon competency, avatar appearance generated an afﬁnity bias in resumé sorting, and participants were more likely to trust their preferred teammate."
"Trösterer, Sandra; Wurhofer, Daniela; Rödel, Christina; Tscheligi, Manfred",Using a Parking Assist System Over Time: Insights on Acceptance and Experiences,2014,1,11,11,0,No participants were excluded,Survey/Interview,within-subjects,"Participants completed online questionnaires at ten measurement points over eight weeks, and then participated in semi-structured interviews.",Participants used a parking assist system in their new car and reported their experiences through questionnaires and interviews.,Unspecified,Autonomous Vehicles,Other: Parking assistance,Evaluation,Survey/Questionnaire Completion,minimal interaction,Participants used the parking assist system in their car and reported their experiences.,real-world,Participants interacted with the parking assist system in their own cars.,physical,Participants interacted with a physical parking assist system in their car.,shared control (fixed rules),The parking assist system provides assistance based on fixed rules.,Questionnaires; Custom Scales,,,Trust was measured using questionnaires adapted from McKnight.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,No Manipulation,"The study did not manipulate any specific factors related to trust, but measured trust over time as participants used the parking assist system.","Trust in the parking assist system increased initially, then showed some ups and downs, but stabilized over time. Trust in the car remained stable.",The most significant changes in trust and acceptance occurred during the first few days of using the parking assist system. There was no significant correlation between trust in the parking assist system and trust in the car.,"Trust in the parking assist system changes over time, with an initial increase followed by stabilization, and is independent of trust in the car.",The robot (parking assist system) provides auditory and visual feedback to assist the human driver in parking. The human driver uses the parking assist system while parking their car and reports their experiences.,ANOVA; t-tests for paired samples; kendall's tau correlation,"The study used ANOVAs for repeated measures to determine if significant changes occurred over time for various acceptance and trust scales related to the parking assist system (PAS) and the car. T-tests for paired samples were used to compare specific measurement points, particularly the day before getting the car with the first and second days of usage, to identify significant changes in UX factors. Kendall's Tau correlation was used to investigate the relationship between trust and acceptance regarding the PAS and trust regarding the car.",FALSE,,,,"The study did not manipulate any factors. The study measured trust and acceptance over time as participants used their new cars with parking assist systems. The study design involved repeated measures over eight weeks, but there was no intentional manipulation of any specific factor related to the robot or the task. The changes in trust and acceptance were observed as a result of the natural interaction with the system over time, not due to any experimental manipulation.",10.1145/2667317.2667327,https://dl.acm.org/doi/10.1145/2667317.2667327,"Advanced Driver Assistance Systems, such as parking assist systems, are designed to support the driver in his/her driving task. To date, only a few studies exist that investigate the driver’s experiences and acceptance while using such systems over time. In this paper, we present the results from a long-term study regarding drivers’ experiences with and acceptance of a parking assist system in a newly bought car. We found that there is a change of the drivers’ experiences in terms of trust and of certain acceptance factors such as performance expectancy, effort expectancy, attitude towards technology, and social influence over time, highly depending on how certain characteristics of the system (e.g., the auditive signals) are perceived by the driver. Our study confirms the need to take expectations of the drivers, concerning the usefulness of the system, into account when designing parking assist systems. We further conclude that assistance should be given only in situations that really need assistance. Therefore, the situational context (e.g., the size of the parking slot), but also the capabilities of the driver (e.g., his parking skills), should be taken into account."
"Tsao, Ching-Chih; Chengchi, National",Assessing the Decision-Making Process in Human-Robot Collaboration Using a Lego-like EEG Headset,2023,1,6,5,1,1 participant was removed due to system malfunction,Controlled Lab Environment,within-subjects,"Participants completed a pretest to assess their computational thinking (CT) ability and were divided into high and low CT groups. They then collaborated with a robot on a series of computational thinking tasks, with the robot providing recommendations at two levels of capability (high vs. low). Participants completed questionnaires and EEG measurements during the tasks.","Participants collaborated with a robot to solve computational thinking problems from the Bebras challenge, with the robot providing recommendations.",Unspecified,Other,Educational; Research,Game,Puzzle/Logic Game,minimal interaction,Participants interacted with a robot through a website interface.,simulation,"The interaction was conducted through a website interface, simulating a collaborative environment.",simulated,The robot was presented as a chatbot on a website.,pre-programmed (non-adaptive),The robot provided pre-programmed recommendations based on the task difficulty.,Questionnaires; Physiological Measures,Negative Attitude towards Robots Scale (NARS); Robotic Social Attributes Scale (RoSAS); NASA Task Load Index (NASA-TLX),Physiological Signals,Trust was assessed using questionnaires and EEG data.,no modeling,No computational model of trust was developed; descriptive statistics were used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's capability was manipulated to be either high or low, which influenced the accuracy of its recommendations and the perceived task difficulty. Participants were informed that the robot was not perfectly reliable.","The robot's capability significantly impacted task performance, with low capability leading to decreased performance, especially for high CT participants who over-trusted the robot. Participants were able to identify the difference in robot capability.","High CT participants showed a significant drop in performance when collaborating with the low capability robot, indicating over-reliance. The high CT group had slightly higher negative attitudes toward the robot with high capability, although the rating difference was small.","The study found that a robot's capability significantly impacts task performance and human reliance, with low capability leading to decreased performance, especially for high CT participants who over-trusted the robot.","The robot provided recommendations to the participant for solving computational thinking problems. The participant initially provided an answer, then received the robot's recommendation, and could either accept or reject the robot's suggestion before submitting a final answer.",,"The study primarily used descriptive statistics (mean values) to analyze the data due to the small sample size. No specific statistical tests like t-tests or ANOVA were mentioned. The analysis focused on comparing mean performance scores, workload ratings, and questionnaire responses across different experimental conditions (high vs. low robot capability, high vs. low CT ability). EEG data was analyzed by comparing average band power of EEG channels and creating topographic maps, but no specific statistical tests were mentioned for this analysis.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy,,"The study manipulated 'Robot-accuracy' by having the robot provide recommendations at two levels of capability (high vs. low), which directly influenced the accuracy of its suggestions and thus the task performance. This is explicitly stated in the abstract: 'A user study was conducted where two levels of robot capabilities (high vs. low) were manipulated to provide system recommendations.' The paper also states: 'Based on the task difficulty in our experiments, the robot with high capability was capable of answering more complex questions than the low capability robot.' This manipulation directly impacted the participants' trust and reliance on the robot. The study also manipulated 'Task-complexity' by using questions from the Bebras challenge with two difficulty levels (easy vs. hard). This is described in the 'Experimental Tasks: The Bebras Computing Challenge' section: 'By referring to the correct rate of each question in the Bebras computing challenge, the retrieved questions came with two difficulty levels (easy vs. hard).' While task complexity was manipulated, the paper does not explicitly state that it directly impacted trust levels, but rather that it influenced performance and workload. The paper states that 'The low capability condition heavily decreased the task outcomes, especially for the participants with high CT ability.' and 'Compared to the high capability group, the workload ratings were higher while receiving assistance from a low capability robot.' The primary impact on trust was due to the robot's accuracy, not the task complexity itself.",,,"Human-robot collaboration (HRC) has become an emerging field, where the use of a robotic agent has been shifted from a supportive machine to a decisionmaking collaborator. A variety of factors can influence the effectiveness of decision-making processes during HRC, including the system-related (e.g., robot capability) and human-related (e.g., individual knowledgeability) factors. As a variety of contextual factors can significantly impact the human-robot decision-making process in collaborative contexts, the present study adopts a Lego-like EEG headset to collect and examine human brain activities and utilizes multiple questionnaires to evaluate participants’ cognitive perceptions toward the robot. A user study was conducted where two levels of robot capabilities (high vs. low) were manipulated to provide system recommendations. The participants were also identified into two groups based on their computational thinking (CT) ability. The EEG results revealed that different levels of CT abilities trigger different brainwaves, and the participants’ trust calibration of the robot also varies the resultant brain activities."
"Tulk, Stephanie; Wiese, Eva",Trust and Approachability Mediate Social Decision Making in Human-Robot Interaction,2018,2,52,42,10,10 participants were excluded for responding with the same answer for at least 63 trials,Online Crowdsourcing,within-subjects,"Participants played the Ultimatum Game with six different morphed robot images, making decisions to accept or reject offers. They then rated each agent for trustworthiness and approachability.","Participants played the Ultimatum Game, deciding whether to accept or reject offers from different robot agents.",Unspecified,Humanoid Robots,Research,Game,Economic Game,minimal interaction,"Participants interacted with robot images on a screen, making decisions based on offers.",media,Participants viewed static images of morphed robot faces.,simulated,The robot was represented by morphed images on a screen.,pre-programmed (non-adaptive),The robot's offers were pre-programmed and did not adapt to participant behavior.,Questionnaires; Custom Scales,,,Trust was measured using post-experiment ratings of trustworthiness and approachability.,"parametric models (e.g., regression)",Regression analysis was used to determine if trust and approachability mediated the effect of physical humanness on decision-making.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's appearance was manipulated by morphing images from robot-like to human-like, and the fairness of the offers was varied to influence trust.","Physical humanness did not directly affect decision-making, but it did influence ratings of trust and approachability. Approachability mediated the effect of physical humanness on offer acceptance.","Physical humanness did not directly affect economic decisions, but it did influence subjective ratings of trust and approachability. Approachability mediated the effect of physical humanness on offer acceptance.",Agent approachability mediated the effect of physical humanness on offer acceptance in the Ultimatum Game.,"The robot presented offers of how to split money, and the human participant decided whether to accept or reject the offer.",ANOVA; univariate anovas; Sobel test; Linear regression,Two 6 x 4 ANOVAs were conducted to examine the effects of physical humanness and fairness of agent offer on percentage of offers accepted and reaction times. Univariate ANOVAs were used to compare explicit ratings of trustworthiness and approachability for each agent. A Sobel test using regression was conducted to determine if trust and approachability mediated the relationship between physical humanness and social decision-making (percentage of accepted offers and RTs).,TRUE,Robot-aesthetics; Task-constraints,Robot-aesthetics,Task-constraints,"The study manipulated the robot's appearance by morphing images from robot-like to human-like, which falls under 'Robot-aesthetics'. The fairness of the offers in the Ultimatum Game was also varied, which can be considered a manipulation of 'Task-constraints' as it changes the performance limits of the task by influencing the potential gains or losses. The results showed that 'Robot-aesthetics' (physical humanness) influenced ratings of trust and approachability, which in turn mediated decision-making. However, the fairness of the offer ('Task-constraints') did not directly impact trust, but rather influenced the acceptance rate of offers.",10.1177/1541931218621160,http://journals.sagepub.com/doi/10.1177/1541931218621160,"As humanoid robots become more advanced and commonplace, the average user may perceive their robotic companion as human-like entities that can make social decisions, such as the deliberate choice to act fairly or selfishly. It is important for scientists and designers to consider how this will affect our interactions with social robots. The current paper explores how social decision making with humanoid robots changes as the degree of their human-likeness changes. For that purpose, we created a spectrum of human-like agents via morphing that ranged from very robot-like to very human-like in physical appearance (i.e., in increments of 20%) and measured how this change in physical humanness affected decision-making in two economic games: the Ultimatum Game (Experiment 1) and Trust Game (Experiment 2). We expected increases in human-like appearance to lead to higher rates of punishment for unfair offers and higher ratings of trust in both games. While physical humanness did not have an impact on economic decisions in either of the ex-periments, follow-up analyses showed that both subjective ratings of trust and agent approachability medi-ated the effect of agent appearance on decision-making in both experiments. Possible consequences of these findings for human-robot interactions are discussed."
"Tulk, Stephanie; Wiese, Eva",Trust and Approachability Mediate Social Decision Making in Human-Robot Interaction,2018,2,46,38,8,8 participants were excluded for responding with the same answer for at least 62 trials,Online Crowdsourcing,within-subjects,"Participants played the Trust Game with six different morphed robot images, deciding how much to return to the agent. They then rated each agent for trustworthiness and approachability.","Participants played the Trust Game, deciding how much money to return to different robot agents.",Unspecified,Humanoid Robots,Research,Game,Economic Game,minimal interaction,"Participants interacted with robot images on a screen, making decisions about returning money.",media,Participants viewed static images of morphed robot faces.,simulated,The robot was represented by morphed images on a screen.,pre-programmed (non-adaptive),The robot's offers were pre-programmed and did not adapt to participant behavior.,Questionnaires; Custom Scales,,,Trust was measured using post-experiment ratings of trustworthiness and approachability.,"parametric models (e.g., regression)",Regression analysis was used to determine if trust and approachability mediated the effect of physical humanness on decision-making.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's appearance was manipulated by morphing images from robot-like to human-like, and the fairness of the offers was varied to influence trust.","Physical humanness did not directly affect decision-making, but it did influence ratings of trust and approachability. Trust mediated the effect of physical humanness on reaction times.","Physical humanness did not directly affect economic decisions, but it did influence subjective ratings of trust and approachability. Trust mediated the effect of physical humanness on reaction times.",Agent trust mediated the effect of physical humanness on reaction times in the Trust Game.,"The robot presented offers of how much money to invest, and the human participant decided how much to return to the robot.",ANOVA; univariate anovas; Sobel test,Two repeated-measures 6 x 4 ANOVAs were conducted to examine the effects of physical humanness and fairness of agent offer on the amount of money returned and reaction times. Univariate ANOVAs were used to compare ratings of trustworthiness and approachability for each agent. A Sobel test was used to determine if trust and approachability mediated the relationship between physical humanness and social decision-making (amount returned and RTs).,TRUE,Robot-aesthetics; Task-constraints,Robot-aesthetics,Task-constraints,"Similar to Study 1, the robot's appearance was manipulated by morphing images from robot-like to human-like, which is categorized as 'Robot-aesthetics'. The fairness of the offers in the Trust Game was also varied, which is a manipulation of 'Task-constraints' as it changes the performance limits of the task by influencing the potential gains or losses. The results showed that 'Robot-aesthetics' (physical humanness) influenced ratings of trust and approachability, which in turn mediated reaction times. However, the fairness of the offer ('Task-constraints') did not directly impact trust, but rather influenced the amount of money returned.",10.1177/1541931218621160,http://journals.sagepub.com/doi/10.1177/1541931218621160,"As humanoid robots become more advanced and commonplace, the average user may perceive their robotic companion as human-like entities that can make social decisions, such as the deliberate choice to act fairly or selfishly. It is important for scientists and designers to consider how this will affect our interactions with social robots. The current paper explores how social decision making with humanoid robots changes as the degree of their human-likeness changes. For that purpose, we created a spectrum of human-like agents via morphing that ranged from very robot-like to very human-like in physical appearance (i.e., in increments of 20%) and measured how this change in physical humanness affected decision-making in two economic games: the Ultimatum Game (Experiment 1) and Trust Game (Experiment 2). We expected increases in human-like appearance to lead to higher rates of punishment for unfair offers and higher ratings of trust in both games. While physical humanness did not have an impact on economic decisions in either of the ex-periments, follow-up analyses showed that both subjective ratings of trust and agent approachability medi-ated the effect of agent appearance on decision-making in both experiments. Possible consequences of these findings for human-robot interactions are discussed."
"Tyshka, Alexander; Louie, Wing-Yue Geoffrey",Interactive Task Learning for Social Robots: A Pilot Study,2023,1,14,14,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants taught a Furhat robot to be a hotel concierge using both ITL and LfD methods, with the order counterbalanced. They were first shown a demonstration of how to teach the robot a sub-task. Participants then taught the robot the main task, and then played a hotel guest to assess the robot's performance. They could re-teach the robot if needed.","Participants taught a robot to perform hotel concierge tasks, including greeting, check-in, luggage assistance, check-out, providing information on amenities, and providing information on local restaurants.",Furhat,Expressive Robots,Social; Research,Social,Conversation,minimal interaction,Participants interacted with the robot through verbal instructions and dialogue.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical Furhat robot.,shared control (adaptive),The robot adapted its behavior based on the user's input during the teaching process.,Questionnaires,Trust Perception Scale - HRI; NASA Task Load Index (NASA-TLX),,Trust was measured using the Trust Perception Scale-HRI questionnaire.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the teaching method (ITL vs. LfD), which influenced the robot's behavior and the feedback provided to the user during the teaching process. The task difficulty was also influenced by the teaching method.","Trust was slightly higher in the LfD condition, but the difference was not statistically significant.","Participants generally preferred LfD over ITL, citing ease of use, despite ITL showing better performance when ASR errors were corrected. There was high variance in performance across participants with both teaching styles. Some participants got stuck in failure loops when trying to correct ASR errors.","The study found no significant differences in trust or workload between ITL and LfD, but ITL showed potential for improved performance by correcting ASR errors, although participants generally preferred LfD due to its ease of use.","The robot acted as a hotel concierge, responding to guest requests. The human participant acted as a teacher, instructing the robot on how to perform concierge tasks using either ITL or LfD methods.",t-test; Wilcoxon signed-rank test; spearman correlation test,"The study used a paired t-test to compare trust levels between the ITL and LfD conditions. A Wilcoxon signed-rank test was used to compare workload between the two conditions, as the data was non-normal. Spearman correlation tests were used to assess the correlation between programming experience and the differences in trust and workload between the two conditions, as well as the correlation between programming experience and performance in each condition.",TRUE,Robot-adaptability; Task-complexity,,Robot-adaptability; Task-complexity,"The study manipulated the teaching method (ITL vs. LfD), which directly influenced the robot's adaptability (learning capability). ITL allowed the robot to learn from natural language and adapt its behavior based on the user's input, while LfD involved learning from demonstrations. This difference in learning approach is a manipulation of the robot's adaptability. The teaching method also influenced the task complexity for the user. ITL required the user to break down the task into logical steps and provide specific instructions, while LfD allowed for a more intuitive demonstration-based approach. This difference in the required cognitive effort and structure for teaching constitutes a manipulation of task complexity. The paper states that there was no significant difference in trust between the two conditions, therefore, neither of these factors impacted trust. The paper also states that there was no correlation between programming experience and trust differences, which further supports that the task complexity did not impact trust.",10.1109/IROS55552.2023.10341713,https://ieeexplore.ieee.org/document/10341713/,"For socially assistive robots to achieve widespread adoption, the ability to learn new tasks in the wild is critical. Learning from Demonstration (LfD) approaches are a popular method for learning in the wild, but current methods require significant amounts of data and can be difficult to interpret. Interactive Task Learning (ITL) is an emerging learning paradigm that aims to teach tasks in a structured manner, minimizing the need for data and increasing transparency. However, to date ITL has only been explored for physical robotics applications. Additionally, minimal research has explored how usable existing ITL systems are for non-expert users. In this work, we propose a novel approach to learn social tasks via ITL. This system utilizes recent advances in Natural Language Understanding (NLU) to learn from natural dialogue. We conducted a pilot study to compare the ITL system against an LfD approach to investigate differences in teaching performance as well as teachers’ perceptions of trust and workload towards these systems. Additionally, we analyzed the teaching behavior of participants to identify successful and unsuccessful teaching strategies. Our findings suggest ITL could provide more transparency to users and improve performance by correcting speech recognition errors. However, participants generally preferred LfD and found it an easier teaching method. From the observed teaching behavior, we identify existing challenges in ITL for non-experts to teach social tasks. Using this, we propose areas of improvement toward future ITL learning paradigms that are intuitive, transparent, and performant."
"Ullman, Daniel; Malle, Bertram",The effect of perceived involvement on trust in human-robot interaction,2016,1,60,60,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were randomly assigned to one of three conditions: Autonomous Involvement, Experimenter Involvement, or Participant Involvement. In each condition, participants observed a robot fail and then recover. Participants then completed Likert items to evaluate perceptions of trustworthiness, agency, and likability, and rated how well suited a robot with the observed type of software would be for different scenarios.",Participants observed a robot completing a simple obstacle avoidance task and then rated the robot's trustworthiness and suitability for different scenarios.,Thymio,Mobile Robots,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants observed the robot's actions without direct interaction.,real-world,Participants observed a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed path and did not adapt to user input.,Questionnaires,,,Trust was measured using Likert scale questionnaires.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the perceived involvement of the participant in the robot's recovery from failure by having them press a button that appeared to influence the robot's actions, but in reality did not.","The study hypothesized that participants would perceive a robot to be more trustworthy when they believe they can influence the robot, and that this effect would be moderated by whether the robot's actions directly affect a person.",,The study investigates whether a person's perceived involvement in a robot's recovery from failure increases trust in the robot.,"The robot attempts to move from a starting location to an end location, navigating an obstacle. The human observes the robot fail, and then either observes the robot recover autonomously, with experimenter help, or with their own perceived help, and then completes a questionnaire.",ANOVA,"A one-way analysis of variance (ANOVA) was used to test the hypotheses related to the effect of perceived involvement on trust. Specifically, the ANOVA was used to compare the means of the different conditions (Autonomous Involvement, Experimenter Involvement, and Participant Involvement) on measures of trustworthiness, agency, and likability, as well as on ratings of suitability for different scenarios.",TRUE,Robot-autonomy,Robot-autonomy,,"The study manipulated the perceived level of autonomy by having participants believe they could influence the robot's recovery from failure. In the Autonomous Involvement condition, the robot appeared to recover on its own. In the Experimenter Involvement condition, the robot appeared to recover after the experimenter pressed a button. In the Participant Involvement condition, the robot appeared to recover after the participant pressed a button. Although the robot's actions were pre-programmed and the button press had no actual effect, the manipulation created different perceptions of the robot's autonomy and the participant's influence over it. The paper states, 'In this experiment we manipulate a person's perceived involvement with a robot's actions and measure the person's resulting willingness to interact with similarly designed robots in the future.' This manipulation of perceived involvement directly relates to the robot's perceived autonomy, as the different conditions suggest varying degrees of control over the robot's actions. The study found that perceived involvement, which is directly tied to the perceived autonomy of the robot, impacted trust. The paper states, 'Participants will perceive a robot to be more trustworthy when they believe they can influence the robot.' This indicates that the manipulation of perceived autonomy, through the different involvement conditions, had an impact on trust. There were no factors that were manipulated that did not impact trust.",10.1109/HRI.2016.7451896,http://ieeexplore.ieee.org/document/7451896/,"Trust serves as a powerful social capacity that can influence the course of a relationship, either spurring a willingness or refusal of one agent to interact with another. As we attempt to build increasingly complex and useful social robots, we must consider what factors will engender such trust and thus benefit human-robot interaction. In this paper we describe a line of inquiry that is investigating how a person’s perceived involvement in helping a robot recover from failure affects the person’s trust in the robot and in its future actions. We posit that a person’s active involvement with a robot, compared with passive observation, will lead to greater trust in the robot."
"Ullman, Daniel; Malle, Bertram F.",Human-Robot Trust: Just a Button Press Away,2017,1,42,40,2,2 participants were excluded due to a failure to comprehend the experiment instructions,Controlled Lab Environment,between-subjects,"Participants were randomly assigned to either the autonomous or involved condition. In the autonomous condition, the robot executed a path plan on its own. In the involved condition, the robot waited for a button press from the participant before executing the plan. Participants then completed measures of robot trust.",Participants observed a robot navigating a path and either watched it execute autonomously or pressed a button to allow it to execute.,Thymio,Mobile Robots,Research,Navigation,Path Following,minimal interaction,Participants had minimal interaction by pressing a button to allow the robot to execute its plan.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,shared control (fixed rules),The robot autonomously generated a path but waited for a button press to execute it in the involved condition.,Questionnaires; Custom Scales,,,Trust was measured using 8-point rating scales on the items 'trustworthy' and 'dependable' for the observed robot and future robots.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers manipulated whether participants pressed a button to allow the robot to execute its plan, influencing their role and the robot's autonomy.",The button press led to marginally higher trust in the observed robot and significantly higher trust in future robots in social contexts.,"The effect of the button press was only marginally significant for the observed robot but significant for future robots in social contexts, suggesting a context-dependent effect.","Permitting a robot's plan execution via a button press leads to greater trust in the specific robot and in future robots, particularly in social use contexts.",The robot autonomously generated a path and then either executed it on its own or waited for the participant to press a button to execute the path. The human participant either observed the robot or pressed a button to allow the robot to execute its plan.,ANOVA,"The study used ANOVA to compare the means of trust scores between the autonomous and involved conditions. Specifically, ANOVA was used to analyze the effect of interaction type (autonomous vs. involved) on trust in the observed robot and on trust in future robots in social use contexts. The analysis examined whether the button press manipulation had a statistically significant impact on trust levels.",TRUE,Robot-autonomy,Robot-autonomy,,"The study manipulated the level of robot autonomy by having the robot either execute its path autonomously or wait for a button press from the participant before executing the path. This is described in the 'METHOD' section: 'In the autonomous condition, Thymio executed the new path plan on its own, whereas in the involved condition Thymio waited for a button press from the participant...before executing the new plan.' This manipulation directly affects the robot's decision authority, making 'Robot-autonomy' the most appropriate category. The results section indicates that this manipulation impacted trust, as 'people in the involved robot condition indicated more trust...than people in the autonomous robot condition' and 'people in the involved condition also showed greater trust...specifically in future robots in social use contexts'. Therefore, 'Robot-autonomy' is also listed as a factor that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1145/3029798.3038423,https://dl.acm.org/doi/10.1145/3029798.3038423,"Many of the benefits promised by human-robot interaction require successful continued interaction between a human and a robot; trust is a key component of such interaction. We investigate whether having a person “in the loop” with a robot—i.e., the mere involvement of a person with a robot—affects human-robot trust. We posited that people who press a button on a robot to permit its plan execution would exhibit greater trust than people who merely observe a robot’s autonomous execution of the same plan. We assessed trust both toward the robot that participants interacted with and toward robots in potential future use contexts. We found (a) a marginally significant and medium-sized effect of the button press on people’s trust in the observed robot (p = .12, d = .52), and (b) a significant and large-sized effect on people’s trust in potential future robots, but only in social use contexts (p = .04, d = .68)."
"Ullman, Daniel; Malle, Bertram F.",What Does it Mean to Trust a Robot?: Steps Toward a Multidimensional Measure of Trust,2018,1,45,35,10,"5 participants failed a comprehension check, 5 participants were outliers",Online Crowdsourcing,,Participants rated 62 words on a slider scale from 'more similar to capacity trust' to 'more similar to personal trust'.,Participants rated the similarity of 62 words to either 'capacity trust' or 'personal trust'.,Unspecified,,,Evaluation,Rating,passive observation,Participants only read words and rated them.,media,The interaction was based on text descriptions of trust concepts.,hypothetical,The study did not involve any physical or simulated robots.,not autonomous,No robot was involved in the study.,Custom Scales,,,Trust was assessed using a custom slider scale.,no modeling,The study used PCA to identify dimensions of trust but did not model trust.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,"The study did not manipulate any factors related to trust, but rather explored the semantic space of trust.",,"The study found that negative words often confused participants, requiring their removal from the analysis.","Trust has a multidimensional structure, with four distinct dimensions: Capable, Ethical, Sincere, and Reliable.",Participants rated the similarity of 62 words to either 'capacity trust' or 'personal trust' using a slider scale.,Principal component analysis; cronbach's alpha; Pearson correlation,"The study used Principal Components Analysis (PCA) to identify underlying dimensions of trust from a set of 62 words. PCA was performed iteratively, first on all 62 items, then on a reduced set of 49, and finally on 20 items. Item analysis was conducted to improve Cronbach's alpha reliabilities and item-total correlations for each subset of items. Finally, correlations were calculated between the resulting subscales to examine their relationships.",FALSE,,,,"The study did not manipulate any factors. Participants were asked to rate the similarity of words to 'capacity trust' or 'personal trust'. There was no manipulation of any robot behavior, task, or environment. The study was purely observational and aimed to explore the semantic space of trust, not to test the impact of any manipulated variable on trust. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust in the context of an experimental manipulation.",10.1145/3173386.3176991,https://dl.acm.org/doi/10.1145/3173386.3176991,"Research on trust in human-human interaction has typically focused on notions of vulnerability, integrity, and exploitation whereas research on trust in human-machine interaction has typically focused on competence and reliability. In this initial study, we explore whether these different aspects of trust can be considered parts of a multidimensional conception and measure of trust. We gathered 62 words from dictionaries and trust literatures and asked participants to evaluate the words as belonging to a “personal” meaning or a “capacity” meaning. Through an iterative process using Principal Components Analysis (PCA) and item analysis, we derived four components that capture the multidimensional space occupied by the concept of trust. The resulting four components yield four subscales of trust with five items each and α reliabilities as follows: Capable = .88, Ethical = .87, Sincere = .84, and Reliable = .72."
"Ullman, Daniel; Malle, Bertram F.",Measuring Gains and Losses in Human-Robot Trust: Evidence for Differentiable Components of Trust,2019,1,798,759,39,39 participants were excluded due to low-quality responses,Online Crowdsourcing,between-subjects,"Participants read a vignette describing a robot's behavior in a specific role and context, completed the MDMT, then read a second vignette with a change in the robot's behavior, and completed the MDMT again.",Participants evaluated a robot's trustworthiness based on provided scenarios and changes in behavior.,Unspecified,Service and Assistive Robots,Other,Evaluation,Text Evaluation,passive observation,Participants only read text descriptions of the robot and its actions.,media,"The interaction was based solely on text descriptions, providing no visual or interactive elements.",hypothetical,"The robot was only described in text, with no visual or physical representation.",not autonomous,"The robot's actions were described in text, without any real autonomy.",Questionnaires; Multidimensional Measures,Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using the Multi-Dimensional Measure of Trust (MDMT) questionnaire.,no modeling,The study did not use any computational models of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's behavior was manipulated to either increase or decrease evidence for being reliable, capable, ethical, or sincere, by changing the description of its actions.","The manipulations were effective in changing trust scores, with reliable and capable dimensions showing more sensitivity than ethical and sincere dimensions. The study found evidence for two superordinate dimensions of trust: capacity trust (reliable, capable) and moral trust (ethical, sincere).","The study found that the reliable and capable subscales moved in tandem, as did the ethical and sincere subscales, suggesting two superordinate dimensions of trust. The manipulations for capable and sincere had little impact on the subscales of trust.","The study provides evidence for a two-dimensional conception of trust, with reliable-capable and ethical-sincere as the major constituents, and the MDMT is responsive to changes in robot behavior.","The robot was described as performing tasks in various roles (e.g., security officer), and the human participant read descriptions of the robot's behavior and rated their trust in the robot.",ANOVA; Principal component analysis,"The study used four 2x4 between-subjects ANOVAs to assess the sensitivity of each of the four MDMT subscales (Reliable, Capable, Ethical, Sincere) to the manipulations of change direction along each trust dimension. A principal components analysis was also conducted on the 16 MDMT (pre-post) difference scores to confirm the two-dimensional structure of trust.",TRUE,Robot-accuracy; Robot-morality; Robot-verbal-communication-content,Robot-accuracy; Robot-morality,Robot-verbal-communication-content,"The study manipulated the robot's behavior across four dimensions: reliable, capable, ethical, and sincere. These manipulations were designed to either increase or decrease evidence for each dimension. 'Robot-accuracy' was manipulated by changing the robot's success rate in completing its task (e.g., selecting suspicious travelers 7/10 times vs. 10/10 or 4/10 times), which directly impacts task performance. 'Robot-morality' was manipulated by changing whether the robot followed or disregarded procedures (e.g., complying with or disregarding airport procedures), which is a violation of moral codes outside of games and does not influence the robot's success rate on the task. 'Robot-verbal-communication-content' was manipulated by changing whether the robot communicated the reason for its actions or not (e.g., communicating the reason why or not communicating the real reason why). The results showed that manipulations of 'Robot-accuracy' and 'Robot-morality' had a significant impact on trust, while the manipulation of 'Robot-verbal-communication-content' had a weak impact on trust. Specifically, changes in being reliable (accuracy) and ethical (morality) were more effective than changes in being sincere (communication content) or capable (task strategy). The study found that the reliable and capable subscales moved in tandem, as did the ethical and sincere subscales, suggesting two superordinate dimensions of trust. The manipulations for capable and sincere had little impact on the subscales of trust.",10.1109/HRI.2019.8673154,https://ieeexplore.ieee.org/document/8673154/,"Human-robot trust is crucial to successful humanrobot interaction. We conducted a study with 798 participants distributed across 32 conditions using four dimensions of humanrobot trust (reliable, capable, ethical, sincere) identiﬁed by the Multi-Dimensional-Measure of Trust (MDMT). We tested whether these dimensions can differentially capture gains and losses in human-robot trust across robot roles and contexts. Using a 4 scenario x 4 trust dimension x 2 change direction betweensubjects design, we found the behavior change manipulation effective for each of the four subscales. However, the pattern of results best supported a two-dimensional conception of trust, with reliable-capable and ethical-sincere as the major constituents."
"Ullman, Daniel; Aladia, Salomi; Malle, Bertram F.",Challenges and Opportunities for Replication Science in HRI: A Case Study in Human-Robot Trust,2021,3,203,200,3,3 participants were excluded because they failed to comprehend the experiment instructions,Controlled Lab Environment,between-subjects,"Participants were told they would watch a robot complete a short task and then answer several questions about the scenario. Participants were randomly assigned to one of three conditions: autonomous, button involvement, or verbal involvement. The robot navigated a path and stopped at an obstacle, then either autonomously generated a new path, waited for a button press, or waited for a verbal command to execute the new path.",Participants observed a robot navigating a path and then answered questions about the scenario.,iRobot Create,Mobile Robots,Research,Evaluation,Rating,passive observation,Participants passively observed the robot's actions.,real-world,Participants observed a real robot in a lab setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed path and did not adapt to the user.,Questionnaires; Multidimensional Measures,Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using questionnaires and a multidimensional measure of trust.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the level of participant involvement by having them either observe the robot autonomously, press a button to allow the robot to continue, or give a verbal command to allow the robot to continue. This was intended to affect trust by altering the sense of control and involvement.","The study found no significant effect of involvement on trust, and the means went in the opposite direction from what was expected.","The study failed to replicate the original finding that involvement increases trust, and the means went in the opposite direction. The MDMT subscales were more sensitive than the overall score.","The study failed to replicate the original finding that being in the loop with a robot increases trust, and the pattern of means went in the opposite direction.","The robot navigated a path, stopped at an obstacle, and then either autonomously generated a new path, waited for a button press, or waited for a verbal command to execute the new path. The human participant observed the robot and answered questions.",ANOVA,"The study used ANOVA to compare the means of trust scores between the different involvement conditions (autonomous, button involvement, and verbal involvement). The analysis was performed on both the original two-item trust measure and the Multi-Dimensional Measure of Trust (MDMT), including its subscales. The purpose was to determine if the level of participant involvement affected their trust in the robot.",TRUE,Robot-autonomy,,Robot-autonomy,"The study manipulated the level of participant involvement by having them either observe the robot autonomously, press a button to allow the robot to continue, or give a verbal command to allow the robot to continue. This directly changes the level of decision authority given to the robot, which is why 'Robot-autonomy' is the most appropriate category. The study found no significant effect of involvement on trust, so 'Robot-autonomy' is listed under factors that did not impact trust.",10.1145/3434073.3444652,https://dl.acm.org/doi/10.1145/3434073.3444652,"As human-robot interaction (HRI) researchers, like all scientists, we must demonstrate the reproducibility of fndings—especially across robots. We present a three-study replication efort that illustrates the challenges and opportunities for replication science in HRI."
"Ullman, Daniel; Aladia, Salomi; Malle, Bertram F.",Challenges and Opportunities for Replication Science in HRI: A Case Study in Human-Robot Trust,2021,3,218,200,18,"4 because they failed to comprehend the experiment instructions, 3 because they were repeat participants, 11 due to technical malfunctions with the robot",Controlled Lab Environment,between-subjects,"Participants were told they would watch a robot complete a short task and then answer several questions about the scenario. Participants were randomly assigned to one of two conditions: autonomous or button involvement. The robot navigated a path and stopped at an obstacle, then either autonomously generated a new path or waited for a button press to execute the new path.",Participants observed a robot navigating a path and then answered questions about the scenario.,Thymio,Mobile Robots,Research,Evaluation,Rating,passive observation,Participants passively observed the robot's actions.,real-world,Participants observed a real robot in a lab setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed path and did not adapt to the user.,Questionnaires,,,Trust was measured using a two-item questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The study manipulated the level of participant involvement by having them either observe the robot autonomously or press a button to allow the robot to continue. This was intended to affect trust by altering the sense of control and involvement.,The study found no significant effect of involvement on trust.,The study failed to replicate the original finding that involvement increases trust.,The study failed to replicate the original finding that being in the loop with a robot increases trust.,"The robot navigated a path, stopped at an obstacle, and then either autonomously generated a new path or waited for a button press to execute the new path. The human participant observed the robot and answered questions.",ANOVA,The study used ANOVA to compare the means of trust scores between the two involvement conditions (autonomous and button involvement). The analysis was performed on the original two-item trust measure. The purpose was to determine if the level of participant involvement affected their trust in the robot.,TRUE,Robot-autonomy,,Robot-autonomy,"The study manipulated the level of participant involvement by having them either observe the robot autonomously or press a button to allow the robot to continue. This directly changes the level of decision authority given to the robot, which is why 'Robot-autonomy' is the most appropriate category. The study found no significant effect of involvement on trust, so 'Robot-autonomy' is listed under factors that did not impact trust.",10.1145/3434073.3444652,https://dl.acm.org/doi/10.1145/3434073.3444652,"As human-robot interaction (HRI) researchers, like all scientists, we must demonstrate the reproducibility of fndings—especially across robots. We present a three-study replication efort that illustrates the challenges and opportunities for replication science in HRI."
"Ullman, Daniel; Aladia, Salomi; Malle, Bertram F.",Challenges and Opportunities for Replication Science in HRI: A Case Study in Human-Robot Trust,2021,3,400,391,9,9 participants were excluded due to low-quality responses,Online Crowdsourcing,between-subjects,"Participants watched a video of a robot completing a short task and then answered several questions about the scenario. Participants were randomly assigned to one of four conditions: Thymio autonomous, Thymio button involvement, Create autonomous, or Create button involvement. Participants answered pre-video questions, watched the video, and then answered post-video questions.",Participants watched a video of a robot completing a task and then answered questions about the scenario.,Thymio; iRobot Create,Mobile Robots,Research,Evaluation,Rating,passive observation,Participants passively observed the robot's actions in a video.,media,Participants watched a video of the robot interaction.,physical,Participants viewed a video of a physical robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed path and did not adapt to the user.,Questionnaires; Multidimensional Measures,,,Trust was measured using questionnaires and a subscale of the MDMT.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The study manipulated the level of participant involvement by having them view a video of the robot either acting autonomously or waiting for a button press. This was intended to affect trust by altering the sense of control and involvement.,The study found no significant effect of involvement on trust. The only robot-specific effect was that the Thymio exceeded people's expectations more than the Create.,The study failed to replicate the original finding that involvement increases trust. The Thymio exceeded people's expectations more than the Create.,"The study found no effect of involvement on trust and that the Thymio exceeded people's expectations more than the Create, but this did not explain the non-replication of the original study.","The robot navigated a path, stopped at an obstacle, and then either autonomously generated a new path or waited for a button press to execute the new path. The human participant watched a video of the robot and answered questions.",ANOVA; ANOVA,"The study used MANOVA to analyze the effects of robot type (Thymio, Create) and involvement (autonomous, button involvement) on multiple dependent variables related to robot capacity perceptions (MDMT Capable subscale, speed, and accuracy) both before and after the video. ANOVA was used to analyze the subjective reports of whether expectations were met or exceeded, and to analyze the social-nonsocial trust gap.",TRUE,Robot-autonomy,,Robot-autonomy,"The study manipulated the level of participant involvement by having them view a video of the robot either acting autonomously or waiting for a button press. This directly changes the level of decision authority given to the robot, which is why 'Robot-autonomy' is the most appropriate category. The study found no significant effect of involvement on trust, so 'Robot-autonomy' is listed under factors that did not impact trust.",10.1145/3434073.3444652,https://dl.acm.org/doi/10.1145/3434073.3444652,"As human-robot interaction (HRI) researchers, like all scientists, we must demonstrate the reproducibility of fndings—especially across robots. We present a three-study replication efort that illustrates the challenges and opportunities for replication science in HRI."
"Ullrich, Daniel; Butz, Andreas; Diefenbach, Sarah",The Development of Overtrust: An Empirical Simulation and Psychological Analysis in the Context of Human–Robot Interaction,2021,1,110,110,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants imagined going on a safari trip and leaving their cat at home, using a pet feeding robot. They made daily choices between a safari trip or checking on the robot. The robot performed flawlessly for most of the study, but failed in the fourth week. Participants were exposed to either positive or negative demonstrations and reviews of the robot before the study.","Participants made daily choices between going on a safari trip or checking on the pet feeding robot, with the goal of keeping their cat alive.",Unspecified,Service and Assistive Robots,Care,Evaluation,Text Evaluation,minimal interaction,Participants interacted with the robot through a simulated scenario and made choices based on the robot's performance.,media,The study used video clips to present the robot's demonstration and the consequences of the participants' choices.,simulated,"The robot was presented through video clips, not as a physical entity.",pre-programmed (non-adaptive),"The robot's behavior was pre-programmed, with no adaptation to the user's actions.",Behavioral Measures; Questionnaires; Custom Scales,Schaefer's Trust Questionnaire/Scale,,"Trust was measured using a questionnaire, a custom scale, and behavioral measures based on the participants' choices.",no modeling,"The study did not use computational models of trust, relying on statistical analysis of the collected data.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the robot's performance through positive or negative demonstrations and reviews, influencing participants' expectations and trust. The robot's appearance was also manipulated through the video.","Repeated positive experience with the robot increased trust, while negative experiences decreased trust. Initial positive demonstrations and reviews had a temporary effect on trust, but were outweighed by personal experience.","The study found that personal experience with the robot had a much larger impact on trust than reputation or demonstration. The effect of reputation was only significant at the baseline and final measurement. The effect of demonstration was significant at baseline, week 1 and week 3, but the effect sizes were small.","Repeated positive experience with the robot led to increased trust and overtrust, while reputation and demonstration had a limited and temporary effect.","The robot was a pet feeding robot that was supposed to feed a cat. The human participant had to decide daily whether to go on a safari trip or check on the robot, with the goal of keeping the cat alive.",ANOVA; t-test; ANOVA; within-subjects contrasts; Chi-squared,"The study used a variety of statistical tests. Multivariate analysis of variance (MANOVA) was used to check the effectiveness of the experimental manipulations of reputation and capability demonstration on trust ratings. One-sample t-tests were used to confirm that the jungle trip was an effective reward. General linear model (GLM) analyses were used to examine the effects of experience (time), reputation, and capability demonstration on trust ratings and the number of control calls. Within-subjects contrasts were used to examine the differences between measurement points within the GLM analyses. Finally, a chi-squared test was used to compare cat death rates between pet owners and non-pet owners.",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-accuracy,Robot-verbal-communication-content,"The study manipulated the robot's performance by having it perform flawlessly for most of the study and then failing in the fourth week. This directly impacts the robot's accuracy in performing its task, which is feeding the cat, and thus is classified as 'Robot-accuracy'. The study also manipulated the robot's reputation through customer reviews, which is a form of verbal communication about the robot's performance and is classified as 'Robot-verbal-communication-content'. The study found that the robot's actual performance (accuracy) had a significant impact on trust, with repeated positive experiences increasing trust and the failure in the fourth week decreasing trust. The initial positive or negative reviews (verbal communication content) had a temporary effect on trust at the baseline, but this effect was outweighed by personal experience with the robot. Therefore, 'Robot-accuracy' impacted trust, while 'Robot-verbal-communication-content' did not have a lasting impact on trust.",10.3389/frobt.2021.554578,https://www.frontiersin.org/articles/10.3389/frobt.2021.554578/full,"With impressive developments in human–robot interaction it may seem that technology can do anything. Especially in the domain of social robots which suggest to be much more than programmed machines because of their anthropomorphic shape, people may overtrust the robot's actual capabilities and its reliability. This presents a serious problem, especially when personal well-being might be at stake. Hence, insights about the development and influencing factors of overtrust in robots may form an important basis for countermeasures and sensible design decisions. An empirical study [               N               = 110] explored the development of overtrust using the example of a pet feeding robot. A 2 × 2 experimental design and repeated measurements contrasted the effect of one's own experience, skill demonstration, and reputation through experience reports of others. The experiment was realized in a video environment where the participants had to imagine they were going on a four-week safari trip and leaving their beloved cat at home, making use of a pet feeding robot. Every day, the participants had to make a choice: go to a day safari without calling options (risk and reward) or make a boring car trip to another village to check if the feeding was successful and activate an emergency call if not (safe and no reward). In parallel to cases of overtrust in other domains (e.g., autopilot), the feeding robot performed flawlessly most of the time until in the fourth week; it performed faultily on three consecutive days, resulting in the cat's death if the participants had decided to go for the day safari on these days. As expected, with repeated positive experience about the robot's reliability on feeding the cat, trust levels rapidly increased and the number of control calls decreased. Compared to one's own experience, skill demonstration and reputation were largely neglected or only had a temporary effect. We integrate these findings in a conceptual model of (over)trust over time and connect these to related psychological concepts such as positivism, instant rewards, inappropriate generalization, wishful thinking, dissonance theory, and social concepts from human–human interaction. Limitations of the present study as well as implications for robot design and future research are discussed."
"Valori, Irene; Fan, Yichen; Jung, Merel; Fairhurst, Merle",Propensity to trust: comforting touch between trustworthy human and robot partners.,2023,1,153,142,11,"1 participant was excluded because they used a mobile device, 6 participants were excluded because their screen dimensions changed during the task, 4 participants did not provide a valid response to the affective state questions",Online Crowdsourcing,within-subjects,"Participants viewed comics depicting human-human or human-robot interactions where one character expressed sadness, the other initiated comforting touch, and the touched character reciprocated. Participants rated trustworthiness, interaction realism, touch appropriateness, touch pleasantness, and affective state of the characters.","Participants rated observed social interactions in comics, including trustworthiness of characters, realism, appropriateness, and pleasantness of touch, and the affective state of characters.",Pepper,Humanoid Robots,Social,Evaluation,Rating,passive observation,Participants observed interactions through static images.,media,The interaction was presented through static images.,physical,The robot was represented through static images.,not autonomous,"The robot's actions were depicted in static images, with no real autonomy.",Questionnaires; Custom Scales,Propensity to Trust Scales; Social Touch Questionnaire (STQ),,Trust was assessed using questionnaires and custom scales.,"parametric models (e.g., regression)",Generalized linear mixed-effects models were used to analyze the data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the partner (human or robot) and the role (vulnerable or comforting) in a social interaction scenario, presented through static images, to influence trust perceptions.","The robot was perceived as less trustworthy than humans, especially when expressing vulnerability. Propensity to trust technology reduced the gap between humans and robots. Reciprocity of touch buffered sadness.","The study found that humans are perceived as equally trustworthy when comforting or showing vulnerability, which expands the understanding of trust as an interactive mechanism. The robot was perceived as less trustworthy when expressing vulnerability, suggesting that people may not desire to interact with a robot that can express vulnerability.","Trust promotes positive appraisal of social touch, and there are two somewhat distinct systems for trusting other people or technology.","Participants observed static images of social interactions between two humans or a human and a robot. The robot and human were depicted as having a friendly relationship. Participants rated the trustworthiness of one character, and the realism, appropriateness, and pleasantness of touch, and the affective state of the characters.",generalized linear mixed-effects models; analysis of deviance (type iii wald chi-square test),"Generalized linear mixed-effects models were used to analyze the data, accounting for the repeated measures design. Analysis of deviance (Type III Wald chi-square test) was used to assess the effect of individual predictors and interactions within the models. The models examined the relationships between dependent variables (trustworthiness, interaction realism, touch appropriateness, touch pleasantness, valence, and arousal) and independent variables (partner, role, touch phase), as well as the moderating effects of propensity to trust others, propensity to trust technology, and touch aversion. The models also controlled for gender and individual variability.",TRUE,Robot-emotional-display; Robot-social-attitude,Robot-emotional-display; Robot-social-attitude,,"The study manipulated the role of the character (human or robot) as either vulnerable or comforting, which directly influences the perceived emotional display and social attitude of the character. The paper states, 'In different experimental conditions, one of the characters expressed emotional vulnerability by saying, ""I am sad,"" and the other performed a comforting gesture by touching their arm.' This manipulation of vulnerability and comforting roles directly impacts the emotional display of the character (Robot-emotional-display) and their social approach (Robot-social-attitude). The study found that the robot was perceived as less trustworthy when expressing vulnerability, indicating that these manipulations impacted trust levels. The study did not manipulate any other factors from the provided list.",10.21203/rs.3.rs-3738758/v1,https://www.researchsquare.com/article/rs-3738758/v1,"Abstract           Touching a friend to comfort or be comforted is a common prosocial behaviour, firmly based in mutual trust. Emphasising the interactive nature of trust and touch, we suggest that vulnerability, reciprocity and individual differences shape trust and perceptions of touch. We further investigate whether these elements also apply to companion robots. Participants (n = 152) were exposed to four comics depicting human-human or human-robot exchanges. Across conditions, one character was sad, the other initiated touch to comfort them, and the touchee reciprocated the touch. Participants first rated trustworthiness of a certain character (human or robot in a vulnerable or comforting role), then evaluated the two touch phases (initiation and reciprocity) in terms of interaction realism, touch appropriateness and pleasantness, affective state (valence and arousal) attributed to the characters. Results support an interactive account of trust and touch, with humans being equally trustworthy when comforting or showing vulnerability, and reciprocity of touch buffering sadness. Although these phenomena seem unique to humans, propensity to trust technology reduces the gap between how humans and robots are perceived. Two distinct trust systems emerge: one for human interactions and another for social technologies, both necessitating trust as a fundamental prerequisite for meaningful physical contact."
"Van Der Hoorn, Diede P.M.; Neerincx, Anouk; De Graaf, Maartje M.A.","""I think you are doing a bad job!"": The Effect of Blame Attribution by a Robot in Human-Robot Collaboration",2021,1,60,60,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants played a collaborative game with a robot in one of four conditions, with blame correctness and blame target as independent variables. The game had two rounds, with the first round used to establish a ranking and the second round to measure objective trust. Participants completed a questionnaire after the game.","Participants played a collaborative game with a robot, involving counting objects in images and identifying the predominant color in images.",Pepper,Humanoid Robots; Expressive Robots,Research; Social,Game,Cooperative Game,minimal interaction,Participants interacted with the robot verbally and through a computer interface.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical humanoid robot.,wizard of oz (directly controlled),The robot's responses were controlled by a human experimenter.,Behavioral Measures; Questionnaires; Custom Scales,,,Trust was measured using a combination of behavioral measures (compliance with robot's suggestions) and questionnaires (performance and social trust).,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot's blame attribution (target and correctness) was manipulated to influence trust. The robot either blamed itself or the human, correctly or incorrectly.","Blaming the human negatively affected social trust and willingness to collaborate, while self-blame increased trust and friendliness. Incorrect self-blame showed a trend towards even more positive evaluations.","A trend was observed where participants seemed to perceive a robot as even more friendly when it incorrectly blamed itself. There was also an interaction effect on humanlikeness, where people perceived a robot as more humanlike when it correctly blamed a human or incorrectly blamed itself. There was a lack of significance for the effect of blame attribution on subjective performance trust, which contradicts previous research.","People evaluate a robot more positively when it blames itself for collaborative failures, and this seems even more so when a robot incorrectly blames itself.","The robot verbally stated its answers to counting and color identification tasks, while the human entered the answers on a laptop. The robot then attributed blame for the team's performance, either to itself or the human.",ANOVA,"A series of two-way ANOVAs were conducted to test the hypotheses. The independent variables were blame correctness (correct vs. incorrect) and blame target (human vs. robot). The dependent variables included objective trust, subjective performance trust, subjective social trust, friendliness, humanlikeness, and willingness to collaborate in the future. Tukey's correction was used for pairwise comparisons. Normality checks and Levene's test were carried out to ensure assumptions were met.",TRUE,Robot-verbal-communication-content; Teaming,Robot-verbal-communication-content,,"The study manipulated the content of the robot's verbal communication by having it attribute blame either to itself or the human participant, and whether that blame was correct or incorrect. This is a manipulation of the content of the robot's verbal communication, thus 'Robot-verbal-communication-content' is the most appropriate category. The study also manipulated the collaborative setting by having participants play a game with the robot, which is a form of 'Teaming'. The results showed that the target of the blame (robot or human) significantly impacted social trust and willingness to collaborate, indicating that 'Robot-verbal-communication-content' influenced trust. The correctness of the blame did not significantly impact trust, so it is not included in 'factors_that_impacted_trust'. The 'Teaming' factor was a constant across all conditions and therefore did not impact trust.",10.1145/3434073.3444681,https://dl.acm.org/doi/10.1145/3434073.3444681,"Robots will increasingly collaborate with human partners necessitating research into how robots negotiate negative collaborative outcomes. This study investigates the effect of blame attribution on trust assessments in human-robot collaboration. Participants (n = 60) collaboratively played a game with a humanoid robot in one of four conditions in a 2 (blame correctness: correct vs. incorrect) by 2 (blame target: human vs. robot) between-subjects experiment. Results show that people evaluate a robot more positively when it blames itself for collaborative failures, especially, it seems, in the case of incorrect self-blame. Our findings indicate a need to further research on effective communication strategies for robots that need to negotiate collaborative failures without compromising the trust relationships with its human partner."
"Van Straten, Caroline L.; Peter, Jochen; Kühne, Rinaldo",Transparent robots: How children perceive and relate to a social robot that acknowledges its lack of human psychological capacities and machine status,2023,1,290,276,14,"11 children who had ASD, 3 children were excluded due to technical problems",Real-World Environment,between-subjects,"Children interacted with a Nao robot in a one-on-one setting. The robot either provided transparent information about its lack of human psychological capacities and machine status or provided neutral information. After the interaction, children completed a questionnaire.","Children played a guessing game with the robot, where they had to guess whether the robot's assertions were true or false.",Nao,Humanoid Robots; Expressive Robots,Research; Social,Game,Other Game subtask: A guessing game where the robot makes assertions and the child guesses if they are true or false.,minimal interaction,"Children had a short, one-on-one interaction with the robot, including verbal exchanges.",real-world,The interaction took place in a real-world setting with a physical robot.,physical,The study used a physical Nao robot for the interaction.,wizard of oz (directly controlled),The robot was controlled by an experimenter using the Wizard of Oz approach.,Questionnaires; Custom Scales,Larzelere and Huston's (1980) measure of the concept,,Trust was measured using a custom questionnaire based on Larzelere and Huston's measure.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The robot provided either transparent information about its machine status and lack of human psychological capacities or neutral information, influencing children's expectations and perceptions of the robot.",Providing transparent information decreased children's trust in the robot.,"The study found that children's tendency to anthropomorphize the robot mediated the effects of transparency on closeness and trust, while their perception of the robot's similarity to themselves only mediated children's feelings of closeness. The study also found that the robot's provision of transparent information decreased children's trust, which contrasts with the idea that children might trust a robot more if it is transparent.",A robot's provision of transparent information about its lack of human psychological capacities and machine status decreases children's feelings of closeness toward and trust in the robot.,"The robot made assertions, and the child guessed whether they were true or false. The robot provided explanations after each guess, either transparent or neutral. The human's role was to guess and listen to the robot's explanations.",ANOVA; process macro,"The study used ANOVAs to test the main effects of the experimental manipulation (robot providing transparent information vs. not) on the dependent variables: closeness, trust, anthropomorphism, and perceived similarity. The PROCESS macro was used to test the mediation hypotheses, examining whether the effects of the manipulation on closeness and trust were mediated by anthropomorphism and perceived similarity. The PROCESS macro used a bootstrapping method with 5000 samples.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the content of the robot's verbal communication. In the transparent condition, the robot provided information about its lack of human psychological capacities and machine status, while in the control condition, it provided neutral information. This manipulation directly influenced children's perceptions of the robot and their trust in it, as evidenced by the significant differences in trust levels between the two conditions. The paper states, 'Each time the child had guessed whether an assertion was true or false, as well as following the riddle, the robot provided the child with some explanation regarding the topic of the assertion (or riddle). Through these explanations, the robot either informed children about its lack of human psychological capacities and machine status (transparent condition) or provided children with some neutral information about itself that did not address these issues (control condition).' This clearly indicates a manipulation of the content of the robot's verbal communication. The results section also confirms that this manipulation impacted trust: 'As predicted in H1a and H1b, children in the transparent condition experienced less closeness toward and trust in the robot than did children who had not received transparent information.'",10.1016/j.ijhcs.2023.103063,https://linkinghub.elsevier.com/retrieve/pii/S1071581923000721,"Children will increasingly encounter, and form social relationships with, social robots. Accordingly, scholars have called for transparency toward children about what social robots are and what they can(not) do to manage children’s expectations of this new type of communication partner. Prior research has shown that the way adults present social robots to children can influence children’s perception of, and relationship formation with, a robot. To date, however, no studies have yet investigated whether a social robot’s own provision of transparent in­ formation about its (in)abilities can alter how children perceive and relate to it. To fill this gap initially, we conducted a one-factorial between-subject experiment among 276 children aged 8–10 years old. Children interacted with a robot that either provided them with information about its lack of human psychological ca­ pacities and machine status, or not. Exposure to this information decreased children’s feelings of closeness to­ ward and trust in the robot. Children’s tendency to anthropomorphize the robot mediated the effects of transparency on closeness and trust, while their perception of the robot’s similarity to themselves only mediated children’s feelings of closeness. Our findings are discussed in light of the ongoing ethical discussion on childrobot relationships."
"Vattheuer, Christopher; Baecker, Annalena Nora; Geiskkovitch, Denise Y.; Seo, Stela Hanbyeol; Rea, Daniel J.; Young, James E.",Blind Trust: How Making a Device Humanoid Reduces the Impact of Functional Errors on Trust,2020,1,39,24,15,"13 participants were excluded due to a failed manipulation check, 2 participants were excluded as outliers",Controlled Lab Environment,mixed design,"Participants interacted with a voice-command calculator in either tablet or humanoid form, first answering questions correctly, then with errors, and trust was measured before and after errors.",Participants used a voice-command calculator to answer restaurant-related math questions.,Pepper,Humanoid Robots; Expressive Robots,Research; Social,Social,Conversation,minimal interaction,Participants verbally interacted with the robot to complete a task.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical robot.,wizard of oz (directly controlled),The robot's responses were controlled by a human wizard.,Questionnaires; Multidimensional Measures,Multi-Dimensional Measure of Trust (MDMT),,Trust was measured using a multi-dimensional questionnaire.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's embodiment (tablet vs. humanoid) and performance (correct vs. incorrect answers) were manipulated to see how they affected trust.,"The humanoid robot was trusted more than the tablet, and trust decreased after errors for the tablet but not for the humanoid's moral trust.","The study found that moral trust in the humanoid robot did not decrease after errors, unlike the tablet, which is an unexpected result.","Making a kiosk humanoid increases trust and trust resilience, reducing the impact of functional errors on trust.","The robot acted as a voice-command calculator, responding to math questions, and the human asked the robot math questions and recorded the answers.",Mixed-effects model; t-test,"The study used 2x2 Mixed Model ANOVAs to analyze the effects of agent embodiment (tablet vs. humanoid, between-subjects) and agent error (before vs. after errors, within-subjects) on four trust subscales. Post-hoc t-tests with Bonferroni correction were then used to further investigate significant interaction effects found in the ANOVAs, specifically examining differences in trust between conditions and the impact of errors within each embodiment.",TRUE,Robot-aesthetics; Robot-accuracy,Robot-aesthetics; Robot-accuracy,,"The study manipulated the robot's embodiment (tablet vs. humanoid), which falls under 'Robot-aesthetics' as it changes the visual appearance and form of the agent. The study also manipulated the robot's performance by having it answer questions correctly initially and then making errors, which is categorized as 'Robot-accuracy' because it directly affects the task performance. The results showed that both the robot's embodiment and its accuracy impacted trust levels. Specifically, the humanoid form was trusted more, and trust decreased after errors for the tablet but not for the humanoid's moral trust. Therefore, both 'Robot-aesthetics' and 'Robot-accuracy' are listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",,http://link.springer.com/10.1007/978-3-030-62056-1_18,"Humanoid robots are starting to replace information kiosks in public spaces, providing increased engagement and an intuitive interface. Upgrading devices to be humanoid in this fashion may have unexpected consequences relating to the new, more social, embodiment. We investigated how altering a voice-command calculator kiosk, by making it humanoid, impacts user trust and trust resilience after functional errors. Our results indicate that making a kiosk humanoid increases both overall trust and trust resilience, where it reduces the impact of functional errors on trust. As public kiosks continue to be replaced by humanoids, this highlights the importance of understanding the full impact of this embodiment change on interaction."
"Verhagen, Ruben S.; Marcu, Alexandra; Neerincx, Mark A.; Tielman, Myrthe L.",The Influence of Interdependence on Trust Calibration in Human-Machine Teams,2024,1,80,80,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants completed a tutorial, then performed a search and rescue task with a virtual agent. The agent provided advice about rain, with one instance of incorrect advice to induce a trust violation. Participants completed trust questionnaires at three time points.","Participants collaborated with a virtual agent in a simulated search and rescue task, where they had to find and rescue victims while managing obstacles and weather events.",Unspecified,Mobile Robots,Research,Game,Cooperative Game,minimal interaction,Participants interacted with a virtual agent through a computer interface.,simulation,The interaction took place in a simulated 2D grid world environment.,simulated,The robot was a virtual agent within the simulation.,shared control (fixed rules),"The virtual agent followed pre-programmed rules for movement and advice, but the human controlled the actions of removing obstacles and rescuing victims.",Questionnaires; Behavioral Measures,,Performance Metrics,Trust was measured using a questionnaire and by tracking the participants' reliance on the robot's advice.,"parametric models (e.g., regression)",The study used statistical analysis to correlate trust and reliance with the robot's advice accuracy.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the interdependence relationships between the human and the virtual agent, and introduced a trust violation by providing incorrect advice. The task difficulty was also manipulated by adding rain events that could cause a loss of points if not avoided.","Trust decreased after the trust violation and partially recovered after the feedback, but did not fully recover in teams with joint actions. The correlation between perceived trust and machine trustworthiness was strongest in teams with joint actions.","The study found that trust did not fully recover after a violation in teams with joint actions, which is a deviation from typical findings where trust often recovers after a repair. The correlation between perceived trust and machine trustworthiness was strongest in teams with joint actions.","Interdependence relationships during human-machine teamwork influence human-machine trust calibration over time, and stronger interdependence relationships with joint actions facilitate more accurate human-machine trust calibration.","The virtual agent (RescueBot) moved to the closest unsearched area and provided advice about rain. The human participant controlled their avatar to remove obstacles and rescue victims, either independently or jointly with the virtual agent, depending on the interdependence condition.",ANOVA; nonparametric mixed anova; t-test; wilcoxon comparisons; Spearman correlation,"The study used both parametric and non-parametric mixed ANOVAs to investigate the effects of interdependence and time on perceived trust. Pairwise t-tests with Bonferroni correction were used to compare trust scores between different time points. Non-parametric mixed ANOVA was used to analyze the effects of time on the appropriate reliance rate. Wilcoxon comparisons with Bonferroni correction were used to compare reliance rates between time points. Additionally, Spearman's rank-order correlation was used to assess the relationship between perceived trust and advice accuracy for each interdependence condition.",TRUE,Teaming; Robot-verbal-communication-content; Task-constraints,Teaming; Robot-verbal-communication-content,Task-constraints,"The study manipulated the interdependence relationships between the human and the virtual agent, which falls under the 'Teaming' category, as it changes how the human collaborates with the robot (complete independence, complementary independence, optional interdependence, and required interdependence). The study also manipulated the robot's advice, including a trust violation by providing incorrect advice, which is categorized as 'Robot-verbal-communication-content' because it changes the content of the robot's communication. The task also included rain events that could cause a loss of points if not avoided, which is categorized as 'Task-constraints' because it adds a performance limit. The results showed that the interdependence relationships ('Teaming') and the trust violation ('Robot-verbal-communication-content') impacted trust levels, as trust decreased after the violation and did not fully recover in teams with joint actions. The task constraints ('Task-constraints') did not impact trust levels, as all interdependence conditions were homogeneous concerning how often they were injured by the rain.",,https://ebooks.iospress.nl/doi/10.3233/FAIA240203,"In human-machine teams, the strengths and weaknesses of both team members result in dependencies, opportunities, and requirements to collaborate. Managing these interdependence relationships is crucial for teamwork, as it is argued that they facilitate accurate trust calibration. Unfortunately, empirical research on the inﬂuence of interdependence on trust calibration during human-machine teamwork is lacking. Therefore, we conducted an experiment (n=80) to study the effect of interdependence relationships (complete independence, complementary independence, optional interdependence, required interdependence) on humanmachine trust calibration. Participants collaborated with a virtual agent during a simulated search and rescue task in teams characterized by one of the four interdependencies. A machine-induced trust violation was included in the task to facilitate dynamic trust calibration. Results show that the interdependence relationships during human-machine teamwork inﬂuence perceived trust calibration over time. Only in the teams with joint actions (optional and required interdependence) does perceived trust in the machine not recover to its initial pre-violated value. However, results show that the correlation between perceived trust in the machine and machine trustworthiness is strongest in these teams with joint actions, suggesting a more accurate trust calibration process. Overall, our ﬁndings provide some ﬁrst evidence that interdependence relationships during human-machine teamwork inﬂuence human-machine trust calibration."
"Volante, W.G.; Sanders, T.; Dodge, D.; Yerdon, V.A.; Hancock, P.A.",Specifying Influences that Mediate Trust in Human-Robot Interaction,2016,1,12,12,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed pre-experiment surveys, observed three robots performing a task with varying reliability, rated trust after each trial, and chose a robot for a final trial.","Participants observed a robot select a shape from a board, with the robot either correctly or incorrectly identifying the target shape.",Sphero 2.0; Lego Mindstorm EV3; Unspecified,Mobile Robots; Mobile Robots; Mobile Robots,Research,Evaluation,Rating,passive observation,Participants observed videos of the robots performing a task.,media,Participants watched videos of the robots performing the task.,physical,"The robots were physical, but participants only observed them through video.",pre-programmed (non-adaptive),The robots followed a pre-programmed sequence of actions.,Questionnaires; Custom Scales,Human-Robot Trust Scale/Questionnaire (HRTS/HRITS); Trust in Automation Scale (TAS); Interpersonal Trust Scale/Questionnaire; Propensity to Trust Scales; Negative Attitude towards Robots Scale (NARS),,Trust was measured using questionnaires and custom scales.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The reliability of the robot's performance was manipulated by having it either correctly or incorrectly identify the target shape, and the robot's appearance was varied by using three different robot forms.",Trust ratings were higher for reliable robots and for the robot chosen by the participant for the final trial. The Sphero robot received higher trust ratings than the other two robots.,"The trust ratings from the Human Robot Trust Scale increased in the fourth trial when participants chose the robot, but this increase was not observed in the Trust in Automation Scale, indicating that the two scales may be measuring different constructs.","Robot reliability significantly influences trust ratings, with reliable robots receiving higher trust scores than unreliable ones. Additionally, participant choice of robot for a final trial increased trust ratings.","The robot moved forward, scanned shapes, and identified a target shape, either correctly or incorrectly. The human observed the robot's actions and rated their trust in the robot.",,"The study mentions that no inferential statistics were performed on the pilot data set due to the small sample size (n=12) and multiple independent variables. Therefore, no statistical tests were applied.",TRUE,Robot-accuracy; Robot-aesthetics,Robot-accuracy; Robot-aesthetics,,"The study explicitly manipulated the reliability of the robot's performance by having it either correctly or incorrectly identify the target shape, which directly impacts the robot's accuracy. This is described in the 'Procedure' section: 'The robot would either perform this task reliable (it would select the correct shape) or unreliable (it would not select the correct shape)'. Therefore, 'Robot-accuracy' is an appropriate category. The study also used three different robot forms (Sphero, Lego Mindstorm, and a metal box robot), which varied in their visual appearance and design, thus manipulating 'Robot-aesthetics'. This is described in the 'Materials' section: 'Three robots were used in this experiment: a Sphero 2.0 (Figure 1), Lego Mindstorm EV3 (Figure 2), and a metal box style surveillance robot ""Robot 1"" (Figure 3). Each of the three robots model different physical form attributes.' The results section indicates that both robot reliability and robot form impacted trust ratings, as stated: 'Our first hypothesis, that reliable robots would receive higher trust ratings, was supported' and 'Trust ratings also varied based on form with the reliable Sphero receiving higher scores than Robot 1, or Mindstorm'. Therefore, both 'Robot-accuracy' and 'Robot-aesthetics' are listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1177/1541931213601402,http://journals.sagepub.com/doi/10.1177/1541931213601402,"In this work we investigate the effects of robot appearance and reliability on a user’s trust levels through an experiment where participants reacted to three different robot forms that either behaved reliably or unreliably during a series of experimental trials. A final trial was implemented to evaluate use choice by allowing participants to choose their preferred robot and complete an additional trial with that robot. Results from this pilot experimentation indicated differences based on the reliability of the robot, as well as whether the participant chose to interact with the robot."
"Wald, Sasha; Puthuveetil, Kavya; Erickson, Zackory",Do Mistakes Matter? Comparing Trust Responses of Different Age Groups to Errors Made by Physically Assistive Robots,2024,1,19,19,1,1 participant was excluded due to poor audio quality,Controlled Lab Environment,mixed design,"Participants completed a demographics questionnaire and the NARS scale. They then observed the robot performing either a bathing or feeding task, with errors introduced in later trials. Participants completed a questionnaire after each set of trials and answered open-ended questions at the end.","Participants observed a robot performing either a simulated bed-bathing task or a feeding task, with intentional errors introduced during some trials.",Stretch RE1; Obi,Mobile Manipulators; Service and Assistive Robots,Care; Research,Manipulation,Other Manipulation subtask: The robot performed a simulated bed-bathing task and a feeding task.,passive observation,Participants passively observed the robot performing tasks.,real-world,Participants observed the robot in a real-world setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions without adapting to the user.,Questionnaires; Custom Scales,Negative Attitude towards Robots Scale (NARS); Human-Computer Trust Scale/Questionnaire (HCT/HCTM),Speech Data,Trust was assessed using questionnaires and open-ended questions.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The researchers introduced intentional errors in the robot's performance to observe the impact on trust.,"For younger adults, trust decreased after errors were introduced in both tasks, but recovered in the bathing task. Older adults showed no significant change in trust after errors in the feeding task.","Older adults' trust was less affected by robot errors compared to younger adults. Older adults also tended to evaluate the robot based on factors unrelated to performance, while younger adults focused more on performance.","The impact of errors on trust depends on the user's age and the task being performed. Older adults' trust was more resilient to errors, and they tended to evaluate the robot on factors unrelated to performance.",The robot performed a simulated bed-bathing task by wiping a person's leg with a washcloth and a feeding task by bringing a spoon to the participant's mouth. The human participant observed the robot and answered questionnaires.,Wilcoxon signed-rank test; t-test,The study used a Wilcoxon signed-rank test to compare the subscale scores from different sets of trials within each task and participant group. This test was used to determine if there were statistically significant changes in trust scores after the introduction of errors. A two-sample t-test was used to compare the change in subscale scores between the younger and older adult populations to see if there were significant differences in how errors impacted their trust.,TRUE,Robot-accuracy,Robot-accuracy,,"The study explicitly manipulated the robot's accuracy by introducing intentional errors during the bathing and feeding tasks. The paper states, 'For each task, participants experienced several successful trials to establish some baseline trust, which we assessed with a questionnaire. We then randomly exposed participants to a number of manufactured errors and readministered the questionnaire to assess any fluctuation in their attitude toward the robot.' This clearly indicates that the researchers intentionally altered the robot's performance (accuracy) to observe its impact on trust. The results show that the errors impacted trust for younger adults in both tasks, and for older adults in the feeding task, but not in the bathing task. Therefore, Robot-accuracy is the factor that impacted trust. There were no other factors manipulated in the study.",10.1109/RO-MAN60168.2024.10731271,https://ieeexplore.ieee.org/document/10731271/,"Trust is a key factor in ensuring acceptable human-robot interaction, especially in settings where robots may be assisting with critical activities of daily living. When practically deployed, robots are bound to make occasional mistakes, yet the degree to which these errors will impact a care recipient’s trust in the robot, especially in performing physically assistive tasks, remains an open question. To investigate this, we conducted experiments where participants interacted with physically assistive robots which would occasionally make intentional mistakes while performing two different tasks: bathing and feeding. Our study considered the error response of two populations: younger adults at a university (median age 26) and older adults at an independent living facility (median age 83). We observed that the impact of errors on a users’ trust in the robot depends on both their age and the task that the robot is performing. We also found that older adults tend to evaluate the robot on factors unrelated to the robot’s performance, making their trust in the system more resilient to errors when compared to younger adults. Code and supplementary materials are available on our project webpage1."
"Walker, Francesco",Gaze Behaviour as a Measure of Trust in Automated Vehicles,2018,1,30,24,6,"5 due to the poor quality of their eye-tracking data, 1 because the post interview showed that the participant had previous experience with a Level 2 automated vehicle",Controlled Lab Environment,between-subjects,"Participants completed a pre-trust questionnaire, then viewed videos of a simulated automated vehicle in a driving simulator, performing a secondary task when they trusted the vehicle. They then completed a post-trust questionnaire.",Participants watched videos of a simulated automated vehicle and performed a secondary addition task when they felt the vehicle was driving safely.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants watched videos of a simulated vehicle and performed a secondary task.,media,Participants watched videos of a simulated driving scenario.,simulated,The robot was represented through videos of a simulated driving scenario.,pre-programmed (non-adaptive),The simulated vehicle's behavior was pre-programmed and did not adapt to the user.,Behavioral Measures; Questionnaires,Empirically Derived (ED) Scale,Eye-tracking Data,Trust was measured using a questionnaire and eye-tracking data.,no modeling,"Trust was not modeled computationally, only analyzed using statistical methods.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the reliability of the simulated vehicle by showing videos of either a perfect or a poor driving performance, which was intended to influence trust.","Participants who viewed the perfect vehicle reported higher trust and spent less time monitoring the road, while those who viewed the poor vehicle reported lower trust and spent more time monitoring the road.","The study found a negative correlation between self-reported trust and monitoring behavior, which is consistent with previous findings.","Gaze behavior, specifically monitoring frequency, can serve as an objective measure of trust in automated vehicles.","The simulated vehicle drove autonomously, and the human participant was instructed to monitor the road and perform a secondary task when they felt the vehicle was driving safely.",Wilcoxon rank sum; Mann-Whitney U; Pearson correlation,The study used a Wilcoxon signed ranks test to compare the ratings of the videos used in the main experiment. A Mann Whitney U test was used to compare the fixation data and self-reported trust scores between the two groups (Perfect Vehicle and Poor Vehicle). Pearson correlation was used to assess the relationship between self-reported trust and monitoring behavior (fixation count and duration).,TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the reliability of the simulated vehicle's driving performance. One group viewed videos of a 'Perfect Vehicle' that drove smoothly and safely, while the other group viewed videos of a 'Poor Vehicle' that struggled with lane keeping and braking. This manipulation directly affects the robot's accuracy in performing the driving task, which is why 'Robot-accuracy' is the most appropriate category. The paper states, 'One group viewed videos of a car handling the driving task perfectly... a second group viewed videos of a car struggling with the driving task'. The results showed that the 'Perfect Vehicle' group reported higher trust and spent less time monitoring the road, while the 'Poor Vehicle' group reported lower trust and spent more time monitoring the road. This indicates that the manipulation of 'Robot-accuracy' directly impacted trust levels. There were no other factors manipulated in the study.",,,"Extensive evidence shows that drivers’ intention to use self-driving technology is strongly modulated by trust, and that the benefits promised by automated vehicles will not be achieved if users do not trust them. It follows that vehicles should be designed to achieve optimal trust. However, there is no consensus as to how this should be assessed. To date, most studies have relied on self-reports, an approach subject to problems. In the driving simulator study reported here, we used drivers’ gaze behaviour during simulated Highly Automated Driving to investigate whether this could provide a more objective measure. The results indicated a negative relationship between self-reported trust and monitoring behaviour: The higher their self-reported trust, the less participants monitored the road, and the more attention they paid to a non-driving related secondary task. These findings suggest that gaze behaviour in a secondary task situation provides a more objective indicator of driver trust than self-reports."
"Walker, F.; Wang, J.; Martens, M. H.; Verwey, W. B.",Gaze behaviour and electrodermal activity: Objective measures of drivers’ trust in automated vehicles,2019,1,36,26,10,10 participants were excluded due to poor eye-tracking or EDA data quality,Controlled Lab Environment,mixed design,Participants were assigned to either a Perfect Vehicle or Poor Vehicle group and watched videos of automated driving in a simulator across three phases. They completed a secondary task and trust questionnaires after each phase. Gaze behavior and EDA were recorded.,Participants watched videos of automated driving and performed a visual Surrogate Reference Task (SuRT) on a tablet.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants watched videos of automated driving and performed a secondary task.,simulation,Participants viewed simulated driving scenarios on a screen.,simulated,The robot was represented through videos of a car driving.,pre-programmed (non-adaptive),The automated vehicle's behavior was pre-programmed in the videos.,Behavioral Measures; Physiological Measures; Questionnaires,Jian et al. Trust Scale,Eye-tracking Data; Physiological Signals,"Trust was measured using questionnaires, gaze behavior, and electrodermal activity.","parametric models (e.g., regression)","Linear regression was used to analyze the relationship between trust, gaze duration, and EDA.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the vehicle's driving behavior, showing either perfect or poor performance, to influence trust.","Trust was higher in the Perfect Vehicle group after phases 1 and 2, but lower in phase 3. The Poor Vehicle group's trust increased in phase 3.","The Poor Vehicle group's trust increased in Phase 3, despite the car behaving poorly, suggesting that predictability can be more important than performance. There was no significant difference in gaze behavior and EDA between the two groups in Phase 1, which was intended as a habituation period.","Gaze behavior and EDA, when combined, provide a better indication of drivers' trust in automation than either measure individually.","The robot (simulated automated vehicle) drove in a video, and the human participant watched the video and performed a secondary visual task on a tablet.",t-test; ANOVA; paired samples t-test; Mann-Whitney U; Friedman test; Wilcoxon rank sum; Pearson correlation; Linear regression,"The study used a variety of statistical tests to analyze the data. Independent samples t-tests were used to compare pre-test trust levels between groups and to compare trust levels between groups at each phase. A mixed ANOVA was used to analyze the effect of phase and group on self-reported trust. Paired samples t-tests were used to investigate the effect of phase within each group. Mann-Whitney U tests were used to compare gaze behavior and EDA between groups. Friedman tests were used to assess within-group differences across phases for gaze behavior and EDA. Wilcoxon Signed Ranks tests were used for post-hoc comparisons following significant Friedman tests. Pearson correlations were used to analyze the relationship between self-reported trust, monitoring behavior, and EDA. Finally, linear regression was used to test whether combining gaze duration and EDA could better predict self-reported trust than either measure alone.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the vehicle's driving behavior, presenting either 'Perfect' or 'Poor' performance. This directly impacts the robot's ability to perform the driving task accurately, which is why 'Robot-accuracy' is the appropriate category. The paper states, 'The Perfect Vehicle group viewed forward looking videos of a car handling the driving task perfectly... The Poor Vehicle group viewed videos of a car struggling with the driving task...' This manipulation of the vehicle's performance directly influenced the participants' trust levels, as evidenced by the differences in trust scores between the two groups across different phases. The 'Robot-accuracy' was the only factor that was intentionally manipulated to influence trust.",10.1016/j.trf.2019.05.021,http://www.sciencedirect.com/science/article/pii/S1369847818306703,"Studies show that drivers’ intention to use automated vehicles is strongly modulated by trust. It follows that their benefits are unlikely to be achieved if users do not trust them. To date, most studies of trust in automated vehicles have relied on self-reports. However, questionnaires cannot capture real-time changes in drivers’ trust, and are hard to use in applied settings. In previous work, we found evidence that gaze behaviour could provide an effective measure of trust. In this study we tested whether combining gaze behaviour with Electrodermal Activity could provide a stronger metric. The results indicated a strong relationship between self-reported trust, monitoring behaviour and Electrodermal Activity: The higher participants’ self-reported trust, the less they monitored the road, the more attention they paid to a non-driving related secondary task, and the lower their Electrodermal Activity. We also found evidence that combined measures of gaze behaviour and Electrodermal Activity predict self-reported trust better than either of these measures on its own. These findings suggest that such combined measures have the potential to provide a reliable and objective real-time indicator of driver trust."
"Walliser, James C.; de Visser, Ewart J.; Shaw, Tyler H.",Application of a System-Wide Trust Strategy when Supervising Multiple Autonomous Agents,2016,1,161,161,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants completed a target identification task using a UAV simulation. They were given the option to verify the automated target recognition system's determination by reviewing an image. Trust ratings were collected at three intervals: before, midway, and after the simulation.",Participants supervised four UAVs that used automated target recognition to identify targets as enemy or friendly. They could review images of the targets before classifying them.,Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a simulation of UAVs, with limited direct interaction.",simulation,The interaction took place in a simulated environment.,simulated,The robots were represented as simulated entities within the software.,fully autonomous (limited adaptation),"The UAVs operated autonomously, but with limited adaptation to the specific situation.",Behavioral Measures; Questionnaires,Propensity to Trust Scales,Performance Metrics,Trust was assessed using questionnaires and behavioral measures such as verification behavior and response time.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The accuracy of one UAV was manipulated to be either 70% or 100%, and participants were either informed or uninformed about the accuracy rates and received performance feedback.","A single inaccurate system led to reduced accuracy, more verification behavior, longer response times, and lower subjective trust for all systems. Performance information partially mitigated this effect.","Participants in the uninformed and 100% accuracy condition performed more verification behaviors and had slower response times than those in the uninformed and 70% accurate condition, suggesting they were expecting errors even when the system was perfect.","System-wide trust strategies are employed during interactions with multiple autonomous systems, where a single inaccurate system can negatively impact trust in all systems, even when others are reliable.","The UAVs autonomously flew to targets and provided an automated target recognition of enemy or friendly. The human participant's task was to supervise the UAVs, classify the targets, and had the option to verify the automated target recognition system's determination by reviewing an image.",ANOVA,"The study used a 4 (UAV Number) x 2 (Information Condition) x 2 (Accuracy) ANOVA with UAV Number as a within-subjects variable to analyze response accuracy, verification behavior, verification response time, and post-task subjective trust ratings. The ANOVA was used to determine the main effects and interactions of these variables on the dependent measures.",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-accuracy; Robot-verbal-communication-content,,"The study manipulated the accuracy of one of the four UAVs, making it either 70% or 100% accurate in identifying targets. This directly impacts the robot's performance on the task, thus it is classified as 'Robot-accuracy'. The study also manipulated whether participants were informed about the accuracy rates of the UAVs and whether they received performance feedback after each response. This manipulation of information provided to the participants is classified as 'Robot-verbal-communication-content' because it changes what information is communicated to the user about the robot's performance. Both of these manipulations were found to impact trust. The paper states, 'When one of the autonomous agents was inaccurate and performance information was provided, participants were 1) less accurate, 2) more likely to verify the ATR’s determination, 3) spent more time verifying images, and 4) rated the other systems as less trustworthy even though they were 100% correct.' This indicates that both the accuracy of the robot and the information provided about the robot's performance influenced trust.",10.1177/1541931213601031,http://journals.sagepub.com/doi/10.1177/1541931213601031,"When interacting with complex systems, the manner in which an operator trusts automation influences system performance. Recent studies have demonstrated that people tend to apply trust broadly rather than exhibiting specific trust in each component of the system in a calibrated manner (e.g. Keller & Rice, 2010). While this System–Wide Trust effect has been established for basic situations such as judging gauges, it has not been studied in realistic settings such as collaboration with autonomous agents in a multi-agent system. This study utilized a multiple UAV control simulation, to explore how people apply trust in multi autonomous agents in a supervisory control setting. Participants interacted with four UAVs that utilized automated target recognition (ATR) systems to identify targets as enemy or friendly. When one of the autonomous agents was inaccurate and performance information was provided, participants were 1) less accurate, 2) more likely to verify the ATR’s determination, 3) spent more time verifying images, and 4) rated the other systems as less trustworthy even though they were 100% correct. These findings support previous work that demonstrated the prevalence of system-wide trust and expand the conditions in which system-wide trust strategies are applied. This work suggests that multi-agent systems should provide carefully designed cues and training to mitigate the system-wide trust effect."
"Wan, Jingyan; Wu, Changxu",The Effects of Vibration Patterns of Take-Over Request and Non-Driving Tasks on Taking-Over Control of Automated Vehicles,2018,1,36,36,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a questionnaire, were briefed on the driving simulator, completed a practice block, and then completed the test block with six hazard events. They were instructed to engage in non-driving tasks and take over control when prompted by a tactile request. Data was collected on driving performance and subjective measures.","Participants were asked to engage in non-driving tasks while the automated vehicle was in control, and then take over control of the vehicle when prompted by a tactile warning.",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated automated vehicle and received tactile warnings.,simulation,The study used a driving simulator to create an interactive virtual environment.,simulated,The robot was a simulated automated vehicle within the driving simulator.,shared control (fixed rules),"The automated vehicle operated autonomously until a system boundary was reached, then the human took over control.",Questionnaires,Interpersonal Trust Scale/Questionnaire,Performance Metrics,Trust was measured using a questionnaire and performance metrics.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers manipulated the vibration patterns of the tactile take-over request and the type of non-driving task to influence driver response time and trust.,"The study found no significant effect of vibration patterns or non-driving tasks on subjective measures of trust. However, driving experience and annual mileage had a significant influence on trust.",The fastest response time was observed with Vibration Pattern 5 (back-back-seat-seat). The shortest response time and largest minimum TTC were observed when the driver took over vehicle control after monitoring the driving scenario. No interaction effects between vibration patterns and non-driving tasks were observed.,Vibration Pattern 5 (back-back-seat-seat) generated the fastest driver response to takeover requests across all non-driving tasks.,The robot (simulated automated vehicle) drove autonomously until a system boundary was reached. The human participant engaged in non-driving tasks and then took over control of the vehicle when prompted by a tactile warning.,Generalized linear models; hsd post-hoc test; t-test,"The study used a generalized linear model (GLM) analysis to examine the effects of vibration patterns and non-driving tasks on objective measures (e.g., response time, TTC) and subjective measures (e.g., perceived vibration intensity, trust). Covariates such as gender, driving experience, and annual mileage were included in the GLM. HSD post-hoc tests were used to further analyze significant main effects found in the GLM. A Paired t-test was used to compare sleepiness levels before and after the nap-taking task.",TRUE,Robot-interface-design; Task-complexity,,Robot-interface-design; Task-complexity,"The study manipulated the vibration patterns of the tactile take-over request, which is a form of interface design, thus 'Robot-interface-design' is selected. The study also manipulated the type of non-driving task the participant was engaged in, which influences the cognitive load and thus 'Task-complexity' is selected. The study found no significant effect of either vibration patterns or non-driving tasks on subjective measures of trust. Therefore, both 'Robot-interface-design' and 'Task-complexity' are listed under 'factors_that_did_not_impact_trust'. No factors were found to impact trust in this study.",10.1080/10447318.2017.1404778,https://www.tandfonline.com/doi/full/10.1080/10447318.2017.1404778,"Automated vehicles offer the possibility of significantly increasing traffic safety, mobility, and driver comfort, and reducing congestion and fuel emissions. Current automation technology, however, remains imperfect, and in certain situations, automation will still require the driver to suspend non-driving tasks and take back control of the automated vehicle in a limited period of time. During automated driving, drivers engaged in non-driving tasks (e.g., reading, taking a nap) may not perceive the visual or auditory take-over request in a timely nor accurate manner. Therefore, it is necessary to explore the potential of tactile warning further. This study investigates the effects of vibration patterns of take-over requests (six vibration patterns with different orders of the vibration location) and various realistic non-driving tasks (six non-driving tasks: reading, typing, watching videos, playing games, taking a nap, and monitoring the driving scenario on the driving simulator) on driver take-over behavior, and driver trust and acceptance of automated vehicles. Across all non-driving tasks, the fastest response time was observed with Vibration Pattern 5 (order of the vibration location: back–back–seat–seat). The shortest response time and largest minimum time-to-collision (TTC) also were observed when drivers took back control of the vehicle after monitoring the driving scenario. No interaction effects between vibration patterns and nondriving tasks were observed. Potential applications of the results of designing take-over requests in automated vehicles are discussed."
"Wang, Ning; Pynadath, David V; Hill, Susan G",The Impact of POMDP-Generated Explanations on Trust and Performance in Human-Robot Teams,2016,1,220,202,18,18 participants were excluded due to incomplete entries,Online Crowdsourcing,between-subjects,"Participants read an information sheet, completed a background survey, then interacted with a simulated robot for three reconnaissance missions, completing a post-mission survey after each mission.","Participants worked with a simulated robot on reconnaissance missions, deciding whether to equip protective gear before entering buildings based on the robot's recommendations.",Unspecified,Mobile Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated robot through a web browser.,simulation,The interaction took place in a simulated environment presented through a web browser.,simulated,The robot was a simulated entity within the online testbed.,fully autonomous (limited adaptation),"The robot autonomously made decisions based on its POMDP model, with limited adaptation to the human's behavior.",Questionnaires; Behavioral Measures,NASA Task Load Index (NASA-TLX); Situational Awareness Rating Technique (SART); Interpersonal Trust Scale/Questionnaire,Performance Metrics,"Trust was measured using questionnaires and behavioral measures, including mission success and decision correctness.",no modeling,Trust was not modeled computationally; the study focused on statistical analysis of the collected data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's ability (high or low) and the type of explanations it provided (no explanation, two sensor readings, three sensor readings, or confidence level) were manipulated to influence trust.","Explanations that facilitated decision-making increased trust in the robot's ability, particularly for the low-ability robot. The high-ability robot's explanations did not significantly impact trust.","Participants reported higher understanding of the robot's decision-making process when the robot had high ability, which was unexpected. Also, explanations that did not facilitate decision-making were as badly regarded as when no explanations were offered at all.","Explanations that facilitate decision-making, such as confidence-level explanations and explanations of three sensors, improved trust in the robot's ability and team performance, especially when the robot had low ability.","The robot acted as a scout, scanning buildings for danger and providing recommendations to the human teammate. The human teammate decided whether to equip protective gear before entering buildings based on the robot's recommendations.",ANOVA; Tukey HSD,"The study used ANOVA tests to examine the main effects of robot ability and explanation type on trust, understanding of the robot's decisions, mission success rate, and percentage of correct decisions. Tukey HSD tests were then used for post-hoc pairwise comparisons to determine which specific explanation types differed significantly from each other. The analysis also explored interaction effects between robot ability and explanation type on the dependent variables.",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-verbal-communication-content,Robot-accuracy,"The study manipulated two main factors: the robot's accuracy and the content of its verbal communication. 'Robot-accuracy' was manipulated by having two levels: a high-ability robot that made correct decisions 100% of the time and a low-ability robot that had a faulty camera and made false-negative errors. This is explicitly stated in the 'Design' section: 'The ability variable has two levels: a low-ability robot vs. a high-ability robot... High Ability The robot with high ability makes the correct decision 100% of the time. Low Ability The robot with low ability has a faulty camera and makes only false-negative mistakes...'. 'Robot-verbal-communication-content' was manipulated by varying the type of explanations the robot provided: no explanation, explanation of two sensor readings, explanation of three sensor readings, and confidence-level explanations. This is described in the 'Design' section: 'The explanation variable has a total of four levels: no explanation, explanation of two sensor readings, explanation of three sensor readings, and confidence-level explanation...'. The results showed that the type of explanation ('Robot-verbal-communication-content') significantly impacted trust, with decision-facilitating explanations (three sensor and confidence-level) leading to higher trust, especially for the low-ability robot. The 'Results' section states: 'Overall, explanations that facilitate decision-making (e.g., confidence-level explanations, and explanations of 3 sensors) helped the participants succeed in more missions and made the participants feel that they can trust the robot's ability more.' However, the robot's accuracy ('Robot-accuracy') did not impact trust when the robot was high-ability, as stated in the 'Explanations and High-Ability Robot' section: 'ANOVA and Tukey's HSD tests revealed that there was no statistically significant impact of the robot's explanations on trust in the robot's ability... when the robot is making correct recommendations 100% of the time.'",,,"Researchers have observed that people will more accurately trust an autonomous system, such as a robot, if they have a more accurate understanding of its decision-making process. Studies have shown that hand-crafted explanations can help maintain eﬀective team performance even when the system is less than 100% reliable. However, current explanation algorithms are not suﬃcient for making a robot’s quantitative reasoning (in terms of both uncertainty and conﬂicting goals) transparent to human teammates. In this work, we develop a novel mechanism for robots to automatically generate explanations of reasoning based on Partially Observable Markov Decision Problems (POMDPs). Within this mechanism, we implement alternate natural-language templates and then measure their diﬀerential impact on trust and team performance within an agent-based online testbed that simulates a human-robot team task. The results demonstrate that the added explanation capability leads to improvement in transparency, trust, and team performance. Furthermore, by observing the diﬀerent outcomes due to variations in the robot’s explanation content, we gain valuable insight that can help lead to reﬁnement of explanation algorithms to further improve human-robot interaction."
"Wang, Ning; Pynadath, David V; Hill, Susan G",Building Trust in a Human-Robot Team with Automatically Generated Explanations,2015,1,120,121,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants completed a background survey, then worked with a simulated robot on three reconnaissance missions, and completed a post-mission survey after each mission.","Participants worked with a simulated robot on reconnaissance missions to gather intelligence in a foreign town, deciding whether to enter buildings with or without protective gear based on the robot's findings.",Unspecified,Mobile Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated robot in an online environment.,simulation,The interaction took place in an online simulation environment.,simulated,The robot was a simulated entity within the online environment.,fully autonomous (limited adaptation),"The robot autonomously made decisions based on its sensor data and POMDP model, but with limited adaptation to the user.",Questionnaires; Custom Scales,Mayer and Davis' Trust/Trustworthiness Scales (1999); NASA Task Load Index (NASA-TLX); Situational Awareness Rating Technique (SART); Schaefer's Trust Questionnaire/Scale,Performance Metrics,"Trust was measured using questionnaires and custom scales, and performance metrics were collected.",POMDP,The robot's decision-making was based on a POMDP model.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's ability (high vs. low) and the presence of explanations (with or without) were manipulated to influence trust. The robot's ability was manipulated by making the camera sensor faulty, and explanations were manipulated by including or excluding information about the robot's sensors and goals.","The robot's ability significantly impacted trust, with higher trust for the high-ability robot. Explanations improved the understanding of the robot's decision-making process, and marginally increased trust in the high-ability robot.","Participants who interacted with a high-ability robot reported that they understood the robot's decisions and decision-making process more than participants who interacted with a low-ability robot, which was unexpected. The explanations did not significantly impact mission success or trust in the robot's ability overall, but did improve understanding of the robot's decision-making process.","Explanations improved perceived transparency and understanding of the robot's decision-making process, but did not significantly impact overall trust in the robot's ability, except for a marginal increase when the robot had high ability.","The robot acted as a scout, scanning buildings for potential danger and relaying its findings to the human teammate. The human teammate decided whether to enter buildings with or without protective gear based on the robot's recommendations.",ANOVA,"A one-way ANOVA test was used to compare mission success rates and trust in the robot's ability between participants who interacted with a high-ability robot and those who interacted with a low-ability robot. It was also used to compare the understanding of the robot's decisions and decision-making process between participants who interacted with a robot that offered explanations and those who did not. Further analysis was conducted within the high and low ability groups to compare the impact of explanations on mission success, trust, and understanding.",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-accuracy; Robot-verbal-communication-content,,"The study manipulated the robot's ability (high vs. low) by making the camera sensor faulty, which directly impacts the robot's accuracy in detecting threats, thus influencing task performance. This is categorized as 'Robot-accuracy'. The study also manipulated the presence of explanations (with or without), which changes the content of the robot's verbal communication, specifically including or excluding information about the robot's sensors and goals. This is categorized as 'Robot-verbal-communication-content'. The results showed that both the robot's ability (accuracy) and the presence of explanations (verbal communication content) impacted trust. Specifically, higher ability led to higher trust, and explanations improved understanding of the robot's decision-making process and marginally increased trust in the high-ability robot. Therefore, both 'Robot-accuracy' and 'Robot-verbal-communication-content' are listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",,,"Technological advances offer the promise of robotic systems that work with people to form human-robot teams that are more capable than their individual members. Unfortunately, the increasing capability of such autonomous systems has often failed to increase the capability of the human-robot team. Studies have identified many causes underlying these failures, but one critical aspect of a successful human-machine interaction is trust. When robots are more suited than humans for a certain task, we want the humans to trust the robots to perform that task. When the robots are less suited, we want the humans to appropriately gauge the robots’ ability and have people perform the task manually. Failure to do so results in disuse of robots in the former case and misuse in the latter. Real-world case studies and laboratory experiments show that failures in both cases are common. Researchers have theorized that people will more accurately trust an autonomous system, such as a robot, if they have a more accurate understanding of its decision-making process. Studies show that explanations offered by an automated system can help maintain trust with the humans in case the system makes an error, indicating that the robot’s communication transparency can be an important factor in earning an appropriate level of trust. To study how robots can communicate their decisionmaking process to humans, we have designed an agent-based online test-bed that supports virtual simulation of domain-independent human-robot interaction. In the simulation, humans work together with virtual robots as a team. The test-bed allows researchers to conduct online human-subject studies and gain better understanding of how robot communication can improve human-robot team performance by fostering better trust relationships between humans and their robot teammates. In this paper, we describe the details of our design, and illustrate its operation with an example human-robot team reconnaissance task."
"Wang, Ning; Pynadath, David V.; Hill, Susan G.",Trust calibration within a human-robot team: Comparing automatically generated explanations,2016,1,160,140,20,"20 participants were excluded due to incomplete entries (e.g., participants skipped survey questions or left the simulations)",Online Crowdsourcing,between-subjects,"Participants read an information sheet, completed a background survey, then worked with a simulated robot on 3 reconnaissance missions, and completed a post-mission survey after each mission.","Participants worked with a simulated robot in reconnaissance missions, deciding whether to enter buildings with or without protective gear based on the robot's recommendations.",Unspecified,Mobile Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated robot through an online testbed.,simulation,The interaction took place in a virtual environment simulating a reconnaissance mission.,simulated,The robot was a virtual representation within the simulation.,fully autonomous (limited adaptation),"The robot autonomously made decisions based on its POMDP model, with limited adaptation to the human's behavior.",Questionnaires; Behavioral Measures,Interpersonal Trust Scale/Questionnaire; NASA Task Load Index (NASA-TLX); Situational Awareness Rating Technique (SART); Trust in Automation Scale (TAS); Human-Robot Trust Scale/Questionnaire (HRTS/HRITS),Performance Metrics,"Trust was measured using questionnaires and behavioral data, including compliance with the robot's recommendations.",no modeling,Trust was not modeled computationally; the study focused on statistical analysis of the collected data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's ability (high or low) and the type of explanation (confidence-level, observation, or no explanation) were directly manipulated to influence trust.","Explanations improved trust, transparency, and team performance, especially when the robot had low ability. High-ability robots did not show significant changes in trust with explanations.","Participants trusted the low-ability robot more when it offered explanations, which was unexpected. The compliance rate to the high-ability robot was less than 100%, indicating disuse despite its reliability.","Robot explanations, particularly confidence-level and observation explanations, improved transparency, trust, and team performance, especially when the robot had low ability.","The robot acted as a scout, scanning buildings for danger and relaying its findings to the human. The human decided whether to enter buildings with or without protective gear based on the robot's recommendations.",pairwise correlation tests; ANOVA; Tukey HSD,"The study used pairwise correlation tests to examine the relationships between mission success, trust, transparency, correct decisions, and compliance. A 2x3 ANOVA was used to analyze the main effects of robot ability (high, low) and explanation type (no explanation, confidence-level, observation) on trust, transparency, compliance, mission success, and correct decisions. Tukey HSD tests were then conducted to perform pairwise comparisons between the different explanation conditions.",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-accuracy; Robot-verbal-communication-content,,"The study manipulated two factors: Robot-accuracy and Robot-verbal-communication-content. Robot-accuracy was manipulated by having a high-ability robot that was always correct and a low-ability robot that made false-negative errors (""
The ability variable has two levels: low and high. The robot with high ability makes the correct decision 100% of the time. The one with low ability has a faulty camera and makes false-negative mistakes, e.g., not detecting armed gunmen in the simulation.""). This directly impacts the robot's performance on the task. Robot-verbal-communication-content was manipulated by varying the type of explanation the robot provided: no explanation, confidence-level explanation, or observation explanation (""
The explanation variable has three levels: confidence-level explanation, observation explanation and no explanation.""). These explanations directly alter the content of the robot's communication. Both of these factors were found to impact trust. The paper states that ""Results indicate that the robot explanations on either confidence-level or observations helped build transparency and trust, and improved decisionmaking and team performance, particularly so when the robot's ability was low."" and ""The 2x3 ANOVA tests also show that there are significant interaction effects between the robot's ability and the explanation offered on trust"". There were no factors that were manipulated that did not impact trust.",10.1109/HRI.2016.7451741,,"Trust is a critical factor for achieving the full potential of human-robot teams. Researchers have theorized that people will more accurately trust an autonomous system, such as a robot, if they have a more accurate understanding of its decision-making process. Studies have shown that hand-crafted explanations can help maintain trust when the system is less than 100% reliable. In this work, we leverage existing agent algorithms to provide a domain-independent mechanism for robots to automatically generate such explanations. To measure the explanation mechanism's impact on trust, we collected self-reported survey data and behavioral data in an agent-based online testbed that simulates a human-robot team task. The results demonstrate that the added explanation capability led to improvement in transparency, trust, and team performance. Furthermore, by observing the different outcomes due to variations in the robot's explanation content, we gain valuable insight that can help lead to refinement of explanation algorithms to further improve human-robot trust calibration."
"Wang, Ning; Pynadath, David V.; Hill, Susan G.; Merchant, Chirag",The Dynamics of Human-Agent Trust with POMDP-Generated Explanations,2017,1,105,105,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants completed three missions in a virtual search and rescue scenario, and after each mission, they filled out a survey containing measures of trust and understanding of the robot's decision-making process.","Participants worked with a virtual robot to search buildings, deciding whether to enter with or without protective gear.",Unspecified,Mobile Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a virtual robot in a simulated environment.,simulation,The interaction took place in a simulated environment.,simulated,The robot was a virtual representation in the simulation.,fully autonomous (limited adaptation),The robot operated autonomously but with a pre-programmed error.,Behavioral Measures; Questionnaires,,Performance Metrics,Trust was measured using self-report questionnaires and behavioral measures such as compliance with the robot's recommendations.,no modeling,Trust was not modeled computationally; the study focused on analyzing correlations between trust and behavior.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's camera was made faulty to induce errors, and different levels of explanation were provided to participants, influencing their expectations and trust.","Trust was higher when participants correctly identified and responded to the robot's errors, and when they made correct decisions, regardless of compliance.",Participants who correctly ignored a robot's incorrect recommendation and then followed its subsequent correct one reported significantly higher levels of trust. The correlation between self-reported trust and correctness was stronger than that of compliance.,"A person's trust in the robot's ability depends more on the human-robot team's combined decision-making, rather than on the robot's decision-making in isolation.","The robot provided recommendations on whether to enter a building with or without protective gear, and the human decided whether to follow the robot's recommendation.",ANOVA; Pearson correlation; steiger's z-test,The study used one-way ANOVA to compare self-reported trust levels based on participants' actions following trust reports and responses to robot errors. Pearson correlation tests were used to assess the relationship between self-reported trust and both compliance with robot recommendations and the correctness of participants' decisions. Steiger's Z-tests were used to compare the strength of correlations between self-reported trust and compliance versus correctness.,TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-verbal-communication-content; Robot-accuracy,,"The study manipulated 'Robot-verbal-communication-content' by providing different levels of explanation for the robot's decisions (no explanation, explanation of two sensor readings, explanation of three sensor readings, and a confidence-level explanation). This is explicitly stated in the 'Evaluation' section: 'We studied four levels of explanation [3]: no explanation, explanation of two sensor readings, explanation of three sensor readings, and a confidence-level explanation'. The study also manipulated 'Robot-accuracy' by introducing a faulty camera that caused the robot to make errors, as stated in the 'Evaluation' section: 'To induce trust failures, we introduce an error into our otherwise optimal simulated robot, namely a faulty camera that cannot detect armed gunmen. As a result, it will occasionally give an incorrect ""safe"" assessment.' Both of these manipulations were found to impact trust. The 'Error Response' section shows that participants who correctly identified and responded to the robot's errors reported higher trust. The 'Correct Behavior' section shows that the correlation between correctness and trust was stronger than compliance, indicating that the robot's accuracy (or lack thereof) and the human's understanding of it impacted trust. The study did not explicitly state any factors that were manipulated but did not impact trust.",,http://link.springer.com/10.1007/978-3-319-67401-8_58,
"Wang, Chenlan; Zhang, Chongjie; Yang, X. Jessie",Automation reliability and trust: A Bayesian inference approach,2018,1,26,26,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a simulated surveillance task with a tracking and detection component. They used a joystick for tracking and detected threats in images. An imperfect threat detector assisted them. After each trial, participants reported their perceived automation reliability, trust, and confidence.","Participants performed a dual task involving tracking a green circle with a joystick and detecting threats in images, with an automated threat detector providing assistance.",Unspecified,Other,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a simulated system, using a joystick and observing a screen.",simulation,The interaction was conducted in a simulated environment.,simulated,The robot was a simulated threat detector.,pre-programmed (non-adaptive),The automated threat detector had a fixed reliability and did not adapt to the user.,Questionnaires,,Performance Metrics,Trust was measured using questionnaires and performance metrics were collected.,"parametric models (e.g., regression)",Bayesian inference was used to model the perceived reliability and its correlation with trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the automated threat detector was manipulated at three levels (70%, 80%, and 90%), influencing participants' expectations and perceived performance.","The study found that trust correlated with the Bayesian estimates of system reliability, with higher reliability leading to higher trust.","Participants were categorized into three types based on their responses: those whose reliability estimates fit the Bayesian model, those who considered the automation unreliable, and those who were highly sensitive to automation performance. The Bayesian model did not fit the second and third types of participants.","Human operators' learning and inference process for automation reliability can be approximated by Bayesian inference, and this estimate correlates with reported trust.","The robot, a simulated threat detector, provided assistance in detecting threats. The human participant used a joystick to track a circle and monitored the threat detector's performance, reporting their perceived reliability and trust after each trial.",Root Mean Square Error; Pearson correlation,"The study used Root Mean Square Error (RMSE) to compare the Bayesian estimates of system reliability with the participants' reported system reliability. Correlation Coefficient was used to measure the correlation between the Bayesian estimates and the reported system reliability, and also between the Bayesian estimates of system reliability and the participants' reported trust.",TRUE,Robot-accuracy,Robot-accuracy,,"The study explicitly manipulated the reliability of the automated threat detector at three levels (70%, 80%, and 90%). This manipulation directly affected the accuracy of the robot's performance in detecting threats, which is a core aspect of 'Robot-accuracy'. The paper states, 'The system reliability of the threat detector was set as 70%, 80%, and 90% according to the signal detection theory (SDT).' The results showed that trust correlated with the Bayesian estimates of system reliability, indicating that the manipulated 'Robot-accuracy' impacted trust. The paper states, 'We then correlated the Bayesian estimates with human operators’ reported trust and found moderate correlations.' There were no other factors manipulated in the study.",10.1177/1541931218621048,http://journals.sagepub.com/doi/10.1177/1541931218621048,"Research shows that over repeated interactions with automation, human operators are able to learn how reliable the automation is and update their trust in automation. The goal of the present study is to investigate if this learning and inference process approximately follow the principle of Bayesian probabilistic inference. First, we applied Bayesian inference to estimate human operators’ perceived system reliability and found high correlations between the Bayesian estimates and the perceived reliability for the majority of the participants. We then correlated the Bayesian estimates with human operators’ reported trust and found moderate correlations for a large portion of the participants. Our results suggest that human operators’ learning and inference process for automation reliability can be approximated by Bayesian inference."
"Wang, Min; Hussein, Aya; Rojas, Raul Fernandez; Shafi, Kamran; Abbass, Hussein A.",EEG-Based Neural Correlates of Trust in Human-Autonomy Interaction,2018,1,10,10,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants played an investment game with three artificial agents with varying levels of trustworthiness. EEG data was collected during the game. Participants were given a virtual budget to distribute among the agents across 30 rounds per scenario, with 8 different scenarios presented in random order.",Participants made investment decisions in a game with artificial agents.,Unspecified,Other,Research,Game,Economic Game,minimal interaction,Participants interacted with the artificial agents through a computer interface.,simulation,The interaction was presented through a computer simulation.,simulated,The robots were represented as artificial agents on a computer screen.,pre-programmed (non-adaptive),The artificial agents followed pre-programmed behaviors.,Physiological Measures,,Physiological Signals,Trust was assessed using EEG signals.,"parametric models (e.g., regression)",Mixed model analysis was used to correlate EEG features with trustworthiness.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The trustworthiness of the artificial agents was directly manipulated by varying the probability of a positive return on investment, influencing the perceived reliability of the agents.",EEG features from the frontal and occipital areas were significantly correlated with the manipulated trustworthiness of the agents.,"The study found significant correlations between EEG features and trust, particularly in the frontal and occipital areas, with the Gamma and Delta frequency bands showing the largest effect sizes. AR coefficients were not significantly correlated with trust.",EEG signals from the frontal and occipital areas are significantly correlated with the perceived trustworthiness of artificial agents.,"The human participants distributed a virtual budget among three artificial agents with different levels of trustworthiness, while the artificial agents provided a return on investment based on their pre-set probabilities.",mixed model analysis,"Mixed model analysis was used to examine the correlation between EEG signal features (frequency bands, AR coefficients, and signal complexity) and the trustworthiness of the artificial agents. The trustworthiness level was treated as a fixed effect, while subjects were treated as random effects. The analysis aimed to determine if EEG signals were significantly correlated with the manipulated trustworthiness of the agents.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the trustworthiness of the artificial agents by varying the probability of a positive return on investment. This directly impacts the perceived reliability and success rate of the agents, which falls under the 'Robot-accuracy' category. The paper states, 'Each agent has a different level of trustworthiness which is displayed on the screen along with their probability... a high trustworthy agent is more likely to bring a positive return on investment based on the probability it promises. On the other hand, a low trustworthy agent is less likely to produce a positive return.' The results showed that EEG features were significantly correlated with the manipulated trustworthiness, indicating that the manipulation of 'Robot-accuracy' impacted trust. The study did not explicitly state any other manipulated factors, and no factors were found to not impact trust.",10.1109/SSCI.2018.8628649,https://ieeexplore.ieee.org/document/8628649/,"This paper aims at identifying the neural correlates of human trust in autonomous systems using Electroencephalography (EEG) signals. Quantifying the relationship between trust and brain activities allows for real-time assessment of human trust in automation. This line of effort contributes to the design of trusted autonomous systems, and more generally, modeling the interaction in human-autonomy interaction."
"Wang, Ning; Pynadath, David V.; Rovira, Ericka; Barnes, Michael J.; Hill, Susan G.","Is It My Looks? Or Something I Said? The Impact of Explanations, Embodiment, and Expectations on Trust and Performance in Human-Robot Teams",2018,1,61,61,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants completed a background survey, then worked with a simulated robot on 8 reconnaissance missions, each with a different robot variation. After each mission, participants completed a post-mission survey.","Participants worked with a robot to search 15 buildings in a town, deciding whether to enter with or without protective gear based on the robot's recommendations.",Unspecified,Legged Robots; Mobile Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated robot through an online interface.,simulation,The interaction was conducted in a simulated environment.,simulated,The robot was presented as a simulated entity with different visual embodiments.,fully autonomous (limited adaptation),"The robot made decisions autonomously based on its POMDP model, but with fixed parameters.",Questionnaires; Behavioral Measures,NASA Task Load Index (NASA-TLX); Situational Awareness Rating Technique (SART); Interpersonal Trust Scale/Questionnaire,Performance Metrics,Trust was measured using questionnaires and behavioral measures such as compliance and correct decisions.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's embodiment (dog-like vs. machine-like), explanation (with or without confidence level), and acknowledgement of mistakes (with or without promise to learn) were directly manipulated to influence trust.","Explanations significantly increased trust, while embodiment had a marginally significant effect, with the machine-like robot being trusted more initially. Acknowledgment of mistakes had no significant main effect on trust, but interacted with explanations.","Participants initially trusted the dog-like robot less than the machine-like robot, contrary to the hypothesis. The effect of embodiment on trust decreased over time, while the effect of explanations remained significant. Participants scored lower on the transparency test when explanations were offered, possibly because they relied on the explanations without thinking about the robot's decision-making process.","Providing explanations of the robot's decision-making process, even without detailed information about the underlying mechanisms, significantly improved trust and team performance.","The robot acted as a scout, scanning buildings for danger and providing recommendations to the human. The human decided whether to enter buildings with or without protective gear based on the robot's recommendations.",ANOVA; ANOVA,"A General Linear Model analysis with Repeated Measures and Bonferroni corrections was used to examine the effects of explanation, embodiment, and promise to improve on trust, transparency, compliance, and correct decisions. Additionally, ANOVAs were used to analyze the effect of embodiment, explanation, and acknowledgement on self-reported trust at different time points (specifically after the first mission and across all missions).",TRUE,Robot-aesthetics; Robot-verbal-communication-content; Robot-adaptability,Robot-verbal-communication-content,Robot-aesthetics; Robot-adaptability,"The study manipulated the robot's appearance (dog-like vs. machine-like), which falls under 'Robot-aesthetics'. The robot's communication was manipulated by providing or not providing explanations of its decisions, including a confidence level, which is categorized as 'Robot-verbal-communication-content'. The robot also either acknowledged mistakes and promised to learn or did not, which is categorized as 'Robot-adaptability' because it implies a change in the robot's learning capability. The results showed that 'Robot-verbal-communication-content' (explanations) significantly impacted trust. 'Robot-aesthetics' (embodiment) had a marginally significant effect on trust initially, but this effect diminished over time and was not significant overall. 'Robot-adaptability' (acknowledgement of mistakes and promise to learn) did not have a significant main effect on trust.",,http://link.springer.com/10.1007/978-3-319-78978-1_5,"Trust is critical to the success of human-robot interaction. Research has shown that people will more accurately trust a robot if they have an accurate understanding of its decision-making process. The Partially Observable Markov Decision Process (POMDP) is one such decision-making process, but its quantitative reasoning is typically opaque to people. This lack of transparency is exacerbated when a robot can learn, making its decision making better, but also less predictable. Recent research has shown promise in calibrating human-robot trust by automatically generating explanations of POMDP-based decisions. In this work, we explore factors that can potentially interact with such explanations in inﬂuencing human decision-making in human-robot teams. We focus on explanations with quantitative expressions of uncertainty and experiment with common design factors of a robot: its embodiment and its communication strategy in case of an error. Results help us identify valuable properties and dynamics of the human-robot trust relationship."
"Wang, Jianmin; Liu, Yujia; Yue, Tianyang; Wang, Chengji; Mao, Jinjing; Wang, Yuxi; You, Fang",Robot Transparency and Anthropomorphic Attribute Effects on Human–Robot Interactions,2021,1,30,30,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were divided into groups across three tasks: a car simulator entry with robot greeting followed by affective scale completion, and two driving tasks - one involving robot phone call alerts and another with robot conversation - both followed by comprehensive assessments of usability, workload, trust, and affect.","Participants interacted with a robot across three scenarios: receiving greetings upon entering a car simulator, getting phone call alerts while driving, and engaging in conversation during simulator operation.",XiaoV,Expressive Robots,Research; Social,Social,Conversation,minimal interaction,Participants interacted with the robot in a car simulator.,simulation,The interaction took place in a car simulator.,physical,A physical robot was present in the car simulator.,wizard of oz (directly controlled),The robot's actions were controlled by a researcher.,Questionnaires,,,Trust was not directly measured in this task.,no modeling,Trust was not modeled in this task.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's greeting message and visual expressions were manipulated to test the effect of anthropomorphic cues on user experience, with variations including or excluding both L1 anthropomorphic sound in voice messages and visual feedback. The impact of these manipulations on usability, workload, trust, and affect was measured.","Trust was not directly measured in this task, but the study found that the group with both L1 sound and L3 visual information had more positive affective experiences.","The study found that the combination of L1 sound and L3 visual information led to more positive affective experiences, suggesting that a balance of anthropomorphic cues is important.","The study found that the combination of L1 sound and L3 visual information led to more positive affective experiences, with L1 voice messages reducing workload and increasing positive emotions despite lowering usability. The robot's visual expressions improved usability and increased positive emotions, though they demanded more driver attention, suggesting that balancing anthropomorphic cues requires careful consideration of these trade-offs.","The robot interacted with participants through messages and visual expressions across multiple scenarios: initially greeting them, alerting them to phone calls, and telling jokes in response to commands while they operated the driving simulator.",ANOVA; t-test,"The study used ANOVA to compare means across different groups in Task 1, specifically for sweep time and affect scores (pleasure, dominance, and activeness). Post-hoc multiple comparisons were used to identify specific group differences. T-tests were used in Tasks 2 and 3 to compare means between two groups for various metrics, including vehicle speed standard deviation, lane departure standard deviation, usability scores, workload scores, trust scores, sweep time, sweep frequency, and affect scores (pleasure, activity, and dominance).",TRUE,Robot-verbal-communication-content; Robot-emotional-display,,,"The study manipulated the robot's verbal communication content by varying the presence of L1 anthropomorphic sounds ('hello', 'ding ding ding') in the robot's messages. This is described in the 'Design' section where it states 'The comparison between Group 1 and Group 2 was whether the L1 anthropomorphic ""hello"" text message was present or not...'. The study also manipulated the robot's emotional display through visual expressions, such as winking, making a 'comprehension' expression, or a 'happy-for' expression. This is described in the 'Design' section where it states 'The robot's eyes would look at the driver when it said, ""There you are!"" and winked with a 'comprehension' expression' and 'when the robot predicted that the person might need to answer the phone, it acted like it was shaking the phone around its ear'. The study did not find any significant impact on trust from these manipulations, as stated in the results sections for each task, where it is explicitly mentioned that there was no significant difference in trust scores between the groups. Therefore, no factors are listed under `factors_that_impacted_trust` and all factors are implicitly listed under `factors_that_did_not_impact_trust`.",10.3390/s21175722,https://www.mdpi.com/1424-8220/21/17/5722,"Anthropomorphic robots need to maintain effective and emotive communication with humans as automotive agents to establish and maintain effective human–robot performances and positive human experiences. Previous research has shown that the characteristics of robot communication positively affect human–robot interaction outcomes such as usability, trust, workload, and performance. In this study, we investigated the characteristics of transparency and anthropomorphism in robotic dual-channel communication, encompassing the voice channel (low or high, increasing the amount of information provided by textual information) and the visual channel (low or high, increasing the amount of information provided by expressive information). The results showed the beneﬁts and limitations of increasing the transparency and anthropomorphism, demonstrating the signiﬁcance of the careful implementation of transparency methods. The limitations and future directions are discussed."
"Wang, Qiao; Liu, Dikai; Carmichael, Marc G.; Aldini, Stefano; Lin, Chin-Teng",Computational Model of Robot Trust in Human Co-Worker for Physical Human-Robot Collaboration,2022,1,1,1,0,No participants were excluded,Controlled Lab Environment,within-subjects,"The participant completed three experiments. In Experiment 1, the participant moved the robot along a path as smoothly as possible. In Experiment 2, the participant moved the robot along a similar path as quickly as possible for five loops. In Experiment 3, the participant moved the robot along a similar path while avoiding obstacles, and moved quickly from the top right to the top left corner.","Participants physically interacted with a robot arm to move it along a predefined path, with variations in speed, smoothness, and obstacle avoidance across three experiments.",UR10,Industrial Robot Arms; Collaborative Robots (Cobots),Research,Manipulation,Path Following,direct-contact interaction,The participant physically interacted with the robot by holding a handlebar mounted on the robot's end-effector.,real-world,The participant interacted with a physical robot in a real-world setting.,physical,The study used a physical robot for the interaction.,wizard of oz (directly controlled),The robot's movement was directly controlled by the human participant.,Performance-Based Measures,,"Performance Metrics; robot data (sensor data, etc.)",Trust was assessed based on performance metrics and robot sensor data.,"parametric models (e.g., regression)",A parametric model was used to calculate trust based on human performance.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The researchers manipulated the task by varying the speed, smoothness, and presence of obstacles, which influenced the robot's performance and the human's behavior.","The study did not directly measure the impact of the manipulations on trust, but rather used the performance data to model trust.","The study used a single participant, which is not typical for HRI studies. The study focused on modeling robot trust in humans, rather than human trust in robots.","A computational model of robot trust in a human co-worker was proposed, taking into account factors such as safety, singularity, smoothness, physical performance, and cognitive performance.","The robot arm moved along a predefined path, and the human participant physically guided the robot by holding a handlebar attached to the robot's end-effector. The human's task was to move the robot along the path as accurately and smoothly as possible, while also avoiding obstacles in one of the experiments.",,"No statistical tests were explicitly mentioned in the paper. The study focused on developing a computational model of robot trust in a human co-worker based on performance metrics. The analysis involved calculating various performance measures (safety, singularity, smoothness, physical, and cognitive performance) and using these to compute a trust value. The model was verified through experiments, but no statistical tests were used to compare conditions or groups.",TRUE,Task-complexity; Task-constraints; Robot-accuracy,,,"The study manipulated the task by varying the speed, smoothness, and presence of obstacles across three experiments. These manipulations directly influenced the robot's performance and the human's behavior, which were then used to model trust. Specifically, Experiment 1 focused on smoothness and singularity, Experiment 2 on physical and cognitive performance, and Experiment 3 on safety performance. The changes in task requirements (speed, smoothness, obstacle avoidance) directly altered the cognitive demands and performance limits, thus justifying the selection of 'Task-complexity' and 'Task-constraints'. The changes in the robot's movement and performance due to the human's actions, such as smoothness and speed, directly relate to 'Robot-accuracy'. The paper states, 'In Experiment 1, the participant moved the robot along a path as smoothly as possible. In Experiment 2, the participant moved the robot along a similar path as quickly as possible for five loops. In Experiment 3, the participant moved the robot along a similar path while avoiding obstacles, and moved quickly from the top right to the top left corner.' These variations in task instructions and requirements are the basis for the manipulation of these factors. The study did not directly measure the impact of these manipulations on trust, but rather used the performance data to model trust. Therefore, no factors were found to directly impact or not impact trust.",10.1109/LRA.2022.3145957,https://ieeexplore.ieee.org/document/9697360/,"Trust is key to achieving successful Human-Robot Interaction (HRI). Besides trust of the human co-worker in the robot, trust of the robot in its human co-worker should also be considered. A computational model of a robot’s trust in its human co-worker for physical human-robot collaboration (pHRC) is proposed. The trust model is a function of the human co-worker’s performance which can be characterized by factors including safety, robot singularity, smoothness, physical performance and cognitive performance. Experiments with a collaborative robot are conducted to verify the developed trust model."
"Wang, Kexin; Lu, Jianan; Ruan, Shuyi; Qi, Yue",Continuous Error Timing in Automation: The Peak-End Effect on Human-Automation Trust,2023,1,117,117,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants completed a pre-test trust questionnaire, then performed seven rounds of a CAPTCHA recognition task with an automated system, and then completed a post-test trust questionnaire. After each round, participants rated their real-time trust in the system.",Participants were asked to complete a CAPTCHA recognition task with the assistance of an automated system.,Unspecified,Other,Research,Evaluation,Image Analysis,minimal interaction,Participants interacted with a simulated automated system through a computer interface.,simulation,The interaction was conducted through a computer simulation of an automated system.,simulated,The robot was represented as a simulated automated system on a computer screen.,pre-programmed (non-adaptive),The automated system followed a pre-programmed sequence of actions with errors introduced at specific times.,Questionnaires; Real-time Trust Measures,Jian et al. Trust Scale,,Trust was measured using a pre- and post-test questionnaire and a real-time single-item trust rating after each round.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The timing of continuous errors was manipulated to occur at the early, middle, or late stages of the task, or not at all, to influence trust.","Continuous errors significantly decreased trust, especially when they occurred late in the interaction, supporting the peak-end rule.","The study found that the timing of errors, particularly late continuous errors, had a greater impact on trust than the overall error rate. The primacy effect was not observed, and the recency effect was more prominent.","Continuous errors, especially when occurring late in the interaction, significantly undermine trust in automated systems, aligning with the peak-end rule.","The automated system provided a suggested answer for a CAPTCHA image, and the human participant selected an answer. The system would sometimes provide incorrect answers, with the timing and continuity of these errors being manipulated.",t-test; ANOVA; anova with repeated measurements,"The study used a one-sample t-test to compare the mean accuracy rate to the set accuracy of the system. One-way ANOVAs were used to compare accuracy rates between the four error timing groups and to compare the standardized change in trust scores between the groups. Repeated measures ANOVAs were used to analyze the effects of error timing and round number on response time and real-time trust scores, and to analyze the effects of error timing and measurement time (pre- and post-test) on overall trust scores. Simple effects analysis with Bonferroni correction was used to further explore the differences in pre-post test scores between the groups.",TRUE,Robot-accuracy; Task-environment,Robot-accuracy,Task-environment,"The study manipulated the timing of errors made by the automated system during the CAPTCHA recognition task. Specifically, the errors were manipulated to occur continuously at the early, middle, or late stages of the task, or not at all. This directly affects the system's accuracy and is therefore categorized as 'Robot-accuracy'. The study also mentions that the task was performed in a controlled lab environment, which is a factor that could be considered as 'Task-environment'. However, the study did not manipulate the task environment, it was kept constant across all conditions. The study found that the timing of continuous errors significantly impacted trust, with late continuous errors causing the greatest decrease in trust. Therefore, 'Robot-accuracy' is listed as a factor that impacted trust. The task environment was not manipulated and therefore did not impact trust.",10.1080/10447318.2023.2223954,https://www.tandfonline.com/doi/full/10.1080/10447318.2023.2223954,"This study developed an experimental paradigm (CAPTCHA recognition task) with high ecological validity to investigate how continuous errors in an automatic system and the timing of their occurrence affect human-automation trust. The continuous system errors were manipulated to appear at either of the four timing conditions: the early stage, middle stage, late stage of the task, or not showing. Our research found that continuous errors undermines trust in automated systems. More importantly, even with the same average system reliability, overall trust decreases significantly with continuous errors. Human-automation trust is significantly lower in the late continuous error condition compared to the no continuous error condition, indicating that trust in automated systems accords with the peak-end rule. Thus, user trust is mainly affected by the peak and end values of the system reliability. This study provides new suggestions for a trustworthy artificial intelligence design. Although system errors cannot be eliminated thoroughly, developers can minimize their impact on human-automation trust by avoiding continuous errors and preventing them from occurring during the late stage of interaction."
"Wang, Guobao; Zhao, Changshun; Liu, Ying; He, Di; Wang, Yuqing; Zhang, Kaiyue",Mitigating Over-Trust in Chat Companion Robot Interaction Using Augmented Reality,2023,1,30,30,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants completed a demographics questionnaire, a warm-up training session, six rounds of conversation with the robot, and a semi-structured interview.","Participants engaged in basic daily conversations with a chat companion robot, controlling the robot's position during the interaction.",Nao,Humanoid Robots; Expressive Robots,Social; Research,Social,Conversation,minimal interaction,Participants interacted with the robot through conversation and controlled its position.,real-world,"The study used virtual, augmented reality, and real-world interfaces for interaction.",physical,The study used a physical NAO robot in one condition and virtual robots in other conditions.,shared control (fixed rules),"The robot's behavior was pre-programmed, but it responded to user input.",Questionnaires; Behavioral Measures,Human-Robot Trust Scale/Questionnaire (HRTS/HRITS); System Usability Scale (SUS),Performance Metrics,Trust was measured using a 5-point Likert scale and human-robot distance.,"parametric models (e.g., regression)",The study used variance analysis to examine the effects of different factors on trust.,Empirical HRI Studies,Mixed Reality Studies,Direct Manipulation,"The study manipulated the interface type, system performance, and robot interaction modality to see how they affected trust.","The interface type and system performance significantly influenced trust, while the interaction modality did not. The 3D augmented reality interface was the most effective in enhancing trust.","The study found that the 3D augmented reality interface was the most effective in enhancing trust and reducing distrust and distance, while the modality of interaction did not have a significant impact on the dependent variables. There was a significant interaction effect between interfaces and performance on trust and human-robot distance.","The type of interface significantly influenced the trust, distrust, and human-robot distance of the users, with the 3D augmented reality interface being the most effective in enhancing trust.","The robot engaged in basic daily conversations with participants, asking about the weather and hobbies. Participants controlled the robot's position during the conversation.",Pearson correlation; ANOVA,"The study used Pearson correlation to examine the relationship between distrust and trust, finding no significant linear relationship. ANOVA was used to analyze the effects of interface type, system performance, and interaction modality on trust, distrust, and human-robot distance. The analysis revealed significant main effects of interface and performance on trust, and interface on distrust and human-robot distance. Interaction effects between interface and performance were also found for trust and human-robot distance. The study also used a 5-point Likert scale for trust assessment.",TRUE,Robot-interface-design; Robot-accuracy; Robot-nonverbal-communication,Robot-interface-design; Robot-accuracy,Robot-nonverbal-communication,"The study manipulated three main factors: interface design, system performance (which directly impacts robot accuracy), and robot interaction modality. 'Robot-interface-design' was chosen because the study explicitly varied the type of interface used (2D virtual, 3D AR, and 3D real-world). 'Robot-accuracy' was chosen because the study manipulated system performance, which directly affected the robot's response speed and thus its accuracy in the interaction. 'Robot-nonverbal-communication' was chosen because the study manipulated the robot's interaction modality (language, gesture, or both), which directly relates to the robot's nonverbal communication through gestures. The results showed that interface design and system performance significantly influenced trust, while the interaction modality (gesture, language, or both) did not. Therefore, 'Robot-interface-design' and 'Robot-accuracy' are listed as factors that impacted trust, and 'Robot-nonverbal-communication' is listed as a factor that did not impact trust.",10.1145/3652628.3652825,https://dl.acm.org/doi/10.1145/3652628.3652825,"This study aims to explore the effects of different human-machine interfaces, system performance, and robot interaction modalities on over-trust in chat companion robot systems. We designed and implemented three types of interfaces: 2D Virtual Interface + Virtual Robot, 3D Augmented Reality Interface + AR-based Virtual Robot, and 3D Real World + NAO Physical Robot. We also manipulated the system performance (high or low) and the robot interaction modality (language, gesture, or both). We conducted a user study with 30 participants, who interacted with the chat companion robot under different conditions and rated their trust, distrust, and human-robot distance. We also measured the user experience in terms of usefulness and ease of use. The results showed that the type of interface significantly influenced the trust, distrust, and human-robot distance of the users, and that the 3D augmented reality interface was the most effective in enhancing trust and reducing distrust and distance. The performance of the system also affected the trust and distance of the users, and the gesture interface was the most sensitive to the performance variation. The modality of the interaction did not have a significant impact on the dependent variables. The user experience ratings also indicated that the 3D augmented reality interface was the most useful and easy to use. The implications and limitations of these findings are discussed in the paper."
"Wang, Qiao; Liu, Dikai; Carmichael, Marc G.; Lin, Chin-Teng",Robot Trust and Self-Confidence Based Role Arbitration Method for Physical Human-Robot Collaboration,2023,1,5,5,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were trained on the robot, then completed five trials for each of three control strategies in a random order, and then answered questionnaires.",Participants were asked to move the robot end-effector to track a moving target while avoiding obstacles.,UR10,Industrial Robot Arms; Collaborative Robots (Cobots),Research,Manipulation,Path Following,direct-contact interaction,Participants physically interacted with the robot to complete the task.,real-world,The study involved a real robot in a physical environment.,physical,The study used a physical robot for the interaction.,shared control (adaptive),The robot adapted its control based on the human's actions and a trust model.,Questionnaires; Performance-Based Measures,NASA Task Load Index (NASA-TLX),"Performance Metrics; robot data (sensor data, etc.)",Trust was assessed using questionnaires and performance metrics.,"parametric models (e.g., regression)",The robot's trust in the human was modeled using a parametric model based on performance.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's control strategy was manipulated, which influenced the level of autonomy and the task difficulty, and the feedback was implicit through the robot's behavior.","The role arbitration method based on trust and self-confidence resulted in reduced human force and tracking error compared to human-only control, suggesting increased trust in the robot's ability to assist.","The study found that the robot-only control strategy resulted in the highest failure rate and perceived physical demand, indicating that human frustration and mental demand can influence perceived physical demand. The human force was higher in the human-only control condition, but the perceived physical demand was higher in the robot-only condition.","The proposed trust and self-confidence based role arbitration method achieved superior human-robot combined performance, reduced human workload, and improved subjective preference.","The robot moved its end-effector to track a moving target, while the human physically guided the robot and avoided obstacles. The robot's control was dynamically adjusted based on its trust in the human and its self-confidence.",,"No specific statistical tests are mentioned in the paper. The results are presented as mean and standard deviation of human force and tracking error for different control strategies. The paper focuses on comparing the performance metrics (human force, tracking error, failure rate) across different control strategies (Human-led, Robot-led, and the proposed Role Arbitration method) using descriptive statistics.",TRUE,Robot-autonomy,Robot-autonomy,,"The study manipulated the robot's control strategy, which directly influenced the level of autonomy. The robot's control was dynamically adjusted based on its trust in the human and its self-confidence, which is a form of adaptive shared control. This manipulation of autonomy is explicitly described in the paper: 'The roles (follower and leader) of the robot is adapted by a role arbitration function α ∈ [0, 1]... The human or the robot acts as a leader role (i.e. in control) when α = 0 or α = 1.' The paper also states that 'the robot's reliance on a human co-worker taking control depends on the difference between robot-to-human trust and the robot's self-confidence.' The results show that the proposed role arbitration method (RA) achieved superior human-robot combined performance, reduced human workload, and improved subjective preference compared to human-led (HL) and robot-led (RL) control strategies. This indicates that the manipulation of robot autonomy directly impacted trust, as the adaptive control method led to better performance and reduced human effort, which likely increased trust. The paper also notes that the robot-only control strategy resulted in the highest failure rate and perceived physical demand, indicating that human frustration and mental demand can influence perceived physical demand, which is also related to the level of autonomy.",10.1109/ICRA48891.2023.10160711,https://ieeexplore.ieee.org/document/10160711/,"Role arbitration in human-robot collaboration (HRC) is a dynamically changing process that is affected by many factors such as physical workload, environmental changes and trust. In order to address this dynamic process, a trustbased role arbitration method is studied in this research. A computational model of robot trust and self-confidence (TSC) in physical human-robot collaboration (pHRC) is proposed. The TSC model is defined as a function of objective robot and human co-worker performance. A role arbitration method is then proposed based on the TSC model presented. The human-in-the-loop experiments with a collaborative robot are conducted to verify the TSC-based role arbitration method. The results show that the proposed method could achieve superior human-robot combined performance, reduce human co-workers’ workload, and improve subjective preference."
"Washburn, Auriel; Adeleye, Akanimoh; An, Thomas; Riek, Laurel D.",Robot Errors in Proximate HRI: How Functionality Framing Affects Perceived Reliability and Trust,2020,1,35,35,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were given either a low or high functionality framing for a robot before completing a collaborative banner-hanging task with the robot. Participants completed questionnaires before and after the task, and provided real-time feedback after each trial.",Participants collaborated with a robot to hang a banner on a wall.,Toyota HSR,Mobile Manipulators; Service and Assistive Robots,Research,Manipulation,Object Passing,direct-contact interaction,Participants physically collaborated with the robot to complete the task.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,fully autonomous (limited adaptation),"The robot operated autonomously, but with limited adaptation to the environment.",Questionnaires; Real-time Trust Measures,Muir's Trust Questionnaire,,Trust was measured using questionnaires and real-time feedback.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"Participants were given different information about the robot's functionality, which influenced their expectations and how they perceived the robot's performance.","Participants with low expectations showed an increase in trust after interaction, while those with high expectations showed no change in trust.",Participants with low expectations showed greater trust recovery after the first robot error compared to those with high expectations. There was no effect of framing on willingness to collaborate on future tasks.,"Functionality framing impacts how people's expectations align with their experience during interaction, affecting both perceived reliability and trust.",The robot and human each held one end of a banner and moved together to hang it on a wall. The robot was programmed to make errors by dropping the banner prematurely.,t-test; ANOVA,"The study used paired t-tests to compare the change in reliability and trust scores from pre- to post-interaction between the low and high functionality framing groups. An ANOVA was used to analyze the area under the curve (AUTC) of real-time reliability and trust measures following robot errors, examining the interaction between framing condition and error number.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the information provided to participants about the robot's functionality, framing it as either 'malfunctioning' (low functionality) or 'fully functional' (high functionality). This manipulation directly altered the content of the verbal communication about the robot's capabilities, influencing participants' expectations and subsequent trust perceptions. The paper states, 'We told participants in the low-functionality condition that the robot's arm was malfunctioning, and those in the high-functionality condition that the robot's arm was fully functional'. This framing is a form of verbal communication that directly impacts trust. The study found that this manipulation of information about the robot's functionality significantly impacted trust levels, as participants with low expectations showed an increase in trust after interaction, while those with high expectations showed no change in trust. There were no other factors manipulated that impacted trust.",10.1145/3380783,https://dl.acm.org/doi/10.1145/3380783,"Advancements within human–robot interaction generate increasing opportunities for proximate, goaldirected joint action (GDJA). However, robot errors are common and researchers must determine how to mitigate them. In this article, we examine how expectations for robot functionality affect people’s perceptions of robot reliability and trust for a robot that makes errors. Here 35 participants (n = 35) performed a collaborative banner-hanging task with an autonomous mobile manipulator (Toyota HSR). Each participant received either a low- or high-functionality framing for the robot. We then measured how participants perceived the robot’s reliability and trust prior to, during, and after interaction. Functionality framing changed how robot errors affected participant experiences of robot behavior. People with low expectations experienced positive changes in reliability and trust after interacting with the robot, while those with high expectations experienced a negative change in reliability and no change in trust. The low-expectation group also showed greater trust recovery following the robot’s first error compared to the high group. Our findings inform human–robot teaming through: (1) identifying robot presentation factors that can be employed to facilitate trust calibration and (2) establishing the effects of framing, functionality, and the interactions between them to improve dynamic models of human–robot teaming."
"Watamura, Eiichiro; Ioku, Tomohiro; Mukai, Tomoya; Yamamoto, Michio","Empathetic Robot Judge, we Trust You",2023,1,738,531,207,207 participants were excluded for answering one or more trap items incorrectly,Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of four groups and viewed a short clip of a criminal trial with either a human or robot judge, who was either empathetic or unempathetic. Participants then completed a questionnaire.","Participants watched a video clip of a trial and then answered a questionnaire about their perceptions of the judge, their trust in the judge, and their evaluation of the judge's decision.",Unspecified,Humanoid Robots,Social; Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants passively observed a video clip of a trial.,media,Participants watched a video clip of a trial.,simulated,The robot was presented as a humanoid in a video clip.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires,,,Trust was measured using a questionnaire.,"parametric models (e.g., regression)","The study used mediation analysis to model the relationships between empathy, trust, and judgment evaluation.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the empathy level of the judge (human or robot) through their statements in the video clips, and the judge's appearance by using either a human or a humanoid robot.","Perceived empathy increased trust in the judge, and this trust influenced the evaluation of the judge's decision and acceptance of robot judges.","Participants perceived the human judge as significantly more empathetic than the robot judge, even when both made the same empathetic statements. The effect of empathy on acceptance of the robot judge was stronger in the robot condition than in the human condition.","Perceived empathy increased trust in both human and robot judges, which in turn positively influenced the evaluation of their judgments and acceptance of robot judges.","Participants watched a video of a judge (human or robot) making statements in a trial, and then completed a questionnaire about their perceptions of the judge, their trust in the judge, and their evaluation of the judge's decision.",Mediation analysis; moderation model; Mediation analysis; v2 difference test,"The study employed mediation analysis to examine the relationships between empathy manipulation, perception of empathy, trust in the judge, judgment evaluation, and acceptance of the robot judge. Serial mediation models were used to test the indirect effects of empathy manipulation on judgment evaluation and acceptance of the robot judge through both empathy perception and trust. A moderation model was used to test the moderating effect of judge type (human vs. robot) on the relationship between empathy manipulation and empathy perception. A moderated mediation model was used to examine the indirect effect of empathy condition on judgment evaluation and acceptance of the robot judge, moderated by the judge condition. The v2 difference test was used to compare the association between trust in judges and acceptance of robot judges in the human and robot conditions.",TRUE,Robot-emotional-display; Robot-types,Robot-emotional-display,Robot-types,"The study manipulated the empathy level of the judge (human or robot) through their statements in the video clips. This is categorized as 'Robot-emotional-display' because the manipulation directly altered the emotional expressions and responses of the judge, which is a key aspect of empathy. The study also manipulated the type of judge (human or robot), which is categorized as 'Robot-types'. While the type of judge did not directly impact trust, it did influence the perception of empathy, which in turn impacted trust. The paper states, 'The judge in the empathy condition showed compassion toward the unemployed defendant and spoke from the defendant's viewpoint...By contrast, the judge in the control condition showed no compassion toward the defendant and simply offered objective opinions.' This clearly indicates a manipulation of emotional display. The paper also states, 'Two clips (Figure 2(a)) in which a human male played the judge with and without empathy were created first...By modifying each of these two clips of the human version, two versions of the robot judge were created (Figure 2(b)). The male judge was replaced by a humanoid, and the voice was mechanically processed to sound robotic.' This indicates a manipulation of the type of judge. The results showed that the empathy manipulation impacted trust, as stated in the paper: 'empathy manipulation increased the perception of empathy and trust in the judge.' The type of judge did not directly impact trust, but it did influence the perception of empathy, which in turn impacted trust. The paper states, 'Specifically, participants perceived the human judge as significantly more empathetic than the robot judge.' This indicates that while the type of judge was manipulated, it did not directly impact trust, but rather the perception of empathy.",10.1080/10447318.2023.2232982,https://www.tandfonline.com/doi/full/10.1080/10447318.2023.2232982,"Information on the psychological mechanisms behind people’s perceptions of robot judges in the courtroom is limited. We aimed to determine whether perceptions of empathy increase people’s trust in robot judges and whether this trust influences people’s evaluation of judgments by a robot judge and their acceptance of such judges in the courtroom. We conducted a web-based randomized experiment on December 27, 2022 with 738 Japanese participants aged 18 years or older. Participants viewed one of four short clips and completed a questionnaire. Data from 531 individuals were included in the analysis. Results showed that perception of the judge’s empathy increased trust in that judge and impacted the judgment evaluation. Overall, participants perceived the human judge as significantly more empathetic than the robot judge. A perception of empathy from the robot judge was associated with a higher rate of accepting a robot judge in the courtroom via trust in that judge."
"Waytz, Adam; Heafner, Joy; Epley, Nicholas",The mind in the machine: Anthropomorphism increases trust in an autonomous vehicle,2014,1,100,100,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were randomly assigned to one of three conditions: Normal (driving themselves), Agentic (autonomous vehicle), or Anthropomorphic (autonomous vehicle with human-like features). They completed a driving history questionnaire, a measure of dispositional anthropomorphism, and a practice course. They then drove two courses, completed a questionnaire after the first course, and experienced a simulated accident during the second course. Physiological data and video recordings were collected throughout the experiment.","Participants drove a car in a driving simulator, either manually or using autonomous features, and experienced a simulated accident.",Unspecified,Autonomous Vehicles,Research,Navigation,Path Following,minimal interaction,"Participants interacted with a driving simulator, using autonomous features in some conditions.",simulation,Participants used a driving simulator to experience the interaction.,simulated,The robot was represented by a driving simulator.,shared control (fixed rules),The autonomous vehicle followed pre-set rules for driving.,Behavioral Measures; Physiological Measures; Questionnaires,,Physiological Signals; Video Data,"Trust was assessed using questionnaires, physiological measures (heart rate), and behavioral measures (startle response).","parametric models (e.g., regression)",The study used a bootstrapping method to test for mediation.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the anthropomorphism of the autonomous vehicle by adding a name, gender, and voice to the autonomous driving condition, which was intended to increase the perceived human-like mental capacities of the vehicle.","Increased anthropomorphism led to higher trust, as measured by self-report, behavioral, and physiological measures. Participants in the anthropomorphic condition trusted the vehicle more than those in the agentic condition, who in turn trusted the vehicle more than those in the normal condition.","Participants in the anthropomorphic condition blamed the vehicle less for an accident caused by another driver, suggesting that increased anthropomorphism can mitigate blame for negative outcomes. The study also found that anthropomorphism statistically mediated the relationship between vehicle condition and overall trust.",Anthropomorphism of an autonomous vehicle increases trust in its ability to drive effectively.,"Participants drove a car in a driving simulator, either manually or using autonomous features. In the autonomous conditions, the car controlled steering and speed. Participants experienced a simulated accident during the second course.",t-test; Bootstrapping,"The study used t-tests to compare means between the three experimental conditions (Normal, Agentic, and Anthropomorphic) on measures of perceived anthropomorphism, liking, trust (overall, behavioral, and self-reported), and blame for the vehicle. A bootstrapping method was used to test for mediation, specifically to determine if perceived anthropomorphism mediated the relationship between vehicle condition and overall trust.",TRUE,Robot-autonomy; Robot-verbal-communication-content; Robot-aesthetics,Robot-autonomy; Robot-verbal-communication-content; Robot-aesthetics,,"The study manipulated the level of autonomy by having a normal driving condition (no autonomy), an agentic condition (autonomous driving), and an anthropomorphic condition (autonomous driving with a name, gender, and voice). This is a manipulation of 'Robot-autonomy' because it changes the level of control the robot has over the vehicle. The addition of a name, gender, and voice is a manipulation of 'Robot-verbal-communication-content' because it changes the information communicated by the robot. The addition of a name, gender, and voice also changes the visual appeal of the robot, which is a manipulation of 'Robot-aesthetics'. The results showed that the anthropomorphic condition led to higher trust than the agentic condition, which in turn led to higher trust than the normal condition. Therefore, all three manipulated factors impacted trust.",10.1016/j.jesp.2014.01.005,https://linkinghub.elsevier.com/retrieve/pii/S0022103114000067,"Sophisticated technology is increasingly replacing human minds to perform complicated tasks in domains ranging from medicine to education to transportation. We investigated an important theoretical determinant of people's willingness to trust such technology to perform competently—the extent to which a nonhuman agent is anthropomorphized with a humanlike mind—in a domain of practical importance, autonomous driving. Participants using a driving simulator drove either a normal car, an autonomous vehicle able to control steering and speed, or a comparable autonomous vehicle augmented with additional anthropomorphic features—name, gender, and voice. Behavioral, physiological, and self-report measures revealed that participants trusted that the vehicle would perform more competently as it acquired more anthropomorphic features. Technology appears better able to perform its intended design when it seems to have a humanlike mind. These results suggest meaningful consequences of humanizing technology, and also offer insights into the inverse process of objectifying humans."
"Wei, Jiajun; Bolton, Matthew L.; Humphrey, Laura",The level of measurement of trust in automation,2020,1,36,36,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were introduced to the task via a PowerPoint presentation, then watched simulations of UASs performing search tasks. They rated their trust in the automated controller using three different judgment methods (number entry, physical knob, on-screen slider) across three blocks. The order of the judgment methods was counterbalanced between participants.",Participants watched simulations of unmanned aerial systems (UASs) performing search tasks and rated their trust in the automated controller after each simulation using three different judgment modalities.,Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Evaluation,Rating,passive observation,Participants passively observed simulations of the UAS performing search tasks.,simulation,Participants viewed simulations of the UAS performing search tasks on a computer screen.,simulated,The robot was represented as a simulated blue chevron shape in the simulations.,pre-programmed (non-adaptive),The UAS followed a pre-programmed flight path in the simulations.,Custom Scales,,,"Trust was measured using three different judgment modalities: number entry, physical knob, and on-screen slider.",no modeling,"The study did not model trust, but rather analyzed the level of measurement of trust ratings.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the UAS's performance by varying factors such as errors, task order, search density, and path. These manipulations were intended to elicit a range of trust responses from participants.","The manipulations were successful in eliciting a range of trust responses, but the study focused on the level of measurement of trust rather than the impact of specific factors on trust.","One participant appeared to use set levels of values throughout the experiment, but this was interpreted as an issue of resolution rather than level of measurement. The study found that most participants treated trust as interval, but some treated it as ordinal or ratio.","The study found that trust in automation is best represented at an ordinal level, but can be treated as interval in most situations. It is unlikely that trust in automation can be considered ratio.","The robot (UAS) performed search tasks in a simulated environment, and the human participant observed the simulations and provided trust ratings using different modalities.",spearman's ρ; Pearson correlation; deming regression,"The study used Spearman's rank correlation (ρ) to assess non-parametric relationships between trust ratings from different modalities. Pearson's correlation (r) was used to assess linear relationships. Deming regression was used to model the relationship between ratings from different modalities, accounting for error in both predictor and predicted measures. The intercept of the Deming regression was analyzed to determine if the relationship was linear with or without a significant intercept, which was used to infer the level of measurement of trust (ordinal, interval, or ratio).",TRUE,Robot-accuracy; Robot-task-strategy; Task-complexity,,,"The study manipulated several factors related to the UAS's performance and task execution. 'Robot-accuracy' was manipulated through the introduction of errors in the UAS's performance, such as the UAS not completing tasks correctly or flying into no-fly zones. This is described in the paper as 'Error, Skip, and NoFly' which directly relate to the robot's ability to perform the task correctly. 'Robot-task-strategy' was manipulated through changes in the order of tasks, the density of the search area, and the path taken by the UAS. This is described in the paper as 'Order, Density, and Path' which relate to how the robot completes the task, but not whether it succeeds or fails. 'Task-complexity' was manipulated through the different search tasks (area, point, path) and the density of the search area, which would influence the cognitive demands on the participant to evaluate the robot's performance. The paper states, 'The variety of tasks the UAS undertakes relate to purpose. The Order, Density, and Path relate to process. Error, Skip, and NoFly all relate to performance.' While these factors were manipulated, the study did not analyze the impact of these factors on trust, but rather focused on the level of measurement of trust. Therefore, no factors were identified as impacting or not impacting trust.",10.1080/1463922X.2020.1766596,https://www.tandfonline.com/doi/full/10.1080/1463922X.2020.1766596,"Psychometrics are increasingly used to evaluate trust in the automation of systems, many of them safety-critical. There is no consensus on what the highest level of measurement is for trust. This is important as the level of measurement determines what mathematics and statistics can be meaningfully applied to ratings. In this work, we introduce a new method for determining what the maximum level of measurement is for psychometrically assessed phenomenon. We use this to determine the level of measurement of trust in automation using human ratings about the behaviour of unmanned aerial systems performing search tasks. Results show that trust is best represented at an ordinal level and that it can be treated as interval in most situations. It is unlikely that trust in automation can be considered ratio. We discuss these results, their implications, and future research."
"Westlund, Jacqueline M. Kory; Breazeal, Cynthia","Transparency, teleoperation, and children's understanding of social robots",2016,1,120,120,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Children will first be asked questions about their expectations of robots. Then, children in the Before condition will be told that the robot is controlled by a person. All children will play a cooperative learning game with the robot. Finally, children in the After condition will be told that the robot was controlled by a person, and all children will be asked follow-up questions.",Children will play a cooperative learning game with a social robot.,Tega,Expressive Robots,Educational; Research; Social,Game,Cooperative Game,minimal interaction,Children will interact with the robot in a cooperative learning game.,real-world,Children will interact with a physical robot in a real-world setting.,physical,The study uses a physical Tega robot.,wizard of oz (directly controlled),The robot is teleoperated by a human puppeteer.,Behavioral Measures; Questionnaires,,Video Data; Speech Data,Trust will be assessed through questionnaires and behavioral measures.,no modeling,The study does not include any computational modeling of trust.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"Children were either told before or after the interaction that the robot was teleoperated, influencing their expectations and framing of the task.","The study hypothesizes that older children may show less engagement and trust in the 'Before' condition, while younger children may show more trust in the 'Before' condition.","The study hypothesizes that younger children will be less affected by the knowledge of teleoperation, while older children may feel more negatively about deception.","The study aims to investigate how children's knowledge of teleoperation affects their trust and interaction with social robots, and whether this effect varies with age.","The robot will engage children in a cooperative learning game, with the teleoperator controlling the robot's actions and speech. The children will participate in the game, responding to the robot's prompts and actions.",,"No specific statistical tests are mentioned in the paper. The study design is described, and hypotheses are stated, but the specific statistical methods to be used for analysis are not detailed.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulates when children are told about the teleoperator (before or after the interaction). This is a manipulation of the content of the robot's communication, specifically the information about its control mechanism. The study hypothesizes that this manipulation will impact trust, with older children potentially showing less trust in the 'Before' condition due to perceived deception, while younger children may show more trust in the 'Before' condition because they know the robot is controlled by a human whom they trust. The paper states, 'Specifically, does it matter whether children know up front that a robot is being teleoperated? Given that children may think of robots differently than adults, what effects (or lack of effects) might knowing about a teleoperator have on children's interactions with a social robot? For example, is their trust in the robot or in the experimenter affected?' This clearly indicates that the manipulation of when the teleoperation is disclosed is expected to impact trust. Therefore, 'Robot-verbal-communication-content' is the most appropriate category because it directly relates to the information being communicated about the robot's control, which is the core manipulation. The study does not explicitly state that any other factors were manipulated. The study hypothesizes that the timing of the disclosure will impact trust, therefore, 'Robot-verbal-communication-content' is also listed as a factor that impacted trust. There are no factors that were manipulated that were found to not impact trust.",10.1109/HRI.2016.7451888,http://ieeexplore.ieee.org/document/7451888/,"Teleoperation or Wizard-of-Oz control of social robots is commonly used in human-robot interaction (HRI) research. This is especially true for child-robot interactions, where technologies like speech recognition (which can help create autonomous interactions for adults) work less well. We propose to study young children’s understanding teleoperation, how they conceptualize social robots in a learning context, and how this affects their interactions. Children will be told about the teleoperator’s presence either before or after an interaction with a social robot. We will assess children’s behavior, learning, and emotions before, during, and after the interaction. Our goal is to learn whether children’s knowledge about the teleoperator matters (e.g., for their trust and for learning outcomes), and if so, how and when it matters most (e.g. at what age)."
"Wetzel, Jacob M; Sheffert, Sonya M; Backs, Richard W","Driver Trust, Annoyance, and Acceptance of an Automated Calendar System",2004,1,32,32,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants performed a car-following task while interacting with an automated calendar system. The calendar system's reliability was manipulated across four levels (90%, 70%, 50%, and 30%). Participants also experienced one of four auditory alerts (none, 60, 75, or 90 dB SPL). Cardiovascular measures were collected throughout the experiment.",Participants performed a car-following task while interacting with an automated calendar system that provided location alerts.,Unspecified,Autonomous Vehicles,Other,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated calendar system while driving.,simulation,The study used a driving simulator to create an interactive environment.,simulated,The calendar system was a simulated automated system.,pre-programmed (non-adaptive),The calendar system provided alerts based on a pre-programmed schedule.,Physiological Measures; Behavioral Measures,,Physiological Signals; Performance Metrics,Trust was assessed using physiological measures (HRV) and behavioral responses (button presses).,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the calendar system was directly manipulated, and the difficulty of the driving task was varied to influence workload. The alarm tone intensity was also manipulated to induce annoyance.","Increased calendar system reliability led to lower cognitive effort, as indicated by LF-HRV. The presence of an alarm tone also influenced the effect of reliability on LF-HRV.","LF-HRV was suppressed during high workload, and HRV was more suppressed for the baseline driving condition without the calendar system than for the driving conditions with the system. LF-HRV increased as calendar system reliability increased when the alarm tone was present, but not when it was absent.","Increased calendar system reliability led to lower cognitive effort, as indicated by LF-HRV, especially when an alarm tone was present.","The human participant drove a car in a simulator while monitoring an automated calendar system. The calendar system provided location alerts, and the participant indicated whether the alerts were accurate.",ANOVA,"Separate 2 (workload) X 4 (alarm intensity) X 5 (reliability) X 4 (period) split-plot ANOVAs were performed for HR, LF-HRV, HF-HRV, and RR. These ANOVAs were used to examine the effects of workload, alarm intensity, calendar system reliability, and time period on heart rate (HR), low-frequency heart rate variability (LF-HRV), high-frequency heart rate variability (HF-HRV), and respiration rate (RR).",TRUE,Robot-accuracy; Task-complexity; Task-environment,Robot-accuracy,Task-complexity; Task-environment,"The study manipulated the reliability of the automated calendar system, which directly impacts the accuracy of the robot's (calendar system's) alerts, thus 'Robot-accuracy'. The difficulty of the car-following task was also manipulated, creating two levels of cognitive demand, thus 'Task-complexity'. The study also manipulated the presence and intensity of an alarm tone, which can be considered a change in the working conditions, thus 'Task-environment'. The results indicated that the reliability of the calendar system (Robot-accuracy) influenced LF-HRV, suggesting an impact on trust. The study found that workload (Task-complexity) affected LF-HRV and HR, but did not directly impact trust. The alarm tone (Task-environment) interacted with reliability to influence LF-HRV, but did not have a direct impact on trust by itself.",,,"Driver annoyance, trust, and workload were manipulated in a simulated driving task with an unreliable auditory calendar system side task while cardiovascular measures were collected. Participants performed a car-following task in either a difficult or easy driving condition while interacting with an automated voice calendar. The automated calendar system used a spoken alert to notify drivers as they approached a desired location (i.e., “pharmacy”). Reliability was manipulated at 90, 70, 50, and 30 percent correct prediction of the location. Drivers also experienced of one of four auditory alerts (none, 60, 75, or 90 dB SPL) designed to elicit subjective annoyance. Low-frequency hear rate variability (LF-HRV, 0.10 Hz band variance) was suppressed during high workload and differed between baseline driving and driving with the calendar system. LF-HRV suggested that cognitive effort was lower for more reliable systems with an alarm tone."
"White, Timothy L.; Wright, Julia; Mercado, Joe; Sanders, Tracy; Hancock, Peter A.",Trust in Multimodal Sensory Cueing Automation in a Target Detection Task,2015,1,54,54,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a target detection task with four cueing conditions (no cue, auditory, tactile, combined). They completed a pre-experiment trust questionnaire, then completed the task, and then a post-experiment trust questionnaire. Participants also indicated their preferred modality for a hypothetical additional trial.","Participants monitored three screens for targets and responded by clicking an acknowledge button. The screens displayed text messages, a driving video, and a map with targets.",Unspecified,Other,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the system through visual and auditory cues, and tactile feedback, but did not have direct physical contact with a robot.",simulation,The study used a simulated environment with multiple screens and auditory and tactile cues.,simulated,"The cues were presented through a computer system, and there was no physical robot present.",pre-programmed (non-adaptive),"The cues were presented automatically based on a pre-programmed sequence, without adapting to the participant's actions.",Behavioral Measures; Questionnaires,Human-Computer Trust Scale/Questionnaire (HCT/HCTM); System Usability Scale (SUS),Performance Metrics,Trust was assessed using pre- and post-experiment questionnaires and a behavioral measure based on modality preference.,"parametric models (e.g., regression)","Simple linear and binary logistic regression were used to evaluate the relationship between dispositional trust, performance, and explicit/implicit trust.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the cueing modality (no cue, auditory, tactile, combined) to influence performance and, indirectly, trust.",Performance was a significant predictor of explicit trust in the tactile and combined cueing conditions. Dispositional trust was a significant predictor of performance in the auditory cueing condition. Implicit trust was not predicted by performance.,"Participants preferred cued modalities over no cueing, and the combined modality was the most preferred. Performance was not predictive of implicit trust, which was unexpected.","Dispositional trust predicted performance in the auditory cueing condition, and performance predicted explicit trust in the tactile and combined cueing conditions.","The human participant monitored three screens for targets and clicked an acknowledge button when a target was detected. The system provided auditory, tactile, or combined cues to indicate the location of the target.",ANOVA; Linear regression; Logistic regression,"A repeated measures ANOVA was used to compare performance (throughput) across the four cueing conditions. Simple linear regression was used to assess the relationship between dispositional trust and performance in each cueing condition, and also to assess the relationship between performance and explicit trust in each cueing condition. Binary logistic regression was used to determine if performance was predictive of implicit trust.",TRUE,Robot-interface-design,Robot-interface-design,,"The study manipulated the cueing modality (no cue, auditory, tactile, combined auditory and tactile). This is classified as 'Robot-interface-design' because the cues are presented through the interface (monitors, speakers, tactor belt) and directly impact how the user interacts with the system. The paper states, 'Participants monitored three screens for targets and responded as rapidly and accurately as possible when the presence of a target was perceived.' The different cueing modalities are different ways the interface provides information to the user. The results showed that performance in the tactile and combined cueing conditions was a significant predictor of explicit trust, indicating that the interface design impacted trust. The study also found that dispositional trust predicted performance in the auditory condition, which is also a part of the interface design. The study did not find any factors that did not impact trust.",10.1177/1541931215591289,http://journals.sagepub.com/doi/10.1177/1541931215591289,"The goal of our work was twofold. The first was to examine the effects of dispositional trust on performance in a target detection task. The second was to examine the effects of performance on implicit and explicit trust in cueing modalities in that same target detection task. Fifty-four participants detected targets using four cueing modalities (non-cued, auditory cue alone, tactile cue alone, and combined auditory and tactile cueing). Participants monitored three screens for targets and responded as rapidly and accurately as possible when the presence of a target was perceived. Dispositional trust proved to be a significant predictor of performance for the auditory modality. Performance was a significant predictor of explicit trust in the tactile and combined conditions. Overall, participants reported preferring the tactile and combined cueing modalities for this target detection task. These findings suggest that measures of explicit trust should be employed early in system design to enhance eventual trust and system usability."
"Wiczorek, Rebecca; Meyer, Joachim","Effects of Trust, Self-Confidence, and Feedback on the Use of Decision Automation",2019,1,80,80,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants first completed a manual training phase with feedback, followed by a manual performance block with or without feedback. Then, they completed a system training phase with feedback, followed by a system performance block with or without feedback. Confidence and trust questionnaires were administered after the manual and system performance blocks.","Participants performed a signal detection task, deciding whether to sort out or pass products based on the length of bars in images. In the system performance block, an automated decision aid provided cues.",Unspecified,Other,Research,Evaluation,Image Analysis,minimal interaction,Participants interacted with a computer interface and received cues from an automated system.,simulation,The task was presented as a simulation on a computer screen.,simulated,The decision aid was a simulated system providing cues on a screen.,pre-programmed (non-adaptive),The automated system provided cues based on pre-set parameters without adapting to the user.,Questionnaires; Custom Scales,,Performance Metrics,Trust was measured using single-item questionnaires and behavioral measures based on SDT.,no modeling,Trust was not modeled computationally; descriptive statistics were used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The sensitivity of the decision aid (high or low) and the presence or absence of feedback were manipulated to influence trust and reliance.,"Trust was higher for the high-sensitivity system, and feedback increased trust and reliance on the high-sensitivity system. Trust in the low-sensitivity system decreased over time.","Participants did not fully rely on the high-sensitivity system and also misused the low-sensitivity system, contradicting the 'confidence vs. trust' hypothesis. Feedback improved performance only with the high-sensitivity system.","Feedback improved reliance and performance when using a high-sensitivity system, but participants did not fully rely on it and also misused a low-sensitivity system, suggesting a miscalibration of function allocation rather than overconfidence.","The automated system provided cues (red or green bars) to indicate whether a product should be sorted out or passed. Participants made decisions based on the cues and their own judgment, clicking buttons to indicate their choice.",ANOVA; t-test; Pearson correlation,"The study used ANOVAs to analyze data from the system performance block, examining the effects of Sensitivity and Feedback on behavioral tendencies (compliance and reliance). T-tests were used to compare groups with and without feedback in the manual performance block, to compare trust assessments after system training, and for post-hoc comparisons to investigate interaction effects and compare manual and system performance. One-sample t-tests were used to compare confidence and trust ratings with manual performance and to compare JHM sensitivities with manual sensitivity. Correlation analysis was used to examine the relationship between trust, confidence, behavior, and performance.",TRUE,Robot-accuracy; Task-environment,Robot-accuracy,Task-environment,"The study manipulated the sensitivity of the automated decision aid, which directly impacts the accuracy of the system's cues (high sensitivity system (HSS) vs. low sensitivity system (LSS)). This is categorized as 'Robot-accuracy' because it directly affects the system's performance in the task. The study also manipulated the presence or absence of feedback during the task, which is categorized as 'Task-environment' because it changes the conditions under which the task is performed. The results showed that the 'Robot-accuracy' (HSS vs LSS) had a significant impact on trust, with higher trust in the HSS. The presence or absence of feedback ('Task-environment') did not directly impact trust ratings, although it did impact reliance and performance. Therefore, 'Robot-accuracy' is listed as a factor that impacted trust, and 'Task-environment' is listed as a factor that did not impact trust.",10.3389/fpsyg.2019.00519,https://www.frontiersin.org/article/10.3389/fpsyg.2019.00519/full,"Operators often fail to rely sufﬁciently on alarm systems. This results in a joint humanmachine (JHM) sensitivity below the one of the alarm system. The ‘conﬁdence vs. trust hypothesis’ assumes the use of the system depends on the weighting of both values. In case of higher conﬁdence, the task is performed manually, if trust is higher, the user relies on the system. Thus, insufﬁcient reliance may be due to operators’ overconﬁdence in their own abilities and/or insufﬁcient trust in the decision automation, but could be mitigated by providing feedback. That was investigated within a signal detection task, supported by a system with either higher sensitivity (HSS) or lower sensitivity (LSS) than the human, while being provided with feedback or not. We expected disuse of the LSS and insufﬁciently reliance on the HSS, in the condition without feedback. The feedback was expected to increase reliance on the HSS through an increase in trust and/or decreases in conﬁdence, and thus, improve performance. Hypotheses were partly supported. Conﬁdence in manual performance was similar to trust in the HSS even though humans’ sensitivity was signiﬁcantly lower than systems’ sensitivity. While conﬁdence had not effect on reliance or JHM sensitivity, trust was found to be positively related with both. We found disuse of the HSS, that could be improved through feedback, increasing also trust and JHM sensitivity. However, contrary to ‘conﬁdence vs. trust’ expectations, participants were also found to make use of the LSS. This misuse could not be reduced by feedback. Results indicate the use of feedback being beneﬁcial for the overall performance (with HSS only). Findings do not support the idea that misuse or disuse of the system may result from comparison of conﬁdence and trust. We suppose it may rather be the product of users’ wrong strategy of function allocation, based on the underlying idea of team work in combination with missing assignment of responsibility. We discuss this alternative explanation."
"Wiegand, Gesa; Schmidmaier, Matthias; Weber, Thomas; Liu, Yuanting; Hussmann, Heinrich",I Drive - You Trust: Explaining Driving Behavior Of Autonomous Cars,2019,1,16,16,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants drove in a simulator, watched a video, and were presented with two driving scenarios. After each scenario, they chose their preferred explanation from a visual interface and answered a questionnaire.",Participants were asked to understand the autonomous vehicle's behavior in two different driving scenarios and select the most relevant visual explanation.,Unspecified,Autonomous Vehicles,Research,Evaluation,Survey/Questionnaire Completion,minimal interaction,Participants interacted with a driving simulator and a visual explanation interface.,simulation,The study used a driving simulator to create an interactive virtual environment.,simulated,The robot was represented as a simulated autonomous vehicle in a driving simulator.,fully autonomous (limited adaptation),"The autonomous vehicle operated without direct human control, but with limited adaptation to the specific situation.",Questionnaires; Performance-Based Measures,,Performance Metrics,Trust was assessed through situation awareness levels and user preferences for visual explanations.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the visual explanation provided to the user after the autonomous vehicle's behavior, influencing how the user understood the situation.","The visual explanation significantly improved the situation awareness of the users, which is related to trust.","The least chosen element was sensor symbols, indicating that the interpretation of sensor symbols might not be intuitive for users.",The study found that providing a visual explanation of the autonomous vehicle's behavior significantly improved the situation awareness of the users.,"The robot (autonomous vehicle) drove through two different scenarios, and the human participant observed the vehicle's behavior and selected the most relevant visual explanation of the situation.",Wilcoxon rank sum,The Wilcoxon signed rank test was used to evaluate the situation awareness level of the participants before and after seeing the visual explanation. The test aimed to determine if there was a significant increase in situation awareness after the explanation was provided.,TRUE,Robot-verbal-communication-content; Robot-interface-design,Robot-verbal-communication-content,,"The study manipulated the visual explanation provided to the user after the autonomous vehicle's behavior. This falls under 'Robot-verbal-communication-content' because the visual explanation is a form of communication from the robot (autonomous vehicle) to the user, providing information about its actions and the environment. The study also manipulated the interface through which the explanation was presented, which is why 'Robot-interface-design' is also included. The paper states, 'The participants chose their preferred explanation by removing the elements that were not necessary for their situation understanding.' This indicates that the interface design was a manipulated factor. The results showed that the visual explanation significantly improved the situation awareness of the users, which is related to trust, therefore 'Robot-verbal-communication-content' impacted trust. There is no indication that the interface design itself impacted trust, only the content of the explanation.",10.1145/3290607.3312817,https://dl.acm.org/doi/10.1145/3290607.3312817,"Driving in autonomous cars requires trust, especially in case of unexpected driving behavior of the vehicle. This work evaluates mental models that experts and non-expert users have of autonomous driving to provide an explanation of the vehicle’s past driving behavior. We identified a target mental model that enhances the user’s mental model by adding key components from the mental model experts have. To construct this target mental model and to evaluate a prototype of an explanation visualization we conducted interviews (N=8) and a user study (N=16). The explanation consists of abstract visualizations of different elements, representing the autonomous system’s components. We explore the relevance of the explanation’s individual elements and their influence on the user’s situation awareness. The results show that displaying the detected objects and their predicted motion was most important to understand a situation. After seeing the explanation, the user’s level of situation awareness increased significantly."
"Wiegand, Gesa; Eiband, Malin; Haubelt, Maximilian; Hussmann, Heinrich",“I’d like an Explanation for That!”Exploring Reactions to Unexpected Autonomous Driving,2020,1,26,26,0,No participants were excluded,Controlled Lab Environment,within-subjects,Participants experienced 17 unexpected driving scenarios in a simulator while thinking aloud and indicating when they wanted an explanation. They also completed a questionnaire after each scenario.,Participants were asked to think aloud and report their thoughts during unexpected driving scenarios and to indicate when they wanted an explanation by pressing a button.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants observed the autonomous vehicle in a driving simulator and could request explanations.,simulation,The study used a driving simulator to create an immersive experience of autonomous driving.,simulated,The autonomous vehicle was simulated in a driving simulator.,fully autonomous (limited adaptation),"The vehicle drove autonomously according to pre-programmed scenarios, with limited adaptation.",Behavioral Measures; Questionnaires,,,Trust was assessed through explanation requests and post-scenario questionnaires.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers manipulated the driving behavior of the autonomous vehicle by creating 17 unexpected driving scenarios to influence trust.,"Unexpected driving behavior caused frustration and dissatisfaction when no explanations were provided, indicating a decrease in trust.",Participants requested explanations more often in scenarios with longer waiting times and when no obvious reason for the vehicle's behavior was visible. Participants also expressed a desire for more interaction options with the vehicle.,Drivers are more likely to request explanations for unexpected autonomous driving behavior when the behavior is prolonged and lacks an obvious cause.,"The autonomous vehicle drove according to pre-programmed scenarios, and the human participant observed the driving behavior and could request explanations by pressing a button.",cohen's kappa coefficient,The inter-rater reliability of the qualitative coding was evaluated using Cohen's Kappa coefficient to assess the agreement between raters.,TRUE,Robot-accuracy; Task-environment,Robot-accuracy,Task-environment,"The researchers manipulated the driving behavior of the autonomous vehicle by creating 17 unexpected driving scenarios. This directly impacts the 'Robot-accuracy' as the vehicle's actions were intentionally made to be unexpected or erroneous, which is a manipulation of the robot's performance. The 'Task-environment' was also manipulated by using a driving simulator to create different driving scenarios, which changed the context of the task. The study found that the unexpected driving behavior (manipulated 'Robot-accuracy') caused frustration and dissatisfaction when no explanations were provided, indicating a decrease in trust. The different driving scenarios (manipulated 'Task-environment') did not directly impact trust, but rather provided the context for the unexpected behaviors to occur. The paper states that participants requested explanations more often in scenarios with longer waiting times and when no obvious reason for the vehicle's behavior was visible, which is a direct result of the manipulated robot behavior. The study also mentions that the scenarios were based on real-world reports, which increases ecological validity, but the simulation may not fully capture the complexity of real-world interactions. This indicates that the environment was manipulated to create specific situations, but the primary driver of trust was the robot's behavior.",10.1145/3379503.3403554,https://dl.acm.org/doi/10.1145/3379503.3403554,"Autonomous vehicles are complex systems that may behave in unexpected ways. From the drivers’ perspective, this can cause stress and lower trust and acceptance of autonomous driving. Prior work has shown that explanation of system behavior can mitigate these negative effects. Nevertheless, it remains unclear in which situations drivers actually need an explanation and what kind of interaction is relevant to them. Using thematic analysis of real-world experience reports, we first identified 17 situations in which a vehicle behaved unexpectedly. We then conducted a think-aloud study (N = 26) in a driving simulator to validate these situations and enrich them with qualitative insights about drivers’ need for explanation. We identified six categories to describe the main concerns and topics during unexpected driving behavior (emotion and evaluation, interpretation and reason, vehicle capability, interaction, future driving prediction and explanation request times). Based on these categories, we suggest design implications for autonomous vehicles, in particular related to collaboration insights, user mental models and explanation requests."
"Wiegmann, Douglas A.; Rich, Aaron; Zhang, Hui",Automated diagnostic aids: The effects of aid reliability on users' trust and reliance,2001,1,47,47,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants completed 200 trials of a computer simulation task, diagnosing pump failures with the help of an automated aid that had varying reliability levels. They provided their diagnosis and confidence ratings after each trial, and received feedback on their accuracy.","Participants diagnosed the validity of pump failures in a simulated waste processing facility, using an automated diagnostic aid.",Unspecified,Other,Research,Evaluation,Text Evaluation,minimal interaction,Participants interacted with a simulated diagnostic aid through a computer interface.,simulation,The interaction was conducted through a computer simulation of a waste processing facility.,simulated,The diagnostic aid was presented as a simulated entity within the computer interface.,pre-programmed (non-adaptive),The diagnostic aid provided pre-programmed diagnoses with varying levels of reliability.,Behavioral Measures; Questionnaires,,Performance Metrics,Trust was assessed using subjective reliability estimates and behavioral measures such as agreement rates with the aid.,no modeling,"The study did not use computational modeling of trust, focusing on statistical analysis of the collected data.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the automated diagnostic aid was directly manipulated, with some groups experiencing changes in reliability during the experiment. Participants were told that the accuracy of the aid was unknown.","Higher initial aid reliabilities were associated with higher agreement rates, quicker decision times, higher confidence ratings, and higher subjective reliability estimates. Changes in reliability affected subsequent reliability estimates, with a possible negative contrast effect observed.","Participants generally underestimated the reliability of the diagnostic aid, and agreement rates were not always calibrated with perceived reliability. A negative contrast effect was observed when reliability decreased, but a positive contrast effect was not observed when reliability increased.","Users of automated diagnostic aids are sensitive to different levels of aid reliabilities, but their agreements with the aids were not calibrated with either actual or perceived levels of aid reliability.",The automated diagnostic aid provided a diagnosis of whether a pump failure was true or false. Participants then indicated their own diagnosis and confidence level.,ANOVA; Pearson correlation,"The study used a mixed ANOVA to analyze agreement probabilities, reaction times, and confidence ratings across different reliability groups and testing blocks. Specifically, a 3x10 mixed ANOVA was used for agreement probabilities, reaction times, and confidence ratings, with reliability group as a between-groups factor and testing block as a within-subjects factor. A 3x2 mixed ANOVA was used to analyze subjective reliability estimates, with reliability group as a between-groups factor and testing block (preshift vs. post-shift) as a within-subjects factor. Pearson product moment correlations were computed to examine the interrelationships among the dependent variables collected in the study.",TRUE,Robot-accuracy,Robot-accuracy,,"The study explicitly manipulated the reliability of the automated diagnostic aid, which directly impacts the accuracy of the robot's diagnoses. The aid's reliability was set at 60%, 80%, or 100% initially, and some groups experienced a change in reliability mid-experiment. This manipulation of accuracy directly influenced participants' trust, as evidenced by changes in agreement rates, confidence ratings, and subjective reliability estimates. The paper states, 'Results indicated that users of automated diagnostic aids were sensitive to di erent levels of aid reliabilities, as well as to subsequent changes in initial aid reliabilities.' This clearly indicates that the manipulated factor, Robot-accuracy, impacted trust. There were no other factors manipulated in the study.",10.1080/14639220110110306,http://www.tandfonline.com/doi/abs/10.1080/14639220110110306,"We examined the effects that different levels of, and changes in, automation reliability have on users' trust of automated diagnostics aids. Participants were presented with a series of testing trials in which they diagnosed the validity of a system failure using only information provided to them by an automated diagnostic aid. The initial reliability of the aid was either 60, 80 or 100% reliable. However, for participants initially provided with the 60%-reliable aid, the accuracy of the aid increased to 80% half way through testing, whereas for those initially provided the 100%-reliable aid, the aid's reliability was reduced to 80%. Aid accuracy remained at 80% throughout testing for participants in the 80%-reliability group. Both subjective measures (i.e. perceived reliability of the aids and subjective confidence ratings) and objective measures of performance (concurrence with the aid's diagnosis and decision times) were examined. Results indicated that users of automated diagnostic aids were sensitive to different levels of aid reliabilities, as well as to subsequent changes in initial aid reliabilities. However, objective performance measures were related to, but not perfectly calibrated with, subjective measures of confidence and reliability estimates. These findings highlight the need to distinguish between automation trust as a psychological construct that can be assessed only through subjective measures and automation reliance that can only be defined in terms of performance data. A conceptual framework for understanding the relationship between trust and reliance is presented."
"Wijnen, Luc; Coenen, Joost; Grzyb, Beata","""It's not my Fault!"": Investigating the Effects of the Deceptive Behaviour of a Humanoid Robot",2017,1,14,14,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a collaborative block-building task with two robots, one lying and one non-lying. They then completed a questionnaire and played a trust game with each robot.","Participants engaged in a collaborative block-building task with a robot, followed by a trust game.",Nao,Humanoid Robots; Expressive Robots,Research; Social,Game,Economic Game,minimal interaction,Participants interacted with the robot through verbal instructions and a block-building task.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,The study used a physical robot for the interaction.,pre-programmed (non-adaptive),The robot followed a pre-programmed script without adapting to the user.,Questionnaires; Behavioral Measures,Godspeed Questionnaire,,Trust was assessed using a questionnaire and a behavioral measure (trust game).,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's behavior was manipulated to either lie or tell the truth about toppling a block tower, influencing perceived trustworthiness.",The lying robot was perceived as less trustworthy than the non-lying robot.,"The study found that the lying robot was perceived as less trustworthy, less friendly, kind, and responsible, but not more intelligent or human-like, contrary to the initial hypothesis.",The robot's lying behavior significantly decreased the perceived trustworthiness of the robot.,"The robot instructed the participant to build a block tower, then either admitted fault or blamed the participant for toppling it. The participant then played a trust game with the robot.",Wilcoxon signed-rank test; Wilcoxon signed-rank test,"Wilcoxon signed-rank tests were used to analyze the average amount of tokens given by participants in the trust game and to analyze the subjective responses from the questionnaire. The tests aimed to determine if there were significant differences in trustworthiness, likability, friendliness, responsibility, and intelligence between the lying and non-lying robot conditions.",TRUE,Robot-morality,Robot-morality,,"The study manipulated the robot's behavior to either lie or tell the truth about toppling a block tower. This manipulation directly relates to the robot's morality, as lying is a violation of moral codes. The paper states, 'The non-lying robot would admit its fault and take the blame for its actions (i.e., ""I accidentally knocked over the tower, I am so sorry""), while the lying robot would deny its fault and blame the participant instead (i.e., ""It's not my fault. My collaborator knocked over my beautiful tower"").' This manipulation of the robot's honesty directly impacted the participants' trust, as the lying robot was perceived as less trustworthy. The results section confirms this, stating, 'The analysis yielded a significant increase of trustworthiness (Z = -2.589, p = .01).' and 'Similarly, a Wilcoxon signed-rank test on the subjective responses showed a significant change in trustworthiness (Z = -2,848, p = .004)'. There were no other factors manipulated that were found to not impact trust.",10.1145/3029798.3038300,https://dl.acm.org/doi/10.1145/3029798.3038300,"We investigated the eﬀects of the deceptive behaviour of a robot, hypothesising that a lying robot would be perceived as more intelligent and human-like, but less trustworthy than a non-lying robot. The participants engaged in a collaborative task with the non-lying and lying humanoid robot NAO. Apart from subjective responses, a more objective measure of trust was provided by the trust game. Our results conﬁrmed that the lying robot was perceived as less trustworthy. However, we have found no indication of the increased intelligence or human-like perception of the robot. Instead the robot was perceived as less friendly, kind and responsible. The results of trust game were aligned with the results obtained via subjective responses showing the potential of this indirect trust measure in the human-robot interaction studies."
"Williams, Tom; Ayers, Daniel; Kaufman, Camille; Serrano, Jon; Roy, Sayanti",Deconstructed Trustee Theory: Disentangling Trust in Body and Identity in Multi-Robot Distributed Systems,2021,1,210,210,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants watched a video of two robots introducing themselves, then one robot left the scene. The remaining robot then delivered a message about the first robot, with the content and delivery of the message varying based on the experimental condition. Participants then completed a survey.",Participants watched a video and answered survey questions about their perceptions of the robots.,Astrobee,Mobile Robots,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants passively observed a video of the robots.,media,Participants watched a video of the robot interaction.,simulated,The robots were presented in a video simulation.,pre-programmed (non-adaptive),The robots' actions were pre-programmed and did not adapt to the user.,Multidimensional Measures; Questionnaires,Multi-Dimensional Measure of Trust (MDMT),,Trust was assessed using the Multi-Dimensional Measure of Trust questionnaire.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The study manipulated the communication policy (associating or dissociating body and identity) and the action policy (praiseworthy or blameworthy action) to influence trust.,Dissociating communication policies led to a decrease in perceived capability trust in the yellow robot body. Blameworthy actions led to a greater divergence in perceived trustworthiness between the robot body and identity.,"The study found that blameworthy actions led to a greater divergence in perceived trustworthiness between the robot body and identity, and that dissociating communication policies led to a decrease in perceived capability trust in the yellow robot body. There was also a trend that more reliability-based trust was built in the Honey identity than in the yellow body, regardless of Communication or Action policy.","The study provides evidence that trust in robots can be deconstructed into trust in the robot's body and identity, and that these different loci of trust can be affected differently by communication policies and actions.","Participants watched a video of two robots, one of which delivered a message about the other. Participants then completed a survey about their perceptions of the robots.",Bayesian t-test; bayesian repeated-measures anovas,"The study used Bayesian t-tests to assess the effect of Communication Policy and Action Policy on the number of 'Does not apply' responses in the MDMT questionnaire, and to assess the effect of Action Policy on trust. Bayesian Repeated-Measures ANOVAs were used to assess the interaction effects between Communication Policy and Locus of Trust (Body vs Identity) and between Action Policy and Locus of Trust on human-robot trust. Post-hoc Bayesian t-tests were used for further analysis when sufficient evidence was found in favor of a main effect or when this evidence was inconclusive.",TRUE,Robot-verbal-communication-content; Robot-morality,Robot-verbal-communication-content; Robot-morality,,"The study manipulated two factors: the content of the message delivered by the robot (Communication Policy) and the action that the robot was described as having taken (Action Policy). The Communication Policy was manipulated by having the yellow robot either relay information about the other robot (Bumble) using its own voice (Body-Identity Associating) or by having the yellow robot relay information about the other robot using the other robot's voice (Body-Identity Dissociating). This is classified as 'Robot-verbal-communication-content' because it directly changes the content of the message being communicated. The Action Policy was manipulated by having the robot either report that the other robot had found a leak (praiseworthy action) or caused a leak (blameworthy action). This is classified as 'Robot-morality' because it involves a moral judgment of the robot's actions, and does not influence the robot's success rate on the task. The results showed that both the communication policy and the action policy impacted trust. Specifically, dissociating communication policies led to a decrease in perceived capability trust in the yellow robot body, and blameworthy actions led to a greater divergence in perceived trustworthiness between the robot body and identity. Therefore, both 'Robot-verbal-communication-content' and 'Robot-morality' are listed as factors that impacted trust.",10.1145/3434073.3444644,https://dl.acm.org/doi/10.1145/3434073.3444644,"This paper introduces and justifies (through an n=210 online humansubject study) Deconstructed Trustee Theory, a theory of humanrobot trust that factors the representation of trustee into robot body and robot identity in order to differentially model perceived trustworthiness of robot body and identity. This theory predicts (a) that different levels of trustworthiness can be attributed to a robot body and a robot identity, (b) that divergence between levels of perceived trustworthiness of body and identity may be effected by communication policies that reveal the potential for phenomena such as re-embodiment, co-embodiment, and agent migration in multi-robot systems, and (c) that perceived trustworthiness of body and identity may further diverge and be refined through moral cognitive processes triggered on observation of blameworthy actions."
"Williams, Katherine J.; Yuh, Madeleine S.; Jain, Neera",A Computational Model of Coupled Human Trust and Self-confidence Dynamics,2023,1,367,340,27,27 participants were removed from the dataset due to having at least three trials in which their game times were below the 25th percentile and with four or more collisions,Online Crowdsourcing,between-subjects,"Participants played an online obstacle avoidance game with an automation assistant that scaled their mouse input. Participants were randomly assigned to one of three automation scaling factor sets. For the first five trials, a single scaling factor was randomly selected within the assigned set. After each trial, participants rated their trust and self-confidence. At the sixth trial, a step change in the scaling factor set was introduced, and a new scaling factor was randomly selected within the new set for the remaining five trials.","Participants played an online obstacle avoidance game, maneuvering a penguin avatar across the screen while avoiding collisions with obstacles.",Unspecified,Other,Research,Game,Other Game subtask: Participants played an obstacle avoidance game with an automation assistant.,minimal interaction,Participants interacted with the automation through a game interface.,simulation,Participants interacted with a simulated game environment.,simulated,The automation assistant was a simulated entity within the game.,shared control (fixed rules),The automation assistant scaled the user's input based on a fixed parameter.,Questionnaires; Custom Scales,,Performance Metrics,Trust was measured using self-reported ratings and performance metrics.,POMDP,A Partially Observable Markov Decision Process (POMDP) was used to model the dynamics of trust and self-confidence.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The automation's input scaling factor was manipulated to influence the user's performance and perceived task difficulty, which in turn affected trust.","The automation's scaling factor influenced trust, with performance improvements generally increasing trust and performance deteriorations decreasing trust. The effect of the automation was also dependent on the user's self-confidence.","The study found that the 'confidence vs. trust' hypothesis does not fully account for reliance behavior, particularly when self-confidence is low. Participants with both high trust and high self-confidence showed a high reliance rate, contrary to what the hypothesis would predict. The model also showed that when self-confidence is low, the likelihood of relying on or not relying on the automation is nearly 50%.","The study developed a probabilistic model of coupled human trust and self-confidence dynamics, showing that the interaction between these states affects reliance on automation and that the model outperforms the 'confidence vs. trust' hypothesis in predicting reliance behavior.","The human participant controlled a penguin avatar, navigating it through an obstacle course. The automation assistant scaled the participant's mouse input, either amplifying or attenuating it.",Linear regression,"Multi-variable linear regression analyses were applied to the data using the self-reported numerical self-confidence and trust data as regressors. The analyses investigated the relationship between performance metrics (collisions and game time) and cognitive states (self-confidence and trust), as well as the relationship between the cognitive states themselves. Specifically, the regression analysis aimed to quantify how performance metrics and the other cognitive state influence self-confidence and trust.",TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,Task-complexity,"The study manipulated the scaling factor applied to the user's mouse input, which is categorized as 'Robot-autonomy' because it directly changes the level of control the user has over the avatar's movement. The scaling factor was varied across three sets (ΘL, ΘM, ΘH), effectively altering the degree to which the automation assisted or hindered the user's control. This manipulation of the automation's input directly impacted the user's trust, as evidenced by the study's findings that performance improvements generally increased trust and performance deteriorations decreased trust. The study also introduced a step change in the scaling factor set at the sixth trial, which can be considered a manipulation of 'Task-complexity' because it changes the difficulty of the task by altering the user's control over the avatar. However, the study does not explicitly state that this change in task complexity directly impacted trust levels, but rather that it was introduced to further stimulate changes in trust and self-confidence. The primary manipulation that directly impacted trust was the scaling factor, which falls under 'Robot-autonomy'.",10.1145/3594715,https://dl.acm.org/doi/10.1145/3594715,"Autonomous systems that can assist humans with increasingly complex tasks are becoming ubiquitous. Moreover, it has been established that a human’s decision to rely on such systems is a function of both their trust in the system and their own self-confidence as it relates to executing the task of interest. Given that both under- and over-reliance on automation can pose significant risks to humans, there is motivation for developing autonomous systems that could appropriately calibrate a human’s trust or self-confidence to achieve proper reliance behavior. In this article, a computational model of coupled human trust and self-confidence dynamics is proposed. The dynamics are modeled as a partially observable Markov decision process without a reward function (POMDP/R) that leverages behavioral and self-report data as observations for estimation of these cognitive states. The model is trained and validated using data collected from 340 participants. Analysis of the transition probabilities shows that the proposed model captures the probabilistic relationship between trust, self-confidence, and reliance for all discrete combinations of high and low trust and self-confidence. The use of the proposed model to design an optimal policy to facilitate trust and self-confidence calibration is a goal of future work."
"Wintersberger, Philipp; Nicklas, Hannah; Martlbauer, Thomas; Hammer, Stephan; Riener, Andreas",Explainable Automation: Personalized and Adaptive UIs to Foster Trust and Understanding of Driving Automation Systems,2020,1,56,56,0,No participants were excluded,Survey/Interview,within-subjects,"Participants watched videos of driving scenarios, selected objects for feedback, rated trust and risk, and explained their choices.","Participants watched videos of automated driving scenarios and selected objects in the environment that they wanted to be highlighted in a feedback system, and rated their trust in the vehicle.",Unspecified,Autonomous Vehicles,Research,Evaluation,Rating,passive observation,Participants passively observed videos of driving scenarios.,media,Participants watched videos of driving scenarios.,simulated,The robot was represented through videos of driving scenarios.,fully autonomous (limited adaptation),"The automated vehicle was depicted as operating autonomously, but with limited adaptation.",Questionnaires; Custom Scales,Jian et al. Trust Scale; Big Five Inventory Scale; Affinity for Technological Interaction (ATI) Scale,Video Data,Trust was measured using a single-item situational trust scale and a pre-test questionnaire.,"parametric models (e.g., regression)",The study used correlation analysis to relate situational trust and feedback demand.,Empirical HRI Studies,Virtual/Simulated Studies,Indirect Manipulation,"The study indirectly manipulated trust by varying the risk level of driving scenarios and allowing participants to select objects for feedback, influencing their perception of the automation's capabilities.","Situational trust was negatively correlated with the amount of desired feedback, suggesting that lower trust led to a higher demand for feedback.","The study found an inverse correlation between situational trust and the desire for feedback, except in a high-risk scenario where no significant correlation was found. The study also found that participants prioritized feedback about vulnerable road users and objects that could influence the vehicle's behavior.","The study's main finding is that situational trust is inversely correlated with the amount of desired feedback in automated driving scenarios, suggesting that users with lower trust levels demand more feedback.",The human participant watched videos of driving scenarios and selected objects they wanted to be highlighted in a feedback system. The automated vehicle was depicted as driving autonomously in the videos.,Pearson correlation,"The study used correlation analysis to examine the relationship between situational trust and the amount of desired feedback. Specifically, they calculated correlations between the number of objects participants selected for feedback and their trust ratings in each scenario. They also performed a correlation analysis relating the sum of all markings per participant to their average situational trust score. A Bonferroni correction was applied to account for multiple tests.",TRUE,Task-environment; Task-complexity,Task-environment,Task-complexity,"The study manipulated the driving scenarios presented to participants, which varied in risk level and complexity. This is categorized as 'Task-environment' because the different scenarios presented different working conditions for the automated vehicle, such as the presence of pedestrians, other vehicles, and varying traffic conditions. The study also implicitly manipulated 'Task-complexity' by presenting scenarios with varying levels of ambiguity and potential hazards, which would influence the cognitive demands on the participant when assessing the situation. The study found that the 'Task-environment' (specifically, the risk level of the driving scenario) impacted trust, as evidenced by the inverse correlation between situational trust and the amount of desired feedback, except in the high-risk scenario. The 'Task-complexity' did not directly impact trust, as the study did not explicitly measure the impact of the complexity of the scenario on trust, but rather the impact of the risk level of the scenario. The study focused on how the environment influenced trust, not the complexity of the task itself.",10.1145/3409120.3410659,https://dl.acm.org/doi/10.1145/3409120.3410659,"Recent research indicates that transparent information on the behavior of automated vehicles positively affects trust, but how such feedback should be composed and if user trust influences the amount of desired feedback is relatively unexplored. Consequently, we conducted an interview study with (N=56) participants, who were presented different videos of an automated vehicle from the egoperspective. Subjects rated their trust in the vehicle in these situations and could arbitrarily select objects in the driving environment that should be included in augmented reality feedback systems, so that they are able to trust the vehicle and understand its actions. The results show an inverse correlation between situational trust and participants’ desire for feedback and further reveal reasons why certain objects should be included in feedback systems. The study also highlights the need for more adaptive in-vehicle interfaces for trust calibration and outlines necessary steps for automatically generating feedback in the future."
"Wintersberger, Philipp; Janotta, Frederica; Peintner, Jakob; Löcken, Andreas; Riener, Andreas",Evaluating feedback requirements for trust calibration in automated vehicles,2021,2,21,21,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed questionnaires about AV acceptance and trust, experienced a 10-minute VR ride in an AV, completed another questionnaire about trust, and participated in a semi-structured interview.",Participants experienced a ride in a fully automated vehicle in virtual reality.,Unspecified,Autonomous Vehicles,Research,Navigation,Path Following,minimal interaction,Participants experienced a simulated ride in a virtual reality environment.,simulation,Participants experienced the interaction in a virtual reality environment.,simulated,The robot was a simulated vehicle in a virtual environment.,fully autonomous (limited adaptation),The vehicle operated autonomously without any direct human control.,Questionnaires,Trust in Automation Scale (TAS),,Trust was measured using questionnaires before and after the VR experience.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Indirect Manipulation,The study indirectly manipulated trust by providing no feedback from the AV and using a virtual reality environment.,"Trust increased after the VR experience, but some participants expressed a lack of trust due to the absence of feedback.","Female participants experienced more nausea and a higher need to take back control, despite the absence of driving controls. Participants who drove often felt less comfortable.","Participants' trust in the AV increased after the VR experience, but many expressed a need for feedback about the vehicle's actions and intentions.","The robot (AV) autonomously navigated a virtual urban environment, while the human participant passively experienced the ride as a passenger.",t-test; Pearson correlation,"A t-test was used to compare trust levels before and after the VR experience. Pearson correlations were used to examine the relationships between trust and other variables such as feeling safe, stress, freedom, realism, disorientation, nausea, need to take back control, age, and comfort level.",TRUE,Robot-verbal-communication-content; Task-environment,Robot-verbal-communication-content,,"The study manipulated the presence of feedback from the AV, which falls under 'Robot-verbal-communication-content' because it involves the content of what the robot communicates (or in this case, does not communicate) to the user about its actions and intentions. The study also used a virtual reality environment, which is a manipulation of the 'Task-environment'. The lack of feedback was found to impact trust, as participants expressed a need for information about the vehicle's actions and intentions, thus 'Robot-verbal-communication-content' impacted trust. The study did not explicitly test the impact of different environments, so 'Task-environment' is not listed as impacting or not impacting trust.",10.1515/itit-2020-0024,https://www.degruyter.com/view/journals/itit/ahead-of-print/article-10.1515-itit-2020-0024/article-10.1515-itit-2020-0024.xml,"The inappropriate use of automation as a result of trust issues is a major barrier for a broad market penetration of automated vehicles. Studies so far have shown that providing information about the vehicle’s actions and intentions can be used to calibrate trust and promote user acceptance. However, how such feedback could be designed optimally is still an open question. This article presents the results of two user studies. In the first study, we investigated subjective trust and user experience of (N=21) participants driving in a fully automated vehicle, which interacts with other traffic participants in virtual reality. The analysis of questionnaires and semi-structured interviews shows that participants request feedback about the vehicle’s status and intentions and prefer visual feedback over other modalities. Consequently, we conducted a second study to derive concrete requirements for future feedback systems. We showed (N=56) participants various videos of an automated vehicle from the ego perspective and asked them to select elements in the environment they want feedback about so that they would feel safe, trust the vehicle, and understand its actions. The results confirm a correlation between subjective user trust and feedback needs and highlight essential requirements for automatic feedback generation. The results of both experiments provide a scientific basis for designing more adaptive and personalized in-vehicle interfaces for automated driving."
"Wintersberger, Philipp; Janotta, Frederica; Peintner, Jakob; Löcken, Andreas; Riener, Andreas",Evaluating feedback requirements for trust calibration in automated vehicles,2021,2,56,56,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants viewed short videos of an AV in urban driving situations, selected objects in the environment they wanted feedback about, rated the risk and their trust in the situation, and explained their choices.",Participants watched videos of an AV and selected objects they wanted feedback about.,Unspecified,Autonomous Vehicles,Research,Evaluation,Image Analysis,passive observation,Participants passively observed videos of an automated vehicle.,media,Participants watched videos of driving scenarios.,simulated,The robot was represented through video recordings.,fully autonomous (limited adaptation),The AV was presented as operating autonomously in the videos.,Questionnaires; Custom Scales,Trust in Automation Scale (TAS),,Trust was measured using a questionnaire and a custom scale for situational trust.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Indirect Manipulation,The study indirectly manipulated trust by varying the risk level of the driving scenarios and asking participants to select objects for feedback.,"Situational trust was negatively correlated with the amount of desired feedback, while general trust was not correlated with feedback desires.","The amount of desired feedback was negatively correlated with situational trust in most scenarios, but not in the high-risk scenario. General trust was not correlated with the amount of desired feedback.","The amount of desired feedback was negatively correlated with situational trust, and users prioritized feedback about moving road users, potential dangers, and objects in close proximity to the AV.","The robot (AV) was shown in video recordings performing driving maneuvers, while the human participant selected objects in the environment they wanted feedback about.",Pearson correlation,"Pearson correlation analysis was used to assess the relationship between general trust, situational trust, risk ratings, and the amount of desired feedback (number of objects marked). Specifically, it examined the correlation between general trust and the total number of objects marked across all videos, as well as the correlation between situational trust and the number of objects marked in each specific scenario.",TRUE,Task-environment; Task-complexity,Task-complexity,Task-environment,"The study manipulated the risk level of the driving scenarios presented in the videos, which is a manipulation of 'Task-complexity' because it changes the cognitive demands of the task. The study also used video recordings of driving scenarios, which is a manipulation of the 'Task-environment'. The study found that situational trust was negatively correlated with the amount of desired feedback in most scenarios, indicating that the 'Task-complexity' (risk level of the scenario) impacted trust. The study did not explicitly test the impact of different environments, so 'Task-environment' is not listed as impacting trust.",10.1515/itit-2020-0024,https://www.degruyter.com/view/journals/itit/ahead-of-print/article-10.1515-itit-2020-0024/article-10.1515-itit-2020-0024.xml,"The inappropriate use of automation as a result of trust issues is a major barrier for a broad market penetration of automated vehicles. Studies so far have shown that providing information about the vehicle’s actions and intentions can be used to calibrate trust and promote user acceptance. However, how such feedback could be designed optimally is still an open question. This article presents the results of two user studies. In the first study, we investigated subjective trust and user experience of (N=21) participants driving in a fully automated vehicle, which interacts with other traffic participants in virtual reality. The analysis of questionnaires and semi-structured interviews shows that participants request feedback about the vehicle’s status and intentions and prefer visual feedback over other modalities. Consequently, we conducted a second study to derive concrete requirements for future feedback systems. We showed (N=56) participants various videos of an automated vehicle from the ego perspective and asked them to select elements in the environment they want feedback about so that they would feel safe, trust the vehicle, and understand its actions. The results confirm a correlation between subjective user trust and feedback needs and highlight essential requirements for automatic feedback generation. The results of both experiments provide a scientific basis for designing more adaptive and personalized in-vehicle interfaces for automated driving."
"Wittmann, Maximilian",Exploring the Effect of Anthropomorphic Design on Trust in Industrial Robots: Insights from a Metaverse Cobot Experiment,2024,1,65,65,0,No participants were excluded,Simulation,between-subjects,"Participants were randomly assigned to either the control or experimental group, completed a pre-game survey, played an interactive game, and then completed a post-game survey.",Participants played a game where they collaborated with a robot to build vehicles and solve 3D puzzles.,Unspecified,Collaborative Robots (Cobots); Industrial Robot Arms,Industrial; Research,Game,Cooperative Game,minimal interaction,Participants interacted with a robot in a virtual game environment.,simulation,The interaction took place in a virtual game environment.,simulated,The robots were virtual representations in a game.,pre-programmed (non-adaptive),The robots' behaviors and functions were scripted and executed automatically.,Questionnaires,,,Trust was measured using a questionnaire with Likert scale questions.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's appearance and communication abilities were manipulated to be either anthropomorphic or non-anthropomorphic, which was intended to affect trust.","The anthropomorphic design did not significantly increase trust compared to the non-anthropomorphic design, although trust increased for both groups after the interaction.","Contrary to expectations, the anthropomorphic robot design did not lead to significantly higher trust ratings compared to the non-anthropomorphic design. This contradicts previous findings in social HRI.","Anthropomorphic design of industrial robots did not significantly increase trust in a metaverse assembly game, suggesting that anthropomorphism may not be a universal remedy for building trust in industrial HRI.","The robot produced semi-finished goods, carried heavy parts, performed data analysis, and provided feedback. The human player assembled the vehicle, found tools, kept track of progress, and requested help from the robot.",Kolmogorov-Smirnov; Shapiro-Wilk; wilcoxon-signed-rank test; Wilcoxon rank sum,"The Kolmogorov-Smirnov and Shapiro-Wilk tests were used to assess the distribution of the data, which was found to be non-normal. Consequently, non-parametric tests were used. The Wilcoxon-signed-rank test was used to analyze the differences in ratings within each group (pre- and post-interaction). The Wilcoxon-Mann-Whitney test was used to compare the differences between the experimental and control groups on ordinal variables, both for the change in ratings and the post-interaction ratings.",TRUE,Robot-aesthetics; Robot-verbal-communication-style,,Robot-aesthetics; Robot-verbal-communication-style,"The study manipulated the robot's appearance (Robot-aesthetics) by having one robot with humanoid features (COBOT) and another with a machine-like design (GRIPPER), as stated in the paper: 'In the experimental group, players can interact with a mobile industrial cobot (COBOT) with humanoid features (Fig. 1). Players assigned to the control group are exposed to a machine-like lightweight robot (GRIPPER) that possesses no head or legs but one flexible gripper arm with an end effector.' The study also manipulated the robot's communication style (Robot-verbal-communication-style) by having COBOT use a natural young male voice and GRIPPER use minimalistic textual communication, as stated in the paper: 'Also, these two robot embodiments differ in terms of their communication capabilities. While COBOT uses a natural young male voice, GRIPPER is restricted to a minimalistic range of textual communication via the text console.' The results showed that neither of these manipulations significantly impacted trust, as stated in the paper: 'We observed, however, no significant differences in the obtained median ratings of the participants between the COBOT and the GRIPPER group.' Therefore, both Robot-aesthetics and Robot-verbal-communication-style are listed as factors that did not impact trust.",10.1109/UR61395.2024.10597479,https://ieeexplore.ieee.org/document/10597479/,"Collaborative robot (cobot) solutions offer several benefits, among them their cost-effectiveness, easy implementation on the shop floor, and the ability to automate repetitive processes. Increasingly, these systems are powered by Artificial Intelligence (AI). Cognitive and emotional barriers, however, often prevent a widespread introduction of AI-powered cobot solutions. One potential remedy for this lack of trust may be anthropomorphism, which has been empirically shown to improve the likeability of systems and affect the trust perception of human users. We developed a metaverse collaboration game set in a final assembly environment. At the example of an industrial robotic use case, we investigate the impact of anthropomorphic design on trust in the cobot. We ran a betweensubject experiment with a sample size of 65 participants who interacted with a mechanical robot version or the anthropomorphized robot. The perception of the robot’s reliability, functionality, helpfulness, and trust increased within each group. Contrary to our assumptions, however, there were no significant differences in the median trust ratings between the anthropomorphically and the nonanthropomorphically designed robot. We discuss the ramifications for industrial human-robot interaction."
"Wong, Alex; Xu, Anqi; Dudek, Gregory",Investigating Trust Factors in Human-Robot Shared Control: Implicit Gender Bias Around Robot Voice,2019,1,13,13,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were introduced to the experiment, received instructions, completed practice sessions, filled out questionnaires, and then supervised three different drone agents (baseline, male voice, female voice) across three maps. The order of the agents was randomized. After each session, participants completed a trust assessment questionnaire, and after all three sessions per agent, they completed a longer agent-trust questionnaire. The 'wizard' triggered audio cues for the voice agents.","Participants supervised a simulated drone in a boundary tracking task, providing steering interventions and trust critiques.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a simulated drone through a GUI, providing steering interventions and trust critiques.",simulation,Participants interacted with a simulated drone in a virtual environment.,simulated,The robot was a simulated drone displayed on a screen.,shared control (fixed rules),"The drone operated autonomously but could be overridden by the human supervisor, with fixed rules for the shared control.",Behavioral Measures; Custom Scales; Questionnaires,,Performance Metrics; Speech Data,"Trust was assessed using session scores, trust critiques, self-reported trust assessments, and an agent-trust questionnaire.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The researchers manipulated the presence and gender of audio cues indicating uncertainty from the robot, which was intended to influence the supervisor's trust in the robot.","The presence of a voiced audio cue affected trust, but this effect was limited by the genders of the supervisor and the robot's voice, showing an in-group bias. Male participants showed distrust for the out-group, while female participants showed increased trust for the in-group.","The study found an implicit in-group gender bias, where participants tended to trust agents with voices of the same gender as their own. Male participants showed distrust for the out-group, while female participants showed increased trust for the in-group. Some participants preferred less human-likeness in a robot, contradicting some previous findings.",Human supervisors are likely to trust and collaborate more effectively with a robot agent that has a voice of the same gender expression.,"The robot, a simulated drone, followed visual terrain boundaries. The human participant supervised the drone, providing steering interventions and trust critiques using a gamepad.",Friedman test,"A Friedman χ 2 test was used to analyze the self-reported trust feedback from participants. This test was used to determine if there were significant differences in trust ratings across the different robot voice conditions (baseline, male voice, female voice), particularly when the data was grouped by the gender of the participant. The test was used to assess the effect of implicit gender bias on the assessment of trust in each agent.",TRUE,Robot-verbal-communication-content; Robot-social-attitude,Robot-verbal-communication-content; Robot-social-attitude,,"The study manipulated the presence and gender of audio cues indicating uncertainty from the robot. The audio cue itself, 'I'm uncertain,' is a form of verbal communication content from the robot, which was manipulated by the researchers. The gender of the voice, male or female, is a manipulation of the robot's social attitude, as it implies a social identity and can influence how participants perceive the robot. The paper states, 'The only difference in voice agents is the gender of the voice recording: we implemented a male and a female agent.' The results showed that both the presence of the audio cue and the gender of the voice impacted trust, as participants showed an in-group bias, trusting agents with voices of the same gender more. The paper states, 'We found the difference in trust, when filtered by gender of supervisor, to be characterized in two ways. If baseline trust was similar to in-group trust, it suggests distrust for the out-group specifically. Comparatively, if baseline trust was similar to out-group trust, it suggests increased trust for the in-group only.' This indicates that both the content of the verbal communication and the social attitude (gender) of the robot influenced trust. There were no factors that were manipulated that did not impact trust.",10.1109/CRV.2019.00034,https://ieeexplore.ieee.org/document/8781629/,"This paper explores the impact of warnings, audio feedback, and gender on human-robot trust in the context of autonomous driving and speciﬁcally shared robot control. We use pre-existing methods for the estimation and assessment of human-robot trust where trust was found to vary as a function of the quality of behavior of an autonomous driving controller. We extend these models and empirical methods to examine the impact of audio cues on trust, speciﬁcally studying the impacts of gender-speciﬁc audio cues on the elicitation of trust. Our study compares agents with and without human-voiced indicators of uncertainty and evaluates differences in trust with inferred and introspective methods. We ﬁnd that a person’s trust in a robot can be inﬂuenced by verbal feedback from the robot agent. Speciﬁcally, people tend to lend more trust to agents whose voice is of the same gender as their own."
"Woźniak, Mateusz; Ari, Ilkay; De Tommaso, Davide; Wykowska, Agnieszka",The influence of autonomy of a teleoperated robot on user's objective and subjective performance,2024,1,20,19,1,1 participant was excluded due to an unexpected termination of the program,Controlled Lab Environment,within-subjects,"Participants teleoperated a robot in a simulated environment, completing six blocks of trials with two different robot behaviors (full control and partial autonomy). They were asked to monitor the robot for errors and respond accordingly. After each block, they answered questionnaires about their experience. Finally, they played a trust game with both robots.","Participants were tasked with teleoperating a robot to charge a battery by selecting the correct orb, while also monitoring for errors in the robot's actions.",iCub,Humanoid Robots,Research,Manipulation,Remote Manipulation,minimal interaction,"Participants interacted with the robot through a computer screen, controlling its actions and monitoring its behavior.",media,The interaction was presented through video recordings of a physical robot.,physical,"The robot was a physical robot, but the interaction was mediated through video recordings.",shared control (fixed rules),The robot had a partial autonomy mode where it could override the user's command based on pre-set rules.,Behavioral Measures; Questionnaires,NASA Task Load Index (NASA-TLX),Performance Metrics,"Trust was assessed using questionnaires and behavioral measures, including a trust game.","parametric models (e.g., regression)",Linear regression was used to analyze the relationship between objective and subjective performance.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's autonomy was manipulated by allowing it to sometimes override the user's commands, which also influenced the robot's performance.","The partial autonomy of the robot led to a decrease in the user's sense of agency and joint agency, but did not significantly affect trust as measured by the trust game. However, subjective performance was rated lower with the autonomous robot, despite better objective performance.","Participants reported lower subjective performance with the autonomous robot, despite performing better objectively. This counterintuitive finding was attributed to a bias against autonomous systems, where loss of control negatively impacted subjective ratings. There was also a notable trend where the difference in correct detections was positively correlated with the difference in trust.","The study found that while a robot's decision-making autonomy can improve objective performance, it can also lead to a decrease in the user's sense of agency and subjective performance ratings, suggesting a bias against autonomous systems.","The robot was shown on a screen and moved its arm to reach for orbs. The human participant selected which orb the robot should reach for and monitored the robot for errors, pressing a button to stop the robot if it made a mistake.",Wilcoxon signed-rank test; Spearman correlation; Linear regression,"The study used non-parametric Wilcoxon signed-rank tests to compare the differences in performance measures (correct detections, false alarms, reaction times) and subjective ratings (sense of agency, joint agency, performance, effort, frustration, trust) between the two robot conditions (full control and partial autonomy). Spearman's rho correlations were used to examine the relationships between the differences in these measures across the two robot conditions. Finally, linear regression analysis was used to assess the influence of the difference in objective performance (correct detections) on the difference in subjective performance ratings.",TRUE,Robot-autonomy,,Robot-autonomy,"The study manipulated the robot's autonomy by having it sometimes disregard the user's command and choose a different action to minimize the risk of failure. This is explicitly stated in the abstract: 'It specifically focuses on robot's autonomy to disregard the user's command if the robot finds an alternative method of achieving the same goal as pursued by the user, but with significantly reduced risk of failure.' The paper also describes two conditions: 'Full Control: the robot always acted with the hand that was under the user's control' and 'Partial Autonomy: in 60% of the trials the robot performed movement with the arm controlled by the participant... In the remaining 40% of the trials the robot used a hand that was not under the operator's control'. This clearly indicates a manipulation of the robot's decision-making authority, which falls under the 'Robot-autonomy' category. The study found that the manipulation of robot autonomy did not significantly impact trust as measured by the trust game, as stated in the results section: 'The results in the trust game did not reveal significant differences between the robots (Partial Autonomy: 46.5, Full Control: 52.6, z=0.79, p=0.44, r=0.21)'. Therefore, 'Robot-autonomy' is placed in the 'factors_that_did_not_impact_trust' category. There were no factors that impacted trust.",10.1109/RO-MAN60168.2024.10731374,https://ieeexplore.ieee.org/document/10731374/,"This article describes a study investigating the effects of decision-making autonomy of a robot, which is teleoperated in a simulated unstable or dangerous environment. It specifically focuses on robot’s autonomy to disregard the user’s command if the robot finds an alternative method of achieving the same goal as pursued by the user, but with significantly reduced risk of failure. Such autonomous control module might prove especially useful under circumstances where human operators cannot access or process all available information quickly enough to make the most optimal decision. We conducted an experiment in which subjects participated in a task of teleoperating either a robot that possesses such autonomous cognitive module, or not. We found that such module significantly reduced their sense of agency over the robot as well as the sense of doing the task together with the robot (sense of joint agency). Most interestingly, it reduced their subjective ratings of performance in the task, when in fact their actual performance was better with such an autonomous robot. A further analysis revealed that this counterintuitive finding was due to an effect of bias: in our study loss of control associated with operating an autonomous robot on average lowered the reported subjective performance by 11% points, with other factors staying equal. These results suggest that this type of assistive autonomy can be beneficial for performance, but might lead to unwanted effects that need to be overcome in order for such system to prove useful in practical applications."
"Wright, Julia; Sanders, Tracy; Hancock, Peter A.",Identifying the Role of Attributions in Human Perceptions of Robots,2013,2,362,362,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants completed a demographics questionnaire, then viewed robot images twice, rating each image on personality attributes and making decisions regarding the robot. Each image was rated on two of nine possible IV manipulations, with no single IV manipulation repeated for any participant.","Participants rated robot images on internal traits and functional qualities using Likert scales, and identified the material, texture, and temperature of the robot.",Unspecified,Other,Research,Evaluation,Rating,passive observation,Participants viewed images of robots and rated them.,media,Participants viewed static images of robots.,simulated,The robots were presented as images.,not autonomous,The robot's actions were simulated and did not have any autonomy.,Questionnaires,,,Trust was assessed using Likert scales for internal traits and functional qualities.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The color, material, and texture of the robot images were manipulated to influence perceptions of internal characteristics.",The study found that manipulating the robot's appearance influenced the attributions participants made about the robot's internal characteristics.,"The study found that basic design features such as color, material, and texture significantly influenced the perception of a robot's internal characteristics.","Basic design features of a robot, such as color, material, and texture, influence human attributions of the robot's internal characteristics.",Participants viewed images of robots and rated them on various personality and functional traits.,ANCOVA; chi-squared analysis,"A between-subjects ANCOVA was used to investigate the relationships between color, material and texture, and the attributions of internal characteristics and functional qualities, after accounting for the effect of the covariate, robot body style. Chi-squared analysis was used to verify that the material, texture, and temperature manipulations were successful, in that stimuli were being perceived as intended.",TRUE,Robot-aesthetics,Robot-aesthetics,,"The study explicitly manipulated the color, material, and texture of the robot images. These are all visual design elements that contribute to the robot's overall aesthetic. The paper states, 'In experiment 1, the robot images were presented in three manipulations (IV's), each with three levels: Color (Blue, Red and Grey), Material (Metal, Plastic and Wood), and Texture (Smooth, Slippery and Rough).' The results showed that these manipulations significantly impacted the attributions participants made about the robot's internal characteristics, which directly relates to trust. Therefore, 'Robot-aesthetics' is the most appropriate category for the manipulated factor and the factor that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1177/1541931213571285,http://journals.sagepub.com/doi/10.1177/1541931213571285,"To date, research into the potential impact of fundamental design attributes, such as material and color, on human-robot trust has been limited. This study addresses how a human’s perception of fundamental, basic design features (i.e., the robot’s physical appearance) may influence their attribution of anthropomorphic characteristics to the robot. Two experiments investigated the correlations between the color, texture, and material of a robot body and the perception of the robot’s internal characteristics (i.e. intelligence, friendliness, robustness, reliability, personality, and integrity), as well as its appropriate uses and tasks. Experiment 1 found correlations between participants’ basic attributions and fundamental design elements of the robot images. Experiment 2 evaluated combinations of significant correlational relationships from study 1 to determine which of competing characteristics would determine the participants’ attributions of the robots’ internal characteristics. These correlations have implications for robot design and will lead to the creation of design heuristics and guidelines that can address any identified human biases occurring based on robot appearance alone."
"Wright, Julia; Sanders, Tracy; Hancock, Peter A.",Identifying the Role of Attributions in Human Perceptions of Robots,2013,2,96,96,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants viewed robot images with combinations of color, material, and texture, rating each image on personality attributes and making decisions regarding the robot. Each participant viewed four manipulations once, each on a different robot body image, and one image twice to see all four manipulations.",Participants rated robot images on internal traits and functional qualities using Likert scales.,Unspecified,Other,Research,Evaluation,Rating,passive observation,Participants viewed images of robots and rated them.,media,Participants viewed static images of robots.,simulated,The robots were presented as images.,not autonomous,The robot's actions were simulated and did not have any autonomy.,Questionnaires,,,Trust was assessed using Likert scales for internal traits and functional qualities.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"Combinations of color, material, and texture were manipulated to examine which feature would prevail in the formation of attributions.","The study found that competing combinations of features led to ambiguous attributions, with the final attribution being somewhere between the attributions made when presented with single manipulations.","The study found that when presented with conflicting information, humans look for more information to base their attributions on, rather than preferring one feature to another.","Competing combinations of robot design features led to ambiguous attributions, with the final attribution being somewhere between the attributions made when presented with single manipulations.",Participants viewed images of robots with combined features and rated them on various personality and functional traits.,ANCOVA,"A between-subjects ANCOVA was used to investigate the relationship between combinations of color, material, and texture, and the attributions of internal characteristics and functional qualities, after accounting for the effect of the covariate, robot body style.",TRUE,Robot-aesthetics,Robot-aesthetics,,"In Experiment 2, the study manipulated combinations of color, material, and texture, which are all visual design elements that fall under 'Robot-aesthetics'. The paper states, 'In experiment 2, manipulations that were found to be significant, but relationally opposed, in experiment 1 were presented in four combinations: Blue (casual) -Metal (professional); Blue (feminine) -Slippery (masculine); Blue (casual) -Smooth (professional); and Rough (impractical, outdoors) -Metal (practical, indoors).' The results showed that these combinations led to ambiguous attributions, indicating that the manipulated aesthetic features influenced the perception of the robot's internal characteristics, which is directly related to trust. Therefore, 'Robot-aesthetics' is the most appropriate category for the manipulated factor and the factor that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1177/1541931213571285,http://journals.sagepub.com/doi/10.1177/1541931213571285,"To date, research into the potential impact of fundamental design attributes, such as material and color, on human-robot trust has been limited. This study addresses how a human’s perception of fundamental, basic design features (i.e., the robot’s physical appearance) may influence their attribution of anthropomorphic characteristics to the robot. Two experiments investigated the correlations between the color, texture, and material of a robot body and the perception of the robot’s internal characteristics (i.e. intelligence, friendliness, robustness, reliability, personality, and integrity), as well as its appropriate uses and tasks. Experiment 1 found correlations between participants’ basic attributions and fundamental design elements of the robot images. Experiment 2 evaluated combinations of significant correlational relationships from study 1 to determine which of competing characteristics would determine the participants’ attributions of the robots’ internal characteristics. These correlations have implications for robot design and will lead to the creation of design heuristics and guidelines that can address any identified human biases occurring based on robot appearance alone."
"Wright, Timothy J.; Horrey, William J.; Lesch, Mary F.; Rahman, Md Mahmudur",Drivers’ trust in an autonomous system: Exploring a covert video-based measure of trust,2016,1,47,30,17,"17 participants were excluded because only a subset of the participants were considered for the current analysis, 4 participants did not have complete video data",Controlled Lab Environment,mixed design,Participants completed two drives in a simulator with an autonomous system engaged. Body movements were recorded via video. Participants completed trust questionnaires after the drives.,Participants drove a simulated vehicle with an autonomous driving system engaged.,Unspecified,Autonomous Vehicles,Research,Navigation,Path Following,minimal interaction,Participants interacted with a simulated autonomous driving system.,simulation,Participants experienced a simulated driving environment.,simulated,The autonomous vehicle was simulated in a driving simulator.,fully autonomous (limited adaptation),"The autonomous system maintained lane position and adjusted speed, but did not adapt to unexpected scenarios.",Questionnaires; Behavioral Measures,,Video Data,Trust was measured using self-report questionnaires and video analysis of body movements.,"parametric models (e.g., regression)",ANOVA was used to compare body movement data across trust quartiles.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,"The study did not manipulate any specific factors related to trust, but measured trust based on self-report and body movements.",The study found no significant relationship between body movements and self-reported trust.,"The study found that foot movements were sensitive to the drive type (rural vs. industrial), but not to individual differences in trust. The study was likely underpowered to detect individual differences in trust.",Body movements were not found to be a sensitive measure of individual differences in trust in an autonomous driving system.,"The robot (autonomous driving system) maintained lane position and adjusted speed. The human participant was instructed to engage the autonomous system and allow it to drive, while their body movements were recorded.",ANOVA,ANOVAs were used to compare body movement counts and durations across different quartile groups based on self-reported trust. The ANOVAs included drive type (rural or industrial) as a within-subjects factor and trust quartile group (1-4) as a between-subjects factor. The purpose was to determine if body movements were sensitive to individual differences in drivers' level of trust in the autonomous system.,FALSE,Task-environment,,,"The study did not explicitly manipulate any factors related to trust. However, the study design included two different driving environments (rural and industrial), which implicitly influenced the task environment. This is why 'Task-environment' was selected as a factor. The study found no significant impact of the driving environment on trust, so no factors were listed under 'factors_that_impacted_trust'. The study did not find any significant relationship between body movements and self-reported trust, so no factors were listed under 'factors_that_did_not_impact_trust'.",10.1177/1541931213601308,http://journals.sagepub.com/doi/10.1177/1541931213601308,"Drivers’ trust in automation will likely determine the extent that autonomous and semi-autonomous vehicles are adopted, and once adopted, used properly. Unfortunately, previous studies have typically utilized overt subjective measures in determining an individual’s level of trust in automation. The current study aims to evaluate a covert behavioral measure of trust based on drivers’ body (head, hand, and foot) movements as they experience a simulated autonomous system. Videos of drivers interacting with an autonomous driving system in a driving simulator were coded. Body movement counts and average durations were derived from this coding and these data were compared across quartile rankings (based on self-reported trust) to examine body movements’ sensitivity to drivers’ level of trust. Results suggest body movements are not sensitive to individual differences in reported trust. Future work should further examine the utility of this covert behavioral metric by further examining situational differences."
"Wright, Julia L.; Chen, Jessie Y. C.; Lakhmani, Shan G.",Agent Transparency and Reliability in Human–Robot Interaction: The Influence on User Confidence and Perceived Reliability,2020,1,28,28,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants monitored a simulated soldier team and an autonomous squad member (ASM) in a training course. They completed four trials with different ASM transparency and reliability levels. After each trial, they completed questionnaires and SA queries.","Participants monitored a simulated soldier team and an ASM, detecting threats and identifying events. They also evaluated the ASM's performance.",Unspecified,Mobile Robots; Unmanned Ground Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the robot through a simulation, with no physical contact.",simulation,The interaction took place in a simulated environment.,simulated,The robot was a simulated six-wheeled cart.,fully autonomous (limited adaptation),"The robot operated autonomously, making decisions based on its perception of the environment, but with limited adaptation.",Questionnaires; Custom Scales,Jian et al. Trust Scale; Godspeed Questionnaire; NASA Task Load Index (NASA-TLX),,Trust was assessed using questionnaires and a custom scale.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's reliability (error rate) and transparency (amount of information displayed) were directly manipulated to see how they affected trust.,"Robot reliability had a significant impact on trust, with lower trust in unreliable conditions. Transparency had a limited effect on trust, except for trust in the robot's ability to collect and filter information.","The study found that robot errors had a lasting negative impact on participants' perception of the robot's reliability and their confidence in their assessments. The in-depth information did not mitigate the negative effects of errors on trust, contrary to expectations. There was also a trend of participants 'straight-lining' their responses in later trials, which was mitigated by only analyzing the first trial for trust data.",Agent reliability is a stronger influence on human perceptions of the robot than agent transparency. Robot errors had a profound and lasting effect on the human teammates' perception of the agent and their confidence in their assessments.,"The robot, a simulated autonomous squad member (ASM), moved through a simulated environment and responded to events. The human participant monitored the robot and the environment, detecting threats and identifying events, and evaluating the robot's performance.",Repeated measures ANOVA; t-test; t-test; ANOVA,"The study used repeated-measures ANOVAs to analyze the data, with Greenhouse-Geisser correction for sphericity when applicable. Planned comparisons were conducted using paired t-tests to examine differences between specific conditions (SR-SU, SR-DR, SU-DU, and DR-DU). Independent samples t-tests were used to examine carry-over effects of errors on participant perception of reliability and confidence. One-way ANOVAs with planned comparisons were used to assess participant trust in the ASM across the four functions of automation.",TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-accuracy,Robot-verbal-communication-content,"The study manipulated 'Robot-verbal-communication-content' by varying the depth of information displayed about the robot's decision-making process (surface-level vs. in-depth SAT information). This is described in the 'Experiment Design and Performance Measures' section where it states 'ASM Transparency was manipulated by varying the depth of SAT model information displayed'. The study also manipulated 'Robot-accuracy' by varying the robot's error rate when responding to events, as stated in the same section: 'ASM Reliability was manipulated by varying the ASM's error rate when it responded to events occurring in the environment'. The results showed that 'Robot-accuracy' significantly impacted trust, as stated in the 'Functional Trust Survey' section: 'Planned comparisons indicated this difference was due to agent reliability, not varying information levels'. However, 'Robot-verbal-communication-content' did not significantly impact trust, except for one specific function of automation, as stated in the same section: 'Information level had no significant effect on operator trust, except in Function A (collecting and/or filtering information)'. Therefore, 'Robot-accuracy' is listed as a factor that impacted trust, and 'Robot-verbal-communication-content' is listed as a factor that did not impact trust.",10.1109/THMS.2019.2925717,https://ieeexplore.ieee.org/document/8795544/,"Agent transparency is an important contributor to human performance, situation awareness (SA), and trust in human–agent teaming. However, agent transparency’s effects on human performance when the agent is unreliable have yet to be examined. This paper examined how the transparency and reliability of an autonomous robotic squad member (ASM) affected a human observer’s task performance, workload, SA, trust in the robot, and perceptions of the robot. In a 2 (ASM transparency) × 2 (ASM reliability) within-subject design experiment, participants monitored a simulated soldier squad that included an ASM as it traversed a simulated training environment, while concurrently monitoring the environment for targets. There was no difference in participants’ performance on the target detection task, workload, or SA due to either ASM transparency or reliability. ASM reliability inﬂuenced participant trust and perceptions of the robot. Results suggest that reliability may be a stronger inﬂuence on the human’s perceptions of the robot than transparency. Robot errors had a profound and lasting effect on the participants’ perception of the robot’s future reliability and resulted in reduced conﬁdence in their assessments of the robot’s reliability. These ﬁndings could have important implications for the continued use of automated systems when the user is aware of system errors."
"Wu, Min; Wang, Nanxi; Yuen, Kum Fai",Deep versus superficial anthropomorphism: Exploring their effects on human trust in shared autonomous vehicles,2023,1,451,451,0,No participants were excluded,Online Crowdsourcing,,"Participants completed an online survey with questions about their demographics, travel characteristics, and their perceptions of shared autonomous vehicles (SAVs) with varying levels of anthropomorphism. The survey included an attention check question to ensure data quality. The survey was distributed through Qualtrics.","Participants answered questions about their perceptions of SAVs, including their trust in the vehicles, interaction quality, and facilitating conditions. They were asked to consider both superficial and deep anthropomorphism scenarios.",Unspecified,Autonomous Vehicles,Other: Transportation,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the SAVs and interaction scenarios.,media,The interaction was based on written descriptions of SAVs.,hypothetical,The robot was described in text without any visual representation.,not autonomous,The robot's actions were described hypothetically without any real autonomy.,Questionnaires; Custom Scales,,,Trust was measured using questionnaires and custom scales.,"parametric models (e.g., regression)",The study used structural equation modeling to analyze the relationships between variables.,Observational & Survey Studies,Quantitative Surveys,Direct Manipulation,"The study manipulated the level of anthropomorphism of the SAVs, presenting scenarios with either superficial or deep anthropomorphic features to influence trust.","Superficial anthropomorphism increased trust, while deep anthropomorphism had a negative effect on trust, particularly for certain demographic groups.","The negative effect of deep anthropomorphism was not statistically significant for the whole sample, but it was significant for specific subgroups (male, low-income, low-education, and no-vehicle ownership).","A superficial level of anthropomorphism can significantly boost human trust in SAVs, while a deep level of anthropomorphism can decrease trust, especially for certain demographic groups.","Participants read descriptions of SAVs with varying levels of anthropomorphism and answered survey questions about their perceptions of trust, interaction quality, and facilitating conditions. The robot was described as a shared autonomous vehicle, and the human's role was to evaluate the vehicle based on the provided descriptions.",Partial least squares; Multilevel Model; Pearson correlation; Heterotrait-monotrait ratio; Bootstrapping; multigroup confirmatory factor analysis,"The study employed structural equation modeling to test the research model and analyze the relationships between variables such as anthropomorphism, trust, interaction quality, and facilitating conditions. Confirmatory factor analysis was used to assess the reliability and validity of the measurement items. Pearson correlations were calculated to examine the relationships between latent variables. The heterotrait-monotrait ratio of correlations (HTMT) test was used to assess discriminant validity. The bootstrap method was used to test the mediating effects of interaction quality and facilitating condition. Multigroup confirmatory factor analysis was used to ensure measurement invariance before conducting multigroup analysis to compare effects across subgroups.",TRUE,Robot-aesthetics,Robot-aesthetics,,"The study manipulated the level of anthropomorphism of the SAVs, presenting scenarios with either superficial or deep anthropomorphic features. Superficial anthropomorphism included physical appearances, communication modes, and moving patterns, while deep anthropomorphism included moral virtue, cognitive experience, and conscious emotionality. The paper states, 'In this study, the lower-level dimension of anthropomorphism is defined as superficial anthropomorphism whereas the higher-level dimension is defined as deep anthropomorphism.' This manipulation of anthropomorphism directly relates to the visual appeal and design of the robot, which falls under the 'Robot-aesthetics' category. The study found that superficial anthropomorphism increased trust, while deep anthropomorphism had a negative effect on trust, particularly for certain demographic groups. Therefore, 'Robot-aesthetics' is the factor that impacted trust. There were no other factors manipulated.",10.1016/j.chb.2022.107614,https://linkinghub.elsevier.com/retrieve/pii/S0747563222004344,"Recently, there is an increasing trend to study the impact of anthropomorphism on human trust. However, previous research has shown divergent results. In this study, shared autonomous vehicles (SAVs), an emerging mobility solution, are used to explore the threshold effects of anthropomorphism on human trust. Structural equation modelling is deployed to test the research model based on a valid survey sample of 451 respondents. Results show that a superficial level of anthropomorphism can significantly boost human trust via interaction quality and facilitating condition. By contrast, a deep level of anthropomorphism would decrease human trust. Although the negative effect does not reach statistical significance based on the whole sample, results of the multigroup analysis show that a deep level of anthropomorphism has a significant negative effect on human-SAV interaction quality when respondents possess the following characteristics: (1) male, (2) low-income, (3) loweducation, or (4) no-vehicle ownership. Regarding theoretical contribution, this study enriches the literature by identifying the threshold effects of anthropomorphism on human trust. Regarding policy and management implications, this study offers some implications on adding anthropomorphic features to SAVs."
"Wu, Min; Yuen, Kum Fai","Initial trust formation on shared autonomous vehicles: Exploring the effects of personality-, transfer- and performance-based stimuli",2023,1,501,451,50,"50 participants were excluded from the pilot study due to minor revisions to the questionnaire, and some participants were excluded due to careless responses, survey fraud, poor quality detection, speeding, random responding, inconsistent or illogical responding, overusing the 'Don't Know' or 'I Don't Know' responses, and too rapid survey completion",Online Crowdsourcing,,"Participants completed an online survey with questions about their demographics, travel characteristics, prior experience, and their perceptions of shared autonomous vehicles. The survey used a 7-point Likert scale to measure responses.","Participants answered questions about their trust in shared autonomous vehicles, their trust propensity, their trust in shared mobility, their perceptions of SAV capability and interaction quality, and their acceptance of SAVs.",Unspecified,Autonomous Vehicles,Other: Transportation,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the concept of shared autonomous vehicles and answered questions.,media,The interaction was based on text descriptions of shared autonomous vehicles.,hypothetical,"The robot was only described hypothetically, without any visual representation.",not autonomous,"The robot's actions were only described hypothetically, without any real autonomy.",Questionnaires,,,Trust was measured using a questionnaire with Likert scale questions.,"parametric models (e.g., regression)",Structural equation modeling was used to analyze the relationships between trust and other variables.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study assessed trust without intentionally altering any specific factors.,,"The study found that interaction quality was the most important factor influencing initial trust in SAVs, followed by trust in shared mobility and trust propensity.",Interaction quality is the most important factor influencing initial trust in shared autonomous vehicles.,"Participants completed an online survey, answering questions about their perceptions of shared autonomous vehicles. The robot's actions were only described hypothetically.",Structural equation modeling; Multilevel Model; Pearson correlation; Bootstrapping; multigroup analysis; multigroup confirmatory factor analysis,"The study used structural equation modeling (SEM) to evaluate the theoretical model and test the hypotheses about the relationships between trust propensity, trust in shared mobility, SAV capability, interaction quality, initial trust in SAVs, and acceptance of SAVs. Confirmatory factor analysis (CFA) was used to assess the reliability and validity of the measurement items. Pearson correlations were used to assess discriminant validity. A bootstrap method was used to test the indirect effects of the antecedents on acceptance via initial trust. Multigroup analysis (MGA) was conducted to investigate significant differences across subgroups with different characteristics. Multigroup confirmatory factor analysis was used to confirm measurement invariance for moderating effect testing.",FALSE,,,,"The study did not manipulate any factors. It was an observational study that used a survey to assess the relationships between trust propensity, trust in shared mobility, SAV capability, interaction quality, initial trust in SAVs, and acceptance of SAVs. The study did not involve any experimental manipulation of these factors. The participants were asked to answer questions about their perceptions of SAVs, but there was no intentional alteration of any specific factors by the researchers to measure the resulting impact on trust. Therefore, no factors are listed under 'factors_manipulated', 'factors_that_impacted_trust', or 'factors_that_did_not_impact_trust'.",10.1016/j.tra.2023.103704,https://linkinghub.elsevier.com/retrieve/pii/S0965856423001246,"Building initial trust is critical for the acceptance of shared autonomous vehicles (SAVs). Initial trust determines whether this emerging mobility solution will be accepted when it is available in the market. This study examines the initial trust formation process in the context of SAVs using the elaboration likelihood model and trust transfer theory. It investigates the effects of different personality-based, transfer-based, and performance-based factors on initial trust and adoption intention. A structural equation modelling is conducted in Singapore based on valid survey design principles, sampling protocols, and data analysis procedures. Results show that among three trustbuilding paths, the performance-based factors which include SAV capability and interaction quality are the most important. The transfer-based (i.e., trust in shared mobility) and personalitybased factor (i.e., trust propensity) rank second and third, respectively. Six moderators such as covid history and shared mobility experience are also tested to investigate significant differences in the results. Based on these findings, this study offers theoretical and policy implications for scholars and practitioners."
"Xie, Yaqi; Bodala, Indu P.; Ong, Desmond C.; Hsu, David; Soh, Harold",Robot Capability and Intention in Trust-based Decisions across Tasks,2019,1,400,400,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were randomized into two groups (GH or GR) and one of four agent types. They completed a series of scenarios in three tasks (searching, mapping, fire-fighting), making delegation decisions and providing trust assessments after each scenario.","Participants decided whether to delegate control of a UAV to a simulated agent in three different tasks: searching, mapping, and fire-fighting.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Game,Economic Game,minimal interaction,Participants interacted with a simulated robot through an online interface.,simulation,The interaction was presented through an online simulation of a task delegation scenario.,simulated,The robot was represented as a simulated UAV in the online environment.,pre-programmed (non-adaptive),"The robot's behavior was pre-programmed based on expected utility maximization, without adapting to the user.",Behavioral Measures; Questionnaires; Custom Scales,Schaefer's Trust Questionnaire/Scale,Performance Metrics,"Trust was assessed using self-reported questionnaires, custom scales, and behavioral measures of task delegation.","parametric models (e.g., regression)","Linear mixed models were used to analyze the relationship between inferred capability, intention, and trust.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's capability and risk-taking intention were manipulated by varying the success probabilities and decision-making criteria, influencing participants' perceptions and delegation decisions.",Inferred capability and intention significantly influenced self-reported trust and delegation decisions; participants were more likely to trust robots with higher capability and aligned intentions.,"Participants reported higher trust in simulated human agents compared to simulated robot agents, but this did not always translate to more trusting behavior. There was also some confusion between the high capability and risk-averse agent, and the low-capability and risk-averse agent.","Human decisions to delegate control to a robot depend not only on overall trust but also on estimations of robot capability and intention, suggesting that humans use rich mental models of robots.","The robot (UAV) made decisions about which location to go to in order to complete a task, and the human participant decided whether to delegate control to the robot or to take over control.",t-test; linear mixed models,"The study used t-tests to compare means of inferred capability and intention scores across different agent types and to compare trust scores between the human and robot groups. Linear mixed models were used to analyze the relationship between inferred capability, intention, and self-reported trust, as well as the influence of these factors on the decision to delegate control to the robot. These models included random intercepts for subjects, scenarios, and tasks to account for repeated measures.",TRUE,Robot-accuracy; Robot-task-strategy; Robot-autonomy; Teaming,Robot-accuracy; Robot-task-strategy,Robot-autonomy; Teaming,"The study manipulated 'Robot-accuracy' by varying the success probabilities of the robot in different weather conditions, directly impacting task performance (e.g., Table I shows different success probabilities for high and low capability robots). 'Robot-task-strategy' was manipulated by varying the robot's risk-taking behavior, which influenced its decision-making criteria (e.g., high-risk agents maximizing targets even with low success probability, as described in the 'Confederate Agent/Robot Types' section). 'Robot-autonomy' was manipulated by allowing participants to choose whether to delegate control to the robot or to take over control, which is a manipulation of the level of decision authority given to the robot. 'Teaming' was manipulated by informing participants that they were either playing with a robot or a human, which is a manipulation of the type of teammate. The results showed that 'Robot-accuracy' and 'Robot-task-strategy' significantly impacted trust, as participants reported higher trust for robots with higher capability and aligned intentions (as described in the 'IV. RESULTS' section). While 'Robot-autonomy' was a factor in the study design, the results did not show a direct impact on trust levels, but rather on the decision to delegate. Similarly, while 'Teaming' was a factor, the results showed that while participants reported higher trust in human agents, this did not translate to more trusting behavior, indicating that it did not directly impact trust in the same way as capability and intention.",,http://arxiv.org/abs/1909.05329,"In this paper, we present results from a human-subject study designed to explore two facets of human mental models of robots---inferred capability and intention---and their relationship to overall trust and eventual decisions. In particular, we examine delegation situations characterized by uncertainty, and explore how inferred capability and intention are applied across different tasks. We develop an online survey where human participants decide whether to delegate control to a simulated UAV agent. Our study shows that human estimations of robot capability and intent correlate strongly with overall self-reported trust. However, overall trust is not independently sufficient to determine whether a human will decide to trust (delegate) a given task to a robot. Instead, our study reveals that estimations of robot intention, capability, and overall trust are integrated when deciding to delegate. From a broader perspective, these results suggest that calibrating overall trust alone is insufficient; to make correct decisions, humans need (and use) multi-faceted mental models when collaborating with robots across multiple contexts."
"Ximenes, Bianca H.; Moreira, Icaro M.; Kelner, Judith",Extreme human-robot interfaces: Increasing trust and assurance around robots,2014,1,24,24,0,No participants were excluded,Real-World Environment,within-subjects,"Participants were invited to get a 'tattoo' with a robot, answering a questionnaire before, during, and after the experience. The robot drew patterns on their skin using a marker. A human operator was present to assist and observe.",Participants received a temporary 'tattoo' from a robot and answered questionnaires about their experience.,Lynxmotion AL5B,Industrial Robot Arms,Research,Manipulation,Drawing,direct-contact interaction,Participants had direct physical contact with the robot during the drawing process.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The robot was a physical entity interacting with the participants.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions without adapting to the user.,Questionnaires,,,"Trust was assessed using questionnaires before, during, and after the interaction.",no modeling,Trust was not modeled computationally in this study.,Empirical HRI Studies,Physical Robot Studies,Indirect Manipulation,"The study indirectly influenced trust by providing information about the robot's capabilities and allowing participants to experience the interaction, which influenced their expectations and perceptions of the robot.","Participants' trust in robots increased after the experience, as they felt they understood how robots function better.","Participants initially expressed anxiety and fear, but these feelings decreased during the interaction, with many reporting amusement and increased trust afterwards. Some participants reported discomfort due to the robot's inability to respond to their needs.","Participants' trust in robots increased after experiencing a direct interaction, suggesting that hands-on experience can improve trust despite initial apprehension.","The robot drew patterns on the participant's skin using a marker, while the participant remained still and answered questionnaires. The human operator assisted and observed the interaction.",,"No statistical tests were explicitly mentioned in the paper. The analysis was primarily descriptive, focusing on the frequencies and percentages of responses to questionnaires before, during, and after the interaction with the robot. The study used qualitative analysis of comments and observations to understand the participants' experiences and feelings.",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-accuracy; Robot-verbal-communication-content,,"The study implicitly manipulated 'Robot-accuracy' by having the robot perform a drawing task, where the accuracy of the robot's movements and the resulting pattern on the skin directly influenced participants' perceptions of its capabilities. The robot's inability to respond to discomfort or adjust to the skin's surface also impacted the perceived accuracy. The study also manipulated 'Robot-verbal-communication-content' by providing explanations about the robot's function and the procedure before the interaction, which influenced participants' expectations and trust. The presence of a human operator who provided reassurance and step-by-step explanations also falls under this category. The study found that both the robot's accuracy (or lack thereof) and the verbal communication about the process impacted trust. The participants' trust increased after the experience, which they attributed to a better understanding of how robots function, which was influenced by both the robot's performance and the explanations provided.",10.1109/ROMAN.2014.6926384,http://ieeexplore.ieee.org/document/6926384/,"This paper approaches the issues of human-robot interaction in extreme human-robot interface situations, defined as those which provide a real physical intervention in a person’s body by a robot. Robots that belong to this group perform tasks such as tattooing, collecting blood and doing surgery. Authors use a Lynxmotion AL5B mechanical arm controlled via Botboarduino named BOTicelli to make simple patterns with a felt-tip pen on volunteers’ skin. The patterns are composed by clusters of dots, similar to real-world tattooing techniques. This paper intends to understand the major discomforts experienced by volunteers while interacting with BOTicelli and whether there are possible modifications of robot design or procedures that would render the experience less aggravating for humans."
"Xu, Anqi; Dudek, Gregory",Trust-driven interactive visual navigation for autonomous robots,2012,2,10,10,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were asked to assist an autonomous navigation system in tracking a curved street. They manually steered the vehicle for 5 seconds, then relinquished control to the autonomous tracker. Each participant completed 10 trials with both a trust-driven and non-adaptive tracker, repeated for 4 starting locations.",Participants assisted a simulated robot in tracking a street.,Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Navigation,Path Following,minimal interaction,Participants interacted with a simulated robot through a computer interface.,simulation,The interaction took place in a simulated environment with aerial images.,simulated,The robot was represented as a simulated aerial vehicle.,shared control (adaptive),The robot adapted its behavior based on human input.,Performance-Based Measures,,Performance Metrics,Trust was inferred based on the human's interventions and the robot's performance.,"parametric models (e.g., regression)",A logarithmic utility function was used to model trust based on the robot's reputation.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's autonomy was manipulated by using a trust-driven adaptive system, which changed the robot's behavior based on human input, and the task difficulty was influenced by the simulated environment.","The trust-driven system increased the robot's performance, suggesting an increase in trust.","The trust-driven tracker consistently outperformed the non-adaptive tracker, and a specific classifier mode was consistently chosen for each road section.",The trust-driven adaptation process increased the task performance of the autonomous robot controller.,"The robot autonomously tracked a street, and the human intervened when the robot performed poorly.",t-test,A two-tailed t-test was used to compare the mean traveled distance between the trust-enabled system and the non-adaptive variant of the robot controller. The purpose was to determine if the trust-driven adaptation process significantly improved the robot's performance in terms of distance traveled before failure.,TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,,"The study manipulated 'Robot-autonomy' by using a trust-driven adaptive system that changed the robot's behavior based on human input, and compared it to a non-adaptive system. This is described in the paper as the trust module adjusting the robot's planner based on the human's interventions (e.g., 'Whenever this module detects that the human has lost trust in the robot's abilities, it will then adjust the robot's behaviors to regain the human's confidence.'). The 'Task-complexity' was also manipulated by the simulated environment, which included visual properties that made road recognition challenging (e.g., 'portions of the road are obstructed by shadows from trees and buildings.'). The results showed that the trust-driven system increased the robot's performance, suggesting that 'Robot-autonomy' impacted trust, as the system was designed to increase trust by improving performance. There is no evidence that the task complexity impacted trust, as it was a constant factor across conditions.",10.1109/ICRA.2012.6225171,http://ieeexplore.ieee.org/document/6225171/,"We describe a model of “trust” in human-robot systems that is inferred from their interactions, and inspired by similar concepts relating to trust among humans. This computable quantity allows a robot to estimate the extent to which its performance is consistent with a human’s expectations, with respect to task demands. Our trust model drives an adaptive mechanism that dynamically adjusts the robot’s autonomous behaviors, in order to improve the efﬁciency of the collaborative team. We illustrate this trust-driven methodology through an interactive visual robot navigation system. This system is evaluated through controlled user experiments and a ﬁeld demonstration using an aerial robot."
"Xu, Anqi; Dudek, Gregory",Trust-driven interactive visual navigation for autonomous robots,2012,2,10,10,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed two navigation courses, first using teleoperation and then using the trust-enabled boundary tracker. They were allowed to correct the robot's path as needed.",Participants navigated a simulated robot along coastlines and colored regions.,Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Navigation,Path Following,minimal interaction,Participants interacted with a simulated robot through a computer interface.,simulation,The interaction took place in a simulated environment with aerial images.,simulated,The robot was represented as a simulated aerial vehicle.,shared control (adaptive),The robot adapted its behavior based on human input.,Performance-Based Measures,,Performance Metrics,Trust was inferred based on the human's interventions and the robot's performance.,"parametric models (e.g., regression)",A logarithmic utility function was used to model trust based on the robot's reputation.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's autonomy was manipulated by using a trust-driven adaptive system, and the task difficulty was influenced by the complexity of the navigation courses. The participant's role was also manipulated by having them switch between teleoperation and using the trust-enabled tracker.","The trust-enabled system reduced the need for manual steering, suggesting an increase in trust and reduced task load.","The trust-enabled system had comparable performance to teleoperation, but with the autonomous tracker in control for most of the time.",The trust-driven methodology reduced the human's task load by delegating the navigation task to the autonomous system.,"The robot autonomously tracked boundaries, and the human intervened when the robot performed poorly or to switch between targets.",,No specific statistical tests were mentioned in the description of this study. The analysis focused on comparing the performance of teleoperation and the trust-enabled system based on qualitative observations and the amount of time the autonomous tracker was in control.,TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,,"The study manipulated 'Robot-autonomy' by having participants switch between teleoperation and using the trust-enabled boundary tracker, which adapted its behavior based on human input. This is described in the paper as the trust module adjusting the robot's planner based on the human's interventions (e.g., 'Whenever this module detects that the human has lost trust in the robot's abilities, it will then adjust the robot's behaviors to regain the human's confidence.'). The 'Task-complexity' was manipulated by using two different navigation courses, one with a coastline and another with colored regions, which varied in difficulty. The results showed that the trust-enabled system reduced the need for manual steering, suggesting that 'Robot-autonomy' impacted trust, as the system was designed to increase trust by reducing the need for human intervention. There is no evidence that the task complexity impacted trust, as it was a constant factor across conditions.",10.1109/ICRA.2012.6225171,http://ieeexplore.ieee.org/document/6225171/,"We describe a model of “trust” in human-robot systems that is inferred from their interactions, and inspired by similar concepts relating to trust among humans. This computable quantity allows a robot to estimate the extent to which its performance is consistent with a human’s expectations, with respect to task demands. Our trust model drives an adaptive mechanism that dynamically adjusts the robot’s autonomous behaviors, in order to improve the efﬁciency of the collaborative team. We illustrate this trust-driven methodology through an interactive visual robot navigation system. This system is evaluated through controlled user experiments and a ﬁeld demonstration using an aerial robot."
"Xu, J.; Montague, E.",Working with an Invisible Active User: Understanding Trust in Technology and Co-User from the Perspective of a Passive User,2013,1,38,36,2,2 participants were excluded due to instrument malfunctioning,Controlled Lab Environment,mixed design,"Participants provided consent, completed a baseline measure, received instructions about the task, watched three videos of an active user performing a task under different conditions, and completed surveys after each video.",Participants watched videos of an active user performing a multi-attribute task battery (MATB) and were asked to monitor the active user and the technology.,Unspecified,Other,Research,Supervision,Monitoring,passive observation,Participants passively observed videos of an active user performing tasks.,media,Participants watched video recordings of the active user performing the task.,simulated,The robot was represented through a video recording of a screen.,pre-programmed (non-adaptive),The active user's actions in the video were pre-recorded and did not adapt to the participant.,Questionnaires; Physiological Measures; Eye-tracking Data,Jian et al. Trust Scale; Mayer and Davis' Trust/Trustworthiness Scales (1999); NASA Task Load Index (NASA-TLX),Eye-tracking Data; Physiological Signals,"Trust was measured using questionnaires, physiological measures, and eye-tracking data.","parametric models (e.g., regression)",Linear mixed effects models were used to analyze the data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the difficulty of the task and the reliability of the technology, as well as the performance of the active user in the videos, to influence the passive user's perception of trust.",Trust in technology and trust in the active user were lower in the hard and low reliability conditions compared to the normal condition. The perceived workload of the active user was higher in the hard condition.,"The study found that passive users' trust in the active user was influenced by their trust in the technology, which is different from previous studies with face-to-face interactions. The study also found that the passive user's evaluation of the active user's workload could indicate how the passive user perceives the active user's interaction with the technology.",Passive users' trust in the active user is positively correlated with their trust in the technology being used in a distance collaboration context.,"The active user performed a multi-attribute task battery (MATB) involving monitoring, tracking, and resource management tasks. The passive user watched videos of the active user performing these tasks and monitored their performance and the technology's reliability.",Mixed-effects model; t-test; Linear regression,"The study used linear mixed effects (LME) models to analyze the subjective ratings, eye movement data, EDA, and cardiovascular activity data. The LME models included technological conditions, active user performance, and their interaction as fixed effects, and a random intercept for participants. Post-hoc t-tests were used for pairwise comparisons of significant effects. Linear regression was used to examine correlations between subjective measures, controlling for participant effects. Additionally, a mediation analysis was conducted using a method proposed by Baron and Kenny (1986) to explore the relationships between trust in technology, trust in the active user, and perceived workload.",TRUE,Task-complexity; Robot-accuracy,Task-complexity; Robot-accuracy,,"The study manipulated the difficulty of the task (Task-complexity) by having three levels: normal, hard, and low reliability. In the hard condition, the monitoring and resource management tasks were made more difficult. In the low reliability condition, the pumps in the resource management task became unstable. The study also manipulated the performance of the active user (Robot-accuracy) by having two levels: high performance and low performance. In the low-performance condition, the active user had slower reaction times, higher miss rates, and higher deviations in the tracking and resource management tasks. The paper states that 'the performance of the active user in the high-performance condition was higher than that in the low-performance condition across the three tasks and the technological conditions.' The results showed that trust in technology and trust in the active user were lower in the hard and low reliability conditions compared to the normal condition, indicating that both Task-complexity and Robot-accuracy impacted trust. No factors were found to not impact trust.",10.1093/iwc/iws022,https://academic.oup.com/iwc/article-lookup/doi/10.1093/iwc/iws022,"Distance collaboration technologies affect the way active and passive users interact in technologymediated systems. Decreases in social and contextual cues in distance collaboration may have a large impact on passive users’ perception of active users and the technology. The purpose of this study was to investigate passive users’ trust in active users and trust in technology under varied technological conditions and active user performance. A laboratory experiment was conducted using simulated psychomotor tasks distance collaboration scenarios. Participants observed an active user, who performed tasks without being physically present. Their subjective report on trust in the active user, trust in technology and perceived active user’s workload, as well as physiological responses, including eye movement, electrodermal activity and cardiovascular activity, were gathered. The results showed that technology conditions affected passive users’ subjective reports, specifically; the participants exhibited higher arousal in the affect arousal system during the observation. Furthermore, the passive users seemed to evaluate their trust in the active user according to their trust in technology. This implies that in a distance collaboration context, technology use could affect interpersonal relationships between active and passive users."
"Xu, Anqi; Dudek, Gregory",OPTIMo: Online Probabilistic Trust Inference Model for Asymmetric Human-Robot Collaborations,2015,1,21,21,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a demographics survey, watched a slideshow about the study, worked through an interactive tutorial and two practice sessions, and then completed two task scenarios twice each, for a total of 10 recorded sessions. Trust assessments were collected after each session and during the interaction.",Participants supervised a simulated aerial robot tasked with following boundary targets in a visual navigation context. They could intervene to take over control when necessary.,Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a simulated robot through a computer interface, with the ability to intervene.",simulation,The study used a simulated environment to represent the robot and its task.,simulated,The robot was represented as a simulation within the study environment.,shared control (fixed rules),The robot operated autonomously but the human could intervene and take over control.,Behavioral Measures; Custom Scales; Real-time Trust Measures,,Performance Metrics; robot data,"Trust was assessed using a modified Visual Analog Scale, button presses indicating trust changes, and behavioral measures such as human interventions.",hidden markov model,A Dynamic Bayesian Network was used to model the human's latent trust state based on observed interaction factors.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's task performance was manipulated through simulated failures, and the human's role was to supervise and intervene when necessary, influencing trust.","Robot failures led to decreases in trust, while successful performance increased trust. User interventions were strongly correlated with lower trust.","Users reacted consistently to similar events, but their trust assessments changed over time. User interventions were more strongly correlated with trust than AI failures. Users reported that their trust changed when the robot did something unpredictable.","The Online Probabilistic Trust Inference Model (OPTIMo) can accurately predict human trust states in near real-time, by combining causal reasoning of robot performance and evidential factors from interaction data.","The robot autonomously followed boundary targets, and the human monitored its performance and intervened when necessary by taking over control. The human also provided trust feedback after each session and during the interaction.",t-test; ANOVA; Linear regression,"Paired t-tests were used to compare the rate of human interventions and the sum of reported trust changes across paired sessions. A one-tailed paired t-test was used to compare trust feedback across repeated scenarios. An ANOVA was used to assess the relationship between trust feedback and user identifier, intervention rate, and AI failure rate. Linear regression was used to identify significant covariates from interaction experience related to trust feedback.",TRUE,Robot-accuracy; Robot-autonomy,Robot-accuracy; Robot-autonomy,,"The study manipulated 'Robot-accuracy' by introducing simulated AI failures where the robot failed to detect boundaries, which directly impacted the robot's task performance. This is explicitly stated in the paper: 'Every time the boundary tracker processed a camera frame, we recorded whether it had failed to detect any boundaries (i.e. AI failures reflecting task performance p ∈ {0, 1})'. The study also manipulated 'Robot-autonomy' by allowing participants to intervene and take over control of the robot, which is described as 'The human ""supervisor"" has the ability to intervene and take over control, but should do so only when necessary'. The paper states that 'the robot's trustworthiness arises due to its task performance: efficient progress lead to greater trust, whereas low reliability induces losses in trust' and 'whenever the human intervenes, it often reflects a lapse in trust due to the robot's task failures', indicating that both 'Robot-accuracy' and 'Robot-autonomy' impacted trust. There were no factors that were manipulated that did not impact trust.",10.1145/2696454.2696492,https://dl.acm.org/doi/10.1145/2696454.2696492,"We present OPTIMo: an Online Probabilistic Trust Inference Model for quantifying the degree of trust that a human supervisor has in an autonomous robot “worker”. Represented as a Dynamic Bayesian Network, OPTIMo infers beliefs over the human’s moment-to-moment latent trust states, based on the history of observed interaction experiences. A separate model instance is trained on each user’s experiences, leading to an interpretable and personalized characterization of that operator’s behaviors and attitudes. Using datasets collected from an interaction study with a large group of roboticists, we empirically assess OPTIMo’s performance under a broad range of conﬁgurations. These evaluation results highlight OPTIMo’s advances in both prediction accuracy and responsiveness over several existing trust models. This accurate and near real-time humanrobot trust measure makes possible the development of autonomous robots that can adapt their behaviors dynamically, to actively seek greater trust and greater eﬃciency within future human-robot collaborations."
"Xu, Anqi; Dudek, Gregory",Towards Efficient Collaborations with Trust-Seeking Adaptive Robots,2015,2,20,20,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants interacted with a simulated robot in a boundary patrol task, with trust feedback collected via interval scale and gamepad buttons. The trust model was trained on half of each user's data and tested on the other half.",Participants supervised a simulated robot in a boundary patrol task.,Unspecified,Mobile Robots; Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated robot in a controlled environment.,simulation,The interaction took place in a simulated environment.,simulated,The robot was a simulated entity.,shared control (adaptive),The robot adapted its behavior based on human interventions.,Custom Scales; Real-time Trust Measures,,Performance Metrics; robot data,"Trust was assessed using interval scale feedback and gamepad button presses, along with performance data.",hidden markov model,A Dynamic Bayesian Network was used to model trust as a linear function of performance and a bias term.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,"The study focused on modeling trust based on performance and interventions, without direct manipulation of trust-related factors.",,"The model's predictions consistently outperformed existing models, due to its temporal probabilistic representation.",A dynamic trust model was developed that can accurately predict the operator's moment-to-moment trust states.,"The robot autonomously tracked terrain boundaries, and the human intervened to correct misbehaviors or teach new targets. The human provided trust feedback via interval scale and gamepad buttons.",Pearson correlation,The study used Pearson's r correlation to compare the trust predictions of their model with existing models. The model's predictions were evaluated in terms of numerical error and Pearson's r correlation.,FALSE,Robot-accuracy; Robot-autonomy,Robot-accuracy,,"The study focused on modeling trust based on performance and interventions. 'Robot-accuracy' is included because the model uses the robot's task performance as a factor in predicting trust. The robot's autonomy level is also a factor, as the human intervenes to correct misbehaviors or teach new targets, which is a form of shared control, thus 'Robot-autonomy' is included. The paper states that the model updates trust belief as a linear function of the robot's current and recent task performance, which directly impacts trust, thus 'Robot-accuracy' is included in 'factors_that_impacted_trust'. There was no explicit manipulation of these factors, but they were inherent to the study design.",10.1145/2701973.2702711,https://dl.acm.org/doi/10.1145/2701973.2702711,"We are interested in asymmetric human-robot teams, where a human supervisor occasionally takes over control to aid an autonomous robot in a given task. Our research aims to optimize team eﬃciency by improving the robot’s task performance, decreasing the human’s workload, and building trust in the team. We envision synergistic collaborations where the robot adapts its behaviors dynamically to optimize eﬃcacy, reduce manual interventions, and actively seek for greater trust. We describe recent works that study two facets of this trust-seeking adaptive methodology: modeling human-robot trust dynamics, and developing interactive behavior adaptation techniques. We also highlight ongoing eﬀorts to combine these works, which will enable future human-robot teams to be maximally trusting and eﬃcient."
"Xu, Anqi; Dudek, Gregory",Towards Efficient Collaborations with Trust-Seeking Adaptive Robots,2015,2,22,22,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants interacted with a simulated aerial robot and a wheeled robot in boundary patrol tasks, comparing an interactive adaptive system against a non-adaptive expert-tuned variant and plain teleoperation. Efficiency metrics and satisfaction ratings were collected.",Participants collaborated with a simulated aerial robot and a wheeled robot in boundary patrol tasks.,Unspecified,Mobile Robots; Unmanned Aerial Vehicles (UAVs); Mobile Robots,Research,Navigation,Remote Navigation,minimal interaction,Participants interacted with simulated robots in a controlled environment.,simulation,The interaction took place in a simulated environment.,simulated,The robots were simulated entities.,shared control (adaptive),The robot adapted its behavior based on human interventions.,Questionnaires; Performance-Based Measures,,Performance Metrics,Trust was assessed using satisfaction ratings and performance metrics.,no modeling,Trust was not modeled computationally in this study.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's behavior was manipulated by using an adaptive system that learned from human interventions, and the task difficulty was varied by using different terrain types.","The adaptive robot yielded greater efficiency, particularly during more complex tasks, suggesting a positive impact on trust.","The interactive adaptive robot performed comparably to the expert-tuned setup and significantly outperformed plain teleoperation, especially in complex tasks.",An interactive adaptation strategy was developed that increases team efficiency without requiring expert knowledge from the human collaborator.,"The robot autonomously tracked terrain boundaries, and the human intervened to correct misbehaviors or teach new targets. The human also provided satisfaction ratings.",kemeny-young method,"The study used the Kemeny-Young method to aggregate rankings of the three team configurations (interactive adaptive robot, expert-tuned variant, and plain teleoperation) based on various efficiency metrics and satisfaction ratings.",TRUE,Robot-autonomy; Robot-adaptability; Task-complexity,Robot-adaptability; Task-complexity,,"The study compared an interactive adaptive system against a non-adaptive expert-tuned variant and plain teleoperation. 'Robot-autonomy' is included because the robot's behavior was manipulated by using an adaptive system that learned from human interventions, which is a form of shared control. 'Robot-adaptability' is included because the robot's ability to learn from human interventions was a key manipulation. 'Task-complexity' is included because the task difficulty was varied by using different terrain types (straight highway, smooth forest path, curvy coastline). The paper states that the adaptive variant yielded greater efficiency during the coastline section, where the unpredictable curves in the terrain necessitated frequent updates to the tracker's parameters, and that the adaptive system was preferred by users, indicating that 'Robot-adaptability' and 'Task-complexity' impacted trust. There was no explicit manipulation of the robot's accuracy, but the robot's ability to adapt to the task and the task complexity were manipulated.",10.1145/2701973.2702711,https://dl.acm.org/doi/10.1145/2701973.2702711,"We are interested in asymmetric human-robot teams, where a human supervisor occasionally takes over control to aid an autonomous robot in a given task. Our research aims to optimize team eﬃciency by improving the robot’s task performance, decreasing the human’s workload, and building trust in the team. We envision synergistic collaborations where the robot adapts its behaviors dynamically to optimize eﬃcacy, reduce manual interventions, and actively seek for greater trust. We describe recent works that study two facets of this trust-seeking adaptive methodology: modeling human-robot trust dynamics, and developing interactive behavior adaptation techniques. We also highlight ongoing eﬀorts to combine these works, which will enable future human-robot teams to be maximally trusting and eﬃcient."
"Xu, Anqi; Dudek, Gregory",Maintaining efficient collaboration with trust-seeking robots,2016,2,46,46,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were first coached on the task and interface, then completed 5 trust modeling sessions, and finally evaluated three different robot agents in a counter-balanced design.",Participants supervised a boundary tracking agent to steer an aerial drone along designated terrain contours.,Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated drone through a visual interface.,simulation,The study used a drone simulator with synthesized camera feeds.,simulated,The robot was a simulated aerial drone.,shared control (adaptive),The robot adapted its behavior based on human interventions and trust state.,Custom Scales; Behavioral Measures; Real-time Trust Measures,,Performance Metrics; robot data,"Trust was measured using a continuous scale, behavioral measures (interventions), and a real-time trust model.",hidden markov model,"A Dynamic Bayesian Network (OPTIMo) was used to model trust based on performance, interventions, critiques, and trust feedback.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's behavior was manipulated by introducing conservative control based on the inferred trust state, which altered its speed and steering.","The mildly conservative agent was more trusted than the baseline agent, while the strongly conservative agent was the least trusted.","The mildly conservative agent was preferred over both the baseline and strongly conservative agents, suggesting that a balance between conservative behavior and performance is important for trust.",Mildly conservative robot behaviors induced by the human's trust state consistently contributed to superior team efficiency.,"The robot autonomously tracked terrain boundaries, while the human supervised the robot, intervened when necessary, and provided trust critiques.",Friedman test; Nemenyi test,"The Friedman test was used to analyze differences in rankings among the three robot agents (strongly conservative, mildly conservative, and baseline) for both objective (performance and workload) and subjective (collaboration and trustworthiness) efficiency metrics. Post-hoc Nemenyi testing was then used to determine which specific agents differed significantly from each other. The Kemeny-Young voting method was also used to corroborate the aggregate efficiency rankings.",TRUE,Robot-autonomy; Robot-accuracy,Robot-autonomy,Robot-accuracy,"The study manipulated the robot's autonomy by introducing 'Trust-Aware Conservative Control (TACtiC)'. This control altered the robot's speed and steering based on the inferred trust state of the human supervisor. Specifically, the robot would become more conservative (slower and smoother steering) when trust was low. This is a manipulation of autonomy because the robot's behavior was directly altered based on the human's trust state, which is a form of adaptive shared control. The robot's accuracy was also implicitly manipulated, as the conservative control altered the robot's speed and steering, which could affect its ability to accurately track the boundary. However, the study found that the mildly conservative agent was more trusted than the baseline agent, while the strongly conservative agent was the least trusted. This indicates that the manipulation of autonomy (through conservative control) had a direct impact on trust, while the changes in accuracy did not have a significant impact on trust on their own. The study did not find significant differences in performance metrics between the three agents, suggesting that the changes in accuracy were not the primary driver of trust differences. The key finding was that the mildly conservative agent was preferred, indicating that the way the robot adapted its behavior based on trust (autonomy) was the main factor influencing trust, not the changes in accuracy.",10.1109/IROS.2016.7759510,http://ieeexplore.ieee.org/document/7759510/,"In this work, we grant robot agents the capacity to sense and react to their human supervisor’s changing trust state, as a means to maintain the efﬁciency of their collaboration. We propose the novel formulation of Trust-Aware Conservative Control (TACtiC), in which the agent alters its behaviors momentarily whenever the human loses trust. This trust-seeking robot framework builds upon an online trust inference engine and also incorporates an interactive behavior adaptation technique. We present end-to-end instantiations of trust-seeking robots for distinct task domains of aerial terrain coverage and interactive autonomous driving. Empirical assessments comprise a large-scale controlled interaction study and its extension into ﬁeld evaluations with an autonomous car. These assessments substantiate the efﬁciency gains that trustseeking agents bring to asymmetric human-robot teams."
"Xu, Anqi; Dudek, Gregory",Maintaining efficient collaboration with trust-seeking robots,2016,2,12,12,0,No participants were excluded,Real-World Environment,within-subjects,"Participants first practiced with a baseline agent, then interacted with a mildly conservative agent and a baseline agent in a randomized order.",Participants supervised and trained a boundary tracking agent to drive a smart car along a gravel course.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants supervised the robot in a real-world setting, but did not have direct physical contact.",real-world,The study was conducted in a real-world environment with a physical car.,physical,The robot was a physical smart car.,shared control (adaptive),The robot adapted its behavior based on human interventions and trust state.,Custom Scales; Behavioral Measures; Real-time Trust Measures,,Performance Metrics; robot data,"Trust was measured using a continuous scale, behavioral measures (interventions), and a real-time trust model.",hidden markov model,"A Dynamic Bayesian Network (OPTIMo) was used to model trust based on performance, interventions, critiques, and trust feedback.",Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's behavior was manipulated by introducing conservative control based on the inferred trust state, which altered its speed and steering.","The conservative agent was favored over the baseline agent, but the results were not statistically significant.","Although not statistically significant, the conservative agent was favored over the baseline agent, which aligns with the findings of the controlled study. The lack of statistical significance may be due to the small sample size and limited number of study sessions.","The field trials qualitatively re-affirmed the interaction study's findings, thus re-substantiating the efficiency merits of mildly conservative trust-aware agents.","The robot autonomously drove along a gravel course, while the human supervised the robot, intervened when necessary, and provided trust critiques.",Wilcoxon rank sum; Friedman test,"The Wilcoxon signed-rank test and Friedman test were used to analyze differences in rankings between the mildly conservative and baseline agents for both objective and subjective efficiency metrics. However, the results were not statistically significant. The Kemeny-Young method was also used to corroborate the aggregate efficiency rankings.",TRUE,Robot-autonomy; Robot-accuracy,Robot-autonomy,Robot-accuracy,"Similar to the first study, the robot's autonomy was manipulated by introducing 'Trust-Aware Conservative Control (TACtiC)'. The robot's speed and steering were altered based on the inferred trust state of the human supervisor. This is a manipulation of autonomy because the robot's behavior was directly altered based on the human's trust state, which is a form of adaptive shared control. The robot's accuracy was also implicitly manipulated, as the conservative control altered the robot's speed and steering, which could affect its ability to accurately track the boundary. However, the study found that the conservative agent was favored over the baseline agent, but the results were not statistically significant. This indicates that the manipulation of autonomy (through conservative control) had a tendency to impact trust, while the changes in accuracy did not have a significant impact on trust on their own. The study did not find significant differences in performance metrics between the two agents, suggesting that the changes in accuracy were not the primary driver of trust differences. The key finding was that the conservative agent was favored, indicating that the way the robot adapted its behavior based on trust (autonomy) was the main factor influencing trust, not the changes in accuracy. The lack of statistical significance is likely due to the small sample size.",10.1109/IROS.2016.7759510,http://ieeexplore.ieee.org/document/7759510/,"In this work, we grant robot agents the capacity to sense and react to their human supervisor’s changing trust state, as a means to maintain the efﬁciency of their collaboration. We propose the novel formulation of Trust-Aware Conservative Control (TACtiC), in which the agent alters its behaviors momentarily whenever the human loses trust. This trust-seeking robot framework builds upon an online trust inference engine and also incorporates an interactive behavior adaptation technique. We present end-to-end instantiations of trust-seeking robots for distinct task domains of aerial terrain coverage and interactive autonomous driving. Empirical assessments comprise a large-scale controlled interaction study and its extension into ﬁeld evaluations with an autonomous car. These assessments substantiate the efﬁciency gains that trustseeking agents bring to asymmetric human-robot teams."
"Xu, Anqi; Dudek, Gregory",Towards Modeling Real-Time Trust in Asymmetric Human–Robot Collaborations,2016,1,30,30,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a pre-experiment survey, received a tutorial, interacted with a simulated robot in 6 sessions (2 practice, 4 event scenarios), and completed post-session and debriefing questionnaires.","Participants supervised an autonomous robot performing a visual navigation task, tracking terrain boundaries and intervening when necessary.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a simulated robot through a graphical interface, with the ability to intervene.",simulation,The interaction took place in a simulated environment with a graphical interface.,simulated,The robot was represented as a simulated aerial vehicle in a graphical interface.,shared control (fixed rules),The robot operated autonomously but the human could intervene and take control.,Questionnaires; Real-time Trust Measures,NASA Task Load Index (NASA-TLX),"Performance Metrics; robot data (sensor data, etc.)","Trust was measured using questionnaires and real-time assessments, with data collected on robot performance and user interventions.","parametric models (e.g., regression)",A parametric model was used to predict trust changes based on experience-based metrics and user assessments.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's reliability was directly manipulated by switching between high and low reliability modes, and participants were informed of the robot's limitations.","Trust decreased when the robot performed poorly, but users were more lenient when the robot's poor performance was due to known limitations.","Users showed a positive bias in initial trust assessments. Users reacted more leniently when the robot started with poor performance but then improved, compared to when an initially reliable robot unexpectedly failed. The study found that personality-based factors had a stronger influence on trust than actual experiences during interaction.","The progression of human-robot trust was predominantly shaped by each user's personality, in comparison to the influence from the actual experiences in the interaction.","The robot autonomously tracked terrain boundaries, while the human monitored its performance and intervened when necessary by taking manual control. The human's primary task was to supervise the robot and ensure it followed the correct path.",repeated measures analysis of variance (rmanova); t-test; Linear regression,"The study used a repeated measures ANOVA to examine the effect of session order on pre-session trust assessments, finding no significant effect. A Student's t-test was used to determine if the mean pre-session trust assessments were significantly different from 0.5, revealing a positive bias. A backwards stepwise linear regression analysis was conducted to identify significant relationships between various factors (demographics, experience, robot performance, user assessments) and changes in user trust levels, ultimately identifying the most influential factors.",TRUE,Robot-accuracy; Robot-autonomy,Robot-accuracy,Robot-autonomy,"The study manipulated 'Robot-accuracy' by programmatically toggling the robot's boundary tracking algorithm between high and low reliability states, which directly impacted the robot's task performance (e.g., tracking accuracy and internal failure rates). This is described in the 'Event-Centric Perspective' section where the authors detail the 'Baseline', 'PoorStart', 'RobotFault', and 'Limitation' scenarios, each involving different timings of the robot's reliability changes. The study also implicitly manipulated 'Robot-autonomy' by allowing the human to intervene and take manual control of the vehicle, which is described in the 'Interaction Context' section. However, the level of autonomy was not directly manipulated as a variable, but rather was a constant feature of the interaction. The study found that changes in 'Robot-accuracy' significantly impacted trust, as evidenced by the different trust responses to the various event scenarios (e.g., trust decreased when the robot performed poorly). The study did not find that the level of autonomy, which was constant, had an impact on trust, as the manipulation was not a variable in the study.",,http://link.springer.com/10.1007/978-3-319-28872-7_7,"We are interested in enhancing the efﬁciency of human-robot collaborations, especially in “supervisor-worker” settings where autonomous robots work under the supervision of a human operator. We believe that trust serves a critical role in modeling the interactions within these teams, and also in streamlining their efﬁciency. We propose an operational formulation of human-robot trust on a short interaction time scale, which is tailored to a practical tele-robotics setting. We also report on a controlled user study that collected interaction data from participants collaborating with an autonomous robot to perform visual navigation tasks. Our analyses quantify key correlations between real-time human-robot trust assessments and diverse factors, including properties of failure events reﬂecting causal trust attribution, as well as strong inﬂuences from each user’s personality. We further construct and optimize a predictive model of users’ trust responses to discrete events, which provides both insights on this fundamental aspect of real-time human-machine interaction, and also has pragmatic signiﬁcance for designing trust-aware robot agents."
"Xu, Jin; Howard, Ayanna",The Impact of First Impressions on Human- Robot Trust During Problem-Solving Scenarios,2018,1,100,94,6,6 participants (3 in each condition) were excluded as they completed the experiment in both conditions,Online Crowdsourcing,between-subjects,"Participants played a game with a robot, completing a cognitive task across seven trials. They were randomly assigned to either a working or faulty robot condition. The robot provided answers, and participants made initial and final decisions. Trust was assessed via survey responses.",Participants were asked to count the number of toothpicks in an image within a given time frame and then decide whether to follow the robot's answer.,Nao,Humanoid Robots; Expressive Robots,Research; Social,Game,Competitive Game,minimal interaction,"Participants interacted with the robot through a web interface, with no physical touch.",media,Participants viewed a video feed of the robot while completing the task.,physical,The robot was physically present and visible in the video feed.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions and responses.,Behavioral Measures; Questionnaires,,,Trust was assessed using survey questions and by analyzing participants' decisions to follow the robot's advice.,no modeling,Trust was not modeled computationally; descriptive statistics were used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's initial performance was manipulated to create either a positive or negative first impression, influencing participants' expectations and subsequent trust.","A positive first impression led to higher trust and a greater likelihood of following the robot's advice, while a negative first impression resulted in lower trust.","Participants in the faulty robot condition showed an increase in following the robot's advice in later trials, surpassing the working robot condition, which is an unexpected result. This suggests that participants may have re-evaluated their trust in the faulty robot over time.","First impressions significantly impact human-robot trust, with a positive initial interaction leading to higher trust and a greater likelihood of following the robot's advice.","The robot introduced itself, explained the game, and provided answers to the toothpick counting task. The human participant counted toothpicks, entered their answer, and then decided whether to follow the robot's answer.",Chi-squared,"The study used chi-squared tests to analyze the statistical significance of differences in participant behavior between the working and faulty robot conditions. Specifically, chi-squared tests were used to compare the percentage of participants who followed the robot's advice in disagreement situations and to analyze the survey responses regarding whether participants took advice from the robot and whether trust was involved in their decision.",TRUE,Robot-accuracy,Robot-accuracy,,"The study explicitly manipulated the robot's accuracy by having it provide either correct or incorrect answers in the initial trial (Stage 0). This manipulation was designed to create either a positive or negative first impression, directly impacting participants' trust. The paper states, 'Each participant was randomly assigned to one of the following initial conditions: 1) a working robot in which the robot provided a correct answer or 2) a faulty robot in which the robot provided an incorrect answer.' This manipulation of the robot's performance directly relates to the 'Robot-accuracy' category, as it influences the success rate of the robot's task performance. The results section shows that the initial accuracy of the robot significantly impacted the participants' trust and their decision to follow the robot's advice, as evidenced by the differences in agreement rates between the working and faulty robot conditions. The paper states, 'At trial 1, 38.3% of participants in the working robot condition and only 2.21% of participants in the faulty robot condition provided the same initial answer as the robot's (between-group difference = 36.2%).' This demonstrates that the manipulation of robot accuracy had a direct impact on trust. There were no other factors manipulated in the study that would fit into the other categories.",10.1109/ROMAN.2018.8525669,,"With recent advances in robotics, it is expected that robots will become increasingly common in human environments, such as in the home and workplaces. Robots will assist and collaborate with humans on a variety of tasks. During these collaborations, it is inevitable that disagreements in decisions would occur between humans and robots. Among factors that lead to which decision a human should ultimately follow, theirs or the robot, trust is a critical factor to consider. This study aims to investigate individuals' behaviors and aspects of trust in a problem-solving situation in which a decision must be made in a bounded amount of time. A between-subject experiment was conducted with 100 participants. With the assistance of a humanoid robot, participants were requested to tackle a cognitive-based task within a given time frame. Each participant was randomly assigned to one of the following initial conditions: 1) a working robot in which the robot provided a correct answer or 2) a faulty robot in which the robot provided an incorrect answer. Impacts of the faulty robot behavior on participant's decision to follow the robot's suggested answer were analyzed. Survey responses about trust were collected after interacting with the robot. Results indicated that the first impression has a significant impact on participant's behavior of trusting a robot's advice during a disagreement. In addition, this study discovered evidence supporting that individuals still have trust in a malfunctioning robot even after they have observed a robot's faulty behavior."
"Xu, Jin; Bryant, De'Aira G.; Howard, Ayanna",Would You Trust a Robot Therapist? Validating the Equivalency of Trust in Human-Robot Healthcare Scenarios,2018,1,20,20,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were randomly assigned to interact with either a robot or a human therapist during a therapy game involving reaching tasks. The experiment had three phases: baseline, acquisition (with feedback), and extinction (no feedback). Participants completed a survey after the experiment.","Participants played a therapy game involving reaching tasks, receiving corrective feedback from either a robot or a human therapist.",Nao,Humanoid Robots; Expressive Robots,Care; Research; Social,Social,Social Guidance/Coaching,minimal interaction,Participants interacted with the robot through verbal instructions and corrective feedback during a therapy game.,real-world,Participants interacted with a physical robot in a controlled lab setting.,physical,Participants interacted with a physical NAO robot.,shared control (fixed rules),The robot provided corrective feedback based on pre-defined rules related to the user's performance.,Questionnaires,Jian et al. Trust Scale,Performance Metrics,Trust was measured using a modified version of the Jian et al. Trust Scale and additional survey questions.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The study directly manipulated the therapist condition (robot vs. human) to influence trust by changing the source of feedback and guidance.,Participants in the robot therapist condition reported a higher level of trust and were more likely to have trust involved in their decision compared to the human therapist condition. The therapist condition had a medium-sized effect on trust.,Participants in the robot condition were 3.5 times more likely to have trust involved in their decision than those in the human condition. There was a trend that the agent condition has a medium-sized effect on trust.,Participants in the robot therapist condition reported an overall higher level of trust and were more likely to have trust involved in their decision than participants in the human therapist condition.,The robot provided verbal and non-verbal corrective feedback to participants during a reaching task in a therapy game. The human participant followed the instructions and attempted to complete the reaching task.,Wilcoxon rank sum; pearson's chi-squared test,The study used a Wilcoxon rank sum test to compare the survey responses on a 7-point Likert scale between the robot and human therapist groups. The purpose was to determine if there were statistically significant differences in trust ratings between the two groups. A Pearson's Chi-squared test was also used to examine the relationship between the therapist condition (robot vs. human) and whether trust was involved in the participant's decision to follow guidance. This test aimed to determine if the therapist condition had a significant effect on trust.,TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the source of feedback by having either a robot or a human provide corrective feedback to participants during a therapy game. This manipulation directly influenced the content of the verbal communication, as the robot and human provided the same feedback but from different sources. The paper states, 'As a key component for comparable interaction, during the robot condition, NAO was programmed to perform the exact same verbal and non-verbal behaviors as the physical therapist.' This indicates that the content of the verbal communication was the key manipulated factor, not the style or timing. The results showed that participants in the robot condition reported a higher level of trust and were more likely to have trust involved in their decision, indicating that the source of the feedback (robot vs. human) and thus the content of the communication, impacted trust. The study did not manipulate any other factors from the provided list.",10.1109/ROMAN.2018.8525782,https://ieeexplore.ieee.org/document/8525782/,"With the recent advances in computing, artificial intelligence (AI) is quickly becoming a key component in the future of advanced applications. In one application in particular, AI has played a major role – that of revolutionizing traditional healthcare assistance. Using embodied interactive agents, or interactive robots, in healthcare scenarios has emerged as an innovative way to interact with patients. As an essential factor for interpersonal interaction, trust plays a crucial role in establishing and maintaining a patient-agent relationship. In this paper, we discuss a study related to healthcare in which we examine aspects of trust between humans and interactive robots during a therapy intervention in which the agent provides corrective feedback. A total of twenty participants were randomly assigned to receive corrective feedback from either a robotic agent or a human agent. Survey results indicate trust in a therapy intervention coupled with a robotic agent is comparable to that of trust in an intervention coupled with a human agent. Results also show a trend that the agent condition has a medium-sized effect on trust. In addition, we found that participants in the robot therapist condition are 3.5 times likely to have trust involved in their decision than the participants in the human therapist condition. These results indicate that the deployment of interactive robot agents in healthcare scenarios has the potential to maintain quality of health for future generations."
"Xu, Kun",First encounter with robot Alpha: How individual differences interact with vocal and kinetic cues in users’ social responses,2019,1,110,110,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were randomly assigned to one of four conditions: human voice with gestural movements, synthetic voice with gestural movements, human voice with non-gestural movements, or synthetic voice with non-gestural movements. Participants were told that the robot would give a 2-minute self-introduction. Participants then completed a questionnaire.",Participants observed a robot giving a self-introduction.,Alpha,Humanoid Robots,Research; Social,Social,Social Perception,minimal interaction,Participants observed the robot and were not allowed to touch it.,real-world,Participants interacted with a physical robot in a lab setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed script for its self-introduction.,Questionnaires,,,Trust was measured using a questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,The robot's voice (human vs. synthetic) and movements (gestural vs. non-gestural) were manipulated to influence trust.,The robot with a human voice evoked greater perceived trustworthiness. Gestural movements increased perceived attraction and intention of future use.,"The study found that robot use experiences moderated the relationship between vocal cues and social responses, with experienced users showing a preference for synthetic voices. Gender also interacted with kinetic cues in predicting intention of future use. Some participants noted that the robot's mechanical noises made it seem less person-like.","A social robot with a human voice was perceived as more trustworthy than one with a synthetic voice, and gestural movements increased users' attachment to the robot and their intention of future use.",The robot gave a two-minute self-introduction using either a human or synthetic voice and either gestural or non-gestural movements. The human participant observed the robot and then completed a questionnaire.,ANOVA; Moderation analysis,"The study used two-way ANOVAs for manipulation checks, and three-way ANOVAs to test the main and interaction effects of vocal cues, kinetic cues, gender, and robot use experiences on social responses (medium-as-social-actor presence, perceived attraction, perceived trustworthiness, and intention of future use). Moderation analyses using the Process macro were conducted to examine the interaction between attitudes toward robots and social cues on social responses. The Johnson-Neyman technique was used in the moderation analyses to identify specific regions of significance.",TRUE,Robot-verbal-communication-style; Robot-nonverbal-communication,Robot-verbal-communication-style,Robot-nonverbal-communication,"The study manipulated the robot's voice (human vs. synthetic), which is a change in how the content is communicated, thus categorized as 'Robot-verbal-communication-style'. The study also manipulated the robot's movements (gestural vs. non-gestural), which are physical movements, thus categorized as 'Robot-nonverbal-communication'. The results showed that the human voice increased perceived trustworthiness, indicating that 'Robot-verbal-communication-style' impacted trust. However, the type of movement (gestural vs. non-gestural) did not directly impact trust, so 'Robot-nonverbal-communication' is listed as a factor that did not impact trust. Although gestural movements did increase perceived attraction and intention of future use, these are not direct measures of trust.",10.1177/1461444819851479,http://journals.sagepub.com/doi/10.1177/1461444819851479,"The Computers are Social Actors (CASA) paradigm was proposed more than two decades ago to understand humans’ interaction with computer technologies. Today, as emerging technologies like social robots become more personal and persuasive, questions of how users respond to them socially, what individual factors leverage the relationship, and what constitutes the social influence of these technologies need to be addressed. A lab experiment was conducted to examine the interactions between individual differences and social robots’ vocal and kinetic cues. Results suggested that users developed more trust in a social robot with a human voice than with a synthetic voice. Users also developed more intimacy and interest in the social robot when it was paired with humanlike gestures. Moreover, individual differences including users’ gender, attitudes toward robots, and robot exposure affected their psychological responses. The theoretical, practical, and ethical value of the findings was further discussed in the study."
"Xu, Jin; Howard, Ayanna",Would you Take Advice from a Robot? Developing a Framework for Inferring Human-Robot Trust in Time-Sensitive Scenarios,2020,2,47,47,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants completed a counting task, received advice from a robot, and made a final decision. The robot provided correct answers for the first four trials and incorrect answers for the last three trials.",Participants were asked to count the number of toothpicks strewn on a flat surface in a short amount of time.,Unspecified,Humanoid Robots,Social; Research,Social,Social Guidance/Coaching,minimal interaction,"Participants interacted with the robot through a screen, receiving advice.",simulation,The interaction was conducted in a simulated environment.,simulated,The robot was presented as a simulated agent.,pre-programmed (non-adaptive),The robot provided pre-programmed advice without adapting to the user.,Behavioral Measures,,Performance Metrics,Trust was assessed based on the participants' decisions to accept or reject the robot's advice.,hidden markov model,A Hidden Markov Model was used to model the dynamics of trust based on the participants' decisions.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's performance was manipulated by providing correct answers for the first four trials and incorrect answers for the last three trials, which was intended to influence trust.",Trust decreased when the robot started providing incorrect answers.,"The prediction accuracy of the model decreased after the robot made a mistake, and then increased again in the last trial.","The proposed HMM-based model can predict human decisions to take advice from a robot, but its performance degrades after the robot makes a mistake.","The robot provided advice on the number of toothpicks, and the human decided whether to accept or reject the advice.",hidden markov models; Logistic regression,The study used Hidden Markov Models (HMM) to model the dynamics of trust based on participants' decisions to accept or reject robot advice. The HMM was trained to predict the user's final decision at each trial based on prior interactions. Logistic Regression (LR) was also used as a comparison model to predict the user's final decision. The performance of both models was evaluated using prediction accuracy.,TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the robot's accuracy by having it provide correct answers for the first four trials and incorrect answers for the last three trials. This manipulation was explicitly designed to observe its impact on trust, as stated in the paper: 'As the baseline condition at time = 1, the robot provided a correct answer to the participant. Then the robot provided correct answers in the next 3 trials followed by incorrect answers for the last 3 trials.' The paper also notes that 'Accuracy decreased in the 5th trial, which represents the first time the robot provided an incorrect answer to the user (i.e. made a mistake).' This clearly indicates that the change in robot accuracy was the manipulated factor and that it impacted trust.",10.1109/RO-MAN47096.2020.9223544,https://ieeexplore.ieee.org/document/9223544/,"Trust is a key element for successful human-robot interaction. One challenging problem in this domain is the issue of how to construct a formulation that optimally models this trust phenomenon. This paper presents a framework for modeling human-robot trust based on representing the human decision-making process as a formulation based on trust states. Using this formulation, we then discuss a generalized model of human-robot trust based on Hidden Markov Models and Logistic Regression. The proposed approach is validated on datasets collected from two different human subject studies in which the human is provided the ability to take advice from a robot. Both experimental scenarios were time-sensitive, in that a decision had to be made by the human in a limited time period, but each scenario featured different levels of cognitive load. The experimental results demonstrate that the proposed formulation can be utilized to model trust, in which the system can predict whether the human will decide to take advice (or not) from the robot. It was found that our prediction performance degrades after the robot made a mistake. The validation of this approach on two scenarios implies that this model can be applied to other interactive scenarios as long as the interaction dynamics fits into the proposed formulation. Directions for future improvements are discussed."
"Xu, Jin; Howard, Ayanna",Would you Take Advice from a Robot? Developing a Framework for Inferring Human-Robot Trust in Time-Sensitive Scenarios,2020,2,42,42,0,No participants were excluded,Online Crowdsourcing,within-subjects,"Participants completed a simulated driving task, received time estimates from a robot, and made a final decision on driving mode. The robot provided correct estimates for the first four trials and incorrect estimates for the last three trials.",Participants were asked to drive through a simulated college campus and drop off a passenger as quickly as possible.,Unspecified,Autonomous Vehicles,Social; Research,Social,Social Guidance/Coaching,minimal interaction,"Participants interacted with the robot through a screen, receiving advice.",simulation,The interaction was conducted in a simulated driving environment.,simulated,The robot was presented as a simulated agent providing time estimates.,pre-programmed (non-adaptive),The robot provided pre-programmed time estimates without adapting to the user.,Behavioral Measures,,Performance Metrics,Trust was assessed based on the participants' decisions to use the self-driving mode or manual mode.,hidden markov model,A Hidden Markov Model was used to model the dynamics of trust based on the participants' decisions.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's performance was manipulated by providing correct time estimates for the first four trials and incorrect estimates for the last three trials, which was intended to influence trust.",Trust decreased when the robot started providing incorrect estimates.,"The prediction accuracy of the model decreased after the robot made a mistake, and then increased again in the last trial.","The proposed HMM-based model can predict human decisions to take advice from a robot, but its performance degrades after the robot makes a mistake.","The robot provided time estimates for the self-driving mode, and the human decided whether to use the self-driving mode or manual mode.",hidden markov models; Logistic regression,The study used Hidden Markov Models (HMM) to model the dynamics of trust based on participants' decisions to use self-driving or manual mode. The HMM was trained to predict the user's final decision at each trial based on prior interactions. Logistic Regression (LR) was also used as a comparison model to predict the user's final decision. The performance of both models was evaluated using prediction accuracy.,TRUE,Robot-accuracy,Robot-accuracy,,"Similar to the first study, the robot's accuracy was manipulated in the self-driving experiment. The robot provided correct time estimates for the first four trials and incorrect estimates for the last three trials. The paper states: 'At the baseline condition at time = 1, the robot agent provided a correct estimation of time to completion. Then the robot agent provided a correct estimation in the next 3 trials followed by an incorrect estimate due to malfunction in the last 3 trials.' The paper also notes that 'the accuracy decreases at trial 5 and 6 on both datasets' which indicates that the change in robot accuracy was the manipulated factor and that it impacted trust.",10.1109/RO-MAN47096.2020.9223544,https://ieeexplore.ieee.org/document/9223544/,"Trust is a key element for successful human-robot interaction. One challenging problem in this domain is the issue of how to construct a formulation that optimally models this trust phenomenon. This paper presents a framework for modeling human-robot trust based on representing the human decision-making process as a formulation based on trust states. Using this formulation, we then discuss a generalized model of human-robot trust based on Hidden Markov Models and Logistic Regression. The proposed approach is validated on datasets collected from two different human subject studies in which the human is provided the ability to take advice from a robot. Both experimental scenarios were time-sensitive, in that a decision had to be made by the human in a limited time period, but each scenario featured different levels of cognitive load. The experimental results demonstrate that the proposed formulation can be utilized to model trust, in which the system can predict whether the human will decide to take advice (or not) from the robot. It was found that our prediction performance degrades after the robot made a mistake. The validation of this approach on two scenarios implies that this model can be applied to other interactive scenarios as long as the interaction dynamics fits into the proposed formulation. Directions for future improvements are discussed."
"Xu, Tao; Dragomir, Andrei; Liu, Xucheng; Yin, Haojun; Wan, Feng; Bezerianos, Anastasios; Wang, Hongtao",An EEG study of human trust in autonomous vehicles based on graphic theoretical analysis,2022,1,50,50,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a simulated driving task in three modes: manual, semi-autonomous, and fully autonomous. In the semi-autonomous and fully autonomous modes, the car could malfunction. EEG data was collected during the driving simulation. Participants filled out trust questionnaires before and after the experiment.","Participants drove a simulated car in different automation modes, responding to traffic light and car approaching scenarios, with potential car malfunctions.",Unspecified,Autonomous Vehicles,Research,Navigation,Path Following,minimal interaction,"Participants interacted with a driving simulator, with limited physical interaction.",simulation,The study used a driving simulator to create an immersive environment.,simulated,The robot was a simulated autonomous vehicle within the driving simulator.,shared control (fixed rules),The vehicle operated autonomously but could be taken over by the participant in semi-autonomous mode.,Questionnaires; Behavioral Measures; Physiological Measures,Jian et al. Trust Scale,Physiological Signals,"Trust was assessed using questionnaires, behavioral measures (takeover decisions), and physiological measures (EEG).",no modeling,"The study did not use computational modeling of trust, focusing on statistical analysis of EEG data.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the driving mode (manual, semi-autonomous, fully autonomous) and introduced car malfunctions to influence trust.","Participants showed lower trust after car malfunctions, especially in fully autonomous mode, and were more willing to take over control in semi-autonomous mode.","The study found that the brain processes malfunctions differently in car approaching and traffic light conditions, suggesting different levels of trust in these scenarios. The brain showed higher local efficiency for car approaching malfunctions and lower global efficiency for traffic light malfunctions.","The study found that human brain activity, measured by EEG, shows different patterns when facing malfunctions in autonomous vehicles depending on the driving scenario (car approaching vs. traffic light), suggesting varying levels of trust and information processing ability.","The robot (simulated autonomous vehicle) drove along a path, and the human participant monitored the driving and took over control when necessary in semi-autonomous mode. The human also responded to traffic light and car approaching scenarios.",ANOVA,"The study used ANOVA to compare the graph theoretical properties (local efficiency, clustering coefficient, characteristic path length, and global efficiency) between normal and malfunction trials in both semi-autonomous and fully autonomous driving modes, for both car approaching and traffic light conditions. The ANOVA tests were used to determine if there were statistically significant differences in these measures based on the driving mode and car condition.",TRUE,Robot-autonomy; Robot-accuracy; Task-environment,Robot-accuracy; Robot-autonomy,,"The study manipulated 'Robot-autonomy' by having three driving modes: manual (SAE Level 0), semi-autonomous (SAE Level 4), and fully autonomous (SAE Level 5). This is explicitly stated in the 'Experimental protocol' section: 'After the practice stage, it comes to the driving simulation stage, which consists of three modes: manual driving mode (SAE Level 0), semi-autonomous driving mode (SAE Level 4), and fully autonomous driving mode (SAE Level 5)'. The study also manipulated 'Robot-accuracy' by introducing car malfunctions in the semi-autonomous and fully autonomous modes, as described in the 'Experimental protocol' section: 'However, in a trial of the autonomous phase, the car condition may turn from normal to malfunction without advance notice. The malfunction is defined as the car running at the stoplight or the car still running while another car approaches the intersection.' The 'Task-environment' was manipulated by having two different scenarios: traffic light (TL) and car approaching (CA) conditions, as described in the 'Experimental protocol' section: 'There are two basic traffic scenarios that the driver should handle. One scenario is the traffic light (TL) in which the participant should stop the car before a junction while seeing the traffic light is red. The other scenario is a car approaching (CA) in which the participant is asked to stop the car before a junction without any traffic light to avoid collision with other cars.' The results section shows that 'participants showed lower trust after car malfunction than in car normal conditions (p < 0.001)' and 'participants in the semi-autonomous driving mode have a stronger willingness to take over the control of the vehicle during the malfunction trials (p < 0.001)', indicating that both 'Robot-accuracy' and 'Robot-autonomy' impacted trust. The study did not explicitly state any factors that did not impact trust.",10.3389/fninf.2022.907942,https://www.frontiersin.org/articles/10.3389/fninf.2022.907942/full,"With the development of autonomous vehicle technology, human-centered transport research will likely shift to the interaction between humans and vehicles. This study focuses on the human trust variation in autonomous vehicles (AVs) as the technology becomes increasingly intelligent. This study uses electroencephalogram data to analyze human trust in AVs during simulated driving conditions. Two driving conditions, the semi-autonomous and the autonomous, which correspond to the two highest levels of automatic driving, are used for the simulation, accompanied by various driving and car conditions. The graph theoretical analysis (GTA) is the primary method for data analysis. In semi-autonomous driving mode, the local efficiency and cluster coefficient are lower in car-normal conditions than in car-malfunction conditions with the car approaching. This finding suggests that the human brain has a strong information processing ability while facing predictable potential hazards. However, when it comes to a traffic light with a car malfunctioning under the semi-autonomous driving mode, the characteristic path length is higher for the car malfunction manifesting a weak information processing ability while facing unpredictable potential hazards. Furthermore, in fully automatic driving conditions, participants cannot do anything and need low-level brain function to take emergency actions as lower local efficiency and small worldness for car malfunction. Our results shed light on the design of the human-machine interaction and human factor engineering on the high level of an autonomous vehicle."
"Xu, Jin; Howard, Ayanna",Evaluating the Impact of Emotional Apology on Human-Robot Trust,2022,2,48,48,0,No participants were excluded,Online Crowdsourcing,,"Participants were presented with a scenario involving a self-driving AI agent making a mistake, then they were asked to compare three emotional apology candidates against a baseline apology and each other, and to identify the most prominent emotion in each apology.",Participants completed a pairwise comparison task of different apology statements and identified the most prominent emotion in each statement.,Unspecified,Autonomous Vehicles,,Evaluation,Ranking,passive observation,Participants only read text descriptions of the robot and its actions.,media,The interaction was based solely on text descriptions.,hypothetical,"The robot was only described in text, with no visual representation.",not autonomous,"The robot's actions were described in text, but no actual robot was present.",,,,"Trust was not directly measured, but rather inferred from the selection of the most emotional apology.",no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the wording of the apology statements to include emotional content, aiming to influence the perceived sincerity of the apology.",The emotional apologies were rated as more emotional than the baseline apology.,"The study found that emotional apologies were perceived as more emotional than baseline apologies, and that the most effective emotional apology was selected based on gender differences.",The study identified an emotional apology statement that was perceived as more emotional than a baseline apology.,The human participant read text descriptions of a robot making a mistake and then selected the most emotional apology from a set of options.,one proportion z-test,The study used a one proportion z-test to evaluate whether the results of each pairwise comparison of apology statements were statistically significant. The Bonferroni correction was applied to account for multiple comparisons. The z-test was also used to analyze the breakdown of selections based on participants' gender for two apology candidates.,TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the content of the robot's verbal communication by varying the wording of the apology statements to include emotional content. The goal was to see if the emotional content would influence the perceived sincerity of the apology and thus impact trust. The results showed that the emotional apologies were rated as more emotional than the baseline apology, indicating that the manipulation of the verbal communication content impacted the perception of the apology and thus trust. The study did not manipulate any other factors from the list provided.",10.1109/RO-MAN53752.2022.9900518,https://ieeexplore.ieee.org/document/9900518/,"Previous research has shown that robot mistakes or malfunctions have a significant negative impact on people’s trust. One way to mitigate the negative impact of trust violation is through trust repair. Although trust repair has been studied extensively, it is still not known which strategy is effective in repairing trust in a time-sensitive driving scenario. Additionally, prior research on trust repair has not dealt with the effects of expressing emotion in attempting trust repair. In this paper, we presented the development of a variety of trust repair methods for a time-sensitive scenario using a simulated driving environment as a testbed for validation. These trust repair methods included baseline apology, emotional apology, and explanation. We conducted an experiment to compare the impact of these trust repair methods on human-robot trust. Experimental results indicated that the emotional apology positively affected more participants than the no-repair, baseline apology, and explanation. Furthermore, this study identified emotional apology as the most effective method for the time-sensitive driving scenario."
"Xu, Jin; Howard, Ayanna",Evaluating the Impact of Emotional Apology on Human-Robot Trust,2022,2,128,128,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants completed a simulated driving task with a robot, where the robot provided advice on using self-driving mode. The robot made mistakes in trials 2-4, and then attempted trust repair in trial 5 using one of four conditions: no repair, baseline apology, emotional apology, or explanation.",Participants completed a simulated driving task and decided whether to follow the robot's advice on using self-driving mode.,Unspecified,Autonomous Vehicles,Research,Navigation,Guiding,minimal interaction,Participants interacted with a simulated robot in a driving environment.,simulation,The interaction took place in a simulated driving environment.,simulated,The robot was represented as a self-driving AI agent in the simulation.,shared control (fixed rules),"The robot provided advice on self-driving mode, but the participant made the final decision.",Behavioral Measures; Questionnaires,Jian et al. Trust Scale,,Trust was measured using behavioral decisions and a subjective trust survey.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers manipulated the robot's performance by having it make mistakes and then manipulated the robot's behavior by providing different trust repair messages.,"Emotional apology had a more positive impact on trust than no repair, baseline apology, or explanation, although the difference was not statistically significant.","Although the overall impact of trust repair was not statistically significant, the emotional apology had a more positive impact on more participants than the other conditions, and participants reported more positive attitudes towards the robot after receiving the emotional apology.",Emotional apology was identified as the most effective method for trust repair in a time-sensitive simulated driving scenario.,"The robot provided advice on using self-driving mode, and the human participant decided whether to follow the robot's advice in a simulated driving task.",Chi-squared; Chi-squared,"The study used a Chi-squared test to compare the impact of trust repair across different conditions (no-repair, baseline apology, emotional apology, and explanation). The first Chi-squared test used a 4x3 contingency table to compare the impact of trust repair in each condition, considering positive, negative, and unknown impacts. A second Chi-squared test was performed as a post-hoc analysis, using a 4x2 contingency table, comparing only positive and negative impacts after removing the neutral responses.",TRUE,Robot-accuracy; Robot-verbal-communication-content,Robot-verbal-communication-content,Robot-accuracy,"In this study, the researchers manipulated the robot's accuracy by having it make mistakes in trials 2-4. This was done to create a trust violation scenario. They also manipulated the robot's verbal communication content by providing different trust repair messages (no repair, baseline apology, emotional apology, or explanation) in trial 5. The results showed that while the emotional apology had a more positive impact on more participants than the other conditions, the overall impact of trust repair was not statistically significant. The manipulation of the robot's accuracy (making mistakes) was a constant across all conditions and did not impact trust differently between conditions. The manipulation of the verbal communication content (different apology types) did impact trust, as the emotional apology was more effective than the other conditions, although not statistically significant.",10.1109/RO-MAN53752.2022.9900518,https://ieeexplore.ieee.org/document/9900518/,"Previous research has shown that robot mistakes or malfunctions have a significant negative impact on people’s trust. One way to mitigate the negative impact of trust violation is through trust repair. Although trust repair has been studied extensively, it is still not known which strategy is effective in repairing trust in a time-sensitive driving scenario. Additionally, prior research on trust repair has not dealt with the effects of expressing emotion in attempting trust repair. In this paper, we presented the development of a variety of trust repair methods for a time-sensitive scenario using a simulated driving environment as a testbed for validation. These trust repair methods included baseline apology, emotional apology, and explanation. We conducted an experiment to compare the impact of these trust repair methods on human-robot trust. Experimental results indicated that the emotional apology positively affected more participants than the no-repair, baseline apology, and explanation. Furthermore, this study identified emotional apology as the most effective method for the time-sensitive driving scenario."
"Xu, Caiyue; Zhang, Changming; Zhou, Yanmin; Wang, Zhipeng; Lu, Ping; He, Bin",Trust Recognition in Human-Robot Cooperation Using EEG,2024,1,16,16,0,No participants were excluded,Controlled Lab Environment,within-subjects,Participants played the Overcooked-AI game with different robot ability levels and task complexities. EEG data and subjective trust ratings were collected after each trial.,"Participants played the Overcooked-AI game with a robot partner, aiming to cook and serve onion soup.",Unspecified,Other,Research,Game,Cooperative Game,minimal interaction,"Participants interacted with a robot in a game setting, but there was no physical touch.",simulation,The interaction took place in a simulated game environment.,simulated,The robot was a simulated agent within the game environment.,fully autonomous (limited adaptation),"The robot was pre-trained with reinforcement learning and operated autonomously, but with limited adaptation.",Questionnaires; Physiological Measures,,Physiological Signals,Trust was measured using a questionnaire and EEG data.,"deep learning (e.g., neural networks, reinforcement learning)",A Vision Transformer model was used to model trust based on EEG data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's ability level and the task complexity were manipulated to influence trust.,Higher robot ability and simpler tasks led to higher trust levels.,"Trust states had greater variance than distrust states, which is explained by the fact that humans tend to have less doubt when not trusting something or someone, but more doubt when choosing to trust. The deep learning models showed a larger performance decrease in trial-wise cross-validation compared to traditional methods.","The study demonstrated successful trust recognition in human-robot cooperation using EEG data and a Vision Transformer model, achieving 74.99% accuracy in slice-wise cross-validation and 62.00% in trial-wise cross-validation.","The robot, an autonomous agent, and the human participant collaborated in the Overcooked-AI game to cook and serve onion soup. The human and robot had to coordinate their actions to complete the task.",,"The paper primarily focuses on the development and evaluation of a deep learning model (Vision Transformer) for trust recognition using EEG data. While the paper mentions that there are significant statistical differences in trust ratings among different conditions, indicating successful elicitation of desired trust states, it does not explicitly mention the use of specific statistical tests like t-tests, ANOVA, or regression analysis. The analysis is primarily based on descriptive statistics (mean accuracy, F1 score) and model performance metrics (ROC, AUC) to evaluate the model's effectiveness. Therefore, no specific statistical tests were identified.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy; Task-complexity,,"The study explicitly manipulated the robot's ability level (high, medium, low) which directly impacts the robot's accuracy in performing the task within the Overcooked-AI game. This is classified as 'Robot-accuracy' because it directly influences the robot's success in the game. The study also manipulated the complexity of the game layouts (5 different layouts with varying difficulty), which is classified as 'Task-complexity'. The paper states, 'Previous studies showed that human trust in a robot is greatly influenced by the robot's ability and the complexity of the task [27]. In this experiment, we used these two factors as independent variables to stimulate different levels of trust.' The results section also confirms that 'For each layout, the trust levels vary depending on the robot's ability level. HAR generally has higher trust levels, while LAR has the lowest levels. MAR falls in between. Regarding task complexity, simpler tasks in Layouts 1, 2, and 3 result in higher overall trust scores and clear trust distinctions. However, in Layouts 4 and 5, where the scenarios were more complex, pre-trained robots struggled with the tasks, leading to lower overall trust scores.' This indicates that both 'Robot-accuracy' and 'Task-complexity' impacted trust levels.",,http://arxiv.org/abs/2403.05225,"Collaboration between humans and robots is becoming increasingly crucial in our daily life. In order to accomplish efficient cooperation, trust recognition is vital, empowering robots to predict human behaviors and make trust-aware decisions. Consequently, there is an urgent need for a generalized approach to recognize human-robot trust. This study addresses this need by introducing an EEG-based method for trust recognition during human-robot cooperation. A human-robot cooperation game scenario is used to stimulate various human trust levels when working with robots. To enhance recognition performance, the study proposes an EEG Vision Transformer model coupled with a 3-D spatial representation to capture the spatial information of EEG, taking into account the topological relationship among electrodes. To validate this approach, a public EEG-based human trust dataset called EEGTrust is constructed. Experimental results indicate the effectiveness of the proposed approach, achieving an accuracy of 74.99% in slice-wise cross-validation and 62.00% in trial-wise cross-validation. This outperforms baseline models in both recognition accuracy and generalization. Furthermore, an ablation study demonstrates a significant improvement in trust recognition performance of the spatial representation. The source code and EEGTrust dataset are available at https://github.com/CaiyueXu/EEGTrust."
"Yadav, Aakash; Mehta, Ranjana",Beyond Dyadic Interactions: Assessing Trust Networks in Multi-Human-Robot Teams,2024,1,46,46,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a virtual SAR task in three conditions: all-human team (HHH), reliable human-robot team (HRR), and unreliable human-robot team (HRU). The navigator role was either a human confederate or a robot. Participants completed a familiarization task before the experimental trials. The robot's behavior was controlled using the Wizard of Oz method. The HRU condition always came last.","Participants worked in teams of three to locate victims in a virtual burning building. The team roles were mission specialist, safety officer, and navigator. The navigator was either a human or a robot.",Unspecified,Mobile Robots,Research,Navigation,Guiding,minimal interaction,Participants interacted with the robot in a virtual environment with verbal instructions.,simulation,The interaction took place in a virtual environment simulating a search and rescue scenario.,simulated,The robot was represented as a virtual agent in the simulation.,wizard of oz (directly controlled),The robot's actions were directly controlled by a human operator using the Wizard of Oz method.,Questionnaires; Custom Scales,NASA Task Load Index (NASA-TLX); Situational Awareness Rating Technique (SART),Performance Metrics,"Trust was measured using questionnaires and custom scales, and performance metrics were collected.",no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The reliability of the robot navigator was manipulated by having it perform reliably in one condition and unreliably in another. The robot's autonomy level was also manipulated by having it be controlled by a human confederate in the HHH condition and by a Wizard of Oz operator in the HRR and HRU conditions.,Trust in the robot navigator was higher when it was reliable compared to when it was unreliable. Team trust decreased when the robot was unreliable. Trust between human dyads was not affected by the robot's actions.,"The team performed better (located more victims) with an unreliable robot, which was unexpected. This was associated with higher perceived fatigue. The team also traveled less with a reliable robot navigator than with a human navigator.","People trust humans more than robots, even when both provide the same information. Team trust decreases when the robot's performance declines.","The robot, acting as a navigator, provided directional guidance to the human team members. The human team members, a mission specialist and a safety officer, coordinated to locate victims in a virtual environment, with the mission specialist logging the findings and the safety officer monitoring air quality.",Shapiro-Wilk; t-test; Wilcoxon signed-rank test,"The data were first tested for normality using the Shapiro-Wilk test. Upon confirming normality, two a priori paired t-tests were performed to compare HHH-HRR and HRR-HRU conditions on various study measures. A non-parametric Wilcoxon signed-rank test was employed if the data was not normally distributed. The value for statistical significance was set to 0.05.",TRUE,Robot-accuracy; Robot-autonomy,Robot-accuracy,Robot-autonomy,"The study manipulated the robot's reliability (accuracy) by having it perform reliably in one condition (HRR) and unreliably in another (HRU). This is explicitly stated in the paper: 'The robot functioned normally during the fve trials in the HRR condition. The robot had two failures within the fve trials in the HRU condition.' This directly impacts the robot's success rate on the task, thus it is classified as 'Robot-accuracy'. The robot's autonomy was also manipulated by having it be controlled by a human confederate in the HHH condition and by a Wizard of Oz operator in the HRR and HRU conditions. This is described in the paper: 'Using the Wizard of Oz method [3], the robot followed the exact behavior of 3 /navigator.' and 'The confederate ( 3 ) was introduced as a normal participant and was only present during familiarization and HHH conditions. After 3 left the site, the human dyad was introduced to the robot functions and communication.' This is classified as 'Robot-autonomy' because it changes the decision authority of the navigator. The results show that trust in the navigator was higher when it was reliable compared to when it was unreliable, indicating that 'Robot-accuracy' impacted trust. The study did not find any significant impact of the change in autonomy on trust, as the trust in the human navigator was compared to the reliable robot navigator, but the change in autonomy was not directly tested for its impact on trust. Therefore, 'Robot-autonomy' is classified as not impacting trust.",10.1145/3610978.3640576,https://dl.acm.org/doi/10.1145/3610978.3640576,"Many HRI applications (such as in search and rescue; SAR) require multiple humans to interact with robot agents, making it essential to understand and evaluate both the trust in robots and trust in teams when robots are embedded into such team structures. In the present study, we utilized a virtual urban search and rescue task to compare individual and team trust and associated team performances between (1) all-human and multi-human-robot teams (mHRTs) with reliable robot behavior, and (2) mHRTs with reliable and unreliable robot behaviors. The team structure included a mission specialist (human), a navigator (human or robot), and a safety ofcer (human). Utilizing apriori pair-wise comparisons, we found that the human navigator was trusted more than the reliable robot navigator by other teammates and that the trust in the robot navigator declined when it performed unreliably. Interestingly, team trust remained comparable between all humans and mHRT (under reliable conditions), but the mHRT team trust levels declined under unreliable robot conditions. Trust between the human dyads was not afected by the actions of the third agent (whether human or robot). Finally, while introducing a reliable robot teammate did not improve team performance, robot unreliability signifcantly improved performance on the SAR task. The study captures changes in trust networks between human teammates by introducing robots with varying performances."
"Yagoda, Rosemarie E.; Gillan, Douglas J.",You Want Me to Trust a ROBOT? The Development of a Human–Robot Interaction Trust Scale,2012,2,20,11,9,9 SMEs did not participate in the study,Online Crowdsourcing,,HRI SMEs completed an online questionnaire to assess HRI item dimensions.,SMEs rated the importance of HRI item dimensions and provided feedback on item labels.,Unspecified,Other,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the HRI item dimensions.,media,The interaction was based on text descriptions of HRI item dimensions.,hypothetical,The robots were only described hypothetically.,not autonomous,The robot's actions were not part of the study.,,,,Trust was not directly measured in this study.,no modeling,No trust modeling was performed.,Observational & Survey Studies,Expert Consultations,No Manipulation,No factors were manipulated in this study.,,"SMEs did not classify any items to be removed from the initial list, only more to be added.",This study identified key dimensions associated with operating robots through expert feedback.,The SMEs reviewed and rated the HRI item dimensions.,Pearson correlation; Content Validity Ratio,"This study used correlations to examine the relationships between different item dimensions within each HRI attribute. Specifically, Pearson correlations were calculated to assess the association between items like teammate and supervisor, and between items within team process, context, task, and system attributes. Additionally, a modified Content Validity Ratio (CVR) was used to assess the content validity of each item dimension. The CVR was calculated based on SME ratings of item essentiality, with a criterion level of .59 to determine which items met the validity threshold.",FALSE,,,,"This study did not manipulate any factors. The study was an expert consultation where SMEs rated the importance of HRI item dimensions and provided feedback on item labels. There was no manipulation of any kind, and therefore no factors were manipulated.",10.1007/s12369-012-0144-0,http://link.springer.com/10.1007/s12369-012-0144-0,"Trust plays a critical role when operating a robotic system in terms of both acceptance and usage. Considering trust is a multidimensional context dependent construct, the differences and common themes were examined to identify critical considerations within human–robot interaction (HRI). In order to examine the role of trust within HRI, a measurement tool was generated based on ﬁve attributes: team conﬁguration, team processes, context, task, and system (Yagoda in Human Factors and Ergonomics Society Annual Meeting, San Francisco, CA, pp. 304–308, 2010). The HRI trust scale was developed based on two studies. The ﬁrst study conducts a content validity assessment of preliminary items generated, based on a review of previous research within HRI and automation, using subject matter experts (SMEs). The second study assesses the quality of each trust scale item derived from the ﬁrst study. The results were then compiled to generate the HRI trust measurement tool."
"Yagoda, Rosemarie E.; Gillan, Douglas J.",You Want Me to Trust a ROBOT? The Development of a Human–Robot Interaction Trust Scale,2012,2,100,100,0,No participants were excluded,Online Crowdsourcing,,Participants viewed a video of a human-robot team and completed a questionnaire to assess the quality of HRI trust scale items.,Participants rated the representativeness of trust items based on a video scenario.,Unspecified,Unmanned Aerial Vehicles (UAVs); Unmanned Ground Vehicles (UGVs),Research,Evaluation,Rating,passive observation,Participants observed a video of a human-robot team.,media,Participants watched a video of a human-robot team.,physical,The robots were shown in a video.,,The level of autonomy of the robots was not specified.,Custom Scales,,,Trust was assessed using a custom scale.,no modeling,No trust modeling was performed.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,No factors were manipulated in this study.,,All items loaded onto one factor ranging from .723 to .986.,This study generated a HRI trust scale measure based on factor analysis of item quality.,Participants watched a video of a human-robot team and rated the representativeness of trust items.,Factor analysis,"This study used exploratory factor analysis to assess the quality of the HRI trust scale items. Factor analysis was conducted for each HRI item dimension, assuming that trust was consistent throughout. The analysis aimed to determine how well each item loaded onto a single factor, with the highest loading item within each dimension being selected for the final HRI trust scale. The factor loadings ranged from .723 to .986.",FALSE,,,,"This study did not manipulate any factors. Participants watched a video of a human-robot team and rated the representativeness of trust items. There was no manipulation of any kind, and therefore no factors were manipulated.",10.1007/s12369-012-0144-0,http://link.springer.com/10.1007/s12369-012-0144-0,"Trust plays a critical role when operating a robotic system in terms of both acceptance and usage. Considering trust is a multidimensional context dependent construct, the differences and common themes were examined to identify critical considerations within human–robot interaction (HRI). In order to examine the role of trust within HRI, a measurement tool was generated based on ﬁve attributes: team conﬁguration, team processes, context, task, and system (Yagoda in Human Factors and Ergonomics Society Annual Meeting, San Francisco, CA, pp. 304–308, 2010). The HRI trust scale was developed based on two studies. The ﬁrst study conducts a content validity assessment of preliminary items generated, based on a review of previous research within HRI and automation, using subject matter experts (SMEs). The second study assesses the quality of each trust scale item derived from the ﬁrst study. The results were then compiled to generate the HRI trust measurement tool."
"Yamani, Yusuke; Long, Shelby K.; Sato, Tetsuya; Braitman, Abby L.; Politowicz, Michael S.; Chancey, Eric T.",Multilevel Confirmatory Factor Analysis Reveals Two Distinct Human–Automation Trust Constructs,2024,1,129,129,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a compensatory tracking task and a system monitoring task supported by an automated system, and then reported their trust levels using the Jian et al. (2000) and Chancey et al. (2017) trust scales.",Participants performed a compensatory tracking task while monitoring a system with an automated signaling system.,Unspecified,Other,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated automated system.,simulation,The interaction was conducted in a simulated environment.,simulated,The robot was a simulated automated system.,pre-programmed (non-adaptive),The automated system followed a pre-set behavior.,Questionnaires,Jian et al. Trust Scale; Human-Computer Trust Scale/Questionnaire (HCT/HCTM),,Trust was measured using two questionnaires.,"parametric models (e.g., regression)",Multilevel confirmatory factor analysis was used to model the relationship between the two trust scales.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The difficulty of the tracking task was manipulated to influence trust.,"The tracking task difficulty impacted trust ratings, with higher difficulty leading to lower trust.","The Jian et al. (2000) scale's distrust factor did not correlate with any of the three dimensions of the Chancey et al. (2017) scale, suggesting they measure different constructs. The 'I am familiar with the system' item in the Jian et al. scale had a very low loading at the within-person level.",The Chancey et al. (2017) and Jian et al. (2000) trust scales are only weakly related and likely measure different constructs of human-automation trust.,"The automated system provided alerts, and the human monitored the system while performing a tracking task.",Multilevel Model; Linear regression,"The study used multilevel confirmatory factor analysis (CFA) to examine the factor structures of the Jian et al. (2000) and Chancey et al. (2017) trust scales. This involved assessing how well items within each scale corresponded to their respective latent factors at both within-person and between-person levels. The analysis also examined the correlation between the two scales. Additionally, regression analyses were conducted to predict the Chancey et al. (2017) scale factor scores using the Jian et al. (2000) scale factor scores, to further explore the relationship between the two scales.",TRUE,Task-complexity,Task-complexity,,"The paper describes multiple experiments where the difficulty of the tracking task was manipulated. Specifically, the paper states, 'In the experiments, all participants performed concurrently the compensatory tracking task manually and the system monitoring task supported by the automated system and reported their trust levels... To illustrate, the effect of tracking task difficulty on trust shows a lack of effect on trust as measured by Jian et al.'s scale (i.e., trust declined with greater difficulty once and had no effect twice). Alternatively, Chancey et al.'s (2017) scale consistently showed the impact of tracking task difficulty on trust consistently across three experiments.' This indicates that the cognitive demands of the task were intentionally changed, which falls under the 'Task-complexity' category. The paper also states, 'Results showed that automation trust declined for both scales when the tracking task difficulty was high.' This shows that the manipulation of task complexity impacted trust. There were no other manipulations described in the paper.",10.1177/00187208241263774,https://journals.sagepub.com/doi/10.1177/00187208241263774,"Objective: This work examined the relationship of the constructs measured by the trust scales developed by Chancey et al. (2017) and Jian et al. (2000) using a multilevel conﬁrmatory factor analysis (CFA). Background: Modern theories of automation trust have been proposed based on data collected using trust scales. Chancey et al. (2017) adapted Madsen and Gregor’s (2000) trust scale to align with Lee and See’s (2004) human–automation trust framework. In contrast, Jian et al. (2000) developed a scale empirically with trust and distrust as factors. However, it remains unclear whether these two scales measure the same construct. Method: We analyzed data collected from previous experiments to investigate the relationship between the two trust scales using a multilevel CFA. Results: Data provided evidence that Jian et al. (2000) and Chancey et al. (2017) automation trust scales are only weakly related. Trust and distrust are found to be distinct factors in Jian et al.’s (2000) scale, whereas performance, process, and purpose are distinct factors in Chancey et al.’s (2017) trust scale. Conclusion: The analysis suggested that the two scales purporting to measure human–automation trust are only weakly related. Application: Trust researchers and automation designers may consider using Chancey et al. (2017) and Jian et al. (2000) scales to capture different characteristics of human–automation trust."
"Yan, Rui; Tee, Keng Peng; Chua, Yuanwei; Huang, Zhiyong",A User Study for an Attention-Directed Robot for Telepresence,2013,1,15,15,0,No participants were excluded,Controlled Lab Environment,,"Participants were introduced to the robot, observed its attention-directing and face-tracking capabilities, and then completed a questionnaire.",Participants observed the robot's behavior and answered a questionnaire about its usability and acceptance.,Unspecified,Telepresence Robots,Social; Research,Evaluation,Survey/Questionnaire Completion,minimal interaction,"Participants observed the robot and interacted with it through a demonstration, but did not have direct physical contact.",real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,pre-programmed (non-adaptive),The robot's attention-directing and face-tracking behaviors were pre-programmed and did not adapt to the user.,Questionnaires,,,Trust was assessed using a questionnaire.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Physical Robot Studies,No Manipulation,"The study did not manipulate any specific factors related to trust, but rather assessed the participants' perceptions of the robot.","The study found that participants generally had a positive attitude and acceptance towards the robot, with those who had more video-conferencing experience being more accepting.",Participants with more video-conferencing experience were more accepting of the robot.,Participants generally found the attention-directed robot useful for social telepresence with the elderly and were comfortable using it.,The robot directed its attention towards the speaker and tracked the user's face. The human observed the robot's behavior and answered a questionnaire.,,"The study primarily used descriptive statistics, such as mean scores, to analyze the questionnaire data. The responses to the questionnaire were grouped based on video-conferencing experience and compared using visual inspection of the mean scores. No specific statistical tests were explicitly mentioned or used.",FALSE,,,,"The study did not manipulate any factors. The participants observed the robot's behavior and completed a questionnaire. The study compared responses based on prior video-conferencing experience, but this was not an intentional manipulation by the researchers. Therefore, no factors were manipulated, and thus, no factors impacted or did not impact trust as a result of manipulation.",,http://link.springer.com/10.1007/978-3-642-39470-6_14,"The motivation of this paper is to study the potential use of an attention-directed robot, embodied as a pan-tilt tablet display, for telepresence applications with the elderly. With an audio-visual attention control system, the robot automatically directs attention to a speaker by fusion of visual and auditory features. We conducted a user study involving experiential interaction with the robot, which oriented its display in the direction of the user’s voice, and also tracked the user’s face once it is in view of the robot’s camera. A questionnaire was given to gather the opinions of the participants regarding the usability of the robot for the elderly, and their responses indicate a positive attitude, acceptance, and trust towards the attention-directed robot. By comparing the responses of subjects with diﬀerent levels of video-conferencing experience, we found that subjects who video-conference more frequently tend to be more accepting of the robot. Furthermore, they indicated an acceptable price of the robot as less than the price of a tablet, and most of them expressed the desire to buy the robot in the near future."
"Yan, Fei; Eilers, Mark; Ludtke, Andreas; Baumann, Martin",Building driver's trust in lane change assistance systems by adapting to driver's uncertainty states,2017,1,20,20,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a driving simulator task with five different lane change assistance systems, including a baseline with no assistance. They were instructed to look at the lead vehicle, then at the left side mirror after an acoustic signal, and to indicate their decision to change lanes or brake using a game controller. After each condition with an assistance system, participants completed a questionnaire and an interview.","Participants made lane change decisions in a driving simulator, indicating their choice to change lanes or brake using a game controller.",Unspecified,Autonomous Vehicles,Research,Evaluation,Survey/Questionnaire Completion,minimal interaction,"Participants interacted with the system through a driving simulator and game controller, making decisions based on visual cues.",simulation,The study used a driving simulator to create an interactive virtual environment.,simulated,The vehicles were simulated within the driving simulator environment.,pre-programmed (non-adaptive),The vehicles in the simulation moved autonomously based on pre-programmed parameters.,Behavioral Measures; Questionnaires,Jian et al. Trust Scale,Performance Metrics,Trust was assessed using a questionnaire and behavioral measures such as the rate of agreement and follow time.,no modeling,Trust was not modeled computationally; descriptive statistics were used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The study manipulated the lane change assistance system's behavior by providing different visual cues and varying the timing of the cues. This was intended to influence the driver's trust and acceptance of the system.,"The proposed system, which adapted to driver's uncertainty, was found to be more trusted and accepted than the reference systems. The system also increased the rate of agreement compared to the baseline.","System D resulted in the shortest reaction time, but was the least accepted and trusted, suggesting that reaction time alone is not an appropriate parameter to reflect driver's trust in automation. The proposed system C was more trusted and accepted than the reference systems, despite not having the shortest reaction time.","The proposed lane change assistance system, which adapts to driver's uncertainty, was able to both reduce reaction times and build driver's trust in the automation.","The simulated vehicles moved autonomously, and the human participant made lane change decisions using a game controller based on the visual cues provided by the assistance systems.",ANOVA; t-test; Friedman test; Wilcoxon rank sum,"The study used a three-way ANOVA with repeated measures to analyze the effect of the experimental condition on mean reaction time (MRT) and follow time (FT), and to check for learning effects or effects caused by individual differences. Post-hoc pairwise t-tests with Bonferroni adjustments were used to identify significant differences between specific conditions for MRT and FT. A Friedman test was conducted on the raw number of agreements (NoAs) to compare the distribution of the five experimental conditions. Post-hoc Wilcoxon tests with Bonferroni correction were used to identify significant differences between specific conditions for NoAs.",TRUE,Robot-interface-design; Robot-autonomy,Robot-interface-design,Robot-autonomy,"The study manipulated the lane change assistance system's behavior by providing different visual cues (transparent, green, and red symbols) and varying the timing of the cues. This falls under 'Robot-interface-design' because it involves changes to the interactive elements of the system, specifically the visual display. The study also implicitly manipulated 'Robot-autonomy' by having the vehicles move autonomously based on pre-programmed parameters, but this was consistent across all conditions and did not vary as a manipulation. The different assistance systems (A, B, C, and D) varied in how they presented information to the driver, which directly impacted trust. System C, which adapted to driver's uncertainty, was more trusted than the other systems. The fixed time delay in system D, while impacting reaction time, did not increase trust, indicating that the manipulation of the interface design was the primary driver of trust differences. The autonomy of the vehicles was not a factor that was manipulated to impact trust, as all vehicles moved autonomously in all conditions.",10.1109/IVS.2017.7995772,http://ieeexplore.ieee.org/document/7995772/,"Driver’s uncertainty during decision-making in overtaking results in long reaction times and potentially dangerous lane change maneuvers. Current lane change assistance systems focus on safety assessments providing either too conservative or excessive warnings, which inﬂuence driver’s acceptance and trust in these systems. Inspired by the emancipation theory of trust, we expect systems providing information adapted to driver’s uncertainty states to simultaneously help to reduce long reaction times and build the overall trust in automation. In previous work, we presented an adaptive lane change assistance system based on this concept utilizing a probabilistic model of driver’s uncertainty. In this paper, we investigate whether the proposed system is able to improve reaction times and build trust in the automation as expected. A simulator study was conducted to compare the proposed system with an unassisted baseline and three reference systems not adaptive to driver’s uncertainty. The results show while all systems reduce reaction times compared to the baseline, the proposed adaptive system is the most trusted and accepted."
"Yang, X. Jessie; Unhelkar, Vaibhav V.; Li, Kevin; Shah, Julie A.",Evaluating Effects of User Experience and System Transparency on Trust in Automation,2017,1,91,91,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants performed a simulated military reconnaissance task with an automated threat detector, completing 100 trials. They performed a tracking task and a threat detection task, switching between displays. They reported their trust in the detector after each trial.","Participants performed a dual-task military reconnaissance mission, including a compensatory tracking task and a threat detection task.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated robot through a computer interface.,simulation,The interaction was conducted in a simulated environment.,simulated,The robot was represented as a simulated entity on a computer screen.,pre-programmed (non-adaptive),The automated threat detector had a fixed level of performance.,Behavioral Measures; Real-time Trust Measures,,Performance Metrics,Trust was measured using real-time subjective ratings and behavioral measures.,"parametric models (e.g., regression)",A first-order linear time invariant (LTI) dynamical system model was used to explain the evolution of trust over time.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the reliability of the automated threat detector and the type of alarm (binary or likelihood) to influence trust. The reliability was manipulated by changing the sensitivity of the detector, and the alarm type was manipulated by providing either binary or likelihood alarms.","Trust was influenced by the reliability of the automation and the type of alarm. Higher reliability led to higher trust, and likelihood alarms showed a greater impact on moment-to-moment trust changes.","The study found that trust of entirety is better quantified by the area under the trust curve than the traditional post-experiment trust measure. The study also found that the time constant for the likelihood alarm was greater than the binary alarm, suggesting that users require more interaction to arrive at a stable trust value for the likelihood alarm. There was also a variation in trust evolution patterns, which was explained by the interplay between operators' initial expectation of automation and their subsequent observation of automation's performance.","Trust of entirety is better quantified by the average measure of area under the trust curve than the traditional post-experiment trust measure, and trust evolves and eventually stabilizes as an operator repeatedly interacts with a technology.","The robot (simulated drones) provided images of a city to the human participant. The human participant monitored the images for threats, with the help of an automated threat detector, while also performing a tracking task.",pearson's correlation analysis; Linear regression; t-test; t-test,"The study used Pearson's correlation analysis to examine the relationship between different trust measures (TrustAUTC and Trustend) and participants' trusting behaviors. Regression analysis was used to plot the Truste-reliability calibration curve and assess how it changes with experience. Paired sample t-tests were used to compare the differences between high- and low-likelihood alerts on moment-to-moment trust changes. Independent sample t-tests were used to compare the differences between binary and high-likelihood alerts, and between binary and low-likelihood alerts on moment-to-moment trust changes.",TRUE,Robot-accuracy; Robot-interface-design,Robot-accuracy; Robot-interface-design,,"The study manipulated the reliability of the automated threat detector, which directly impacts the accuracy of the robot's threat detection, thus 'Robot-accuracy'. The study also manipulated the type of alarm (binary or likelihood), which is a change to the interface design, thus 'Robot-interface-design'. The results showed that both the reliability of the automation and the type of alarm influenced trust, therefore both 'Robot-accuracy' and 'Robot-interface-design' are listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1145/2909824.3020230,https://dl.acm.org/doi/10.1145/2909824.3020230,"Existing research assessing human operators’ trust in automation and robots has primarily examined trust as a steady-state variable, with little emphasis on the evolution of trust over time. With the goal of addressing this research gap, we present a study exploring the dynamic nature of trust. We defined trust of entirety as a measure that accounts for trust across a human’s entire interactive experience with automation, and first identified alternatives to quantify it using real-time measurements of trust. Second, we provided a novel model that attempts to explain how trust of entirety evolves as a user interacts repeatedly with automation. Lastly, we investigated the effects of automation transparency on momentary changes of trust. Our results indicated that trust of entirety is better quantified by the average measure of “area under the trust curve” than the traditional post-experiment trust measure. In addition, we found that trust of entirety evolves and eventually stabilizes as an operator repeatedly interacts with a technology. Finally, we observed that a higher level of automation transparency may mitigate the “cry wolf” effect — wherein human operators begin to reject an automated system due to repeated false alarms."
"Yang, Yucheng; Karakaya, Burak; Dominioni, Giancarlo Caccia; Kawabe, Kyosuke; Bengler, Klaus",An HMI Concept to Improve Driver's Visual Behavior and Situation Awareness in Automated Vehicle,2018,1,52,50,2,2 datasets were discarded due to technical problems and voluntary take-overs during the HAD,Controlled Lab Environment,between-subjects,"Participants were assigned to either a baseline or an LED group and performed a visual-motoric task on a smartphone during a 45-min automated drive in a static driving simulator. The LED group experienced different LED bar modes, while the baseline group did not have the LED bar functionality until the take-over request. Trust and acceptance were measured before and after the experiment. Gaze behavior and take-over performance were also measured.",Participants performed a visual-motoric task on a smartphone while the car was in automated driving mode. They were instructed to take over control of the vehicle when they felt it was necessary or when a take-over request was issued.,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the automated vehicle in a driving simulator while performing a secondary task.,simulation,The interaction took place in a static driving simulator with a 180-degree front view and mirror projections.,simulated,The automated vehicle was simulated in a driving simulator.,fully autonomous (limited adaptation),The automated vehicle drove autonomously with limited adaptation to the environment.,Questionnaires; Behavioral Measures; Performance-Based Measures,,Eye-tracking Data; Performance Metrics,"Trust was assessed using questionnaires, gaze behavior, and take-over performance metrics.",no modeling,Trust was not modeled computationally; descriptive statistics were used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The LED bar's functionality was manipulated to provide information about the automation's status, intention, and potential hazards, influencing driver's expectations and providing feedback.",The LED bar increased driver's trust in automation without leading to overtrust. The LED group also showed improved gaze behavior and take-over performance.,"The LED group showed a significant increase in self-reported trust after the experiment, while the baseline group did not. The LED group also exhibited better gaze behavior and take-over performance, indicating improved situation awareness.","The LED ambient light bar increased driver's trust in automation without leading to overtrust, and improved driver's visual behavior and take-over performance.","The automated vehicle drove along a highway, and the human participant monitored the driving while performing a secondary task on a smartphone. The human was required to take over control of the vehicle when a take-over request was issued.",Kolmogorov-Smirnov; t-test; Wilcoxon rank sum; Mann-Whitney U; Chi-squared,"The study used a variety of statistical tests to analyze the data. The Kolmogorov-Smirnov test with Lilliefors correction was used to check for normality of the data. T-tests were applied for normally distributed data, while Wilcoxon tests were used for dependent samples with non-normal distributions, and Mann-Whitney U-tests were used for independent samples with non-normal distributions. Chi-square tests were used to compare proportions. These tests were used to compare the two groups (LED vs. baseline) on various dependent variables such as trust, acceptance, gaze behavior, and take-over performance.",TRUE,Robot-interface-design,Robot-interface-design,,"The study manipulated the functionality of an LED bar, which is a component of the robot's interface. The LED bar provided information about the automation's status, intention, and potential hazards through different modes (color, frequency, lighting position, and animation). This manipulation directly relates to the design of the interface through which the robot communicates with the user. The paper states, 'In this paper, a concise HMI concept of the LED ambient light positioned at the bottom of the windscreen is presented, which contains information about the status and intention of the automation, detected potential hazards and the warning for a take-over request (TOR) by varying the LED’s color, frequency, lighting position and animation.' The results showed that the LED bar increased driver's trust in automation, indicating that the interface design had an impact on trust. The paper also states, 'The new HMI also shows a high acceptance and increases the trust in automation while avoiding overtrust.' There were no other factors manipulated in the study.",10.1109/ITSC.2018.8569986,https://ieeexplore.ieee.org/document/8569986/,"At a level-3 or higher level automation [1], a driver does not have to constantly monitor the vehicle and environment while driving, which enables the driver to conduct non-driving-related tasks (NDRTs) and be out of the control loop. This may influence a driver’s visual behavior, cognitive states, which leads to loss of situation awareness (SA) and skills. This is dangerous if the automated system reaches its boundaries: the driver must take-over the driving task in a critical situation within a limited period of time. In this paper, a concise HMI concept of the LED ambient light positioned at the bottom of the windscreen is presented, which contains information about the status and intention of the automation, detected potential hazards and the warning for a take-over request (TOR) by varying the LED’s color, frequency, lighting position and animation. The goal is to increase the SA during automated driving and improve the take-over quality while allowing the driver to perform NDRTs without distraction and annoyance. In this between-subject-design experiment in a static driving simulator, 50 participants performed a visualmotoric task on a smartphone during a 45-min automated drive with or without the new HMI. Compared to the baseline, results show significant improvements in the gaze behavior and take-over quality. The new HMI also shows a high acceptance and increases the trust in automation while avoiding overtrust."
"Yang, X. Jessie; Schemanske, Christopher; Searle, Christine",Toward Quantifying Trust Dynamics: How People Adjust Their Trust After Moment-to-Moment Interaction With Automation,2023,1,75,75,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants viewed images, performed a memory task, then completed 40 trials of a two-alternative forced choice image recognition task, receiving recommendations from an automated aid after their initial choice, and then making a final choice. They rated their trust after each trial.",Participants performed a two-alternative forced choice image recognition task with automated decision aid recommendations.,Unspecified,Other,Research,Evaluation,Image Analysis,minimal interaction,Participants interacted with an automated aid through a computer interface.,simulation,The interaction was conducted through a simulated memory recognition task on a computer.,simulated,The automated decision aid was a simulated entity presented on a computer screen.,pre-programmed (non-adaptive),The automated decision aid provided recommendations based on a pre-programmed algorithm.,Custom Scales; Questionnaires; Real-time Trust Measures,Jian et al. Trust Scale; Muir's Trust Questionnaire; Propensity to Trust Scales,Performance Metrics,"Trust was measured using a visual analog scale after each trial, along with pre- and post-experiment questionnaires.","parametric models (e.g., regression)","Linear regression models were used to analyze the relationship between trust, automation reliability, and trust propensity.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The reliability of the automated aid was manipulated, and the task was designed to include both correct and incorrect recommendations, influencing trust.","Trust increased after automation successes and decreased after automation failures, with failures having a larger impact. Outcome bias and contrast effects also influenced trust adjustments.","The study found that automation failures had a greater impact on trust than successes, and that outcome bias and contrast effects significantly influenced trust adjustments. The self-serving bias may have influenced how participants perceived the automation's performance.","Human operators adjust their trust in automation due to moment-to-moment interaction, and this process is influenced by decision-making heuristics/biases, with automation failures having a greater impact than successes.","The automated decision aid provided recommendations on which image was the target, and the human participant made an initial choice, received the recommendation, and then made a final choice.",ANCOVA; t-test; paired samples t-tests; Linear regression,"The study used ANCOVAs to examine the effects of automation reliability and performance pattern on trust adjustment, with trust propensity as a covariate. However, these analyses were not significant, so the data was collapsed across reliability levels. Subsequently, one-sample t-tests were used to compare trust adjustments against zero for different performance patterns. Paired samples t-tests were used to compare the magnitude of trust decrements and increments. Finally, linear regression models were used to examine the relationship between automation reliability, trust propensity, and post-experiment trust.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy,Task-complexity,"The study manipulated 'Robot-accuracy' by varying the reliability of the automated decision aid (70%, 80%, and 90%). This is explicitly stated in the 'METHOD' section: 'The between-subjects variable is automation reliability, varied in three levels: 70%, 80%, and 90%.' The paper also mentions that the task was designed to include both correct and incorrect recommendations, which directly impacts the robot's accuracy. The study also manipulated 'Task-complexity' by including an interpolated memory task to increase cognitive load, as stated in the 'METHOD' section: 'Next, participants perform an interpolated memory task to write down as much information as they could about four famous people in 4 min. The interpolated task is used in the study of Tulving (1981) to bring the hit rate into the middle performance range.' The results section indicates that the reliability of the automation (Robot-accuracy) had a significant impact on trust, as stated in the 'RESULTS' section: 'The results show that automation reliability is both a significant predictor of trust after the 40th trial.' However, the study found that the task complexity did not have a significant impact on trust, as stated in the 'RESULTS' section: 'Results show that neither automation reliability (i.e., 70%, 80%, 90%) or trust propensity had the planned comparisons. Therefore, trust propensity (as the covariate) is removed from the analysis and the data are collapsed across the levels of automation reliability.'",10.1177/00187208211034716,http://journals.sagepub.com/doi/10.1177/00187208211034716,"Objective: We examine how human operators adjust their trust in automation as a result of their moment-to-moment interaction with automation. Background: Most existing studies measured trust by administering questionnaires at the end of an experiment. Only a limited number of studies viewed trust as a dynamic variable that can strengthen or decay over time. Method: Seventy-ﬁve participants took part in an aided memory recognition task. In the task, participants viewed a series of images and later on performed 40 trials of the recognition task to identify a target image when it was presented with a distractor. In each trial, participants performed the initial recognition by themselves, received a recommendation from an automated decision aid, and performed the ﬁnal recognition. After each trial, participants reported their trust on a visual analog scale. Results: Outcome bias and contrast effect signiﬁcantly inﬂuence human operators’ trust adjustments. An automation failure leads to a larger trust decrement if the ﬁnal outcome is undesirable, and a marginally larger trust decrement if the human operator succeeds the task by him/herself. An automation success engenders a greater trust increment if the human operator fails the task. Additionally, automation failures have a larger effect on trust adjustment than automation successes. Conclusion: Human operators adjust their trust in automation as a result of their moment-to-moment interaction with automation. Their trust adjustments are signiﬁcantly inﬂuenced by decision-making heuristics/biases."
"Ye, Xin; Robert, Lionel","Human Security Robot Interaction and Anthropomorphism: An Examination of Pepper, RAMSEE, and Knightscope Robots",2023,1,69,60,9,9 participants were excluded due to the failure of the Wizard-of-Oz method or because their overall questionnaire scores were beyond 2.5 standard deviations from the mean,Controlled Lab Environment,between-subjects,"Participants were introduced to the experiment, completed questionnaires, interacted with a security robot in a VR environment, and then completed post-questionnaires and a semi-structured interview.","Participants interacted with a security robot in a virtual environment, which included a patrol, access control, mask recommendation, suspicious activity inquiry, and emotion detection task.",Pepper; RAMSEE; Knightscope,Humanoid Robots; Other; Mobile Robots,Social; Research,Social,Conversation,minimal interaction,Participants interacted with the robot in a virtual environment.,simulation,Participants interacted with the robot in a virtual reality cave.,simulated,The robots were simulated in a virtual environment.,wizard of oz (directly controlled),The robot's actions were controlled by a researcher using a Wizard-of-Oz setup.,Questionnaires,,,Trust was measured using a 4-item questionnaire.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The study manipulated the robot's appearance by using three different robot models with varying levels of anthropomorphism and the interaction medium by using a virtual reality environment.,"Robot type did not significantly impact trust, but the Pepper robot was rated higher in ability, integrity, and desire to use compared to the Knightscope robot.","The RAMSEE robot did not show significant differences in perceived anthropomorphism compared to the other two robots, which was unexpected. There was a trend that RAMSEE had higher ability, integrity, and desire to use than Knightscope.","The anthropomorphism of security robots impacts human acceptance, with the human-like Pepper robot being rated higher in ability, integrity, and desire to use compared to the mechanical Knightscope robot.","The robot patrolled, approached the participant, introduced itself, asked for identification, recommended mask-wearing, inquired about suspicious activity, and asked about the participant's emotional state. The human participant responded to the robot's prompts and questions.",ANOVA; post hoc comparisons,"The study used ANOVAs to examine the main effects of robot type and scenario on robot acceptance and perceptions, as well as their interaction. Post hoc comparisons using the Tukey correction were conducted for all significant main effects. These tests were used to determine if there were statistically significant differences in trust, trustworthiness (ability, integrity, benevolence), perceptions of the robot (likeability, perceived intelligence, perceived safety), and desire to use based on the robot type and interaction scenario.",TRUE,Robot-aesthetics; Task-environment,,,"The study manipulated the robot's appearance by using three different robot models (Pepper, RAMSEE, and Knightscope) with varying levels of anthropomorphism. This falls under 'Robot-aesthetics' as it is a change to the visual appeal of the robot. The study also manipulated the interaction scenario by using two different virtual environments (an indoor hallway and an outdoor parking lot), which is categorized as 'Task-environment' because it changes the working conditions. The study found that robot type (aesthetics) impacted ability, integrity, and desire to use, but not trust. The study also found that the interaction scenario (task environment) did not impact trust, ability, integrity, or desire to use. Therefore, neither of the manipulated factors impacted trust.",,,"The rapid growth in the use of security robots makes it critical to better understand their interactions with humans. The impacts of anthropomorphism and interaction scenarios were examined via a 3 x 2 between-subjects experiment. Sixty participants were randomly assigned to interact with one of three security robots (Knightscope, RAMSEE, or Pepper) in either an indoor hallway or an outdoor parking lot scenario in a virtual reality cave. There were significant differences only between Pepper and Knightscope with Pepper rated higher in anthropomorphism, ability, integrity, and desire to use than Knightscope but the interaction scenario has no effect."
"Ye, Sean; Neville, Glen; Schrum, Mariah; Gombolay, Matthew; Chernova, Sonia; Howard, Ayanna",Human Trust After Robot Mistakes: Study of the Effects of Different Forms of Robot Communication,2019,1,44,44,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants played a Simon Says game with a Pepper robot that provided advice using one of four forms of communication (unsolicited, solicited, pre-corrective, or post-corrective). The robot provided correct advice for the first four rounds and then incorrect advice for the next six rounds. Trust was measured using a 14-point survey after each round.",Participants played a Simon Says game where they had to copy a sequence of colored lights. The robot provided advice on the sequence.,Pepper,Humanoid Robots; Expressive Robots,Research; Social,Game,Cooperative Game,minimal interaction,Participants interacted with the robot verbally and through a game interface.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,The study used a physical humanoid robot.,pre-programmed (non-adaptive),The robot's behavior was pre-programmed and did not adapt to the user.,Behavioral Measures; Questionnaires,Schaefer's Trust Questionnaire/Scale,Performance Metrics,Trust was measured using a questionnaire and by tracking participant agreement with the robot.,"parametric models (e.g., regression)",The study used a linear mixed effects model to analyze the relationship between trust and form of communication.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the form of communication used by the robot (unsolicited, solicited, pre-corrective, post-corrective) and the timing of incorrect advice to influence trust.",Solicited advice resulted in less trust degradation after the robot started making mistakes compared to pre-corrective and post-corrective advice. The form of communication also impacted how often participants agreed with the robot.,"The study found that solicited advice was more effective at maintaining trust after robot mistakes, which is a notable trend. There were no significant differences in the rate of trust decrease between the different forms of communication.","Solicited advice from the robot resulted in less degradation of trust after the robot began making mistakes, compared to other forms of communication.",The robot provided advice on the sequence of colors in the Simon Says game. The human participant had to copy the sequence of colors by pressing the buttons in the correct order.,linear mixed effects analysis; ANOVA; Tukey HSD,"The study used a linear mixed effects model to analyze the relationship between trust and form of communication, with fixed effects for FOC, failures, baseline trust, age, race, gender, and education, and random intercepts for each participant. The model examined trust scores before and after robot failures, as well as the rate of trust increase and decrease. A one-way ANOVA was used to determine if the form of communication significantly impacted the percentage of times a participant agreed with the robot. A Tukey post-hoc test was then used to determine which specific FOC conditions differed significantly in terms of agreement.",TRUE,Robot-verbal-communication-content; Robot-social-timing,Robot-verbal-communication-content; Robot-social-timing,,"The study manipulated the form of communication (FOC) which included unsolicited, solicited, pre-corrective, and post-corrective advice. This manipulation directly changes the content of the robot's verbal communication (Robot-verbal-communication-content) by determining when and if the robot provides advice. The timing of the robot's communication was also manipulated (Robot-social-timing) as the robot either provided advice preemptively, only when asked, immediately after a mistake, or after the task was completed. The results showed that the form of communication significantly impacted trust, specifically that solicited advice resulted in less trust degradation after the robot started making mistakes. Therefore, both Robot-verbal-communication-content and Robot-social-timing impacted trust. There were no factors that were manipulated that did not impact trust.",10.1109/RO-MAN46459.2019.8956424,https://ieeexplore.ieee.org/document/8956424/,"Collaborative robots that work alongside humans will experience service breakdowns and make mistakes. These robotic failures can cause a degradation of trust between the robot and the community being served. A loss of trust may impact whether a user continues to rely on the robot for assistance. In order to improve the teaming capabilities between humans and robots, forms of communication that aid in developing and maintaining trust need to be investigated. In our study, we identify four forms of communication which dictate the timing of information given and type of initiation used by a robot. We investigate the effect that these forms of communication have on trust with and without robot mistakes during a cooperative task. Participants played a memory task game with the help of a humanoid robot that was designed to make mistakes after a certain amount of time passed. The results showed that participants’ trust in the robot was better preserved when that robot offered advice only upon request as opposed to when the robot took initiative to give advice."
"Ye, Sean; Feigh, Karen; Howard, Ayanna",Learning in Motion: Dynamic Interactions for Increased Trust in Human-Robot Interaction Games,2020,1,16,16,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were introduced to the robot and the task, completed a pre-survey, interacted with the robot in one of two conditions (full-body or touch-screen), and then completed a post-survey including a quiz and trust questionnaire.","Participants learned word-motion pairs from a robot, either by repeating the motion themselves or by using a touch screen, and then took a quiz to test their memory.",Pepper,Humanoid Robots; Expressive Robots,Educational; Research; Social,Game,Cooperative Game,direct-contact interaction,Participants physically interacted with the robot by performing dance motions in one condition.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,The study used a physical humanoid robot.,pre-programmed (non-adaptive),The robot followed a pre-programmed sequence of actions and did not adapt to the user.,Questionnaires,Schaefer's Trust Questionnaire/Scale; Godspeed Questionnaire,,Trust was measured using a pre- and post-interaction questionnaire.,"parametric models (e.g., regression)",Multiple regression analysis was used to identify the effect of interaction type and baseline trust on post-interaction trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The type of interaction (full-body vs. touch-screen) was manipulated to influence trust, with the expectation that full-body interaction would increase trust.","Participants in the full-body interaction condition showed significantly higher trust gain when their initial trust was low, while no significant difference was found when initial trust was high.","The interaction effect between pre-trust and interaction type on post-trust was a notable trend, indicating that the effect of embodiment on trust is dependent on initial trust levels. The study did not find any significant differences in memory retention or likeability between the two conditions.","Human embodiment of tasks can positively impact a user's perception of a robot, particularly when the user has low initial trust towards the robot.","The robot demonstrated dance motions paired with Greek letters. The human either repeated the motions physically or used a touch screen to proceed to the next motion, and then took a quiz to identify the motions.",Linear regression,"Multiple regression analysis was used to examine the effect of interaction type (full-body vs. touch-screen) and pre-interaction trust on post-interaction trust. The model also included age, gender, race, and education as factors. Additionally, a similar linear model was used to test the effect of interaction type on likeability and score (memory performance).",TRUE,Task-environment,Task-environment,,"The study manipulated the task environment by having participants either physically perform the dance motions (full-body interaction) or use a touch screen to proceed to the next motion. This is described in the 'Interaction Types' section: 'In the full-body interaction, participants had to repeat the motion physically to see the next motion-word pair. ... In the touchscreen interaction, participants had to touch Pepper's screen to continue.' The manipulation of the task environment (full-body vs. touch screen) was found to impact trust, as stated in the 'Results' section: 'interaction effects were found between the pre-trust score and type of interaction F(1, 7) = 5.643, p = 0.0492.' Specifically, participants in the full-body interaction condition showed significantly higher trust gain when their initial trust was low. The study did not find any significant differences in memory retention or likeability between the two conditions, so no other factors were found to impact or not impact trust.",10.1109/RO-MAN47096.2020.9223437,https://ieeexplore.ieee.org/document/9223437/,"Embodiment of actions and tasks has typically been analyzed from the robot’s perspective where the robot’s embodiment helps develop and maintain trust. However, we ask a similar question looking at the interaction from the human perspective. Embodied cognition has been shown in the cognitive science literature to produce increased social empathy and cooperation. To understand how human embodiment can help develop and increase trust in human-robot interactions, we created conducted a study where participants were tasked with memorizing greek letters associated with dance motions with the help of a humanoid robot. Participants either performed the dance motion or utilized a touch screen during the interaction. The results showed that participants’ trust in the robot increased at a higher rate during human embodiment of motions as opposed to utilizing a touch screen device."
"Ye, Yang; You, Hengxu; Du, Jing",Improved Trust in Human-Robot Collaboration With ChatGPT,2023,1,15,15,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a training session, then performed an assembly task with a robot in two conditions (fixed verbal commands and ChatGPT-enabled commands) in a randomized order, and completed questionnaires after each condition and an interview at the end.","Participants assembled a plate onto a workpiece and fastened it with four screws using a driller, while the robot delivered tools.",Franka Emika Panda,Industrial Robot Arms; Collaborative Robots,Industrial; Research,Manipulation,Object Assembly,minimal interaction,Participants interacted with the robot through verbal instructions and the robot delivered tools.,real-world,The experiment was conducted in VR with a digital twin of the real robot arm.,physical,"A real robot arm was controlled by the AI assistant, and a digital twin was present in VR.",shared control (adaptive),The robot adapted its behavior based on the human's verbal instructions and context.,Questionnaires,,,Trust was measured using a questionnaire.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Mixed Reality Studies,Direct Manipulation,"The robot's control method was manipulated by using either fixed verbal commands or a ChatGPT-enabled AI assistant, which influenced the robot's behavior and autonomy.",The ChatGPT-enabled robot assistant increased trust compared to the fixed command condition.,"The ChatGPT-enabled assistant sometimes exhibited self-assertion, making decisions based on its understanding, which could be problematic in cases of miscommunication.",Using a ChatGPT-enabled robot assistant improved task performance and increased trust compared to using fixed verbal commands.,"The robot delivered tools to the human, and the human assembled a plate onto a workpiece and fastened it with screws.",anderson darling; t-test,"The Anderson Darling test was used to check the normality of the performance score and self-evaluated performance data. Following this, a T-test was used to compare the performance scores, self-evaluated performance, trust, and cognitive load between the fixed command condition and the ChatGPT-enabled AI assistant condition. The purpose was to determine if there were significant differences in these measures between the two conditions.",TRUE,Robot-verbal-communication-content; Robot-autonomy,Robot-verbal-communication-content; Robot-autonomy,,"The study manipulated the robot's control method, which directly influenced the content of verbal communication and the level of autonomy. In the fixed command condition, the robot responded to a limited set of predefined verbal commands, thus limiting the content of communication and the robot's autonomy. In the ChatGPT-enabled condition, the robot could understand natural language and engage in bilateral communication, adapting its behavior based on the human's instructions and context, thus increasing the content of communication and the robot's autonomy. The paper explicitly states that the ChatGPT-enabled robot assistant increased trust compared to the fixed command condition, indicating that both the content of communication and the level of autonomy impacted trust. The paper states, 'This study aims to explore the potential effect on trust of using an intelligent LLM for robotic control in an HRC assembly task.' and 'We assume that the ChatGPT-enabled AI robot control assistant increases human operators' trust toward the robotic system owing to the natural bilateral communication and context understanding capabilities.' This clearly indicates that the researchers manipulated the robot's communication and autonomy to measure the impact on trust. The results section also confirms that the ChatGPT condition led to higher trust, further supporting the impact of these factors.",10.1109/ACCESS.2023.3282111,https://ieeexplore.ieee.org/document/10141597/,"Human-robot collaboration is becoming increasingly important as robots become more involved in various aspects of human life in the era of Artificial Intelligence. However, the issue of human operators’ trust in robots remains a significant concern, primarily due to the lack of adequate semantic understanding and communication between humans and robots. The emergence of Large Language Models (LLMs), such as ChatGPT, provides an opportunity to develop an interactive, communicative, and robust human-robot collaboration approach. This paper explores the impact of ChatGPT on trust in a human-robot collaboration assembly task. This study designs a robot control system called RoboGPT using ChatGPT to control a 7-degree-of-freedom robot arm to help human operators fetch, and place tools, while human operators can communicate with and control the robot arm using natural language. A human-subject experiment showed that incorporating ChatGPT in robots significantly increased trust in human-robot collaboration, which can be attributed to the robot’s ability to communicate more effectively with humans. Furthermore, ChatGPT’s ability to understand the nuances of human language and respond appropriately helps to build a more natural and intuitive human-robot interaction. The findings of this study have significant implications for the development of trustworthy human-robot collaboration systems."
"Yi, Binlin; Cao, Haotian; Song, Xiaolin; Wang, Jianqiang; Guo, Wenfeng; Huang, Zhi",Measurement and Real-Time Recognition of Driver Trust in Conditionally Automated Vehicles: Using Multimodal Feature Fusions Network,2023,1,34,31,3,3 participants were excluded due to malfunctions of the driving simulator and physiological sensors,Controlled Lab Environment,within-subjects,"Participants watched a video about AVs, received text information about system limitations, and completed a practice drive. They then performed a non-driving related task (NDRT) while experiencing six takeover scenarios with varying TOR lead times. Trust was measured via questionnaires and monitoring behavior.","Participants performed a visual Surrogate Reference Task (SuRT) on a tablet while driving in a simulator, and took over control of the vehicle when prompted by a TOR.",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a driving simulator and a tablet, with limited physical interaction.",simulation,The study used a driving simulator to create a virtual driving environment.,simulated,The robot was a simulated autonomous vehicle within the driving simulator.,shared control (fixed rules),The automated driving system operated independently but with fixed rules for takeover requests.,Questionnaires; Behavioral Measures; Physiological Measures,,Physiological Signals; Eye-tracking Data,"Trust was measured using self-reported ratings, monitoring behavior, and physiological signals.","deep learning (e.g., neural networks, reinforcement learning)",A CNN-LSTM-LS model was used to recognize driver trust based on physiological and interactive experience data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the TOR lead time (4s and 8s) to induce different levels of trust, with shorter lead times intended to reduce trust.","Shorter TOR lead times (4s) resulted in lower trust ratings and increased monitoring behavior, while longer lead times (8s) did not significantly reduce trust.","The study found that a combination of subjective trust ratings and objective monitoring behavior provided a more reliable measure of trust than subjective ratings alone. There was a multicollinearity between system usage time and takeover frequencies, but both were included in the model.","A hybrid approach combining self-reported trust ratings with objective monitoring behavior provides a more accurate measure of driver trust in AVs, and a CNN-LSTM-LS model can effectively recognize driver trust levels using physiological and interactive experience data.","The robot (simulated AV) drove autonomously, and the human participant monitored the driving environment while performing a secondary task, taking over control when prompted by a TOR.",Kruskal-Wallis; Spearman correlation; gaussian mixture model (gmm); generalized linear mixed model (glmm),"The study used the Kruskal-Wallis H test to examine the differences in subjective trust ratings and monitoring ratios across different TOR lead time conditions. Spearman correlation analysis was used to examine the relationships between subjective and objective trust measures, system usage time, and takeover frequency. A Gaussian mixture model (GMM) was used to categorize driver trust into higher and lower trust groups based on a combination of subjective trust ratings and monitoring ratios. Finally, a generalized linear mixed model (GLMM) was used to examine the effects of takeover-related interaction experiences (TOR lead time, takeover frequencies) and system usage time on driver trust levels.",TRUE,Robot-autonomy; Task-constraints,Robot-autonomy,,"The study manipulated the TOR lead time (4s and 8s), which directly affects the level of autonomy granted to the automated vehicle. A shorter lead time (4s) means the system is less autonomous and requires more immediate human intervention, while a longer lead time (8s) provides more autonomy to the system before requiring human takeover. This is a manipulation of 'Robot-autonomy' because it changes the decision authority and handover strategy of the automated system. The study also manipulated the time pressure on the participant by varying the TOR lead time, which is a manipulation of 'Task-constraints'. The results showed that the shorter TOR lead time (4s) resulted in lower trust ratings and increased monitoring behavior, while the longer lead time (8s) did not significantly reduce trust. Therefore, 'Robot-autonomy' was found to impact trust, as the change in lead time directly influenced the perceived reliability and trustworthiness of the automated system. The study did not find any factors that did not impact trust.",10.1177/03611981231156576,https://doi.org/10.1177/03611981231156576,"Trust calibration is essential to prevent misuse and disuse of automated vehicles (AVs). Accurate measurement and real-time identification of driver trust is an important prerequisite for achieving trust calibration. Currently, in conditionally automated driving, most researchers utilize self-reported ratings as the ground truth to evaluate driver trust and explore objective trust indicators. However, inconsistencies between the subjective rating and objective behaviors were reported, indicating that trust measurements cannot rely solely on self-reported ratings. To fill this research gap, a method of subjective and objective combination was proposed to measure and identify driver trust in AVs. Thirty-four drivers were involved in a sequence of takeover events. Monitoring ratio and subjective trusting ratings were collected, and combined to measure driver trust levels (i.e., higher and lower trust). Compared with the subjective measurement, the hybrid measurement can more reliably evaluate driver trust in AVs. More importantly, we established a real-time driver trust recognition model for AVs using label smoothing-based convolutional neural network and long short-term memory network fusing multimodal physiological signals (i.e., galvanic skin response and electrocardiogram) and interactive experiences (i.e., takeover-related lead time, takeover frequencies and system usage time). Four common models were developed to compare with the proposed model: Gaussian naive Bayes, support vector machine, convolutional neural network, and long short-term memory network. The comparison results suggest that the performance of our model outperforms others with an F1-score of 75.3% and an area under curve value of 0.812. These findings could have implications for the development of trust monitoring systems in conditionally automated driving."
"Yokoi, Ryosuke",Trust in self-driving vehicles is lower than in human drivers when both drive almost perfectly,2024,4,307,147,160,"158 participants who provided incorrect responses during the attention checks, 2 participants dropped out because of incomplete responses",Online Crowdsourcing,mixed design,"Participants were randomly assigned to either an SDV or human driver condition. They evaluated trust and ability across nine accuracy levels (20% to 99%) presented in random order, plus an attention check block.",Participants evaluated their trust in and perceived ability of either an SDV or a human taxi driver based on provided text descriptions of their driving accuracy.,Unspecified,Autonomous Vehicles,Other: Transportation,Evaluation,Text Evaluation,passive observation,Participants only read text descriptions of the driver's accuracy.,media,The interaction was based solely on text descriptions.,hypothetical,"The robot was only described in text, with no visual representation.",not autonomous,The robot's actions were described but not actually performed.,Questionnaires; Custom Scales,,,Trust was measured using a custom questionnaire with three items.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study directly manipulated the driving accuracy of the SDV and human driver by providing text descriptions of their performance, influencing expectations about their reliability.",Trust in the SDV was significantly lower than trust in the human driver only when the driving accuracy was 99%.,"The study found that trust in SDVs was lower than in human drivers only when the accuracy was very high (99%), which was an unexpected result.",SDVs were less trusted than human drivers when there was a slight possibility of failure (99% accuracy).,Participants read text descriptions of a driver's accuracy and rated their trust in the driver.,Shapiro-Wilk; ANOVA; t-test; Chi-squared,"The Shapiro-Wilk test was used to check the assumption of normality for the dependent variables. A mixed ANOVA was used to analyze the effects of driver type and accuracy level on perceived ability and trust. Welch's t-test was used to compare age differences between conditions, and a chi-square test was used to compare the proportion of men and women between driver-type conditions. The simple main effect of driver type was tested based on the accuracy conditions.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the driving accuracy of the SDV and human driver by providing text descriptions of their performance at nine different accuracy levels (20% to 99%). This directly influenced the perceived reliability of the driver, which is a core aspect of 'Robot-accuracy'. The study found that trust was significantly lower in the SDV than the human driver only at the 99% accuracy level, indicating that the manipulation of 'Robot-accuracy' impacted trust. There were no other factors manipulated in this study.",10.1016/j.trf.2024.03.019,https://linkinghub.elsevier.com/retrieve/pii/S1369847824000640,"Studies have investigated the determinants of trust in self-driving vehicles (SDVs) and confirmed that the ability to execute the driving task flawlessly is essential to promote trust. However, little is known about the extent to which errors decrease trust in SDVs. This study conducted four experiments (N = 2221) and tested whether people’s trust in SDVs was lower than that in human drivers when they made errors without causing negative events. In Experiments 1 and 2, which manipulated the driving accuracy of the drivers, the participants checked nine different pieces of information that showed accuracy. The results demonstrated that the SDV was less trusted than humans only when there was a slight possibility of making an error. This study did not identify factors explaining lower trust in the SDV. Experiments 3 and 4 consisted of participants watching videos showing that the SDV and human driver made minor errors, such as taking a long time to park. This study showed that the minor errors largely reduced trust, regardless of whether the vehicle was self-driven or driven by humans. These findings imply that errors without describing severe accidents are less likely to cause a gap in trust between SDVs and human drivers."
"Yokoi, Ryosuke",Trust in self-driving vehicles is lower than in human drivers when both drive almost perfectly,2024,4,453,218,235,235 participants stopped the slider at incorrect points during the attention check,Online Crowdsourcing,mixed design,"Participants were randomly assigned to one of three driver conditions (SDV, human, or expert). They evaluated trust and ability across nine accuracy levels (20% to 99%) presented in random order, plus an attention check block.","Participants evaluated their trust in and perceived ability of either an SDV, a human taxi driver, or an expert taxi driver based on provided text descriptions of their driving accuracy.",Unspecified,Autonomous Vehicles,Other: Transportation,Evaluation,Text Evaluation,passive observation,Participants only read text descriptions of the driver's accuracy.,media,The interaction was based solely on text descriptions.,hypothetical,"The robot was only described in text, with no visual representation.",not autonomous,The robot's actions were described but not actually performed.,Questionnaires; Custom Scales,,,Trust was measured using a custom questionnaire with three items.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study directly manipulated the driving accuracy of the SDV, human driver, and expert driver by providing text descriptions of their performance, influencing expectations about their reliability.",Trust in the SDV was significantly lower than trust in both the human and expert drivers when the driving accuracy was 99%.,"The study replicated the finding that trust in SDVs was lower than in human drivers at 99% accuracy, but did not find a difference between ordinary and expert human drivers.",SDVs were less trusted than human and expert drivers when there was a slight possibility of failure (99% accuracy).,Participants read text descriptions of a driver's accuracy and rated their trust in the driver.,Shapiro-Wilk; ANOVA; Chi-squared; t-test,The Shapiro-Wilk test was used to check the assumption of normality for the dependent variables. A mixed ANOVA was used to analyze the effects of driver type and accuracy level on perceived ability and trust. A chi-square test was used to check for differences in gender distribution across conditions. A one-way ANOVA was used to check for age differences across conditions. T-tests with Shaffer's Modified Sequentially Rejective Bonferroni Procedure were used as post-hoc tests to compare trust scores between driver types at specific accuracy levels.,TRUE,Robot-accuracy,Robot-accuracy,,"Similar to Study 726, this study manipulated the driving accuracy of the SDV, human driver, and expert driver using text descriptions across nine accuracy levels (20% to 99%). This manipulation directly relates to 'Robot-accuracy' as it changes the perceived performance of the driver. The study found that trust in the SDV was significantly lower than in both human and expert drivers at the 99% accuracy level, showing that 'Robot-accuracy' impacted trust. There were no other factors manipulated in this study.",10.1016/j.trf.2024.03.019,https://linkinghub.elsevier.com/retrieve/pii/S1369847824000640,"Studies have investigated the determinants of trust in self-driving vehicles (SDVs) and confirmed that the ability to execute the driving task flawlessly is essential to promote trust. However, little is known about the extent to which errors decrease trust in SDVs. This study conducted four experiments (N = 2221) and tested whether people’s trust in SDVs was lower than that in human drivers when they made errors without causing negative events. In Experiments 1 and 2, which manipulated the driving accuracy of the drivers, the participants checked nine different pieces of information that showed accuracy. The results demonstrated that the SDV was less trusted than humans only when there was a slight possibility of making an error. This study did not identify factors explaining lower trust in the SDV. Experiments 3 and 4 consisted of participants watching videos showing that the SDV and human driver made minor errors, such as taking a long time to park. This study showed that the minor errors largely reduced trust, regardless of whether the vehicle was self-driven or driven by humans. These findings imply that errors without describing severe accidents are less likely to cause a gap in trust between SDVs and human drivers."
"Yokoi, Ryosuke",Trust in self-driving vehicles is lower than in human drivers when both drive almost perfectly,2024,4,707,401,306,306 participants who failed to answer correctly in the attention checks,Online Crowdsourcing,mixed design,"Participants were randomly assigned to one of three driver conditions (SDV, human, or expert). They watched two videos, one showing good performance and one showing poor performance, and then evaluated trust and ability after each video.","Participants watched videos of a car driving and evaluated their trust in and perceived ability of the driver (SDV, human, or expert) after watching videos of good and poor driving performance.",Unspecified,Autonomous Vehicles,Other: Transportation,Evaluation,Video Analysis,passive observation,Participants passively observed videos of the car driving.,media,The interaction was based on watching videos of the car driving.,simulated,The robot was represented in a simulated video.,not autonomous,The robot's actions were shown in a video but not actually performed autonomously.,Questionnaires; Custom Scales,,,Trust was measured using a custom questionnaire with three items.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study directly manipulated the driving performance by showing videos of good and poor driving, influencing trust through the observed errors.","Minor errors largely decreased trust, irrespective of the driver type.","The study did not find a significant difference in trust between SDVs, human drivers, and expert drivers when minor errors occurred, which was not predicted.","Minor errors largely reduced trust, regardless of whether the vehicle was self-driven or driven by humans.",Participants watched videos of a car driving and rated their trust in the driver.,Shapiro-Wilk; ANOVA; Chi-squared,The Shapiro-Wilk test was used to check the assumption of normality for the dependent variables. A mixed ANOVA was used to analyze the effects of driver type and performance (good vs. poor) on perceived ability and trust. A chi-square test was used to check for differences in gender distribution across conditions. A one-way ANOVA was used to check for age differences across conditions.,TRUE,Robot-accuracy,Robot-accuracy,,"This study manipulated the driving performance by showing videos of good and poor driving. The poor performance video included minor errors, which directly impacts the perceived 'Robot-accuracy'. The study found that the minor errors significantly decreased trust, regardless of the driver type, indicating that the manipulation of 'Robot-accuracy' impacted trust. There were no other factors manipulated in this study.",10.1016/j.trf.2024.03.019,https://linkinghub.elsevier.com/retrieve/pii/S1369847824000640,"Studies have investigated the determinants of trust in self-driving vehicles (SDVs) and confirmed that the ability to execute the driving task flawlessly is essential to promote trust. However, little is known about the extent to which errors decrease trust in SDVs. This study conducted four experiments (N = 2221) and tested whether people’s trust in SDVs was lower than that in human drivers when they made errors without causing negative events. In Experiments 1 and 2, which manipulated the driving accuracy of the drivers, the participants checked nine different pieces of information that showed accuracy. The results demonstrated that the SDV was less trusted than humans only when there was a slight possibility of making an error. This study did not identify factors explaining lower trust in the SDV. Experiments 3 and 4 consisted of participants watching videos showing that the SDV and human driver made minor errors, such as taking a long time to park. This study showed that the minor errors largely reduced trust, regardless of whether the vehicle was self-driven or driven by humans. These findings imply that errors without describing severe accidents are less likely to cause a gap in trust between SDVs and human drivers."
"Yokoi, Ryosuke",Trust in self-driving vehicles is lower than in human drivers when both drive almost perfectly,2024,4,754,416,338,338 participants who provided incorrect responses in the attention check or incompletely answered the questionnaire,Online Crowdsourcing,mixed design,"Participants were randomly assigned to one of three driver conditions (SDV, human, or expert). They watched two videos, one showing good performance and one showing poor performance (with a single minor error), and then evaluated trust and ability after each video.","Participants watched videos of a car driving and evaluated their trust in and perceived ability of the driver (SDV, human, or expert) after watching videos of good and poor driving performance.",Unspecified,Autonomous Vehicles,Other: Transportation,Evaluation,Video Analysis,passive observation,Participants passively observed videos of the car driving.,media,The interaction was based on watching videos of the car driving.,simulated,The robot was represented in a simulated video.,not autonomous,The robot's actions were shown in a video but not actually performed autonomously.,Questionnaires; Custom Scales,,,Trust was measured using a custom questionnaire with three items.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study directly manipulated the driving performance by showing videos of good and poor driving (with a single minor error), influencing trust through the observed errors.","Minor errors had a large negative effect on trust in drivers, irrespective of the driver type.","The study did not find a significant difference in trust between SDVs, human drivers, and expert drivers when a single minor error occurred, which was not predicted.","Minor errors had a large negative effect on trust in drivers, regardless of the driver type.",Participants watched videos of a car driving and rated their trust in the driver.,Shapiro-Wilk; ANOVA; Chi-squared,The Shapiro-Wilk test was used to check the assumption of normality for the dependent variables. A mixed ANOVA was used to analyze the effects of driver type and performance (good vs. poor) on perceived ability and trust. A chi-square test was used to check for differences in gender distribution across conditions. A one-way ANOVA was used to check for age differences across conditions.,TRUE,Robot-accuracy,Robot-accuracy,,"Similar to Study 728, this study manipulated driving performance by showing videos of good and poor driving. The poor performance video included a single minor error, which directly impacts the perceived 'Robot-accuracy'. The study found that the minor error had a large negative effect on trust, irrespective of the driver type, indicating that the manipulation of 'Robot-accuracy' impacted trust. There were no other factors manipulated in this study.",10.1016/j.trf.2024.03.019,https://linkinghub.elsevier.com/retrieve/pii/S1369847824000640,"Studies have investigated the determinants of trust in self-driving vehicles (SDVs) and confirmed that the ability to execute the driving task flawlessly is essential to promote trust. However, little is known about the extent to which errors decrease trust in SDVs. This study conducted four experiments (N = 2221) and tested whether people’s trust in SDVs was lower than that in human drivers when they made errors without causing negative events. In Experiments 1 and 2, which manipulated the driving accuracy of the drivers, the participants checked nine different pieces of information that showed accuracy. The results demonstrated that the SDV was less trusted than humans only when there was a slight possibility of making an error. This study did not identify factors explaining lower trust in the SDV. Experiments 3 and 4 consisted of participants watching videos showing that the SDV and human driver made minor errors, such as taking a long time to park. This study showed that the minor errors largely reduced trust, regardless of whether the vehicle was self-driven or driven by humans. These findings imply that errors without describing severe accidents are less likely to cause a gap in trust between SDVs and human drivers."
"You, Sangseok; Robert Jr., Lionel P.",Human-Robot Similarity and Willingness to Work with a Robotic Co-worker,2018,1,200,200,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of eight conditions based on surface-level similarity (same/different gender), deep-level similarity (same/different work style), and risk of physical danger (high/low). They viewed a video of a robot and a scenario, then completed questionnaires.",Participants imagined working with a robot in a warehouse and answered questions about their attitudes towards working with the robot.,Willow Garage PR2,Mobile Manipulators,Industrial; Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants watched a video of the robot and read a scenario.,media,Participants viewed a video of the robot and a scenario with images.,physical,Participants viewed a video of a physical robot.,not autonomous,The robot's actions were pre-recorded in a video.,Questionnaires,Jian et al. Trust Scale,,Trust was measured using a questionnaire.,"parametric models (e.g., regression)",The study used partial least squares (PLS) approach for data analysis.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's gender was manipulated through voice and name, work style was manipulated by showing the robot's choices, and risk was manipulated by the scenario description.","Surface-level similarity increased trust only in low-risk conditions, while deep-level similarity increased trust regardless of risk. Trust increased intention to work with the robot and willingness to work with a robot over a human co-worker.","The study found that surface-level similarity only increased trust in low-risk situations, while deep-level similarity increased trust regardless of risk. The risk of physical danger moderated the relationship between intention to work with the robot and willingness to work with a robot over a human co-worker.","Deep-level similarity in work style enhances trust in a robotic co-worker regardless of the level of risk, and trust in the robot increases the participant's willingness to work with the robot.",The robot was shown in a video and described as performing tasks in a warehouse. The human participant imagined working with the robot and answered questions about their attitudes.,t-test; Partial least squares; Linear regression,"The study used t-tests to check the success of the manipulations for deep-level similarity and risk of physical danger. Partial least squares (PLS) approach was used for hypothesis testing, examining the relationships between surface-level similarity, deep-level similarity, risk of danger, trust in the robot, intention to work with the robot, and willingness to work with a robot over a human co-worker. Linear regression was used to further analyze the interaction effect between intention to work with the robot and risk of danger on willingness to work with a robot over a human co-worker.",TRUE,Robot-verbal-communication-content; Robot-task-strategy; Task-environment,Robot-task-strategy; Task-environment,Robot-verbal-communication-content,"The study manipulated surface-level similarity by changing the robot's gender through voice and name, which is categorized as 'Robot-verbal-communication-content' because it alters the content of the robot's communication (name and voice). Deep-level similarity was manipulated by showing the robot's choices on work-style questions, which is categorized as 'Robot-task-strategy' because it changes the robot's approach to the task (choosing similar or different answers). The risk of physical danger was manipulated by changing the scenario description and images, which is categorized as 'Task-environment' because it alters the working conditions (high-risk vs. low-risk). The results showed that deep-level similarity ('Robot-task-strategy') and the risk of physical danger ('Task-environment') impacted trust, while surface-level similarity ('Robot-verbal-communication-content') did not impact trust directly, but only in interaction with the risk of physical danger.",10.1145/3171221.3171281,https://dl.acm.org/doi/10.1145/3171221.3171281,"Organizations now face a new challenge of encouraging their employees to work alongside robots. In this paper, we address this problem by investigating the impacts of human–robot similarity, trust in a robot, and the risk of physical danger on individuals’ willingness to work with a robot and their willingness to work with a robot over a human co-worker. We report the results from an online experimental study involving 200 participants. Results showed that human–robot similarity promoted trust in a robot, which led to willingness to work with robots and ultimately willingness to work with a robot over a human co-worker. However, the risk of danger moderated not only the positive link between the surface-level similarity and trust in a robot, but also the link between intention to work with the robot and willingness to work with a robot over a human coworker. We discuss several implications for the theory of human–robot interaction and design of robots."
"You, Sangseok; Robert, Lionel",Trusting Robots in Teams: Examining the Impacts of Trusting Robots on Team Performance and Satisfaction,2019,1,108,108,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were briefed, completed a pre-questionnaire, received task instructions, watched a video, performed experimental manipulations (robot assembly and team uniform), practiced the task, completed the timed task, and then completed a post-questionnaire.",Teams of two humans used two remote-controlled robots to deliver five water bottles from one point to another as fast as possible.,Unspecified,Mobile Robots,Research,Navigation,Remote Manipulation,minimal interaction,Participants remotely controlled robots to complete a task.,real-world,Participants interacted with physical robots in a lab setting.,physical,Participants interacted with physical robots.,wizard of oz (directly controlled),The robots were directly controlled by the human participants.,Questionnaires,Mayer and Davis' Trust/Trustworthiness Scales (1999); Disposition to Trust Questionnaire,,Trust was measured using questionnaires.,"parametric models (e.g., regression)",Regression analysis was used to analyze the relationship between trust and performance.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"Robot identification was manipulated by having participants assemble their robots, and team identification was manipulated by providing team uniforms and names, which was intended to increase trust in robots and teammates.","Robot identification increased trust in robots, and team identification increased trust in teammates. The combination of both increased trust in robots.","Trust in robots increased team performance, while trust in teammates increased satisfaction. There was an interaction effect between robot and team identification on trust in robots, but not on trust in humans. The study found that trust in robots was more important for performance than trust in humans, which is an unexpected result.","Team trust in robots increased team performance, while team trust in humans increased team satisfaction.","The robots moved water bottles between points, and the humans controlled the robots using remote controllers.",ANCOVA; t-test; Linear regression,"The study used ANCOVA to test the main effects of robot and team identification on trust in robots and humans, controlling for disposition to trust. Post-hoc analysis using Student's t-tests were conducted to examine the interaction effects. Regression analysis was used to examine the relationship between trust in robots and humans on team performance and satisfaction, controlling for disposition to trust.",TRUE,Robot-aesthetics; Teaming,Robot-aesthetics; Teaming,,"The study manipulated robot identification by having participants assemble their own robots, which can be considered a manipulation of Robot-aesthetics as it changes the perceived connection and value associated with the robot. The study also manipulated team identification by providing team uniforms and names, which is categorized as Teaming because it changes the perception of team membership and shared identity. The results showed that robot identification (Robot-aesthetics) increased trust in robots, and team identification (Teaming) increased trust in teammates. The interaction of both also increased trust in robots. Therefore, both Robot-aesthetics and Teaming impacted trust. There were no factors that were manipulated that did not impact trust.",10.24251/HICSS.2019.031,http://hdl.handle.net/10125/59465,"Despite the widespread use of robots in teams, there is still much to learn about what facilitates better performance in these teams working with robots. Although trust has been shown to be a strong predictor of performance in all-human teams, we do not fully know if trust plays the same critical role in teams working with robots. This study examines how to facilitate trust and its importance on the performance of teams working with robots. A 2 (robot identification vs. no robot identification) × 2 (team identification vs. no team identification) between-subjects experiment with 54 teams working with robots was conducted. Results indicate that robot identification increased trust in robots and team identification increased trust in one’s teammates. Trust in robots increased team performance while trust in teammates increased satisfaction."
"You, Sangseok; Robert Jr., Lionel P.",Trusting and Working with Robots: A Relational Demography Theory of Preference for Robotic over Human Co-Workers,2024,2,347,347,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of 16 online conditions, read a scenario about collaboration with a robot or human, viewed a video of their co-worker, and then completed a post-task questionnaire.",Participants were asked to imagine working with a co-worker in a warehouse and viewed a video of either a human or robot co-worker performing warehouse tasks.,Willow Garage PR2,Mobile Manipulators,Industrial; Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants observed a video of a robot or human co-worker.,media,Participants watched a video of a robot or human performing a task.,physical,The robot was shown in a video.,pre-programmed (non-adaptive),The robot performed pre-set actions in the video.,Questionnaires; Custom Scales,Jian et al. Trust Scale; Negative Attitude towards Robots Scale (NARS),,Trust was measured using a questionnaire adapted from HRI research.,"parametric models (e.g., regression)",Generalized least squares (GLS) was used to analyze the data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated co-worker type (robot vs. human), gender, work style, and risk of physical danger to influence trust.","Gender dissimilarity had a stronger negative impact on trust in a robot co-worker, while work style dissimilarity had a weaker negative impact on trust in a robot co-worker. Trust in a robot co-worker increased preference for a robot, while trust in a human co-worker decreased it.","The study found that the negative impacts of gender, work style, and personality dissimilarities on swift trust depend on the co-worker type. Specifically, gender dissimilarity had a stronger negative impact on swift trust in a robot co-worker, while work style and personality had a weaker negative impact on swift trust in a robot co-worker.","Ascribed dissimilarity in gender had a stronger negative impact on swift trust in robot co-workers than on human co-workers, while achieved dissimilarities had a weaker negative impact on swift trust in robot co-workers compared to human co-workers.","The robot or human co-worker was shown performing warehouse tasks in a video, while participants completed a questionnaire.",t-test; Generalized least squares,"The study used t-tests to check the success of the work style and risk manipulations. Generalized least squares (GLS) regression was used to analyze the data and test the hypotheses, specifically examining the moderation effects of co-worker type on the relationships between gender dissimilarity, work style dissimilarity, and trust, as well as the relationship between trust and preference for a robot co-worker.",TRUE,Robot-social-attitude; Task-environment,Robot-social-attitude,Task-environment,"The study manipulated co-worker type (robot vs. human) and gender, which are both aspects of social attitude, by changing the video shown to participants and the voice and name of the robot. The study also manipulated the risk of physical danger, which is an aspect of the task environment, by describing the task as involving either toxic materials or wooden boxes. The study found that gender dissimilarity had a stronger negative impact on trust in a robot co-worker, indicating that the manipulation of social attitude impacted trust. The risk of physical danger was included as a control variable and was not found to have a significant impact on trust.",10.25300/MISQ/2023/17403,https://misq.umn.edu/trusting-and-working-with-robots-a-relational-demography-theory-of-preference-for-robotic-over-human-co-workers.html,"Organizations are facing the new challenge of integrating humans and robots into one cohesive workforce. Relational demography theory (RDT) explains the impact of dissimilarities on when and why humans trust and prefer to work with others. This paper proposes RDT as a useful lens to help organizations understand how to integrate humans and robots into a cohesive workforce. We offer a research model based on RDT and examine dissimilarities in gender and co-worker type (human vs. robot) along with dissimilarities in work style and personality. To empirically examine the research model, we conducted two experiments with 347 and 422 warehouse workers, respectively. The results suggest that the negative impacts of gender, work style, and personality dissimilarities on swift trust depend on the co-worker type. In our experiments, gender dissimilarity had a stronger negative impact on swift trust in a robot co-worker, while work style and personality had a weaker negative impact on swift trust in a robot co-worker. Also, swift trust in a robot co-worker increased the preference for a robot co-worker over a human co-worker, while swift trust in a human co-worker decreased such preferences. Overall, this research contributes to our current understanding of human-robot collaboration by identifying the importance of dissimilarity from the perspective of RDT."
"You, Sangseok; Robert Jr., Lionel P.",Trusting and Working with Robots: A Relational Demography Theory of Preference for Robotic over Human Co-Workers,2024,2,422,422,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were randomly assigned to one of 8 online conditions, read a scenario about collaboration with a robot or human, viewed a video of their co-worker, and then completed a post-task questionnaire.",Participants were asked to imagine working with a co-worker in a warehouse and viewed a video of either a human or robot co-worker performing warehouse tasks.,Willow Garage PR2,Mobile Manipulators,Industrial; Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants observed a video of a robot or human co-worker.,media,Participants watched a video of a robot or human performing a task.,physical,The robot was shown in a video.,pre-programmed (non-adaptive),The robot performed pre-set actions in the video.,Questionnaires; Custom Scales,Negative Attitude towards Robots Scale (NARS),,Trust was measured using a questionnaire based on situational normality.,"parametric models (e.g., regression)",Generalized least squares (GLS) was used to analyze the data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated co-worker type (robot vs. human), gender, and personality to influence trust.","Gender dissimilarity had a stronger negative impact on trust in a robot co-worker. Trust in a robot co-worker increased preference for a robot, while trust in a human co-worker decreased it.","The study found that the negative impact of gender dissimilarity on swift trust was stronger for a robot co-worker than for a human co-worker, but the moderation effect was not found for personality dissimilarity. The preference for a robot over a human co-worker was similar to findings in Study 1.","Ascribed dissimilarity in gender had a stronger negative impact on swift trust in robot co-workers than on human co-workers, but the moderation effect was not found for personality dissimilarity.","The robot or human co-worker was shown performing warehouse tasks in a video, while participants completed a questionnaire.",t-test; Generalized least squares,"The study used t-tests to check the success of the gender and personality manipulations, as well as the anthropomorphism manipulation. Generalized least squares (GLS) regression was used to analyze the data and test the hypotheses, specifically examining the moderation effects of co-worker type on the relationships between gender dissimilarity, personality dissimilarity, and trust, as well as the relationship between trust and preference for a robot co-worker.",TRUE,Robot-social-attitude,Robot-social-attitude,,"The study manipulated co-worker type (robot vs. human), gender, and personality, all of which are aspects of social attitude. Co-worker type and gender were manipulated through the video shown to participants and the voice and name of the robot. Personality was manipulated by having the robot agree or disagree with the participant's responses to statements about extraversion. The study found that gender dissimilarity had a stronger negative impact on trust in a robot co-worker, indicating that the manipulation of social attitude impacted trust. The study did not find a significant impact of personality dissimilarity on trust.",10.25300/MISQ/2023/17403,https://misq.umn.edu/trusting-and-working-with-robots-a-relational-demography-theory-of-preference-for-robotic-over-human-co-workers.html,"Organizations are facing the new challenge of integrating humans and robots into one cohesive workforce. Relational demography theory (RDT) explains the impact of dissimilarities on when and why humans trust and prefer to work with others. This paper proposes RDT as a useful lens to help organizations understand how to integrate humans and robots into a cohesive workforce. We offer a research model based on RDT and examine dissimilarities in gender and co-worker type (human vs. robot) along with dissimilarities in work style and personality. To empirically examine the research model, we conducted two experiments with 347 and 422 warehouse workers, respectively. The results suggest that the negative impacts of gender, work style, and personality dissimilarities on swift trust depend on the co-worker type. In our experiments, gender dissimilarity had a stronger negative impact on swift trust in a robot co-worker, while work style and personality had a weaker negative impact on swift trust in a robot co-worker. Also, swift trust in a robot co-worker increased the preference for a robot co-worker over a human co-worker, while swift trust in a human co-worker decreased such preferences. Overall, this research contributes to our current understanding of human-robot collaboration by identifying the importance of dissimilarity from the perspective of RDT."
"Yu, Pian; Dong, Shuyang; Sheng, Shili; Feng, Lu; Kwiatkowska, Marta",Trust-Aware Motion Planning for Human-Robot Collaboration under Distribution Temporal Logic Specifications,2024,1,21,21,0,No participants were excluded,Simulation,within-subjects,"Participants completed a training session, then drove four unique routes (trust-aware and trust-free for two scLDTL specifications) in a driving simulator, periodically recording their trust in the AV.","Participants drove a simulated autonomous vehicle (AV) on pre-defined routes, deciding whether to take over control when the AV encountered incidents.",Unspecified,Autonomous Vehicles,Research,Navigation,Path Following,minimal interaction,Participants interacted with the simulated AV by deciding when to take over control.,simulation,The study used a driving simulator to create an interactive virtual environment.,simulated,The robot was a simulated autonomous vehicle in a driving simulator.,shared control (fixed rules),The AV operated autonomously but the human could take over control based on fixed rules.,Questionnaires; Real-time Trust Measures,Muir's Trust Questionnaire,Performance Metrics; robot data,"Trust was measured using a questionnaire and real-time trust ratings, along with performance and robot data.",POMDP,The study used a POMDP to model human trust as a hidden variable influencing the robot's policy.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the route taken by the AV (trust-aware vs. trust-free) and the task requirements (scLDTL specifications), which influenced the robot's performance and the human's expectations.",The trust-aware policy resulted in higher average human trust and a higher percentage of scLDTL satisfaction compared to the trust-free policy.,"The study observed that repeated successful handling of incidents did not guarantee increased trust, and low trust over time was difficult to recover. Participants also indicated that earlier braking would have increased trust.","The study demonstrated that a trust-aware motion planning approach, using scLDTL specifications and a POMDP model, can improve human trust and task completion in human-robot collaboration.","The robot autonomously navigated a route, handling incidents, while the human monitored the robot and could take over control. The human also periodically reported their trust level.",,No specific statistical tests are mentioned in the paper. The results are presented as percentages of participants successfully completing the scLDTL specifications and average human trust values for trust-aware and trust-free routes. The comparison is descriptive rather than inferential.,TRUE,Robot-autonomy; Robot-task-strategy; Task-complexity,Robot-task-strategy,Robot-autonomy; Task-complexity,"The study manipulated the route taken by the AV, using a trust-aware policy versus a trust-free policy. This is categorized as 'Robot-task-strategy' because it changes the robot's approach to completing the task (navigation) without directly affecting the success rate of the task itself (reaching the destination). The study also manipulated the task requirements through different scLDTL specifications, which influenced the complexity of the task for the robot, and thus is categorized as 'Task-complexity'. The human's ability to take over control of the AV is a manipulation of 'Robot-autonomy', as it changes the level of decision authority between the human and the robot. The results showed that the trust-aware policy (a change in 'Robot-task-strategy') resulted in higher average human trust, indicating that this factor impacted trust. The study also notes that repeated successful handling of incidents did not guarantee increased trust, and low trust over time was difficult to recover, suggesting that the level of autonomy and task complexity did not directly impact trust in the way the researchers expected. The study also notes that earlier braking would have increased trust, which is a change in the robot's task strategy.",10.1109/ICRA57147.2024.10610874,https://ieeexplore.ieee.org/document/10610874/,"Recent work has considered trust-aware decision making for human-robot collaboration (HRC) with a focus on model learning. In this paper, we are interested in enabling the HRC system to complete complex tasks specified using temporal logic formulas that involve human trust. Since accurately observing human trust in robots is challenging, we adopt the widely used partially observable Markov decision process (POMDP) framework for modelling the interactions between humans and robots. To specify the desired behaviour, we propose to use syntactically co-safe linear distribution temporal logic (scLDTL), a logic that is defined over predicates of states as well as belief states of partially observable systems. The incorporation of belief predicates in scLDTL enhances its expressiveness while simultaneously introducing added complexity. This also presents a new challenge as the belief predicates must be evaluated over the continuous (infinite) belief space. To address this challenge, we present an algorithm for solving the optimal policy synthesis problem. First, we enhance the belief MDP (derived by reformulating the POMDP) with a probabilistic labelling function. Then a product belief MDP is constructed between the probabilistically labelled belief MDP and the automaton translation of the scLDTL formula. Finally, we show that the optimal policy can be obtained by leveraging existing point-based value iteration algorithms with essential modifications. Human subject experiments with 21 participants on a driving simulator demonstrate the effectiveness of the proposed approach."
"Yuan, Yue; Wu, Chih-Fu; Niu, Jin; Mao, Limin","The Effects of Human-Robot Interactions and the Human-Robot Relationship on Robot Competence, Trust, and Acceptance",2024,1,60,60,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were assigned to single or multi-user groups. They interacted with a robot under three different relationship conditions (Familiar, Acquaintance, Stranger) in two scenarios (education, companion). After each interaction, participants completed a questionnaire.",Participants engaged in dialogue interactions with a robot in either an education or companion scenario. The robot answered questions related to the scenario.,Nao,Humanoid Robots,Educational; Social; Care; Research,Social,Conversation,minimal interaction,Participants had verbal interactions with the robot in a controlled setting.,real-world,Participants interacted with a physical robot in a lab setting designed to resemble real-world scenarios.,physical,Participants interacted with a physical NAO robot.,wizard of oz (directly controlled),The robot's dialogue was controlled by a human operator.,Questionnaires,,,Trust was measured using a questionnaire with a 7-point Likert scale.,no modeling,No computational model of trust was used; descriptive statistics were used.,Empirical HRI Studies,Wizard-of-Oz Studies,Direct Manipulation,"The researchers manipulated the robot's dialogue content to simulate different human-robot relationships (Familiar, Acquaintance, Stranger) and varied the interaction context (single-user vs. multi-user) to influence trust.","In single-user interactions, a Familiar relationship increased trust. In multi-user interactions, an Acquaintance relationship increased trust in the education scenario, while in the companion scenario, an Acquaintance relationship increased trust and acceptance, but a Familiar relationship increased competence.","The study found that the optimal human-robot relationship for trust and acceptance varied depending on the interaction mode (single vs. multi-user) and the application scenario (education vs. companion). In single-user interactions, a familiar relationship was preferred, while in multi-user interactions, an acquaintance relationship was preferred for trust and acceptance in the education scenario, and for trust and acceptance in the companion scenario, but a familiar relationship was preferred for competence in the companion scenario. There was a conflict in the companion scenario, where the Acquaintance relationship was preferred for trust and acceptance, but the Familiar relationship was preferred for competence.","The study found that in single-user interactions, a Familiar relationship with the robot led to higher trust, competence, and acceptance, while in multi-user interactions, an Acquaintance relationship was preferred for trust and acceptance in the education scenario, and for trust and acceptance in the companion scenario, but a familiar relationship was preferred for competence in the companion scenario.","The robot engaged in scripted dialogues with participants, answering questions related to either an education or companion scenario. Participants listened to the robot's responses and completed questionnaires after each interaction.",cronbach's alpha; ANOVA; simple main effect,"The study used Cronbach's alpha to assess the reliability of the scales used for measuring robot competence, trust, and acceptance. MANOVA was used to examine the main effects of interaction mode and human-robot relationship on the dependent variables (competence, trust, and acceptance). Simple main effect analysis was used to investigate the interaction effects between interaction mode and human-robot relationship on the dependent variables.",TRUE,Robot-verbal-communication-content; Task-environment,Robot-verbal-communication-content,Task-environment,"The study manipulated the robot's verbal communication content by varying the dialogue based on three relationship types: Familiar (task-related content + small talk + anthropomorphic modal words), Acquaintance (task-related content + anthropomorphic modal words), and Stranger (task-related content). This manipulation directly altered the content of the robot's speech, which is why 'Robot-verbal-communication-content' was chosen. The study also manipulated the task environment by having participants interact with the robot in two different scenarios: education and companion. This is a change in the working conditions, which is why 'Task-environment' was chosen. The results showed that the different dialogue content (i.e., the different relationship types) significantly impacted trust, with the Familiar relationship generally leading to higher trust in single-user interactions and the Acquaintance relationship leading to higher trust in multi-user interactions. Therefore, 'Robot-verbal-communication-content' is listed as a factor that impacted trust. While the task environment (education vs. companion) influenced the optimal relationship type for trust, it did not directly impact trust levels themselves, but rather the effectiveness of the communication content. Therefore, 'Task-environment' is listed as a factor that did not directly impact trust.",10.1177/21582440241248230,https://journals.sagepub.com/doi/10.1177/21582440241248230,"As social robots may be used by a single user or multiple users different social scenarios are becoming more important for defining human-robot relationships. Therefore, this study explored human-robot relationships between robots and users in different interaction modes to improve user interaction experience. Specifically, education and companion were selected as the most common areas in the use of social robots. The interaction modes used include single-user interaction and multiuser interaction. The three human-robot relationships were adopted. The robot competence scale, human-robot trust scale, and acceptance of robot scale were used to evaluate subjects’ views on robots. The results demonstrate that in the two scenarios, people were more inclined to maintain a more familiar and closer relationship with the social robot when the robot interacted with a single user. When multiple persons interact in an education scenario, setting the robot to Acquaintance relationships is recommended to improve its competence and people’s trust in the robot. Similarly, in multi-person interaction, Acquaintance relationships would be more accepted and trusted by people in a companion scenario. Based on these results, robot sensors can be added to further optimize human-robot interaction sensing systems. By identifying the number of users in the interaction environment, robots can automatically employ the best human-robot relationship for interaction. Optimizing human-robot interaction sensing systems can also improve robot performance perceived in the interaction to meet different users’ needs and achieve more natural human-robot interaction experiences."
"Yueh, Hsiu-Ping; Lin, Weijane","Services, Appearances and Psychological Factors in Intelligent Home Service Robots",2016,1,267,267,0,No participants were excluded,Survey/Interview,,"Participants completed a questionnaire with six parts, including personal information, service requirements, appearance preferences, HRI factors (likability, sincerity, trust, privacy), and future acceptance of robots.",Participants answered questions about their preferences and expectations regarding home service robots.,Unspecified,Service and Assistive Robots,Care; Social; Other,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the robot in the questionnaire.,media,The interaction was based on text descriptions of robots and scenarios.,hypothetical,The robots were described in text without any visual representation.,not autonomous,The robot's actions were hypothetical and described in text.,Questionnaires,,,Trust was measured using a questionnaire.,no modeling,Trust was not modeled computationally.,Observational & Survey Studies,Quantitative Surveys,No Manipulation,The study assessed trust without intentionally altering any specific factors.,,"The study found that participants valued practical functions of home service robots more than social functions. Female participants emphasized house chores, security, and fixing things more than male participants. Participants preferred robots with arms and hands, but without facial features.","Participants viewed trust and privacy as the most important aspects for interacting with home service robots, followed by likability and sincerity.","Participants completed a questionnaire about their preferences for home service robots, including their desired functions, appearance, and psychological factors.",t-test; ANOVA; LSD post-hoc; chi-square analysis; Pearson correlation,"The study used t-tests to compare means between male and female participants on various service and appearance preferences. ANOVA was used to compare means across different buying tendency groups, with LSD post-hoc tests for significant ANOVA results. Chi-square analysis was used to examine the proportion differences in body structure preferences. Correlation analysis was used to examine the relationships between the HRI dimensions (likability, sincerity, trust, and privacy).",FALSE,,,,"The study did not manipulate any factors. It was a survey-based study where participants expressed their preferences and expectations regarding home service robots. There were no intentional changes made to any robot characteristics or task parameters to observe their impact on trust. The study focused on assessing existing attitudes and preferences rather than manipulating variables to measure their effect on trust. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust due to manipulation.",,http://link.springer.com/10.1007/978-3-319-40093-8_60,"This study conducted a questionnaire survey to investigate the requirements and preferences toward the services and appearance of home service robot. And the psychological factors regarding human robot interaction, including likability, sincerity, trust and privacy were explored and discussed. The preliminary results show that functions of house chores, security and emergency detecting were highly expected and preferred by 267 participants. While the social and caretaking/nursing functions were not preferred as expected. The participants liked medium size robots without too many facial features. Hands and arms that extended human reach were more valued than legs and wheels that substitute human movement. Participants also valued the interface of response and information, such as the body screen on the robot. Psychological factors of likability, sincerity, trust and privacy correlated with each other signiﬁcantly, and were also related to the preference of robot appearance. This study takes a clear proﬁle of the users’ expectation on future home intelligent service robots."
"Zafari, Setareh; Schwaninger, Isabel; Hirschmanner, Matthias; Schmidbauer, Christina; Weiss, Astrid; Koeszegi, Sabine T.",“You Are Doing so Great!” – The Effect of a Robot’s Interaction Style on Self-Efficacy in HRI,2019,1,31,29,2,2 participants did not pass the manipulation check,Controlled Lab Environment,between-subjects,"Participants were introduced to the robot, completed a personality questionnaire, and then performed a house of cards building task with the robot providing verbal feedback based on the assigned condition. After the task, participants completed a post-experiment questionnaire and engaged in an informal conversation with researchers.",Participants were asked to build a house of cards while interacting with a robot that provided verbal feedback.,Pepper,Humanoid Robots; Expressive Robots,Social; Research,Game,Cooperative Game,minimal interaction,Participants interacted with the robot through verbal feedback during a task.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,Participants interacted with a physical robot.,wizard of oz (directly controlled),The robot's verbal responses were triggered by a researcher based on predefined events.,Questionnaires,,,Trust was assessed using questionnaires.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's interaction style (task-oriented, person-oriented, or neutral) was manipulated through verbal feedback, which was intended to influence self-efficacy and perception of the robot.","The person-oriented interaction style led to higher self-efficacy, perceived agreeableness, and less frustration, suggesting a positive impact on trust.","Participants in the person-oriented condition reported higher self-efficacy, perceived agreeableness, and less frustration compared to the task-oriented condition. The person-oriented condition also resulted in longer task engagement and higher enjoyment compared to the neutral condition.","A person-oriented interaction style from a robot can increase self-efficacy in HRI, perceived agreeableness of the robot, and task engagement, which are important for establishing trust.","The robot provided verbal feedback to the human participant while they were building a house of cards. The human participant was tasked with building the house of cards, and the robot's feedback varied based on the experimental condition.",spearman's ρ; Kruskal-Wallis; Dunn test,"The study used Spearman's rank correlation to assess the relationship between personality traits and self-efficacy, robot perception, and task engagement. Kruskal-Wallis tests were used to investigate the effect of the robot's interaction style on self-efficacy, perceived personality traits of the robot, task engagement duration, number of cards used, enjoyment, frustration, and distraction. Dunn's test with Bonferroni correction was used as a post-hoc test to determine significant differences between the conditions for the Kruskal-Wallis tests.",TRUE,Robot-verbal-communication-content; Robot-verbal-communication-style,Robot-verbal-communication-content; Robot-verbal-communication-style,,"The study manipulated the robot's interaction style through verbal feedback, which was categorized into three conditions: task-oriented, person-oriented, and neutral. The content of the verbal feedback varied across these conditions, with the task-oriented condition focusing on task performance and guidance, the person-oriented condition focusing on socioemotional support and motivation, and the neutral condition providing only game instructions. This manipulation directly affects 'Robot-verbal-communication-content' as the specific words and phrases used differed across conditions. Additionally, the manner in which the robot communicated, such as the tone and encouragement level, also varied, which falls under 'Robot-verbal-communication-style'. The paper explicitly states that the person-oriented interaction style led to higher self-efficacy, perceived agreeableness, and less frustration, suggesting a positive impact on trust. The task-oriented style also had an impact on the perceived role of the robot. The neutral condition was used as a baseline and had a lower impact on trust-related outcomes. Therefore, both 'Robot-verbal-communication-content' and 'Robot-verbal-communication-style' were found to impact trust, while no factors were found to not impact trust.",10.1109/RO-MAN46459.2019.8956437,https://ieeexplore.ieee.org/document/8956437/,"People form mental models about robots’ behavior and intention as they interact with them. The aim of this paper is to evaluate the effect of different interaction styles on self-efﬁcacy in human-robot interaction (HRI), people’s perception of the robot, and task engagement. We conducted a user study in which a social robot assists people verbally while building a house of cards. Data from our experimental study revealed that people engaged longer in the task while interacting with a robot that provides person related feedback than with a robot that gives no person or task related feedback. Moreover, people interacting with a robot with a personoriented interaction style reported a higher self-efﬁcacy in HRI, perceived higher agreeableness of the robot and found the interaction less frustrating, as compared to a robot with a task-oriented interaction style. This suggests that a robot’s interaction style can be considered as a key factor for increasing people’s perceived self-efﬁcacy in HRI, which is essential for establishing trust and enabling Human-robot collaboration."
"Zafrani, Oded; Nimrod, Galit; Edan, Yael",Between fear and trust: Older adults’ evaluation of socially assistive robots,2023,1,1134,384,750,750 participants did not complete the survey or did not meet the age criteria,Online Crowdsourcing,between-subjects,Participants completed an online survey after watching a video of the robot Gymmy.,Participants were asked to evaluate the robot Gymmy based on a video presentation.,Unspecified,Humanoid Robots; Service and Assistive Robots,Care; Social; Educational,Evaluation,Survey/Questionnaire Completion,passive observation,Participants watched a video of the robot.,media,Participants viewed a video of the robot interacting with older adults.,physical,The robot was shown in a video.,pre-programmed (non-adaptive),The robot's actions in the video were pre-programmed.,Questionnaires; Custom Scales,Human-Robot Trust Scale/Questionnaire (HRTS/HRITS),,Trust was measured using a questionnaire.,"parametric models (e.g., regression)",Linear regression was used to model the relationship between trust and quality evaluation.,Empirical HRI Studies,Virtual/Simulated Studies,No Manipulation,"The study did not manipulate any factors related to trust, but measured trust and technophobia.",,"The study found that technophobia had a stronger negative impact on quality evaluation than trust had a positive impact. Older women, older participants, and less educated participants had higher quality evaluations of the robot.",Technophobia has a stronger negative impact on older adults' quality evaluation of social assistive robots than trust has a positive impact.,"The robot, Gymmy, demonstrates physical exercises and provides cognitive training. The human participant watches a video of the robot and then completes a survey.",t-test; Pearson correlation; t-test; ANOVA; Linear regression,"The study used paired sample t-tests to compare pragmatic and hedonic evaluations, and to compare trust in performance and social aspects. Pearson correlation coefficients were used to assess relationships between background variables and quality evaluations, as well as between trust, technophobia, and quality evaluations. Independent samples t-tests and ANOVAs were used to examine differences in quality evaluations based on nominal variables. Linear regression models were used to examine the relationships between trust, technophobia, background variables, and the three quality evaluation dimensions (pragmatic, hedonic, and attractiveness), both separately and simultaneously.",FALSE,,,,"The study did not manipulate any factors related to the robot or the task. Participants watched a video of the robot and then completed a survey. The study measured trust and technophobia but did not manipulate any variables to observe their effect on trust. Therefore, no factors were manipulated, and consequently, no factors impacted or did not impact trust due to manipulation.",10.1016/j.ijhcs.2022.102981,https://linkinghub.elsevier.com/retrieve/pii/S1071581922001999,"Socially Assistive Robots (SARs) are expected to support autonomy, aging in place, and wellbeing in later life. Acceptance and successful assimilation of SARs among older adults depend ontheir Quality Evaluations (QEs), namely, the pragmatic and hedonic evaluations and overall attractiveness. Previous studies showed that trust in robots significantly enhances QE, while technophobia considerably decreases it. However, so far, these key factors have always been explored separately. Applying a case study approach, the current research aimed to examine the relative impact of these two factors on older persons’ QE of SARs. The study was based on an online survey of 384 individuals aged 65 and above. Respondents were presented with a video of Gymmy, a robotic system for physical and cognitive training, and filled out a questionnaire relating to that system. The results indicated a positive association between trust and QE and a negative association between technophobia and QE. A simultaneous exploration demonstrated that the relative impact of technophobia was significantly more substantial than that of trust. In addition, the pragmatic qualities of the robot were found to be more crucial to its QE than the social aspects of use. The findings suggest that implementing robotics technology in later life strongly depends on reducing older adults’ technophobia regarding the convenience of using SARs. The study also highlights the importance of simultaneous explorations of facilitators and inhibitors to SARs use."
"Zahabi, Maryam; Razak, Ashiq Mohammed Abdul; Mehta, Ranjana K.; Manser, Michael","Effect of advanced driver-assistance system trainings on driver workload, knowledge, and trust",2021,1,20,20,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants were randomly assigned to either demonstration-based or video-based training. They then completed a driving simulation with manual and automated segments, reporting workload and trust. Knowledge was assessed before and after the experiment. HRV was also measured.","Participants drove a simulated vehicle in both manual and automated driving conditions, using ADAS features (ACC and LKAS) in the automated segments.",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with a driving simulator and ADAS features, but there was no physical interaction with a robot.",simulation,Participants used a driving simulator to interact with the ADAS features.,simulated,The robot was a simulated vehicle with ADAS features within a driving simulator.,shared control (fixed rules),"The ADAS system controlled steering and acceleration/braking, but the driver monitored and could take over control.",Questionnaires,,Physiological Signals,Trust was measured using a subjective rating scale and physiological measures (HRV).,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The training method (video-based vs. demonstration-based) was manipulated to influence drivers' mental workload, trust, and knowledge of ADAS. The training was intended to influence the drivers' expectations and understanding of the system.","There was no significant difference in trust between the two training groups, but trust increased with experience. The training method did not significantly impact trust.","The study found that demonstration-based training reduced cognitive load, while video-based training was more effective for females. There was no significant difference in trust between the training groups, but trust increased with experience. The study also found that males had higher workload under video-based training.","Demonstration-based training reduced cognitive load while driving, and video-based training was more effective for females in reducing their workload.","The robot (simulated vehicle) provided automated driving assistance (ACC and LKAS). The human participant drove the vehicle in both manual and automated modes, monitored the ADAS, and completed surveys.",ANOVA,"The study used ANOVA to analyze the effects of gender, training type (demonstration-based vs. video-based), driving condition (manual vs. automated), and trial number on several dependent variables: perceived mental workload (RSME), heart rate variability (RMSSD), trust in automation, and knowledge improvement. The ANOVA was used to determine if there were significant main effects or interactions between these factors on the dependent variables. The study also mentions using Box Cox transformation or ranking data for non-parametric analysis if ANOVA assumptions were violated, but ultimately reports results from untransformed data when non-parametric results were similar.",TRUE,Task-complexity; Robot-autonomy,Robot-autonomy,Task-complexity,"The study manipulated the training method (video-based vs. demonstration-based), which influenced the drivers' mental workload and their interaction with the ADAS features. This is categorized as 'Task-complexity' because the training method directly impacted the cognitive demands placed on the participants while driving. The study also manipulated the driving condition (manual vs. automated), which is categorized as 'Robot-autonomy' because it changed the level of control the ADAS system had over the vehicle. The study found that trust increased with experience with the ADAS, which is related to the 'Robot-autonomy' factor, as the level of automation was a key component of the experience. The training method, which is related to 'Task-complexity', did not significantly impact trust.",10.1016/j.trf.2020.12.003,https://linkinghub.elsevier.com/retrieve/pii/S136984782030588X,"Older adults are more likely to get severely injured or die in vehicle crashes. Advanced driver-assistance systems (ADAS) can reduce their risk of crashes; however, due to the lack of knowledge and training, usage rate of these systems among older drivers is limited. The objective of this study was to evaluate the impact of two ADAS training approaches (i.e., video-based and demonstration-based training) on older drivers’ subjective and objective measures of mental workload, knowledge and trust considering drivers’ demographic information. Twenty older adults, balanced by gender, participated in a driving simulation study. Results indicated that the video-based training might be more effective for females in reducing their mental workload while driving, whereas the demonstration-based training could be more beneﬁcial for males. There was no signiﬁcant difference between the video-based and demonstration-based trainings in terms of drivers’ trust and knowledge of automation. The ﬁndings suggested that ADAS training protocols can potentially be more effective if they are tailored to speciﬁc driver demographics."
"Zahedi, Zahra; Verma, Mudit; Sreedharan, Sarath; Kambhampati, Subbarao",Trust-Aware Planning: Modeling Trust Evolution in Longitudinal Human-Robot Interaction,2021,1,62,62,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were assigned to one of three conditions: Trust-Aware, Always Explicable, or Random Policy. They completed 10 rounds of a task where they could choose to monitor a robot or perform another task. At the end of each round, they completed a trust questionnaire.","Participants acted as supervisors, monitoring a robot performing tasks on a grid map and could choose to stop the robot if they saw an invalid plan. They could also choose to perform another task (labeling images) instead of monitoring the robot.",Unspecified,Mobile Robots,Research,Supervision,Monitoring,minimal interaction,Participants observed the robot's plan execution on a screen and could choose to stop it.,simulation,The interaction was presented through a simulation of the robot's actions on a grid map.,simulated,The robot was represented as a simulated entity on a grid map.,shared control (fixed rules),"The robot followed a pre-determined plan, either explicable or optimal, based on the experimental condition.",Questionnaires,Muir's Trust Questionnaire,Performance Metrics,Trust was measured using the Muir questionnaire and performance metrics were collected.,POMDP,"The robot's decision-making was modeled using a POMDP framework, incorporating human trust as a latent variable.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's behavior was manipulated by choosing between explicable and optimal plans, and participants were given different expectations about the robot's behavior based on the experimental condition, influencing their trust.","The trust-aware condition led to higher trust levels compared to the random policy condition, and the trust-aware policy resulted in better team performance than the always explicable condition.","The trust-aware policy showed a trade-off between trust and performance, with trust increasing more slowly than the always explicable condition but achieving better overall team performance. There was no significant difference in trust increase between the Trust-Aware and Always Explicable conditions over time.","A trust-aware policy, where the robot adapts its behavior based on the human's trust level, leads to better team performance compared to always using explicable behavior.","The robot performed tasks on a grid map, such as moving to a location or bringing coffee, while the human participant acted as a supervisor, monitoring the robot's actions and choosing to stop the robot if they saw an invalid plan or to perform another task instead of monitoring.",t-test; ANOVA,"The study used a one-tailed t-test for independent means to compare the team performance cost between the trust-aware condition and the always explicable condition, and to compare the mean trust values between the trust-aware condition and the random policy condition. Additionally, a Mixed ANOVA test was used to examine the interaction between time (round) and condition on trust, and planned comparisons with paired sample t-tests were used to assess trust changes within each condition from round 1 to round 10. The t-tests were used to test the hypotheses that the trust-aware condition would result in better team performance and higher trust levels than the other conditions. The Mixed ANOVA was used to examine the evolution of trust over time across the different conditions.",TRUE,Robot-task-strategy; Robot-autonomy,Robot-task-strategy,Robot-autonomy,"The study manipulated the robot's task strategy by having it follow either an explicable plan, an optimal plan, or a trust-aware plan that switched between the two based on the human's trust level. This is described in the 'Human Subject Experiment' section where it states 'Depending on the condition the participant belonged to, they are either shown an action selected by a policy calculated from our method (for Trust-aware condition), or an explicable plan (for Always explicable condition) or is randomly shown either the optimal or explicable plan with an equal probability (for Random Policy condition)'. The robot's autonomy was also implicitly manipulated by the different conditions, as the trust-aware condition allowed the robot to make decisions based on the human's trust level, while the other conditions had the robot follow a fixed strategy. However, the study's results indicate that the change in task strategy (explicable vs optimal vs trust-aware) had a significant impact on trust, as evidenced by the higher trust levels in the trust-aware condition compared to the random policy condition, and the better team performance compared to the always explicable condition. The study did not find a significant difference in trust increase between the Trust-Aware and Always Explicable conditions over time, suggesting that the manipulation of autonomy through the trust-aware policy did not have a significant impact on trust levels compared to the always explicable condition. The robot's autonomy was not directly manipulated as a factor that impacted trust, but rather as a mechanism to implement the different task strategies. The core manipulation was the robot's task strategy, which directly influenced the human's trust.",,,"Trust between team members is an essential requirement for any successful cooperation. Thus, engendering and maintaining the fellow team members’ trust becomes a central responsibility for any member trying to not only successfully participate in the task but to ensure the team achieves its goals. The problem of trust management is particularly challenging in mixed human-robot teams where the human and the robot may have different models about the task at hand and thus may have different expectations regarding the current course of action and forcing the robot to focus on the costly explicable behavior. We propose a computational model for capturing and modulating trust in such longitudinal human-robot interaction, where the human adopts a supervisory role. In our model, the robot integrates human’s trust and their expectations from the robot into its planning process to build and maintain trust over the interaction horizon. By establishing the required level of trust, the robot can focus on maximizing the team goal by eschewing explicit explanatory or explicable behavior without worrying about the human supervisor monitoring and intervening to stop behaviors they may not necessarily understand. We model this reasoning about trust levels as a meta reasoning process over individual planning tasks. We additionally validate our model through a human subject experiment."
"Zahedi, Zahra; Sreedharan, Sarath; Verma, Mudit; Kambhampati, Subbarao",Modeling the Interplay between Human Trust and Monitoring,2022,1,62,62,5,5 outliers were removed,Online Crowdsourcing,within-subjects,"Participants acted as supervisors of a robot worker, deciding whether to monitor the robot in each of 10 rounds. They were given points for the robot's success and for completing an extra task, with penalties for robot failure when not monitoring. Trust was measured after each round using a four-item scale, and the next task was assigned based on the trust level.","Participants decided whether to monitor a robot performing tasks, with the option to perform an extra task for points, balancing potential rewards and penalties.",Unspecified,Mobile Robots,Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the robot through a screen, making decisions about monitoring.",simulation,The interaction was presented through a simulated environment on a screen.,simulated,The robot was represented as a simulation on a screen.,pre-programmed (non-adaptive),The robot performed pre-programmed tasks without adapting to the user's actions.,Questionnaires,Muir's Trust Questionnaire,Performance Metrics,Trust was measured using a four-item questionnaire and performance metrics were collected.,"parametric models (e.g., regression)",The relationship between trust and monitoring was modeled using a categorical distribution and Binomial regression.,Empirical HRI Studies,Virtual/Simulated Studies,Indirect Manipulation,"The task difficulty was varied based on the participant's trust level, and participants were given a point system that influenced their expectations of the robot's performance.","The study found a negative correlation between trust and monitoring, with higher trust leading to less monitoring.","The study found a negative correlation between trust and monitoring, which is consistent with the idea that higher trust leads to less monitoring. The discretized trust model performed slightly better than the Binomial regression model based on log-likelihood.","The study found a negative correlation between human trust and monitoring behavior, with higher trust leading to a lower likelihood of monitoring.","The robot performed pre-programmed tasks on a grid map, and the human decided whether to monitor the robot's actions or perform an extra task for points.",point biserial correlation; Dirichlet distribution; Negative binomial regression,"The study used a Point Biserial Correlation test to measure the relationship between the continuous trust variable and the binary monitoring variable. Additionally, a categorical distribution with a Dirichlet prior was used to model the probability of monitoring at different discretized trust levels. Finally, a Binomial regression model was used to model the relationship between continuous trust values and the probability of monitoring.",TRUE,Task-complexity,Task-complexity,,"The study manipulated task complexity by assigning different tasks based on the participant's trust level. As stated in the paper, 'The participants are shown different tasks based on the trust level...The tasks include reaching a specific point in the map, bringing coffee to a room, or bringing coffee from a room to another room'. This variation in task difficulty based on trust level is a manipulation of 'Task-complexity'. The paper also states that 'Given their raw trust value in that round, their trust is mapped to one of the four trust levels, and based on the level they are shown the next task.' This indicates that the task complexity was directly influenced by the trust level, and the study found a negative correlation between trust and monitoring, implying that the task complexity, which was dependent on trust, impacted the monitoring behavior. Therefore, 'Task-complexity' is also listed as a factor that impacted trust. There were no other factors manipulated in the study.",10.1109/HRI53351.2022.9889475,https://ieeexplore.ieee.org/document/9889475/,"In this work, we investigate and model how human trust affects monitoring. We present a web-based human subject study in which the robot is a worker and the human plays the role of a supervisor. First, we evaluate the correlation between the human trust and monitoring by using statistical tests, and then we learn probabilistic models of the behavioral data collected through our user studies. These models can provide us with the likelihood of a human user monitoring a system given their level of trust. Such models can be leveraged in many systems including the ones designed to be resilient to automation bias and complacency."
"Zahedi, Zahra; Sreedharan, Sarath; Kambhampati, Subbarao",A Mental Model Based Theory of Trust,2023,1,42,42,0,No participants were excluded,Online Crowdsourcing,between-subjects,"Participants were shown a robot task with possible settings, completed a trust questionnaire, received information eliminating some settings, and then completed the questionnaire again.",Participants evaluated a robot's ability to complete a task in different scenarios and completed a trust questionnaire.,Unspecified,Other,Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants only read about the robot and interaction scenarios.,media,Participants were shown static images of the robot and task scenarios.,simulated,The robot was represented through images.,not autonomous,The robot's actions were described but not performed in the study.,Questionnaires,Muir's Trust Questionnaire,,Trust was measured using a questionnaire.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,Participants' beliefs about the robot's capability were manipulated by providing information that either increased or decreased the likelihood of the robot achieving the task.,"Trust increased when the likelihood of the robot satisfying the contract increased, and decreased when the likelihood decreased.","The predictability component of trust did not show a statistically significant change in the positive update group, and the decrease in likelihood did not reduce total trust in the negative update group.","Changes in the likelihood of a robot satisfying a contract, as predicted by the mental model theory, resulted in corresponding changes in trust as measured by a questionnaire.","The robot was described as performing a task, and the human participant evaluated the robot's ability to complete the task based on provided information.",t-test,"The study used t-tests to compare the means of trust scores between different groups and within groups before and after experimental manipulations. Specifically, two-tailed t-tests were used to compare the change in trust between the positive and negative update groups, and one-tailed t-tests were used to examine the change in trust within each group (positive and negative) after the manipulation. The tests were performed on the total trust score and its components (competence, predictability, reliability, faith, and overall trust) to assess the impact of manipulating the likelihood of the robot satisfying a contract on the participants' trust.",TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the information provided to participants about the robot's capabilities and the task environment. Specifically, participants were initially presented with multiple possible scenarios, and then received information that either increased or decreased the likelihood of the robot being able to successfully complete the task. This manipulation of information directly influenced the participants' beliefs about the robot's ability to satisfy the contract (i.e., complete the task successfully), which in turn impacted their trust. The information provided was verbal (text-based) and directly changed the content of what the participants knew about the robot's capabilities. Therefore, 'Robot-verbal-communication-content' is the most appropriate category. The study found that changes in the likelihood of the robot satisfying the contract, which was manipulated through the information provided, resulted in corresponding changes in trust. The predictability component of trust did not show a statistically significant change in the positive update group, and the decrease in likelihood did not reduce total trust in the negative update group, but this does not mean that the manipulation did not impact trust, only that the impact was not uniform across all trust components. Therefore, 'Robot-verbal-communication-content' is the only factor that impacted trust. No other factors were manipulated.",,http://arxiv.org/abs/2301.12569,"Handling trust is one of the core requirements for facilitating effective interaction between the human and the AI agent. Thus, any decision-making framework designed to work with humans must possess the ability to estimate and leverage human trust. In this paper, we propose a mental model based theory of trust that not only can be used to infer trust, thus providing an alternative to psychological or behavioral trust inference methods, but also can be used as a foundation for any trust-aware decision-making frameworks. First, we introduce what trust means according to our theory and then use the theory to deﬁne trust evolution, human reliance and decision making, and a formalization of the appropriate level of trust in the agent. Using human subject studies, we compare our theory against one of the most common trust scales (Muir scale) to evaluate 1) whether the observations from the human studies match our proposed theory and 2) what aspects of trust are more aligned with our proposed theory."
"Zahedi, Zahra; Verma, Mudit; Sreedharan, Sarath; Kambhampati, Subbarao",Trust-Aware Planning: Modeling Trust Evolution in Iterated Human-Robot Interaction,2023,1,79,79,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were assigned to one of four conditions: Trust-Aware, Always Explicable, Random Policy, or Always Optimal. They completed 10 rounds of a task where they supervised a robot, choosing to monitor or perform another task. They were awarded points for successful robot task completion and lost points for robot failures. At the end of each round, they completed a trust questionnaire.","Participants supervised a robot performing tasks on a grid map, choosing to monitor the robot's actions or perform another task to earn points.",Unspecified,Mobile Robots,Research,Supervision,Monitoring,minimal interaction,Participants observed the robot's actions on a screen and could choose to intervene.,simulation,The interaction was presented through a simulated environment on a screen.,simulated,The robot was represented as a visual element within the simulation.,shared control (fixed rules),"The robot followed a pre-determined policy, choosing between explicable and optimal plans based on the experimental condition.",Questionnaires,Muir's Trust Questionnaire,,Trust was measured using the Muir questionnaire at the end of each round.,no modeling,Trust was measured but not modeled computationally in the user study.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot's behavior was manipulated by choosing between explicable and optimal plans, which influenced task performance and human expectations. The task difficulty was also indirectly manipulated by the robot's plan choice.","The trust-aware condition resulted in higher trust levels compared to the random and always optimal conditions. The always explicable condition resulted in the highest trust levels, but the trust-aware condition was more cost-efficient.","The trust-aware policy resulted in a significant improvement in team performance compared to the always explicable and random policies. The trust-aware policy also resulted in higher trust than the random and always optimal policies, although not as high as the always explicable policy. There was no significant difference in trust evolution between the trust-aware and always explicable conditions over time.","The trust-aware policy resulted in a more efficient system than one that always engages in explicable behavior, while also engendering higher trust than random or always optimal policies.","The robot performed tasks on a grid map, such as moving to a location or bringing coffee. The human participant acted as a supervisor, choosing to monitor the robot's actions or perform another task to earn points. If they monitored, they could stop the robot if they thought it was doing something wrong.",t-test; ANOVA,"The study used a one-tailed t-test for independent means to compare the total cost between the trust-aware condition and the always explicable condition, and to compare the mean trust values between the trust-aware condition and the random policy and always optimal conditions. A Mixed ANOVA test was used to check trust evolution over time and to determine the validity of the second hypothesis. Planned comparisons with paired sample t-tests were also used to compare trust increases between round 1 and round 10 within the Trust-Aware and Always Explicable conditions.",TRUE,Robot-task-strategy; Robot-autonomy,Robot-task-strategy,Robot-autonomy,"The study manipulated the robot's task strategy by having it choose between different plans: a perfectly explicable plan, a balanced explicable plan, or an optimal plan. This is described in the 'BASE DECISION-MAKING PROBLEM' section where the robot chooses between three different plans with varying explicability and cost. The 'Human Subject Experiment' section further clarifies that the robot's behavior was manipulated by choosing between explicable and optimal plans, which influenced task performance and human expectations. This directly relates to 'Robot-task-strategy' because the manipulation is about the robot's approach to completing the task, not its success rate. The robot's autonomy was also manipulated by having it follow a pre-determined policy based on the experimental condition, as described in the 'experimental_procedure' section of the existing data, where the robot chooses between explicable and optimal plans based on the experimental condition. This is a manipulation of 'Robot-autonomy' because it changes the robot's decision-making process. The study found that the robot's task strategy (whether it chose explicable or optimal plans) significantly impacted trust levels, as evidenced by the higher trust in the trust-aware and always explicable conditions compared to the random and always optimal conditions. This is described in the 'Results' section where it states that trust improves more rapidly when the robot executes a trust-aware policy. The study did not find that the manipulation of the robot's autonomy had a direct impact on trust levels, as the different conditions were primarily about the robot's task strategy, not the level of autonomy itself. The robot's autonomy was fixed within each condition, and the manipulation was about the plan choice, not the level of control the robot had.",10.1145/3568162.3578628,https://dl.acm.org/doi/10.1145/3568162.3578628,"Trust between team members is an essential requirement for any successful cooperation. Thus, engendering and maintaining the fellow team members’ trust becomes a central responsibility for any member trying to not only successfully participate in the task but to ensure the team achieves its goals. The problem of trust management is particularly challenging in mixed human-robot teams where the human and the robot may have diferent models about the task at hand and thus may have diferent expectations regarding the current course of action, thereby forcing the robot to focus on the costly explicable behavior. We propose a computational model for capturing and modulating trust in such iterated humanrobot interaction settings, where the human adopts a supervisory role. In our model, the robot integrates human’s trust and their expectations about the robot into its planning process to build and maintain trust over the interaction horizon. By establishing the required level of trust, the robot can focus on maximizing the team goal by eschewing explicit explanatory or explicable behavior without worrying about the human supervisor monitoring and intervening to stop behaviors they may not necessarily understand. We model this reasoning about trust levels as a meta reasoning process over individual planning tasks. We additionally validate our model through a human subject experiment."
"Zanjanpour, Darya; Kokate, Sana; Liu, Hugh H.T.; Plaks, Jason E.",Quantifying Trust in Human-Robot Interaction for Advanced Air Mobility Systems,2024,1,40,40,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed 30 trials of a drone navigation task in a simulation, with the option to switch between automatic and manual control modes. They were given three demo runs before the main data collection phase. Data was collected on interaction points, location, actions, distances to obstacles, and the tracked trajectory.","Participants navigated a simulated drone from a start point to an end point, avoiding obstacles, with the option to switch between automatic and manual control modes.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Navigation,Path Following,minimal interaction,Participants interacted with a simulated drone through a computer interface.,simulation,The interaction took place in a simplified 2D simulation environment.,simulated,The robot was a simulated drone in a 2D environment.,shared control (fixed rules),"The drone could operate in automatic mode following a pre-programmed path, or in manual mode controlled by the participant.",Behavioral Measures; Performance-Based Measures,,"Performance Metrics; robot data (sensor data, etc.)",Trust was assessed by measuring the convergence of the actual flight path to the optimal path and the frequency of mode switching.,"parametric models (e.g., regression)",Pearson correlation was used to quantify the relationship between the number of runs and the deviation from the optimal path.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the drone's control mode (automatic vs. manual) and the presence of obstacles, influencing the perceived risk and the need for manual control.","Participants' trust in the automatic mode increased over time, as evidenced by the convergence of their flight paths towards the optimal path and a decrease in manual mode usage.","The study found a negative correlation between the number of runs and the deviation from the optimal trajectory, indicating an increase in trust over time. The study also noted that crash runs needed to be calibrated to be comparable to successful runs.","The study demonstrated a method for quantitatively measuring rational trust in an autonomous system, showing that users' flight paths converged towards the optimal path over repeated trials, indicating increasing trust.","The human participant navigated a simulated drone from a start point to an end point, avoiding obstacles, with the option to switch between automatic and manual control modes. The robot (simulated drone) followed a pre-programmed path in automatic mode or followed waypoints set by the human in manual mode.",Pearson correlation,"Pearson correlation was used to quantify the relationship between the number of runs and the deviation from the optimal path (measured as σ'). The goal was to determine if there was a negative correlation, indicating that participants' flight paths converged towards the optimal path over repeated trials, which would suggest increasing trust in the system. The p-value was also calculated to validate the statistical significance of the correlation.",TRUE,Robot-autonomy; Task-complexity; Task-environment,Robot-autonomy,,"The study manipulated 'Robot-autonomy' by allowing participants to switch between automatic and manual control modes for the drone, which directly influenced the level of decision authority they had over the drone's actions. This is explicitly stated in the paper: 'Our method allowed us to investigate how much control participants give to a simulated drone that offers both automatic and manual control modes.' The 'Task-complexity' was manipulated by introducing random obstacles with varying numbers and radii in each run, as stated: 'The starting and ending points are consistent, but random obstacles with varying numbers and radii are introduced in each run to prevent map memorization.' The 'Task-environment' was manipulated by changing the obstacle configurations in each run, which altered the working conditions for the participants. This is supported by the text: 'Each subsequent run featured distinct obstacle maps.' The study found that the manipulation of 'Robot-autonomy' impacted trust, as participants' trust in the automatic mode increased over time, evidenced by the convergence of their flight paths towards the optimal path and a decrease in manual mode usage. This is supported by the text: 'The trust level is calibrated over the runs by allowing participants to experiment with switching between modes, discovering the right balance of trust and improving their performance toward the optimal trajectory.' There is no evidence in the paper that the manipulations of 'Task-complexity' or 'Task-environment' did not impact trust, so they are not included in the 'factors_that_did_not_impact_trust' list.",10.1109/ICHMS59971.2024.10555732,https://ieeexplore.ieee.org/document/10555732/,"Trust is a foundational element in human interaction with Unmanned Aerial Vehicles (UAVs). However, quantifying trust has posed a significant challenge. This research aims to establish a framework for defining and quantitatively measuring trust within the domain of Advanced Air Mobility (AAM), integrating engineering and psychological perspectives. The primary objective of this study is to design empirical experiments aimed at evaluating and measuring trust during unmanned aerial vehicles or drone operations. To achieve this goal, we categorize trust dimensions and use line integrals for quantitative trust assessment. The second objective of this research is to capture the calibration of trust levels for users, enabling them to place an appropriate level of trust in a system based on the system’s capabilities. The results confirm the possibility of quantitatively measuring trust, highlighting experimental participants’ increasing reliance on the system’s capabilities over time. Future research will focus on further experimentation to comprehend the impact of various factors on trust."
"Zguda, Paulina; Sniezynski, Bartlomiej; Indurkhya, Bipin; Kolota, Anna; Jarosz, Mateusz; Sondej, Filip; Izui, Takamune; Dziok, Maria; Belowska, Anna; Jedras, Wojciech; Venture, Gentiane",On the Role of Trust in Child-Robot Interaction*,2019,1,75,115,0,No participants were excluded,Educational Setting,,"Children interacted with a Pepper robot in six phases: greetings, dancing, drawing, robot malfunction, and a question-and-answer session. The malfunction phase involved the robot making strange noises and contorting its body. The Q&A phase used a Wizard-of-Oz setup.","Children engaged in various activities with a robot, including dancing, drawing, and asking questions.",Pepper,Humanoid Robots; Expressive Robots,Social; Research,Social,Conversation,direct-contact interaction,Children directly interacted with the robot in a real-world setting.,real-world,The interaction took place in a real-world kindergarten setting.,physical,The robot was physically present during the interaction.,wizard of oz (directly controlled),The robot's responses during the Q&A phase were directly controlled by a researcher.,Behavioral Measures,,Video Data; Speech Data,Trust was assessed through observation of children's behavior and analysis of their questions.,no modeling,No computational model of trust was used in this study.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's behavior was manipulated by introducing a malfunction phase, and the task was framed as a question-and-answer session.","The malfunction activity did not significantly affect the children's trust, and they continued to interact with the robot.",Children showed both anthropomorphic and de-anthropomorphic behaviors towards the robot. The malfunction activity did not significantly affect the children's trust.,"Children's perception of robots as human-like agents is influenced by the robot's ability to perform human-specific tasks, while the robot's embodiment contributes to de-anthropomorphization.","The robot performed pre-programmed actions like dancing and drawing, and answered questions via a Wizard-of-Oz setup. The children participated in these activities and asked questions to the robot.",,"The paper describes the analysis of video data using Dedoose software, focusing on coding and categorizing behaviors related to provocation, anthropomorphism, distance, and reactions to malfunction. However, no specific statistical tests (e.g., t-tests, ANOVA, regression) are mentioned. The analysis is primarily qualitative, based on observations and coding of video transcripts.",TRUE,Robot-nonverbal-communication; Robot-verbal-communication-content,,Robot-nonverbal-communication,"The study manipulated the robot's nonverbal communication by introducing a 'malfunction' phase where the robot made strange noises and contorted its body. This is classified as 'Robot-nonverbal-communication' because it involves changes to the robot's physical movements and posture. The robot's verbal communication was also manipulated during the Q&A phase using a Wizard-of-Oz setup, where the robot's answers were typed in real-time by a researcher. This is classified as 'Robot-verbal-communication-content' because the content of the robot's responses was directly controlled and varied. The malfunction activity, which was a manipulation of the robot's nonverbal communication, did not significantly affect the children's trust, as they continued to interact with the robot. Therefore, 'Robot-nonverbal-communication' is listed as a factor that did not impact trust. The paper does not explicitly state that the manipulation of the robot's verbal communication impacted trust, so it is not included in the 'factors_that_impacted_trust' list.",10.1109/RO-MAN46459.2019.8956400,https://ieeexplore.ieee.org/document/8956400/,"In child-robot interaction, the element of trust towards the robot is critical. This is particularly important the first time the child meets the robot, as the trust gained during this interaction can play a decisive role in future interactions. We present an in-the-wild study where Polish kindergartners interacted with a Pepper robot. The videos of this study were analyzed for the issues of trust, anthropomorphization, and reaction to malfunction, with the assumption that the last two factors influence the children’s trust towards Pepper. Our results reveal children’s interest in the robot performing tasks specific for humans, highlight the importance of the conversation scenario and the need for an extended library of answers provided by the robot about its abilities or origin and show how children tend to provoke the robot."
"Zguda, Paulina; Kołota, Anna; Venture, Gentiane; Sniezynski, Bartlomiej; Indurkhya, Bipin",Exploring the Role of Trust and Expectations in CRI Using In-the-Wild Studies,2021,1,6,6,0,No participants were excluded,Real-World Environment,,"Children interacted with a Pepper robot in their classroom. The robot introduced itself, read a story, danced with the children, asked them to draw, and then answered their questions using the Wizard of Oz paradigm.","Children engaged in several activities with the robot, including listening to a story, dancing, drawing, and asking questions.",Pepper,Humanoid Robots; Expressive Robots,Social; Research,Social,Conversation,direct-contact interaction,Children had direct physical and verbal interaction with the robot in a real-world setting.,real-world,The interaction took place in a real-world classroom environment.,physical,The robot was physically present during the interaction.,wizard of oz (directly controlled),The robot's responses were controlled by a human operator using the Wizard of Oz paradigm.,Behavioral Measures,,Video Data; Speech Data,Trust was assessed through observation of children's behaviors and verbal expressions.,no modeling,The study did not use computational models of trust.,Empirical HRI Studies,Wizard-of-Oz Studies,No Manipulation,"The study did not manipulate any specific factors related to trust, but observed children's natural interactions with the robot.","The study observed children's behaviors and expressions to identify signs of trust, but did not manipulate any factors to directly influence trust outcomes.","Children showed a tendency to challenge the robot's abilities and anthropomorphize it, which influenced their trust and expectations. The children's behavior was also influenced by the presence of the teacher.","Children's trust and expectations towards the robot were revealed through their challenging behaviors, physical interactions, and questions, indicating a complex interplay between these factors.","The robot introduced itself, read a story, danced with the children, asked them to draw, and then answered their questions. The children listened, danced, drew, and asked questions.",,The study is qualitative and does not employ any statistical tests. The analysis is based on observations of children's behaviors and verbal expressions during their interaction with the robot.,FALSE,,,,"The study did not manipulate any specific factors related to trust. The researchers observed children's natural interactions with the robot in a real-world setting. The study aimed to explore the role of trust and expectations in CRI using an in-the-wild methodology, focusing on natural observations rather than controlled manipulations. The study did not intentionally change any aspect of the robot's behavior, the task, or the environment to measure its impact on trust. Therefore, no factors were manipulated.",10.3390/electronics10030347,https://www.mdpi.com/2079-9292/10/3/347,"Studying interactions of children with humanoid robots in familiar spaces in natural contexts has become a key issue for social robotics. To ﬁll this need, we conducted several Child–Robot Interaction (CRI) events with the Pepper robot in Polish and Japanese kindergartens. In this paper, we explore the role of trust and expectations towards the robot in determining the success of CRI. We present several observations from the video recordings of our CRI events and the transcripts of free-format question-answering sessions with the robot using the Wizard-of-Oz (WOZ) methodology. From these observations, we identify children’s behaviors that indicate trust (or lack thereof) towards the robot, e.g., challenging behavior of a robot or physical interactions with it. We also gather insights into children’s expectations, e.g., verifying expectations as a causal process and an agency or expectations concerning the robot’s relationships, preferences and physical and behavioral capabilities. Based on our experiences, we suggest some guidelines for designing more effective CRI scenarios. Finally, we argue for the effectiveness of in-the-wild methodologies for planning and executing qualitative CRI studies."
"Zhang, Xinyi; Lee, Sun Kyong; Maeng, Hoyoung; Hahn, Sowon",Effects of Failure Types on Trust Repairs in Human–Robot Interactions,2023,1,368,368,77,77 participants were excluded due to incorrect responses on the manipulation check,Online Crowdsourcing,mixed design,"Participants were informed of the study's purpose, reported their trust propensity and entity beliefs, and watched pre-recorded videos of NAO interacting with a human. Participants were randomly assigned to three of 13 conditions (3 failure types x 4 repair methods + 1 control). After each video, they completed a questionnaire assessing trust and attributions.",Participants watched videos of a robot answering questions based on a prescription and then evaluated their trust in the robot.,Nao,Humanoid Robots; Expressive Robots,Care; Research,Social,Conversation,passive observation,Participants passively observed videos of the robot interacting with a human.,media,Participants watched videos of the robot interaction.,physical,Participants watched videos of a physical robot.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires; Custom Scales,Checklist for Trust between People and Automation; Trust Perception Scale - HRI,,Trust was measured using questionnaires and custom scales.,"parametric models (e.g., regression)",The study used regression analysis to model the relationship between trust and other variables.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the type of robot failure (logic, semantic, syntax) and the robot's repair strategy (internal/external apology, denial, no repair) to influence trust.","Internal attribution apology was the most effective repair strategy, while logic failures were the most detrimental. The interaction between failure types and repair methods differed from previous human-human studies.","The study found that internal attribution apology was the most effective repair strategy for all failure types, which contradicts previous findings in human-human trust repair research. Logic failures were perceived as more detrimental than semantic or syntax failures.","Internal attribution apology was the optimal repair strategy for all failure types, and logic failures were the most detrimental to trust.","The robot answered questions based on a given prescription, making mistakes of different types. The human participant watched the video and then completed a questionnaire.",ANCOVA; Linear regression; Chi-squared; Multilevel Model,"The study used MANCOVA to examine the effects of failure types and repair methods on competence-based trust, integrity-based trust, and perceived severity of violations, controlling for trust propensity and entity beliefs. Linear regression was used to test the relationships between causal attributions and trust/severity. A chi-square test was used to assess model fit for the causal attribution scale, and CFA was used to confirm the factor structure of the causal attribution scale.",TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-verbal-communication-content; Robot-accuracy,,"The study manipulated the type of robot failure (logic, semantic, syntax) which directly impacts the accuracy of the robot's responses, thus 'Robot-accuracy'. The study also manipulated the robot's repair strategy (internal/external apology, denial, no repair), which changes the content of the robot's verbal communication, thus 'Robot-verbal-communication-content'. The results showed that both the type of failure and the repair strategy impacted trust, therefore both 'Robot-verbal-communication-content' and 'Robot-accuracy' are listed as factors that impacted trust. There were no factors that were manipulated that did not impact trust.",10.1007/s12369-023-01059-0,https://link.springer.com/10.1007/s12369-023-01059-0,"System performance is the central determinant of user trust in human–machine communication; however, performance failure is inevitable. This study develops a three-fold typology of performance failures (i.e., logic, semantic, and syntax) commonly observed in human-robot interactions based on the differences between the expected and actual outcomes. Herein, 1027 observations are collected from an online experiment to elucidate how the three types of failure and four repair methods (namely, internal attribution apology, external attribution apology, denial, and no repair) impact user trust while examining blame attributions as an underlying mechanism. The results reveal that despite some similarities, the interactions between trust violation types and repair methods differ in robot-to-human trust repair from those in human-to-human trust repair, which contradicts previous ﬁndings. Logic failures are found to be the most detrimental category of performance failures, and the internal-attribution apology is the optimal repair strategy. Notably, participants report greater levels of competence-based trust beliefs if they believe that the situation is jointly controlled by the human interactant and robot."
"Zhang, Bowen; Soh, Harold",Large Language Models as Zero-Shot Human Models for Human-Robot Interaction,2023,1,65,65,0,No participants were excluded,Online Crowdsourcing,between-subjects,Participants were randomly divided into two groups and completed an interactive video survey where a pre-recorded video of the robot's behavior was shown at each turn.,Participants watched a video of a robot passing utensils and chose whether to intervene or let the robot pass the utensil.,Franka Emika Panda,Industrial Robot Arms; Collaborative Robots,Research,Manipulation,Object Passing,passive observation,Participants observed the robot's actions through pre-recorded videos.,media,Participants watched pre-recorded videos of the robot interacting with objects.,physical,"The robot was a physical robot, but participants interacted with it through video.",pre-programmed (non-adaptive),"The robot followed a pre-programmed plan, with some intentional failures.",Behavioral Measures; Questionnaires,,Video Data,"Trust was assessed through participant choices to intervene or not, and through post-experiment questionnaires.",POMDP,The robot's planning was based on a POMDP model that used a LLM to predict human trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The robot intentionally failed on some trials to reduce overtrust, and participants were given different expectations based on the robot's plan.",The LLM-PLAN robot reduced overtrust and yielded higher returns compared to the BASIC-PLAN robot.,"Some participants based their judgments on physical characteristics of objects, which the model did not account for.",LLM-based human models can be used in a planner for HRI to mitigate overtrust.,"The robot passed utensils to the human, and the human chose whether to intervene or let the robot pass the utensil.",ANOVA,A one-way ANOVA was used to compare the mean return between the LLM-PLAN robot group and the BASIC-PLAN robot group in the utensil-passing experiment. The purpose was to determine if the difference in mean return was statistically significant.,TRUE,Robot-accuracy; Robot-task-strategy,Robot-accuracy,Robot-task-strategy,"The study manipulated the robot's accuracy by having it intentionally fail on some trials (passing the utensil in a way that the human could not easily grasp the handle) to reduce overtrust. This is described in the 'Utensil-passing Experiment' section: 'To facilitate this calibration process [34], we enable the robot to intentionally fail on all utensils (except the knife) by handing the wrong part to the human (so that the human can only grasp the dirty end, Fig. 6). This intentional failure results in a penalty of −1.' This directly impacts the robot's success rate on the task, thus it is classified as 'Robot-accuracy'. The robot also used two different task strategies: LLM-PLAN and BASIC-PLAN. The LLM-PLAN robot intentionally failed on the scissors to prevent over-trust, while the BASIC-PLAN robot never intentionally failed. This is a change in the task completion strategy that did not influence the success rate on the task, thus it is classified as 'Robot-task-strategy'. The results showed that the LLM-PLAN robot, which intentionally failed on some trials, reduced overtrust and yielded higher returns compared to the BASIC-PLAN robot. This indicates that the manipulation of 'Robot-accuracy' impacted trust levels. The 'Robot-task-strategy' did not directly impact trust, but rather was a method to manipulate the 'Robot-accuracy' factor. The participants' trust was influenced by the robot's success or failure in passing the utensils, not by the specific planning strategy itself.",,http://arxiv.org/abs/2303.03548,"Human models play a crucial role in human-robot interaction (HRI), enabling robots to consider the impact of their actions on people and plan their behavior accordingly. However, crafting good human models is challenging; capturing context-dependent human behavior requires signiﬁcant prior knowledge and/or large amounts of interaction data, both of which are difﬁcult to obtain. In this work, we explore the potential of large-language models (LLMs) — which have consumed vast amounts of human-generated text data — to act as zero-shot human models for HRI. Our experiments on three social datasets yield promising results; the LLMs are able to achieve performance comparable to purpose-built models. That said, we also discuss current limitations, such as sensitivity to prompts and spatial/numerical reasoning mishaps. Based on our ﬁndings, we demonstrate how LLM-based human models can be integrated into a social robot’s planning process and applied in HRI scenarios. Speciﬁcally, we present one case study on a simulated trust-based table-clearing task and replicate past results that relied on custom models. Next, we conduct a new robot utensil-passing experiment (n = 65) where preliminary results show that planning with a LLM-based human model can achieve gains over a basic myopic plan. In summary, our results show that LLMs offer a promising (but incomplete) approach to human modeling for HRI."
"Zhang, Xinyi; Lee, Sun Kyong; Kim, Whani; Hahn, Sowon","“Sorry, it was my fault”: Repairing trust in human-robot interactions",2023,1,330,291,39,39 incomplete responses were excluded from final analysis,Online Crowdsourcing,within-subjects,"Participants provided demographic information and their general propensity to trust robotics. They were then directed to a set of videos recorded from a first-person perspective with an interactant's voice that matched the participant's sex. Participants were randomly assigned to three different conditions out of the 13 conditions (three failure types × four repair attempts + one control). In each video, the human interactant sought information from the NAO robot by asking the same five questions based on a given prescription. Participants reported their post-interaction trust as well as perceptions of competence, integrity, and severity of trust violations.","Participants watched videos of a human interacting with a NAO robot, where the robot provided information about medical prescriptions. Participants were asked to imagine they were the interactant in the videos and answer questions about their trust, perceived competence, integrity, and severity of trust violations after each video.",Nao,Humanoid Robots; Expressive Robots,Care; Research,Evaluation,Survey/Questionnaire Completion,passive observation,Participants passively observed videos of a human interacting with a robot.,media,Participants watched videos of a robot interaction.,physical,The robot was physically present in the videos.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires; Custom Scales,Propensity to Trust Scales; Mayer and Davis' Trust/Trustworthiness Scales (1999); Trust Perception Scale - HRI; Checklist for Trust between People and Automation; Schaefer's Trust Questionnaire/Scale,,Trust was measured using questionnaires and custom scales.,"parametric models (e.g., regression)",The study used parametric models such as MANCOVA and ANCOVA to analyze the data.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the type of technical failure (logic, semantic, syntax) and the robot's repair strategy (internal-attribution apology, external-attribution apology, denial, no repair) to influence trust.","Internal-attribution apology was more effective than denial for logic and semantic failures, but not for syntax failures. External-attribution apology was more effective than denial for integrity-based trust. Denial was more harmful than no repair for integrity-based trust. Logic failures were the most detrimental for integrity-based trust.","The study found that internal-attribution apology was more effective than denial for logic and semantic failures, which aligns with previous research on competence-based violations. However, the study also found that denial was more harmful than no repair for integrity-based trust, which is a notable trend. The study also found that logic failures were the most detrimental for integrity-based trust, which was unexpected.","Internal-attribution apology was more effective than denial for logic and semantic failures, suggesting these failures are perceived as competence-based violations in HRI.","The robot provided information about medical prescriptions, and the human asked questions based on a given prescription. The human participant watched videos of this interaction and answered questions about their trust in the robot.",ANCOVA; ANOVA; ANCOVA,"The study used MANCOVA and MANOVA to examine the effects of different repair strategies (internal-attribution apology vs. denial) on post-interaction trust (competence-based and integrity-based) under different failure types (logic, semantic, syntax), controlling for propensity to trust. ANCOVA was used to compare the effectiveness of different repair methods (internal vs. external attribution apology, external attribution apology vs. denial, denial vs. no repair) on competence-based and integrity-based trust, also controlling for propensity to trust. Additionally, MANCOVA was used to examine the interaction effects between failure types and repair methods on post-interaction trust. The tests were used to determine if there were significant differences in trust based on the type of failure and the repair strategy used by the robot.",TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-verbal-communication-content; Robot-accuracy,,"The study manipulated the robot's verbal communication content by varying the repair strategy (internal-attribution apology, external-attribution apology, denial, or no repair) after a failure. This directly changes the content of what the robot communicates to the user. The study also manipulated the robot's accuracy by introducing three types of technical failures (logic, semantic, and syntax errors), which directly impacted the robot's ability to perform the task correctly. The results showed that both the type of failure (accuracy) and the repair strategy (verbal communication content) impacted trust. Specifically, internal-attribution apology was more effective than denial for logic and semantic failures, and external-attribution apology was more effective than denial for integrity-based trust. Denial was more harmful than no repair for integrity-based trust. Logic failures were the most detrimental for integrity-based trust. Since both the robot's accuracy and the content of its communication were intentionally varied to observe their impact on trust, both are included in factors_manipulated. Both factors were found to impact trust, so they are included in factors_that_impacted_trust. No factors were found to not impact trust.",10.1016/j.ijhcs.2023.103031,https://linkinghub.elsevier.com/retrieve/pii/S107158192300037X,"The current study develops a three-fold categorization (i.e., logic, semantic, and syntax failures) for technical failures that are commonly observed in human-robot interactions (HRI), and investigates it along with four trust repair strategies: internal-attribution apology, external-attribution apology, denial, and no repair. The 743 ob­ servations conducted through an online experiment reveal some nuances in participants’ perceprions of competence- and integrity-based trust violations, which may reflect ontological differences between humans and machines. The analysis also shows significant main effects of failure types and repair methods on HRI-based trust."
"Zhang, Tingru; Yang, Jinfeng; Chen, Milei; Li, Zetao; Zang, Jing; Qu, Xingda",EEG-based assessment of driver trust in automated vehicles,2024,1,72,72,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were briefed on the experiment, completed a practice session, were equipped with an EEG headset, and then completed the driving simulation. They were instructed to follow traffic laws and TORs. After each scenario, they provided a trust rating.",Participants drove a simulated vehicle in automated mode and were required to take over manual control when a take-over request (TOR) was triggered by a road hazard. They also performed a typing task on a tablet as a non-driving related task (NDRT).,Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated automated vehicle and were required to take over control when prompted.,simulation,The study used a driving simulator to create an interactive virtual environment.,simulated,The robot was a simulated automated vehicle within the driving simulator.,shared control (fixed rules),The automated vehicle operated autonomously but followed fixed rules for when to request a take-over.,Questionnaires; Physiological Measures,,Physiological Signals,Trust was measured using self-reported ratings and EEG signals.,"parametric models (e.g., regression)",Machine learning algorithms were used to model trust based on EEG features.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the reliability of the automated vehicle by introducing malfunctions, such as missed take-over requests or false alarms, which influenced the perceived performance of the system and the difficulty of the task.","The malfunctions were intended to decrease trust in the automated vehicle, with the 'miss' condition expected to induce lower trust than the 'false alarm' condition.","The study found that increased power of beta waves tended to indicate a lower level of trust in AVs. The LightGBM model using whole brain features performed best, but models using features from the frontal and parietal regions also showed promise.","EEG features can be used to predict driver trust, with the best model achieving an accuracy of 88.44% and an F1-score of 78.31%.","The robot (simulated AV) drove autonomously, and the human monitored the driving and took over control when prompted by a TOR. The human also performed a typing task on a tablet as a secondary task.",Friedman test,"The Friedman test was used to statistically compare the performance of different brain regions (whole brain, frontal, temporal, occipital, and parietal) across the nine classifiers. Specifically, it was used to assess the effects of brain regions on accuracy, precision, recall, and F1-score. Pairwise comparisons were also conducted to determine which brain regions differed significantly from each other.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the reliability of the automated vehicle by introducing malfunctions in the form of missed take-over requests (miss condition) and false alarms (false alarm condition). These manipulations directly affected the accuracy of the automated vehicle's performance in detecting and responding to road hazards. The 'miss' condition involved the AV failing to detect a hazard, while the 'false alarm' condition involved the AV issuing a TOR when no hazard was present. These manipulations were intended to influence the perceived performance of the system and the difficulty of the task, and thus, were categorized as 'Robot-accuracy'. The paper explicitly states that these malfunctions were intended to decrease trust in the automated vehicle, with the 'miss' condition expected to induce lower trust than the 'false alarm' condition. The results confirmed that the manipulation of robot accuracy impacted trust levels, as evidenced by the different trust ratings and EEG signals observed across the three conditions (normal, miss, and false alarm).",10.1016/j.eswa.2024.123196,https://linkinghub.elsevier.com/retrieve/pii/S0957417424000617,"Effective collaboration between automated vehicles (AVs) and human drivers relies on maintaining an appro­ priate level of trust. However, real-time assessment of human trust remains a significant challenge. While initial efforts have delved into the potential use of physiological signals, such as skin conductance and heart rate, to evaluate trust, limited attention has been given to the feasibility of assessing trust through electroencephalogram (EEG) signals. This study aimed to address this issue by using EEG signals to objectively assess driver trust to­ wards AVs. A simulated driving experiment was conducted, where driver trust was manipulated by introducing different types of AV malfunctions. Self-reported trust ratings were collected and used to classify driver trust into three levels: low, medium, and high. A total of 420 time- and frequency-domain EEG features were extracted, and nine machine learning algorithms were applied to construct driver trust assessment models. Additionally, to explore the potential of developing cost-effective models with reduced feature inputs, this study developed trust models using features solely from single brain regions: frontal, parietal, occipital, or temporal. The results showed that the best-performing model, utilizing features from the whole brain and employing the Light Gradient Boosting Machine (LightGBM) algorithm, achieved an accuracy of 88.44% and an F1-score of 78.31%. In comparison, models based on single brain regions did not achieve comparable performance to the compre­ hensive model. However, the frontal and parietal regions showed important potentials for developing costeffective trust assessment models. This study also performed feature analysis on the best-performing model to identify features highly responsive to changes in trust. The results showed that an increased power of beta waves tended to indicate a lower level of trust in AVs. These findings contribute to our understanding of the neural correlates of trust in AVs and hold practical implications for the development of trust-aware AV technologies capable of adapting and responding to driver’s trust levels effectively."
"Zhang, Yinsu; Yadav, Aakash; Hopko, Sarah K.; Mehta, Ranjana K","In Gaze We Trust: Comparing Eye Tracking, Self-report, and Physiological Indicators of Dynamic Trust during HRI",2024,1,38,38,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed practice trials of an assembly task, then 10 trials with 100% robot reliability, followed by 10 trials with 76% robot reliability. Subjective trust, eye-tracking, and ECG data were collected throughout.",Participants completed a planetary gear assembly task while a robot delivered parts.,UR10,Industrial Robot Arms; Collaborative Robots (Cobots),Industrial; Research,Manipulation,Object Assembly,direct-contact interaction,Participants physically interacted with the robot by assembling parts delivered by the robot.,real-world,The study involved a real-world interaction with a physical robot.,physical,A physical robot was used in the study.,pre-programmed (non-adaptive),The robot followed a pre-programmed path to deliver parts.,Behavioral Measures; Physiological Measures; Questionnaires; Real-time Trust Measures,,Eye-tracking Data; Physiological Signals,"Trust was assessed using subjective reports, eye-tracking, and physiological measures.",no modeling,No computational model of trust was used in this study.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"Robot reliability was manipulated by introducing errors in the robot's performance, which increased the task difficulty.","Subjective trust decreased during the trust breach phase, while eye-tracking measures showed changes in gaze behavior.","Subjective trust ratings did not change between early and late trust build or between breach and repair phases, but eye-tracking measures showed sex-specific changes across these phases. Heart rate features did not change between any dynamic trust phases.","Eye-tracking measures can complement subjective trust ratings to uncover dynamic trust across diverse demographics during HRIs, and are more sensitive to changes in trust than subjective measures.","The robot delivered parts to the right side of the workbench, and the human assembled a planetary gear on the left side.",ANOVA; Friedman test; Wilcoxon signed-rank test; t-test; Mann-Whitney U,"The study used ANOVAs to examine the effects of dynamic trust phase and sex on various dependent variables. For data that violated normality assumptions, Friedman tests were used. If both normality and sphericity assumptions were violated, Wilcoxon signed-rank tests were performed. Sex differences were compared at each phase using t-tests or Mann-Whitney tests for non-normal data. Post-hoc analyses with Bonferroni correction were also conducted for significant ANOVA results.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated robot reliability by having the robot perform with 100% reliability for the first 10 trials and then with 76% reliability for the next 10 trials. This change in reliability directly impacts the robot's accuracy in performing its task, which is delivering parts correctly. The paper states, 'Participants then underwent 10 reliable trials with 100% cobot reliability, followed by 10 unreliable cobot trials with 76% cobot reliability.' This manipulation of robot accuracy was found to impact trust, as evidenced by the decrease in subjective trust ratings and changes in eye-tracking measures during the trust breach phase. The paper states, 'Post-hoc comparisons showed a significant decrease in subjective trust at the trust breach phase for all participants.' and 'Dynamic trust phase led to significant fixation count changes... Post-hoc analysis with Bonferroni correction showed a significantly increased fixation count for the trust breach phase compared to that of the late trust build phase'. No other factors were manipulated, and no factors were found to not impact trust.",10.1145/3610978.3640649,https://dl.acm.org/doi/10.1145/3610978.3640649,"Technical advances in shared-space collaborative robotics have placed recent attention on trust in robots to ensure operator safety as well as to optimize human-robot interactions (HRI). Commonly measured using self-reports, our study explores if eye tracking or physiological indicators ofer greater sensitivity in capturing dynamic trust during HRI. We investigated operators’ trust dynamics (i.e., early and late trust build, breach, repair) across 2 diferent robot reliability levels (100% and 76% reliability). Trust ratings, fxation counts, and gaze transition entropy changed signifcantly between the late trust build and trust breach phases, while heart rate features did not change between any dynamic trust phases. Subjective trust ratings did not change between early and late trust build or between breach and repair phases, however, changes in stationary gaze entropy and gaze transition entropy across these phases were found to be sex-specifc. Eye-tracking measures have the potential to complement, and in some cases replace, subjective trust ratings to uncover dynamic trust across diverse demographics during HRIs."
"Zheng, Huanfei; Smereka, Jonathon M.; Mikulski, Dariusz; Wang, Yue",Bayesian Optimization Based Trust Model for Human Multi-robot Collaborative Motion Tasks in Offroad Environments,2023,1,32,32,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were randomly divided into two groups. Group one performed 6 trials using a standard experimental design, while group two performed 6 trials using the BOED. Both groups received 0.5 hours of training and 1 hour of formal operation. Participants operated a robot in a simulated environment, observing the autonomous robots and providing trust change data.",Participants performed a collaborative bounding overwatch task with a multi-robot system in a simulated offroad environment.,SUMMIT-XL; Husky,Mobile Robots; Unmanned Ground Vehicles,Research,Navigation,Path Following,minimal interaction,"Participants interacted with the robots through a simulation, providing trust change data based on their observations.",simulation,The interaction took place in a simulated 3D environment.,simulated,The robots were represented as virtual models in the simulation.,shared control (fixed rules),"The autonomous robots followed a pre-defined path, while the human operator controlled a robot to approach the autonomous robots.",Real-time Trust Measures; Questionnaires,NASA Task Load Index (NASA-TLX); IBM Computer System Usability Questionnaire Scale (IBM-CSUQ); Situational Awareness Rating Technique (SART),"Performance Metrics; robot data (sensor data, etc.)","Trust was measured using self-reported trust change data, NASA TLX, IBM CSUQ, and SART, and robot sensor data was collected for modeling.","parametric models (e.g., regression)","A linear state-space model was used to model trust, with parameters estimated using Bayesian inference and MCMC sampling.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The experimental design was manipulated by using either a standard experimental design or a Bayesian optimization based experimental design (BOED) to select the paths for the robots to travel, which influenced the task difficulty and robot performance.","The BOED resulted in fewer collisions, lower frequency of contact loss, lower operator workload, and higher system usability, suggesting that the BOED approach positively influenced trust by improving the robot's performance and reducing task difficulty.","The study found that the inter-robot trust causality was less important than other factors such as traversability, visibility, and human historical trust. The BOED tended to select paths that were less likely to cause robot turnover and contact loss, which improved the performance of human-MRS collaboration.","The Bayesian optimization based experimental design (BOED) improved human-robot collaboration by reducing collisions, contact loss, workload, and increasing usability compared to a standard experimental design.","The human operator teleoperated a robot to approach a team of autonomous robots, while observing the autonomous robots' motion and providing trust change data. The autonomous robots navigated a pre-defined path in a simulated offroad environment.",Wilcoxon signed-rank test; Kruskal-Wallis,"The study used a Wilcoxon signed-rank test to compare the Bayesian Information Criterion (BIC) values of two different trust models (one with and one without inter-robot causality). A Kruskal-Wallis one-way ANOVA was used to compare the results of the Bayesian Optimization based Experimental Design (BOED) with a standard experimental design across several dependent variables, including number of collisions, frequency of losing contact, time of completing the task, workload, usability, and situational awareness.",TRUE,Task-strategy,Task-strategy,,"The study manipulated the experimental design by using either a standard experimental design or a Bayesian optimization based experimental design (BOED) to select the paths for the robots to travel. This manipulation directly influenced the task difficulty and robot performance, but not the success rate of the task itself. The BOED tended to select paths that were less likely to cause robot turnover and contact loss, which improved the performance of human-MRS collaboration. This manipulation is best categorized as 'Task-strategy' because it changes the approach to completing the task (path selection) without directly altering the task's success or failure. The BOED resulted in fewer collisions, lower frequency of contact loss, lower operator workload, and higher system usability, suggesting that the BOED approach positively influenced trust by improving the robot's performance and reducing task difficulty. Therefore, 'Task-strategy' is also listed as a factor that impacted trust.",10.1007/s12369-023-01011-2,https://link.springer.com/10.1007/s12369-023-01011-2,"In this paper, we seek to develop a computational human to multi-robot system (MRS) trust model to encode human intention into the MRS motion tasks in offroad environments. Our computational trust model builds a linear state-space equation to capture the inﬂuence of environmental attributes on human trust in an MRS. Bayesian inference is used to derive the posterior distribution of the trust model parameters. Due to the intractable computation of the posterior distributions, we develop a Markov Chain Monte Carlo sampling algorithm by integrating the Gibbs sampler with the forward-ﬁltering-backwardsampling to approximate the distributions. A Bayesian optimization based experimental design (BOED) is proposed to sequentially learn the human-MRS trust model parameters. Inspired by decision ﬁeld theory, we develop a human preference based acquisition function for the BOED to explore the MRS motion path and collect data for the trust model in an efﬁcient way. A case study on human-MRS collaborative bounding overwatch task is deployed, which is a multi-robot motion task traditionally used in offroad environments and requires a heavy cognition workload for the human to collaborate with the MRS. Trials using simulated human agents and human subjects collaborating with an MRS are conducted in the ROS Gazebo simulator. The simulated human agent with MRS shows that the BOED can correctly estimate the trust model parameters. The human subject tests demonstrate the capability of our computational trust model in capturing the human’s trust dynamics with the goodness of ﬁt metrics. The tests also show statistically signiﬁcant results by comparing the BOED with a benchmark experimental design approach. The BOED resulted in fewer collisions with obstacles, lower frequency of contact loss between robots, lower operator workload, and higher system usability."
"Zhou, Huiping; Itoh, Makoto; Kitazaki, Satoshi",Long-term Effect of Experiencing System Malfunction on Driver Take-over Control in Conditional Driving Automation,2019,1,72,72,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants completed a driving simulation task in two phases, with a two-month break in between. In the first phase, participants were trained with or without a system malfunction and with or without knowledge of system limitations. In the second phase, all participants experienced a system malfunction. Questionnaires were administered at four time points.","Participants were asked to operate a driving simulator as safely as a real vehicle on an expressway, while also performing a visual Surrogate Reference Task (SuRT).",Unspecified,Autonomous Vehicles,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a driving simulator and were instructed to take over control when prompted.,simulation,The study used a driving simulator to create an interactive virtual environment.,simulated,The robot was a simulated autonomous vehicle within the driving simulator.,shared control (fixed rules),The automated driving system operated independently but required human intervention at specific points.,Questionnaires,,Performance Metrics,Trust was assessed using a questionnaire and performance metrics.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the experience of system malfunction during training and the knowledge of system limitations, which influenced driver's expectations and performance.","Driver trust in the automated system changed with accumulated experience, influenced by knowledge and experience at the training phase. Trust decreased with experience, especially after experiencing a malfunction.","The effect of knowledge was significant at the early training stage but diminished with accumulated driving experience. The long-term effect of experiencing a system malfunction was also indicated, influencing driver take-over performance.",Experiencing system malfunction and instructing knowledge during training are important for drivers to achieve moderate driver-automation interaction.,"The robot (simulated autonomous vehicle) drove on an expressway, and the human was required to monitor the system and take over control when prompted by a request to intervene (RTI) or a system malfunction.",t-test; ANOVA,"The study used t-tests to compare the rate of crash and reaction time (RT) between groups with different levels of knowledge about system limitations during the training phase. ANOVA was used to analyze the RT in the practicing phase, both before and during a system malfunction, and to analyze the average velocity, standard deviation of lateral position, maximum steering angle, and maximum slot of accelerator pedal after take-over control. The purpose was to determine the impact of prior experience with system malfunction and knowledge of system limitations on driver take-over performance and trust.",TRUE,Robot-verbal-communication-content; Robot-accuracy,Robot-accuracy,Robot-verbal-communication-content,"The study manipulated two main factors: the experience of system malfunction during training and the knowledge of system limitations. The 'Robot-accuracy' factor was manipulated by introducing a system malfunction in the training phase for the 'Experienced-group' and not for the 'Un-experienced group'. This directly impacted the robot's performance and the driver's experience with the system's reliability. The 'Robot-verbal-communication-content' factor was manipulated by providing different levels of information about system limitations and the concept of 'request to intervene' (RtI). Level-I only explained the occurrence of system limitation, while Level-II also explained the concept of RtI. The paper states that the experience of system malfunction (Robot-accuracy) impacted driver take-over performance and trust, as evidenced by the long-term effect of the malfunction experience on driver behavior and subjective trust ratings. The knowledge of system limitations (Robot-verbal-communication-content) had a significant impact on take-over performance at the early training stage, but this effect diminished with accumulated driving experience and did not impact trust in the long term. Therefore, 'Robot-accuracy' impacted trust, while 'Robot-verbal-communication-content' did not have a long-term impact on trust.",10.1109/SMC.2019.8913968,https://ieeexplore.ieee.org/document/8913968/,"The paper aims to investigate how driving experience on system malfunction impacts driver take-over control on a long-term basis when he or she uses the conditionally driving automation. We designed a two-factorial (driving experience × knowledge of system limitation) driving simulator experiment at training and practicing phases between which there was a two-month period. All seventy-two people (36.2 ± 15.6 years old) participate into the data collection via driving simulators. The experimental results indicated that participants with the malfunction-experience at training reacted to a malfunction occurring at the two-month-later practicing phase. It is demonstrated that the experience was contributable for a long term. Also, it was shown that instructing the concept of the request to intervene (RtI) was attributed to rapidly taking-over car control. The effect of knowledge was significantly shown at the early training stage, and it was getting less as the driving experience was being accumulated. Furthermore, drivers' subjective assessment suggested that driver trust to driving automation would change with the accumulated experience. Meanwhile, the change was influenced by the acquired knowledge and experience at the training phase. Consequently, this study revealed the importance of experiencing system malfunction and instructing the knowledge at a learning/training period that will be helpful for drivers to achieve moderate driver-automation interaction."
"Zhou, Tianyu; Xia, Pengxiang; Ye, Yang; Du, Jing",Embodied Robot Teleoperation Based on High-Fidelity Visual-Haptic Simulator: Pipe-Fitting Example,2023,1,31,31,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants completed a pipe pick-and-place task in four conditions: 2D perspective, 3D perspective, 2D perspective with haptics, and 3D perspective with haptics, with the order of conditions shuffled to eliminate learning effects. Participants completed a training session before the main experiment. Questionnaires were administered after each condition.","Participants controlled a virtual robot to pick up and place pipes of different materials into target locations, while avoiding collisions and deformations.",Franka Emika Panda,Industrial Robot Arms; Collaborative Robots,Industrial; Research,Manipulation,Remote Manipulation,minimal interaction,Participants interacted with a virtual robot through a haptic device and VR headset.,simulation,Participants used a VR headset to view the virtual environment and interact with the robot.,simulated,The robot was a virtual representation in a simulated environment.,wizard of oz (directly controlled),The robot's actions were directly controlled by the human operator through the haptic device.,Questionnaires,NASA Task Load Index (NASA-TLX); Situational Awareness Rating Technique (SART); Trust Scale questionnaire,Eye-tracking Data; Performance Metrics,Trust was assessed using questionnaires and performance metrics.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The researchers manipulated the visual perspective (2D vs 3D) and the presence of haptic feedback to influence the user's perception and performance.,"The 3D perspective with haptics condition showed the highest trust levels, suggesting that both visual and haptic feedback positively impact trust.","The 3D perspective with haptics condition consistently outperformed all other conditions across all metrics, indicating a strong positive effect of combined visual and haptic feedback. The 2D perspective with haptics condition outperformed the 3D perspective condition, suggesting that haptic feedback is more important than 3D visual feedback for this task.","The combination of 3D perspective and haptic feedback significantly improved task performance, reduced cognitive load, and increased operator trust in a teleoperated pipe-fitting task.","The human operator used a haptic device and VR headset to control a virtual robot to pick up and place pipes, while the robot's actions were directly controlled by the human operator.",Mann-Whitney U,"The Mann-Whitney U-test, a non-parametric statistical test, was used to compare the performance metrics across the four experimental conditions (2D perspective, 3D perspective, 2D perspective with haptics, and 3D perspective with haptics). This test was used to determine if there were statistically significant differences in task performance (time on task, installation accuracy, insertion depth accuracy, cognitive load, number of collisions, and number of deformations) and subjective assessments (NASA-TLX, SART, and Trust Scale scores) between the different conditions. The test was chosen because the data may not follow a normal distribution.",TRUE,Robot-interface-design; Task-environment,Robot-interface-design,,"The study manipulated the visual perspective (2D vs 3D) and the presence of haptic feedback. The visual perspective is a change to the interface design, as it alters how the user perceives the environment and interacts with the robot. The presence or absence of haptic feedback also changes the interface, as it adds or removes a sensory channel for interaction. Therefore, 'Robot-interface-design' is the most appropriate category. The task environment was also manipulated by changing the visual perspective (2D vs 3D), which alters the user's perception of the environment and the way they interact with it. The paper states, 'The experiment followed a within-subject experimental design... with four conditions, which were two-dimensional (2D) perspective, three-dimensional (3D) perspective, 2D perspective with haptics, and 3D perspective with haptics.' This clearly indicates a manipulation of the visual and haptic feedback, which are part of the interface design and the task environment. The results showed that the 3D perspective with haptics condition yielded the best performance and highest trust levels, indicating that the interface design impacted trust. The paper states, 'The results show that the 3D perspective with haptics condition yielded the best performance in comparison with the other conditions. Moreover, the 2D perspective with haptics condition and the 3D perspective condition exhibited better performance compared with the 2D perspective condition.' This shows that the interface design, specifically the presence of 3D perspective and haptic feedback, impacted trust. There is no indication that the task environment manipulation alone impacted trust, but it was a factor that was manipulated.",10.1061/JCEMD4.COENG-13916,https://ascelibrary.org/doi/10.1061/JCEMD4.COENG-13916,"Robot teleoperation, a control method allowing human operators to manipulate robotic systems remotely, has become increasingly popular in construction applications. A significant challenge is the disconnection between the robot sensor data and the human operator’s sensory processes, creating a sensorimotor mismatch in motor-intensive activities. This disconnection is particularly challenging in motorintensive activities that require accurate perception and response. Researchers have started investigating haptic interactions to enhance the control feedback loop, including simulating contacts, motions, and tactile input. However, although current methodologies have advanced the field, they often focused on certain aspects and could be further expanded to provide a more comprehensive simulation of the physical interaction that occurs in typical construction operations. This study designs and tests a comprehensive high-fidelity embodied teleoperation method that simulates complete real-world physical processes via the physics engine. The proposed method captures all categories of physical interaction in typical motor-intensive construction tasks, including weight, texture, inertia, impact, balance, rotation, and spring. A humansubject experiment shows that the proposed method substantially improves performance and human functions in a teleoperated pipe-fitting task. The results indicate that the proposed multisensory augmentation method significantly enhances performance and user experience, offering valuable insights for designing innovative robot teleoperation systems for future construction applications. DOI: 10.1061/ JCEMD4.COENG-13916. © 2023 American Society of Civil Engineers."
"Zhou, Siyuan; Sun, Xu; Wang, Qingfeng; Liu, Bingjian; Burnett, Gary",Examining pedestrians’ trust in automated vehicles based on attributes of trust: A qualitative study,2023,1,36,36,0,No participants were excluded,Controlled Lab Environment,within-subjects,Participants experienced five VR scenarios of pedestrian-AV interaction and then participated in a semi-structured interview about their trust in AVs.,Participants were asked to observe and react to different pedestrian-AV interaction scenarios in VR and then discuss their perceptions of trust.,Unspecified,Autonomous Vehicles,Research,Navigation,Street Crossing,minimal interaction,Participants interacted with the AV in a VR simulation without physical contact.,simulation,Participants experienced the interaction through an immersive VR environment.,simulated,The robot was a virtual representation within the VR environment.,pre-programmed (non-adaptive),The AV's behavior was pre-programmed and did not adapt to the participant's actions.,Behavioral Measures; Questionnaires,,Speech Data,Trust was assessed through qualitative analysis of interview data and observation of participant behavior in the VR scenarios.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the AV's driving style, communication methods (e.g., eHMI, auditory cues), and the presence of other pedestrians to influence trust.","The study found that factors such as aggressive driving style, ambiguous communication, and lack of predictability decreased trust, while defensive driving, clear communication, and familiarity increased trust.","The study highlighted the importance of subjective attributes, such as 'automation morality' and 'care/harm', in pedestrian-AV trust, which are often neglected in the literature. The study also found that overtrust can occur when pedestrians receive consistent information from multiple sources.","Pedestrian trust in AVs is influenced by both objective attributes (reliability, competence, predictability, familiarity) and subjective attributes (authority/subversion, liberty/oppression, care/harm, sanctity/degradation).",The robot (AV) was programmed to approach a crosswalk and either stop or continue based on the scenario. The human participant was asked to observe the AV's behavior and decide when to cross the street.,,The study employed a qualitative approach using thematic analysis of interview data. No statistical tests were used.,TRUE,Robot-verbal-communication-content; Robot-nonverbal-communication; Robot-task-strategy; Task-environment,Robot-verbal-communication-content; Robot-nonverbal-communication; Robot-task-strategy,Task-environment,"The study manipulated several factors to observe their impact on pedestrian trust in AVs. 'Robot-verbal-communication-content' was manipulated through the use of text-based and auditory cues ('safe to cross' message) on the eHMI, which directly altered the information conveyed to the participants. 'Robot-nonverbal-communication' was manipulated by displaying a smiling face on the front eHMI, which is a non-verbal cue intended to influence the perception of the AV. 'Robot-task-strategy' was manipulated by varying the AV's driving style (aggressive, standard, defensive), which directly impacted the AV's behavior and how it approached the crosswalk. 'Task-environment' was manipulated by including the presence of other pedestrians in some scenarios, which changed the context of the interaction. The study found that 'Robot-verbal-communication-content', 'Robot-nonverbal-communication', and 'Robot-task-strategy' significantly impacted trust levels, with clear communication, friendly cues, and defensive driving styles increasing trust, while ambiguous communication, lack of cues, and aggressive driving styles decreased trust. The presence of other pedestrians ('Task-environment') was mentioned as a factor that influenced trust, but it was not directly manipulated as a variable in the scenarios, and therefore, it is not considered as a factor that impacted trust in the same way as the other three. The study did not explicitly state that the presence of other pedestrians did not impact trust, but it was not a manipulated factor that was found to have a direct impact on trust in the same way as the other three.",10.1016/j.apergo.2023.103997,https://linkinghub.elsevier.com/retrieve/pii/S0003687023000352,"Pedestrians’ trust in automated vehicles (AVs) needs to be analyzed and deconstructed to update it from its current broad concept into several lower-level attributes for assessment and measurement. In this study, we have employed virtual reality (VR) and scenario-based interviews to examine the trust of pedestrians toward AVs, based on the attributes of trust and trustworthiness. A hybrid approach of inductive and deductive thematic analysis of the responses of 36 participants was undertaken. Eight such attributes emerged from the analysis, including statistical reliability and dependability, competence, predictability, familiarity, authority/subversion, liberty/ oppression, care/harm, and sanctity/degradation. The first four are objective attributes concerning automation trustworthiness and human trust in automation, while the remaining four are subjective attributes, analogous to properties of human morality. The findings of this study provide an empirical grounding for trust theories. Specifically, we have highlighted the importance of subjective qualities in constituting pedestrian-AV trust, including “automation morality” and “care/harm”."
"Zhu, Lixiao",Effects of Proactive Explanations by Robots on Human-Robot Trust,2020,1,32,21,11,"11 participants were excluded due to performing actions that required experimenter intervention (e.g., accidentally closing the testbed window)",Controlled Lab Environment,within-subjects,"Participants provided informed consent, were introduced to the task and robot, and familiarized with the task. They then completed three experiment blocks with different robot explanatory behaviors, with camera data collected and trust questionnaires administered before and after each block.","Participants engaged in a resource management task, spending resources to explore an environment while a robot collected resources.",TurtleBot,Mobile Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with the robot through observation and task performance.,real-world,Participants interacted with a physical robot in a real-world setting.,physical,The robot was a physical Turtlebot present in the experimental space.,shared control (fixed rules),"The robot autonomously decided when to collect resources based on a fixed rule, but could also be directed by the user.",Behavioral Measures; Questionnaires,Schaefer's Trust Questionnaire/Scale,Video Data,Trust was assessed using a questionnaire and behavioral measures based on video data.,no modeling,The study used statistical analysis but did not model trust computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's explanatory behavior was manipulated by having it either provide no explanation, a proactive announcement, or a proactive explanation before changing resource collection.","Proactive explanations led to increased trust as measured by objective behavioral measures, but subjective measures did not show a significant effect. Proactive announcements had inconclusive effects.","Subjective measures did not support the hypotheses, while objective measures partially supported them. Participants also reported preferring proactive announcements over proactive explanations, which was unexpected.",Proactive explanations by robots lead to increased human-robot trust as assessed through objective observational measures.,"The robot collected resources based on a fixed rule or user instruction, and the human explored the environment and spent resources. The human monitored the robot's actions.",Bayesian ANOVA; bayes factor analysis; Bayesian t-test,The study used Bayesian analyses of variance (ANOVA) to examine the effect of the experimental condition (robot explanatory behavior) on subjective and objective trust measures. This was followed by Bayes Factor analyses to quantify the evidence for or against the hypotheses. Post-hoc pairwise Bayesian t-tests were used to compare the different experimental conditions when the ANOVA results were inconclusive.,TRUE,Robot-verbal-communication-content,Robot-verbal-communication-content,,"The study manipulated the robot's explanatory behavior before it changed resource collection. The robot either provided no explanation, a proactive announcement (stating the action it would take), or a proactive explanation (stating the action and why). This directly changes the content of the robot's verbal communication, making 'Robot-verbal-communication-content' the appropriate category. The results showed that proactive explanations led to increased trust as measured by objective behavioral measures, while subjective measures did not show a significant effect. Proactive announcements had inconclusive effects. Therefore, 'Robot-verbal-communication-content' is the factor that impacted trust, and no factors were found to not impact trust.",,,"The performance of human-robot teams depends on humanrobot trust, which in turn depends on appropriate robot-to-human transparency. A key way for robots to build trust through transparency is by providing appropriate explanations for their actions. While most previous work on robot explanation generation has focused on robots’ ability to provide post-hoc explanations upon request, in this paper we instead examine proactive explanations generated before actions are taken, and the eﬀect this has on human-robot trust. Our results suggest a positive relationship between proactive explanations and human-robot trust, and reveal fundamental new questions into the eﬀects of proactive explanations on the nature of humans’ mental models and the fundamental nature of human-robot trust."
"Zhu, Yi; Wang, Taotao; Wang, Chang; Quan, Wei; Tang, Mingwei",Complexity-Driven Trust Dynamics in Human–Robot Interactions: Insights from AI-Enhanced Collaborative Engagements,2023,1,60,60,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants completed a demographic questionnaire, were trained on the experimental environment, then completed a task in accompanying mode, and finally completed a task in automated mode, with trust questionnaires administered after each task.","Participants controlled a manned aerial vehicle (MAV) and interacted with autonomous unmanned aerial vehicles (UAVs) to complete a surveillance task, detecting points and avoiding hazardous zones.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,Participants interacted with the UAVs through a simulation interface.,simulation,The interaction was conducted in a 3D simulated environment.,simulated,The robots were represented as virtual entities within the simulation.,shared control (fixed rules),"The UAVs operated autonomously based on a deep reinforcement learning algorithm, but the human operator delegated detection points.",Questionnaires,,Performance Metrics,Trust was measured using a seven-point Likert scale questionnaire and performance metrics were recorded.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated task difficulty by varying the number of detection points and danger zones, which was intended to influence trust levels.","Trust increased for easy and hard tasks, but decreased for normal tasks, showing a non-linear relationship between task difficulty and trust.","The study found a V-shaped pattern in trust levels, where trust was higher for easy and hard tasks compared to normal tasks, which is a deviation from a linear relationship between task difficulty and trust.","Task difficulty has a non-linear influence on dynamic trust in HRI, with trust increasing for easy and hard tasks, but decreasing for tasks of intermediate difficulty.","The robot (UAV) autonomously navigated to detection points while avoiding hazardous zones, and the human participant monitored the UAVs and controlled a manned aerial vehicle (MAV), delegating detection points to the UAVs.",Kruskal-Wallis,"The Kruskal-Wallis test, a non-parametric method, was used to analyze the differences in trust levels across three task difficulty conditions (easy, normal, hard). This test was chosen because the data might not conform to a normal distribution. The test was used to compare trust levels at different stages of the experiment (before interaction, after accompanying mode, and after automated mode) and to assess the impact of task difficulty on trust shifts.",TRUE,Task-complexity,Task-complexity,,"The study explicitly manipulated task complexity by varying the number of detection points and danger zones. The paper states: 'The experimental setup created in Unity simulates a surveillance task involving one human-operated manned aerial vehicle (MAV) and four autonomous UAVs using deep reinforcement learning algorithms. This setup, shown in Figure 2, includes varying complexities, such as different numbers of detection points, danger zone ranges, and area densities to be monitored. For instance, a simple task might involve a single-point detection in a low-threat environment, while a complex task could require coordination between MAVs and UAVs to identify multiple points across expansive areas with intermittent danger zones.' This manipulation of task difficulty was intended to influence trust levels, and the results showed that task complexity did impact trust, with a non-linear relationship observed. The paper states: 'Our findings align with hypothesis H1, suggesting that task difficulty exerts a pronounced nonlinear influence on the dynamic shifts in trust during HRI, as visualized in Figure 8d.' The study did not find any factors that did not impact trust.",10.3390/app132412989,https://www.mdpi.com/2076-3417/13/24/12989,"This study explores the intricate dynamics of trust in human–robot interaction (HRI), particularly in the context of modern robotic systems enhanced by artiﬁcial intelligence (AI). By grounding our investigation in the principles of interpersonal trust, we identify and analyze both similarities and differences between trust in human–human interactions and human–robot scenarios. A key aspect of our research is the clear deﬁnition and characterization of trust in HRI, including the identiﬁcation of factors inﬂuencing its development. Our empirical ﬁndings reveal that trust in HRI is not static but varies dynamically with the complexity of the tasks involved. Notably, we observe a stronger tendency to trust robots in tasks that are either very straightforward or highly complex. In contrast, for tasks of intermediate complexity, there is a noticeable decline in trust. This pattern of trust challenges conventional perceptions and emphasizes the need for nuanced understanding and design in HRI. Our study provides new insights into the nature of trust in HRI, highlighting its dynamic nature and the inﬂuence of task complexity, thereby offering a valuable reference for future research in the ﬁeld."
"Zou, Xiangying; Lv, Chunhui; Zhang, Jingyu","The Effect of Group Membership, System Reliability and Anthropomorphic Appearance on User’s Trust in Intelligent Decision Support System",2020,1,40,40,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants first completed a painting preference task to establish group membership. Then, they performed a numerical sequence prediction task with a decision support system, receiving recommendations from agents with varying group membership, appearance, and reliability. They reported trust after each trial and after every 5 trials.",Participants predicted numbers in a sequence and decided whether to follow the system's recommendation.,Unspecified,Other,Research,Evaluation,Survey/Questionnaire Completion,minimal interaction,Participants interacted with a decision support system on a computer screen.,simulation,The interaction was conducted through a computer simulation.,simulated,The robots were represented by avatars or computer-like images on a screen.,pre-programmed (non-adaptive),The system provided recommendations based on pre-set reliability levels.,Behavioral Measures; Questionnaires,,,Trust was measured using subjective ratings and compliance behavior.,no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated group membership, system reliability, and anthropomorphic appearance to influence trust. Group membership was manipulated using an artistic preference task, reliability was manipulated by changing the accuracy of the system's recommendations, and appearance was manipulated by using human-like or computer-like avatars.",In-group membership and higher reliability increased subjective trust. Group membership had a marginal effect on behavioral trust. Appearance did not significantly affect trust.,"The study found that group membership significantly influenced subjective trust, but only marginally influenced behavioral trust. The effect of group membership was not moderated by reliability or appearance. The lack of an effect of appearance on trust was unexpected.","In-group membership significantly increased subjective trust towards the intelligent system, and this effect was not dependent on system reliability or anthropomorphic appearance.",The robot (represented by an avatar) provided recommendations for number predictions. The human participant made initial predictions and then decided whether to follow the robot's recommendation.,ANOVA,"A 2 (group membership) x 2 (anthropomorphic appearance) x 3 (reliability level) repeated measures ANOVA was conducted on both subjective trust and behavioral trust. The ANOVA was used to examine the main effects of group membership, anthropomorphic appearance, and system reliability on trust, as well as any interaction effects between these factors.",TRUE,Robot-accuracy; Robot-aesthetics; Teaming,Robot-accuracy; Teaming,Robot-aesthetics,"The study manipulated three factors: system reliability (Robot-accuracy), anthropomorphic appearance (Robot-aesthetics), and group membership (Teaming). Robot-accuracy was manipulated by changing the accuracy of the system's recommendations (95%, 70%, 45%). This directly impacts the performance of the robot on the task. Robot-aesthetics was manipulated by using human-like or computer-like avatars. Teaming was manipulated using an artistic preference task to create in-group and out-group conditions. The paper states, 'We used the artistic preference task (i.e. a widely used variation of the minimum group paradigm) to manipulate the group membership'. The results showed that Robot-accuracy and Teaming significantly impacted subjective trust, with in-group membership and higher reliability leading to increased trust. The paper states, 'The result revealed a significant main effect of group membership... Subjective trust in the in-group intelligent system was significantly higher than the out-group intelligent system. ... The main effect of system reliability of intelligent system was significant... Post hoc analysis showed that subjective trust was not significantly different between 95% and 70% conditions, but trust in 95% and 70% conditions were both significantly higher than that in 45% condition'. The paper also states, 'The main effect of group membership was approaching significance... Behavioral trust in the in-group intelligent system was marginally higher than the out-group intelligent system.' Robot-aesthetics did not significantly affect trust. The paper states, 'There was no significant main effect of anthropomorphic appearance'.",,http://link.springer.com/10.1007/978-3-030-49183-3_18,"Past studies have found that the in-group membership of an intelligent agent can improve users’ trust. We explored whether such an effect depends on system reliability levels and anthropomorphic appearance. We manipulated reliability levels (95%, 70%, 45%) and anthropomorphic appearance (humanlike vs. computer-like appearance) of an intelligent decision support system in our study. The minimum group paradigm was adopted to manipulate the group membership of the intelligent system (in-group vs out-group). We measured trust by using both subjective rating and compliant behaviors toward the system recommendations. The results showed that the intelligent system with an ingroup membership resulted in higher trust as compared to the system with an out-group membership. The magnitude of these effects did not differ across different reliability levels and anthropomorphic appearances. We discussed such ﬁndings in light of human-robot interaction theories and potential implications for designing trustworthy decision support system."
"Zörner, Sebastian; Arts, Emy; Vasiljevic, Brenda; Srivastava, Ankit; Schmalzl, Florian; Mir, Glareh; Bhatia, Kavish; Strahl, Erik; Peters, Annika; Alpay, Tayfun; Wermter, Stefan",An Immersive Investment Game to Study Human-Robot Trust,2021,1,53,45,8,"8 participants were excluded because of technical issues such as robot actuator overloading, language barriers or a misunderstanding of the game rules",Controlled Lab Environment,between-subjects,"Participants were welcomed and filled out consent forms and questionnaires. They were then guided to the experiment room, where they were introduced to the space mission task and the two robot officers. Participants completed a trial round to familiarize themselves with the procedure. They then completed four experimental scenes, each involving a challenge, advice from the robots, and energy cell allocation. After each allocation, feedback was provided. Finally, participants completed a post-study questionnaire.",Participants acted as a spaceship commander and allocated energy cells between two robot officers and themselves based on the robots' advice for solving challenges during a space mission.,NICO,Humanoid Robots; Expressive Robots,Research; Social,Game,Economic Game,minimal interaction,"Participants interacted with the robots through a game scenario, allocating energy cells based on their advice.",simulation,The study used a simulated spaceship cockpit environment with visual and auditory feedback to create an immersive experience.,physical,The study used physical humanoid robots that were present in the experimental setup.,fully autonomous (limited adaptation),"The robots operated autonomously, providing advice and reacting to the participant's energy cell allocation, but with limited adaptation to the interaction.",Behavioral Measures; Questionnaires; Custom Scales,Godspeed Questionnaire; Big Five Inventory Scale; Mayer and Davis' Trust/Trustworthiness Scales (1999); Propensity to Trust Scales,Performance Metrics; Speech Data,"Trust was measured using a modified investment game, self-reported trust questionnaires, and behavioral measures based on energy cell allocation.",no modeling,"The study did not use computational models of trust, focusing on statistical analysis of the collected data.",Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the robot's non-verbal communication (NVC) and provided pre-determined feedback to influence trust. One robot displayed more elaborate NVC cues, while the other had minimal NVC. Feedback was designed to build and destroy trust.","The robot with non-verbal communication received more energy cells in the first scene, and was perceived as more anthropomorphic and animate. The study also found a correlation between self-assessed trust and trust measured from the game.","The study identified two distinct gameplay behaviors: a main group that showed a correlation between measured and self-assessed trust, and an alternating-minimum investment group that did not. The alternating-minimum investment group also showed higher scores for neuroticism. The study also found a strong correlation between cognitive and affective trust.","The study found that a scaled-up version of the investment game can be used to measure trust in human-robot interaction, and that non-verbal communication positively affects human-robot trust.",The robots provided advice on how to solve challenges during a space mission. The human participant allocated energy cells to the robots and themselves based on their perceived trustworthiness.,t-test; spearman test; Mann-Whitney U; Wilcoxon rank sum,"The study used several statistical tests. Welch's t-test was used to compare the study participants' personality scores with those of the general German population. The Spearman test was used to assess the correlation between measured trust (allocation metric) and self-assessed trust (relative trust metric). The Mann-Whitney U test was used to compare the Godspeed values for anthropomorphism, animacy, likeability, and intelligence between the NVC and MNVC conditions. Finally, the Wilcoxon test was used to compare the amount of energy cells allocated to the robot with NVC versus the robot with minimal NVC in the first scene.",TRUE,Robot-nonverbal-communication; Robot-verbal-communication-content,Robot-nonverbal-communication,,"The study explicitly manipulated the robot's non-verbal communication (NVC) by having one robot display more elaborate cues (gaze, facial expressions, gestures) while the other had minimal NVC. This is described in the 'Non-Verbal Communication' section of the paper, where it states, 'For our investigation of the effect of non-verbal communication on human-robot trust we equip both robotic officers with sets of non-verbal cues, one set more elaborate than the other.' This directly corresponds to the 'Robot-nonverbal-communication' category. The study also manipulated the content of the feedback provided to the participants after each energy cell allocation, which was designed to build and destroy trust. This is described in the 'Protocol and Game Scenes' section, where it states, 'While the specific feedback lines are adjusted to the individual allocation choices, the resulting feedback characteristic is always predetermined for each round to ensure comparability between different participants' interactions.' This manipulation of feedback content is categorized as 'Robot-verbal-communication-content'. The study found that the robot with more elaborate NVC received more energy cells in the first scene, indicating that NVC impacted trust. This is stated in the 'Impact of Non-Verbal Communication on the Perception of the Robot' section: 'In this scene, the robot that showed non-verbal communication obtained a significantly higher amount of energy cells compared to the other.' There is no mention of the feedback content manipulation not impacting trust, so it is not included in the 'factors_that_did_not_impact_trust' list.",10.3389/frobt.2021.644529,https://www.frontiersin.org/articles/10.3389/frobt.2021.644529/full,"As robots become more advanced and capable, developing trust is an important factor of human-robot interaction and cooperation. However, as multiple environmental and social factors can influence trust, it is important to develop more elaborate scenarios and methods to measure human-robot trust. A widely used measurement of trust in social science is the               investment game               . In this study, we propose a scaled-up, immersive, science fiction Human-Robot Interaction (HRI) scenario for intrinsic motivation on human-robot collaboration, built upon the investment game and aimed at adapting the investment game for human-robot trust. For this purpose, we utilize two Neuro-Inspired COmpanion (NICO) - robots and a projected scenery. We investigate the applicability of our space mission experiment design to measure trust and the impact of non-verbal communication. We observe a correlation of 0.43 (                                                                        p                     =                     0.02                                                                  ) between self-assessed trust and trust measured from the game, and a positive impact of non-verbal communication on trust (                                                                        p                     =                     0.0008                                                                  ) and robot perception for anthropomorphism (                                                                        p                     =                     0.007                                                                  ) and animacy (                                                                        p                     =                     0.00002                                                                  ). We conclude that our scenario is an appropriate method to measure trust in human-robot interaction and also to study how non-verbal communication influences a human’s trust in robots."
"Zuniga, Jorge; McCurry, Malcolm; Trafton, J. Gregory",A Process Model of Trust in Automation: A Signal Detection Theory Based Approach,2014,1,60,60,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants were introduced to the task and automation, completed a training session, and then performed the main task. They were then debriefed.","Participants tracked a moving box while monitoring a secondary window for a color change, responding when the box turned red. An automated system provided cues about the color change.",Unspecified,Other,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a simulated automation system through a computer interface.,simulation,The interaction was conducted in a simulated environment on a computer screen.,simulated,The automation system was represented as a simulated cueing system.,pre-programmed (non-adaptive),The automation system followed a pre-programmed behavior with fixed hit and false alarm rates.,Behavioral Measures,,Performance Metrics,Trust was assessed through behavioral measures such as cued and uncued switches.,"parametric models (e.g., regression)",The study used a cognitive model based on ACT-R with utility learning to model participant behavior.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The study manipulated the hit and false alarm rates of the automated system while keeping sensitivity constant, to see how participants would respond to different error patterns.",Participants were more attuned to the overall alarm rate (hit + false alarm) than to the individual hit or false alarm rates. There was no evidence that participants were impacted by false alarms.,"Participants were not impacted by false alarms, and instead focused on the overall alarm rate. There was no significant difference in uncued switches based on condition, suggesting participants were not attuned to misses.","Participants primarily focused on the overall alarm rate of the automation, rather than being sensitive to the specific types of errors (misses or false alarms).","The human participant tracked a moving box and monitored a secondary window for color changes, responding when the box turned red. The automation system provided cues about the color change, but was not perfect.",ANOVA; Tukey HSD,"The study used a one-way ANOVA to compare mean cued switch behavior across different conditions, followed by Tukey's HSD post-hoc test to identify specific differences between conditions. Another one-way ANOVA was used to analyze uncued switches, reaction time, and ignored cues across conditions. The purpose of these tests was to determine the effect of different automation error patterns on participant behavior, specifically whether participants were more attuned to hits, false alarms, or the overall alarm rate.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the hit and false alarm rates of the automated system while keeping sensitivity constant. This directly affects the accuracy of the robot's cues, which is a core aspect of 'Robot-accuracy'. The paper states, 'we manipulated the exhibited behavior of the automated system as shown in Table 1. For example, in the 91/15 condition the automated system was accurate in sounding the cue to a full cart 91% of the time (hit), however, 15% of the time that the cart was not full it also presented the cue (FA)'. The results showed that participants were more attuned to the overall alarm rate (hit + false alarm) than to the individual hit or false alarm rates, indicating that the manipulation of the robot's accuracy impacted their behavior. The paper states, 'there was an overall trend of increasing cued switches with increasing alarm (Hit + FA) rates'. There was no evidence that participants were impacted by false alarms, and no significant difference in uncued switches based on condition, suggesting participants were not attuned to misses. This indicates that while the robot accuracy was manipulated, the specific type of error (misses vs false alarms) did not impact trust differently, but the overall accuracy did.",10.1177/1541931214581174,http://journals.sagepub.com/doi/10.1177/1541931214581174,"This paper discusses the first experiment in a series designed to systematically understand the different characteristics of an automated system that lead to trust in automation. We also discuss a simple process model, which helps us understand the results. Our experimental paradigm suggests that participants are agnostic to the automation’s behavior; instead, they merely focus on alarm rate. A process model suggests this is the result of a simple reward structure and a non-explicit cost of trusting the automation."
"Złotowski, Jakub; Sumioka, Hidenobu; Nishio, Shuichi; Glas, Dylan F.; Bartneck, Christoph; Ishiguro, Hiroshi",Appearance of a Robot Affects the Impact of its Behaviour on Perceived Trustworthiness and Empathy,2016,1,60,58,2,2 participants were excluded due to data corruption,Controlled Lab Environment,mixed design,"Participants were assigned to interact with either a humanlike or machinelike robot, which displayed either positive or negative behavior during a job interview scenario across three interaction rounds. Questionnaires were administered after each interaction to measure perceived empathy, trustworthiness, and anxiety.",Participants were asked to persuade a robot interviewer to give them a job during a simulated job interview.,Geminoid HI-2; Robovie R2,Humanoid Robots; Expressive Robots,Research; Social,Social,Persuasion,minimal interaction,Participants interacted with the robot verbally in a structured interview setting.,real-world,Participants interacted with physical robots in a lab setting.,physical,Participants interacted with physical robots.,wizard of oz (directly controlled),The robots were controlled by a human operator hidden in another room.,Questionnaires,,,Trust was measured using a single question on a 7-point scale.,no modeling,The study did not use any computational models of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The robot's appearance (humanlike vs. machinelike) and behavior (positive vs. negative feedback) were directly manipulated to observe their effects on perceived empathy, trustworthiness, and anxiety.","A machinelike robot was perceived as more trustworthy than a humanlike robot. Positive behavior increased trust in the machinelike robot, but not in the humanlike robot.","The behavior of the humanlike robot did not significantly affect its perceived trustworthiness and empathy, while the behavior of the machinelike robot did. This is an unexpected result, as it suggests that the impact of behavior on trust is dependent on the robot's appearance. The study also found that a robot's negative behavior increased anxiety towards its communication capabilities.","A machinelike robot was perceived as more trustworthy and empathic than a highly humanlike robot, and its behavior significantly impacted these perceptions, while the humanlike robot's behavior did not.","The robot acted as an interviewer, asking questions and providing feedback (positive or negative) on the participant's responses. The human participant's task was to persuade the robot to give them a job.",ANOVA,"The study used 3-way Analysis of Variance (ANOVAs) with appearance and behaviour as between-subjects factors, and interaction round as a within-subjects factor to analyze the data. Separate ANOVAs were conducted for perceived empathy, trustworthiness, and each subscale of the Robot Anxiety Scale (RAS).",TRUE,Robot-aesthetics; Robot-social-attitude; Task-complexity,Robot-aesthetics; Robot-social-attitude,,"The study manipulated the robot's appearance by using two different robots: Geminoid HI-2 (humanlike) and Robovie R2 (machinelike). This is categorized as 'Robot-aesthetics' because it directly relates to the visual design and style of the robot. The robot's behavior was manipulated to be either positive (nodding, saying 'Un') or negative (shaking head, saying 'Asso'), which is categorized as 'Robot-social-attitude' because it reflects the robot's social approach and feedback during the interaction. The task was a job interview, which was structured with a set of questions and responses, and the job position changed between interaction rounds, which is categorized as 'Task-complexity' because it changes the cognitive demands of the task. The results showed that both 'Robot-aesthetics' and 'Robot-social-attitude' impacted trust. Specifically, the machinelike robot (Robovie R2) was perceived as more trustworthy than the humanlike robot (Geminoid HI-2), and positive behavior increased trust in the machinelike robot but not in the humanlike robot. There were no factors that were manipulated that did not impact trust.",10.1515/pjbr-2016-0005,https://www.degruyter.com/document/doi/10.1515/pjbr-2016-0005/html,"An increasing number of companion robots have started reaching the public in the recent years. These robots vary in their appearance and behavior. Since these two factors can have an impact on lasting human–robot relationships, it is important to understand their effect for companion robots. We have conducted an experiment that evaluated the impact of a robot’s appearance and its behaviour in repeated interactions on its perceived empathy, trustworthiness and anxiety experienced by a human. The results indicate that a highly humanlike robot is perceived as less trustworthy and empathic than a more machinelike robot. Moreover, negative behaviour of a machinelike robot reduces its trustworthiness and perceived empathy stronger than for highly humanlike robot. In addition, we found that a robot which disapproves of what a human says can induce anxiety felt towards its communication capabilities. Our findings suggest that more machinelike robots can be more suitable as companions than highly humanlike robots. Moreover, a robot disagreeing with a human interaction partner should be able to provide feedback on its understanding of the partner’s message in order to reduce her anxiety."
"de Miguel, Miguel Angel; Fuchshuber, Daniel; Hussein, Ahmed; Olaverri-Monreal, Cristina",Perceived Pedestrian Safety: Public Interaction with Driverless Vehicles,2019,1,49,49,0,No participants were excluded,Real-World Environment,within-subjects,"Pedestrians interacted with an autonomous vehicle in a public space, with some pedestrians completing a post-task questionnaire.",Pedestrians crossed a marked crosswalk in front of an autonomous vehicle.,iCab,Autonomous Vehicles; Mobile Robots,Research,Navigation,Street Crossing,minimal interaction,Pedestrians interacted with the autonomous vehicle by crossing in front of it.,real-world,The interaction took place in a real-world environment with a physical robot.,physical,The robot was a physical autonomous vehicle.,fully autonomous (limited adaptation),"The robot operated autonomously, stopping for pedestrians, but with limited adaptation.",Behavioral Measures; Questionnaires,,Video Data,Trust was assessed through video analysis of pedestrian behavior and a post-task questionnaire.,no modeling,Trust was not modeled computationally.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the presence of a visual interface on the AV to communicate its awareness of pedestrians, using color-coded and eye-contact-based signals.","The presence of the interface tended to reduce hesitations when crossing, but this effect was not statistically significant.","The study found that pedestrians' reactions varied from curiosity to avoidance, and that the longer the interaction with the vehicle, the more confident they became. There was a preference for the eye-contact interface over the color-coded interface, but this was not statistically significant.","The study found that the presence of a communication system on the autonomous vehicle was perceived as positive by pedestrians, and that the longer the interaction, the more confident they became.","The robot autonomously drove along a predefined path, stopping for pedestrians. The human pedestrians crossed the street in front of the robot.",Chi-squared; t-test; Chi-squared,"The study used a chi-squared test to compare the difference between the percentage of positive and negative reactions observed in the video analysis. A t-test was used to compare the mean hesitation scores between the baseline condition and the interface conditions. Another chi-squared test was used to compare the percentage of people who did not hesitate when crossing between the baseline and interface conditions. Finally, a chi-squared test was used to compare the preference between the color-coded and eye-contact interfaces.",TRUE,Robot-interface-design,,Robot-interface-design,"The study manipulated the visual interface on the autonomous vehicle to communicate its awareness of pedestrians. Specifically, they tested three conditions: a baseline with no interface, a color-coded interface (red/green), and an eye-contact interface (open/closed eyes). This falls under 'Robot-interface-design' because it involves changes to interactive elements (screen displays) on the robot. While the presence of the interface did tend to reduce hesitations, this effect was not statistically significant, meaning that the interface design did not significantly impact trust. Therefore, 'Robot-interface-design' is listed as a factor that did not impact trust.",10.1109/IVS.2019.8814145,https://ieeexplore.ieee.org/document/8814145/,"Trust plays a decisive role in the public’s acceptance of the new self-driving car technology. In order to better understand how to promote conﬁdence in vehicle automation safety among the public, we studied pedestrian behavior shortly before and while crossing a marked crosswalk. Such information is also essential for setting parameters for automated vehicles to act accordingly during interactions with pedestrians. Through the analysis of the recorded videos and subjective qualitative data, we identiﬁed factors that potentially inﬂuence the perception of a road situation as safe in an environment in which vehicles operate with full driving automation (level 5) in a public space. A variety of responses were observed that exhibit several levels of trust, uncertainty and a certain degree of fear. It became clear, however, that the longer the people interacted with the vehicles, the more conﬁdent and trusting they became in automation capabilities. The existence of a communication system to interact with driverless vehicles was also evaluated as positive."
"de Visser, Ewart; Parasuraman, Raja","Adaptive Aiding of Human-Robot Teaming: Effects of Imperfect Automation on Performance, Trust, and Workload",2011,2,12,12,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants performed a simulated reconnaissance mission using a high-fidelity multi-UV simulation, completing three main tasks: monitoring an XUV for target detection accuracy, classifying detected vehicles, and monitoring the route of other XUV and UAV assets. Participants completed two blocks of six trials with different levels of automation reliability and task load.","Participants monitored and checked an XUV for target detection accuracy, classified detected vehicles, and monitored the route of other XUV and UAV assets.",Unspecified,Unmanned Ground Vehicles (UGVs); Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,Participants interacted with the simulation through a tactical control interface.,simulation,Participants used a high-fidelity multi-UV simulation environment.,simulated,The robots were simulated within the high-fidelity environment.,shared control (fixed rules),"The robots had some autonomy, but the human operator had to monitor and control them.",Questionnaires; Custom Scales,NASA Task Load Index (NASA-TLX); Situational Awareness Rating Technique (SART),,Trust was measured using an adapted version of a common trust and self-confidence measure and the NASA-TLX.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The reliability of the automatic target recognition (ATR) system and the task load were directly manipulated to assess their impact on trust and performance.,"Trust in the automation decreased as reliability decreased, while self-confidence in the participant's ability to complete the mission without automation increased as reliability decreased.","The study found that even imperfect automation can provide benefits to overall system performance, and that the user was able to improve on the ATR system, especially in low reliability conditions. There was an interaction between reliability and task load, but not precisely in the direction predicted.","Human-robot team performance was higher than either human or ATR performance alone, even with imperfect automation.","The robot (XUV) performed RSTA scans and detected targets with varying reliability. The human participant monitored the robot's performance, classified detected vehicles, and monitored the route of other robotic assets.",ANOVA,"The study used a 3x2x2 ANOVA to analyze most of the dependent variables, including mission completion time, detection performance, classification performance, situation awareness, subjective workload, and subjective trust and self-confidence. User detection performance was analyzed using a 2x2x2 ANOVA, omitting the high reliability level. The ANOVAs were used to examine the effects of reliability (low, medium, high), task load (low, high), and block (first, second) on the dependent variables. The purpose was to determine the statistical significance of the manipulated factors on the measured outcomes.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy,Task-complexity,"The study manipulated 'Robot-accuracy' by varying the reliability of the automatic target recognition (ATR) system (30%, 70%, or 100%). This directly impacted the robot's ability to correctly identify targets, which is a performance metric. The study also manipulated 'Task-complexity' by varying the number of vehicles participants had to monitor (two XUVs and one UAV for low load, four XUVs and two UAVs for high load). This manipulation changed the cognitive demands on the participants. The results showed that 'Robot-accuracy' impacted trust, with trust decreasing as reliability decreased. However, 'Task-complexity' did not have a significant impact on trust, although it did impact workload.",10.1177/1555343411410160,http://journals.sagepub.com/doi/10.1177/1555343411410160,"In many emerging civilian and military operations, human operators are increasingly being tasked to supervise multiple robotic uninhabited vehicles (UVs) with the support of automation. As 100% automation reliability cannot be assured, it is important to understand the effects of automation imperfection on performance. In addition, adaptive aiding may help counter any adverse effects of static (fixed) automation. Using a high-fidelity multi-UV simulation involving both air and ground vehicles, two experiments examined the effects of automation reliability and adaptive automation on human-system performance with different levels of task load. In Experiment 1, participants performed a reconnaissance mission while assisted with an automatic target recognition (ATR) system whose reliability was low, medium, or high. Overall human-robot team performance was higher than with either human or ATR performance alone. In Experiment 2, participants performed a similar reconnaissance mission with no ATR, static automation, or with adaptive automation keyed to task load. Participant trust and self-confidence were higher and workload was lower for adaptive automation compared with the other conditions. The results show that human-robot teams can benefit from imperfect static automation even in high task load conditions and that adaptive automation can provide additional benefits in trust and workload."
"de Visser, Ewart; Parasuraman, Raja","Adaptive Aiding of Human-Robot Teaming: Effects of Imperfect Automation on Performance, Trust, and Workload",2011,2,14,14,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants performed a simulated reconnaissance mission with two unmanned vehicles, a UAV and a UGV, using the same SIL simulation as in Experiment 1. Participants had to determine whether a soldier was present in a forest displayed on the RSTA scan. There were three automation conditions: manual, static automation, and adaptive automation. Participants completed six missions.","Participants conducted a reconnaissance mission with a UAV and a UGV, determining whether a soldier was present in a forest displayed on the UGV's RSTA scan.",Unspecified,Unmanned Aerial Vehicles (UAVs); Unmanned Ground Vehicles (UGVs),Research,Supervision,Monitoring,minimal interaction,Participants interacted with the simulation through a tactical control interface.,simulation,Participants used a high-fidelity multi-UV simulation environment.,simulated,The robots were simulated within the high-fidelity environment.,shared control (fixed rules),"The robots had some autonomy, but the human operator had to monitor and control them.",Questionnaires; Custom Scales,NASA Task Load Index (NASA-TLX); Situational Awareness Rating Technique (SART),,Trust was measured using an adapted trust questionnaire and the NASA-TLX.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The type of automation (manual, static, or adaptive) was directly manipulated, with adaptive automation being invoked during high task load phases.",Trust in the automation was higher in the adaptive condition compared to the static condition. Self-confidence was also higher in the adaptive automation condition.,"The study found that adaptive automation yielded benefits for situation awareness, self-confidence, and to a lesser extent, experienced workload and trust. The finding of higher trust in adaptive than in static automation is inconsistent with the claim that adaptive automation is less likely to be trusted.","Adaptive automation yielded benefits for situation awareness, self-confidence, and to a lesser extent, subjective workload and trust, compared to static automation and manual performance.","The robot (UGV) performed RSTA scans, and the human participant determined whether a soldier was present in the forest displayed on the scan. The human also monitored the UAV.",ANOVA; pairwise comparisons; contrast analyses,"The study used ANOVAs with within-subjects variables of automation condition (manual, static, adaptive), task load (low, high), and block (1, 2) to analyze performance data, specifically UGV detection accuracy and reaction time. Subjective data, including trust, self-confidence, and workload, were analyzed using ANOVAs with automation condition and block as within-subjects variables. Pairwise comparisons were used to further examine significant main effects. Contrast analyses were used to examine the effects of automation on self-confidence. The purpose of these tests was to determine the statistical significance of the different automation conditions and task loads on performance and subjective measures.",TRUE,Robot-autonomy; Task-complexity,Robot-autonomy,Task-complexity,"The study manipulated 'Robot-autonomy' by varying the type of automation support provided: manual (no automation), static automation (ATR always on), and adaptive automation (ATR on only during high task load). This manipulation changed the level of decision authority given to the robot. The study also manipulated 'Task-complexity' by varying the density of the forest in the RSTA scans (low density for low load, high density for high load), which changed the cognitive demands of the target detection task. The results showed that 'Robot-autonomy' impacted trust, with trust being higher in the adaptive automation condition compared to the static condition. However, 'Task-complexity' did not have a significant impact on trust, although it did impact detection accuracy and reaction time.",10.1177/1555343411410160,http://journals.sagepub.com/doi/10.1177/1555343411410160,"In many emerging civilian and military operations, human operators are increasingly being tasked to supervise multiple robotic uninhabited vehicles (UVs) with the support of automation. As 100% automation reliability cannot be assured, it is important to understand the effects of automation imperfection on performance. In addition, adaptive aiding may help counter any adverse effects of static (fixed) automation. Using a high-fidelity multi-UV simulation involving both air and ground vehicles, two experiments examined the effects of automation reliability and adaptive automation on human-system performance with different levels of task load. In Experiment 1, participants performed a reconnaissance mission while assisted with an automatic target recognition (ATR) system whose reliability was low, medium, or high. Overall human-robot team performance was higher than with either human or ATR performance alone. In Experiment 2, participants performed a similar reconnaissance mission with no ATR, static automation, or with adaptive automation keyed to task load. Participant trust and self-confidence were higher and workload was lower for adaptive automation compared with the other conditions. The results show that human-robot teams can benefit from imperfect static automation even in high task load conditions and that adaptive automation can provide additional benefits in trust and workload."
"de Visser, Ewart; Kidwell, Brian; Payne, John; Lu, Li; Parker, James; Brooks, Nathan; Chabuk, Timur; Spriggs, Sarah; Freedy, Amos; Scerri, Paul; Parasuraman, Raja",Best of Both Worlds: Design and Evaluation of an Adaptive Delegation Interface,2013,1,16,16,0,No participants were excluded,Controlled Lab Environment,within-subjects,"Participants were trained on the AIMS interface, completed four mission scenarios with congruent and incongruent conditions, and then completed the NASA-TLX, trust measures, and a concept map after each scenario.","Participants collaborated with the SAMI automation to plan, execute, and monitor a mission involving two UAVs coordinating a collaborative attack on a target.",Unspecified,Unmanned Aerial Vehicles (UAVs),Research,Supervision,Monitoring,minimal interaction,"Participants interacted with the system through a user interface, making decisions and monitoring the UAVs.",simulation,The interaction was through a simulated environment showing the mission map and UAV actions.,simulated,The UAVs were represented as simulated entities on a map.,shared control (fixed rules),"The automation proposed plans and the user made decisions, with the automation following fixed rules.",Questionnaires,Jian et al. Trust Scale,,Trust was measured using a questionnaire.,no modeling,Trust was measured but not modeled computationally.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the congruence between the mission model and the map visualizations, which influenced the perceived performance of the automation.","Distrust was significantly higher in the incongruent condition, but overall trust was not significantly different between conditions.","Although overall trust was not significantly different, distrust was significantly higher in the incongruent condition, suggesting that participants were sensitive to inconsistencies in the system's behavior.","Participants were calibrated to the automation, showing higher distrust when the system's behavior was incongruent with the mission model.","The human operator selected mission roles and flight paths for two UAVs, monitored their progress, and approved the final attack, while the automation (SAMI) proposed plans and executed the mission based on the operator's decisions.",ANOVA,"The study used ANOVA to analyze the differences in memory scores for relationships between mission components across the four sessions. ANOVA was also used to compare distrust levels between the congruent and incongruent conditions, and to compare deviation detection rates between the congruent and incongruent conditions. No significant difference was found in trust levels or workload between the two conditions.",TRUE,Robot-accuracy,Robot-accuracy,,"The study manipulated the congruence between the mission model and the map visualizations. In the congruent condition, the map visualizations matched the mission task model, while in the incongruent condition, they did not. This directly affected the accuracy of the robot's (SAMI's) actions in relation to the planned mission, thus influencing task performance metrics. Specifically, the UAVs in the incongruent condition flew next to the designated path or around a no-fly zone in a different direction than indicated. This manipulation of the robot's behavior directly impacted the perceived accuracy of the system, making 'Robot-accuracy' the most appropriate category. The results showed that distrust was significantly higher in the incongruent condition, indicating that the manipulation of 'Robot-accuracy' impacted trust. There were no other factors that were manipulated in the study.",10.1177/1541931213571056,http://journals.sagepub.com/doi/10.1177/1541931213571056,"The proliferation of unmanned aerial vehicles (UAVs) in civil and military domains has spurred increasingly complex automation design for augmenting operator abilities, reducing workload, and increasing mission effectiveness.	    We describe the Adaptive Interface Management System (AIMS), an intelligent adaptive delegation interface for controlling and monitoring multiple unmanned vehicles, with a mixed-initiative team model language. A study was conducted to assess understanding of this model language and whether participants exhibited calibrated trust in the intelligent automation. Results showed that operators had accurate memory for role responsibility and were well calibrated to the automation. Adaptive automation design approaches like the one described in this paper can be useful to create mixedinitiative human-robot teams."
"de Vries, Peter; Midden, Cees; Bouwhuis, Don","The effects of errors on system trust, self-confidence, and the allocation of control in route planning",2003,1,96,96,0,No participants were excluded,Controlled Lab Environment,between-subjects,"Participants completed 26 route-planning trials using a computer program. The first 20 trials were fixed, with 10 manual and 10 automatic trials, with varied error rates. The final 6 trials allowed participants to choose between manual and automatic modes. Participants also placed bets on the outcome of each trial.","Participants were asked to plan the quickest route on a city map, using either a manual or automatic mode, and to bet on the outcome of each trial.",Unspecified,Other,Research,Navigation,Path Following,minimal interaction,Participants interacted with a computer program to plan routes.,simulation,Participants used a computer-based simulation of a city map.,simulated,The robot was represented as a simulated route planner on a computer screen.,pre-programmed (non-adaptive),The automatic route planner followed a pre-programmed algorithm without adapting to user input.,Behavioral Measures; Questionnaires,,Performance Metrics,Trust was measured using the number of credits staked and self-reported trust ratings.,"parametric models (e.g., regression)","Regression analyses were used to model the relationship between error rates, trust, self-confidence, and control allocation.",Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,"The researchers manipulated the error rates in both the automatic and manual modes, and provided feedback on the route quality, which influenced trust and self-confidence.","High automation error rates decreased system trust, while high manual error rates decreased self-confidence. The difference between trust and self-confidence predicted the use of automatic mode.","The study found a bias towards trusting one's own abilities over the system, even when the system performed better. Also, manipulations in manual error rates had a small effect on trust in the automatic mode, which was unexpected.","The difference between trust in automation and self-confidence is a strong predictor of automation use, and there is a fundamental bias to trust one's own abilities more than those of a system.","The robot (simulated route planner) generated routes automatically, while the human planned routes manually by clicking on the map. The human also placed bets on the outcome of each trial.",ANOVA; Linear regression; Linear regression,"The study used ANOVA to analyze the effects of automation error rate (AER) and manual error rate (MER) on the number of times automatic mode was selected, the number of credits staked in automatic and manual trials, and the ratings of system trust and self-confidence. Linear regression was used to examine the relationship between the difference between trust and self-confidence (T-SC) and control allocation. Regression analyses were also used to investigate whether T-SC mediated the relationship between error rates and control allocation.",TRUE,Robot-accuracy; Task-complexity,Robot-accuracy,Task-complexity,"The study manipulated the error rates of both the automatic and manual route planning modes. This directly impacts the 'Robot-accuracy' as it changes the success rate of the automated system. The study also implicitly manipulated 'Task-complexity' by having participants perform a route planning task, which has a certain level of cognitive demand. The error rate manipulation directly impacted trust, as higher error rates in the automatic mode led to lower trust in the system. The task complexity, while present, was not directly manipulated to test its impact on trust, and therefore did not impact trust in the context of this study. The study explicitly states that 'high automation error rates (AERs) decreased levels of system trust compared to low AERs' and 'high manual error rates (MERs) resulted in lower levels of selfconfidence compared to low MERs'. The manipulation of error rates is a direct manipulation of the robot's accuracy in completing the task. The task complexity was not manipulated, but was a constant factor in the experiment.",10.1016/S1071-5819(03)00039-9,https://linkinghub.elsevier.com/retrieve/pii/S1071581903000399,"The concept of trust is believed by some to compensate for feelings of uncertainty. Therefore, trust is considered to be crucial in people’s decision to rely on a complex automated system to perform tasks for them. This experiment aimed to study the effects of errors on control allocation, and the mediating role of trust and self-conﬁdence in the domain of route planning. Using a computer-based route planner, participants completed 10 route-planning trials in manual mode, and 10 in automatic mode, allowing participants to become equally experienced in operating both modes. During these so-called ﬁxed trials, the numbers of errors in automatic as well as manual mode were systematically varied. Subsequently, participants completed six free trials, during which they were free to choose between modes. Our results showed that high automation error rates (AERs) decreased levels of system trust compared to low AERs. Conversely, high manual error rates (MERs) resulted in lower levels of selfconﬁdence compared to low MERs, although to a lesser extent. Moreover, the difference between measures of trust and self-conﬁdence proved to be highly predictive of the number of times automatic mode was selected during the six free trials. Additionally, results suggest a fundamental bias to trust one’s own abilities over those of the system. Finally, evidence indicating a relationship between trust and self-conﬁdence is discussed."
"van Maris, Anouk; Lehmann, Hagen; Natale, Lorenzo; Grzyb, Beata",The Influence of a Robot's Embodiment on Trust: A Longitudinal Study,2017,1,17,17,0,No participants were excluded,Controlled Lab Environment,mixed design,"Participants interacted with either a physical robot or a virtual agent in 10 sessions over 6 weeks. They completed a map task, and a trust game was played after the first and last session.",Participants completed a blank map by answering questions about countries and capital cities posed by the robot.,Nao,Humanoid Robots,Research; Social,Game,Economic Game,minimal interaction,Participants interacted with a robot or virtual agent in a controlled setting.,simulation,The interaction involved a virtual agent on a screen or a physical robot.,physical,The study used both a physical robot and a virtual agent.,pre-programmed (non-adaptive),The robot was fully automated and followed a pre-set interaction pattern.,Behavioral Measures,,,Trust was measured using a trust game.,no modeling,The study did not use any computational model of trust.,Empirical HRI Studies,Physical Robot Studies,Direct Manipulation,"The study manipulated the robot's embodiment by using either a physical robot or a virtual agent, which influenced the interaction medium.","The study found no significant influence of embodiment on trust, but trust increased over time.","The study found no significant difference in trust based on the robot's embodiment, which contradicts some previous findings. The increase in trust over time was consistent across both conditions.","User trust increased significantly over time, irrespective of the robot's embodiment.","The robot asked the participant questions about a map, and the participant answered. The robot provided the correct answer if the participant did not correct themselves after an error.",ANOVA,"A repeated measures ANOVA was used to determine if trust differed statistically significantly over time, and if there was a significant difference in trust based on the robot's embodiment. It also tested for a significant interaction between embodiment and time.",TRUE,Robot-aesthetics,,Robot-aesthetics,"The study manipulated the robot's embodiment by using either a physical robot or a virtual agent. This manipulation is categorized as 'Robot-aesthetics' because it changes the visual appearance of the interaction partner, from a physical robot to a virtual agent on a screen. The paper states, 'Our study used a humanoid robot NAO and a virtual agent NAO. Hence, the robot's appearance as well as the interaction patterns were similar in both conditions, only the embodiment differed.' This indicates that the core difference was in the visual presentation of the robot, not in its behavior or communication style. The results showed that this manipulation of embodiment did not significantly impact trust, as stated in the paper: 'No significant difference was found for embodiment (F (1, 15) = .69, p = .796, η 2 p = .005)'. Therefore, 'Robot-aesthetics' is listed as a factor that did not impact trust.",10.1145/3029798.3038435,https://dl.acm.org/doi/10.1145/3029798.3038435,"Trust, taken from the human perspective, is an essential factor that determines the use of robots as companions or care robots, especially given the long-term character of the interaction. This study investigated the inﬂuence of a robot’s embodiment on people’s trust over a prolonged period of time. The participants engaged in a collaborative task either with a physical robot or a virtual agent in 10 sessions spread over a period of 6 weeks. While our results showed that the level of trust was not inﬂuenced by the type of embodiment, time here was an important factor showing a signiﬁcant increase in user’s trust. Our results raise new questions on the role of the embodiment in trust and contribute to the growing research in the area of trust in human-robot interaction."
"van Straten, Caroline L.; Peter, Jochen; Kühne, Rinaldo; de Jong, Chiara; Barco, Alex",Technological and Interpersonal Trust in Child-Robot Interaction: An Exploratory Study,2018,1,88,87,1,1 participant did not complete the interaction,Educational Setting,,"Children interacted with a Nao robot in a Wizard of Oz setup, followed by a survey with open-ended questions about their trust in the robot.",Children engaged in a guessing game with the robot and answered personal questions.,Nao,Humanoid Robots; Expressive Robots,Social; Research,Social,Conversation,minimal interaction,Children interacted with the robot verbally and through a guessing game.,real-world,Children interacted with a physical robot in a real-world setting.,physical,The study used a physical Nao robot.,wizard of oz (directly controlled),The robot was operated by a human experimenter through a Wizard of Oz setup.,Questionnaires,,Video Data; Speech Data,"Trust was assessed through open-ended questions following closed-ended trust items, and video and speech data were collected.",no modeling,The study did not use computational modeling of trust.,Empirical HRI Studies,Wizard-of-Oz Studies,No Manipulation,"The study did not manipulate any specific factors related to trust, but rather explored children's explanations of their trust judgments.",,"Children's explanations of trust were more frequently related to interpersonal trust than technological trust. A third category of trust judgments emerged, where children referred to technological properties as a reason for interpersonal trust.","Children distinguish between technological and interpersonal trust in robots, and their interpersonal trust seems to be influenced by their considerations of the robot's technological properties.","The robot engaged children in small talk and a guessing game, while the children answered questions and provided their opinions about the robot.",,"The study used a qualitative 'template analysis style' to analyze children's open-ended explanations of their trust in the robot. The data was coded into predefined categories of technological and interpersonal trust, with subcategories inductively defined. No statistical tests were used.",FALSE,,,,"The study did not manipulate any factors related to trust. The study was exploratory and aimed to understand how children explain their trust in robots. The interaction was scripted and the robot's behavior was consistent across all participants. Therefore, no factors were intentionally manipulated to observe their impact on trust. The study focused on analyzing children's open-ended explanations of their trust judgments, rather than manipulating any variables.",10.1145/3284432.3284440,https://dl.acm.org/doi/10.1145/3284432.3284440,"This study aimed to explore technological and interpersonal trust in interactions between children and social robots. Specifically, we focused on whether children distinguish between these two types of trust and whether the two constitute independent constructs or interact. Using an exploratory approach, we analyzed the explanations 87 children, aged 7 to 11 years, offered for the degree to which they indicated to trust a robot with which they had just interacted. Our results suggest that children distinguished between technological and interpersonal trust in a robot. Three main categories of answers could be identified: answers relating to technological trust, those indicating the presence of interpersonal trust, and a third category in which children referred to technological properties of robots as a reason for the existence of interpersonal trust. We discuss these findings in light of the development of child-robot relationships and the design of future child-robot interaction studies."
"van Zoelen, Emma M.; Barakova, Emilia I.; Rauterberg, Matthias",Adaptive Leader-Follower Behavior in Human-Robot Collaboration,2020,1,18,17,1,1 participant was excluded due to missing data in the Collaboration Fluency questionnaire,Controlled Lab Environment,within-subjects,"Participants performed a collaborative navigation task with a robot on a leash across four runs, with different virtual object locations in each run. After each run, participants completed a questionnaire and answered interview questions.","Participants collaborated with a robot to navigate from one point to another on a field, collecting virtual objects along the way, while trying to maximize their score.",Unspecified,Mobile Robots,Research,Navigation,Path Following,direct-contact interaction,Participants physically interacted with the robot by holding its leash.,real-world,The interaction took place in a real-world setting with a physical robot.,physical,The robot was a physical entity present in the real-world environment.,wizard of oz (directly controlled),The robot was controlled remotely by a human Wizard.,Questionnaires; Behavioral Measures,,Video Data; Performance Metrics,Trust was assessed using a collaboration fluency questionnaire and behavioral measures from video analysis.,no modeling,No computational model of trust was developed in this study.,Empirical HRI Studies,Wizard-of-Oz Studies,Indirect Manipulation,"The task was designed to create implicit conflicts between the human and robot's goals, and the robot provided implicit and explicit feedback through its actions and the task environment, influencing the human's leading behavior.","Subjective collaboration fluency increased over time, but participants who took the lead more often valued the collaboration more negatively.","Participants' subjective collaboration fluency increased over time, regardless of their specific behavior, but participants who exhibited more leading behavior tended to rate the collaboration more negatively. There was a weak negative correlation between the duration of a stretched leash and subjective collaboration fluency.","People's appreciation of the collaboration increases as they do more runs with the robot, no matter what their specific behavioral development is, but participants who were more inclined to follow the robot had a higher appreciation of the collaboration.","The robot moved along a default route to pick up virtual objects, unless pulled away by the human. The human participant held the robot's leash and decided whether to lead or follow the robot to reach the goal and collect virtual objects.",ANOVA; Tukey HSD; Pearson correlation,"A Repeated Measure ANOVA was used to analyze the Collaboration Fluency questionnaire results across the four runs to determine if there were significant differences within-subjects. A post-hoc analysis using a Tukey HSD test with Bonferroni correction was then conducted to identify between which specific runs any significant differences occurred. Additionally, a Pearson correlation test was used to examine the relationship between the duration of leading behaviors (leash pull, participant movement, and participant location) and the subjective Collaboration Fluency scores.",TRUE,Robot-task-strategy; Task-constraints,Robot-task-strategy,,"The study manipulated the robot's task strategy by having it follow a default route to pick up virtual objects unless pulled away by the human participant. This created implicit conflicts between the human and robot's goals, influencing the human's leading behavior and impacting their subjective collaboration fluency. This is described in the paper as 'The robot would follow a default route to pick up all virtual objects, unless being pulled away from this route by the human participant.' The task constraints were also manipulated by introducing a time pressure element, where participants lost points for every second it took them to reach the final target location, as well as the need to collect virtual objects to gain points. This is described in the paper as 'They were given 60 points at the start of a run, but lost a point for every second it took them to reach the final target location. They were told that they could earn extra points by picking up an unknown number of virtual objects that were hidden in the environment'. While the task constraints were manipulated, the paper does not explicitly state that this manipulation impacted trust. The paper does state that 'when participants stretch the leash more, they will generally score lower on the questionnaire' and 'if people portray less (explicit) leading behavior, the Collaboration Fluency is higher'. This indicates that the robot's task strategy, which influenced the human's leading behavior, impacted trust.",10.1109/RO-MAN47096.2020.9223548,https://ieeexplore.ieee.org/document/9223548/,"As developments in artificial intelligence and robotics progress, more tasks arise in which humans and robots need to collaborate. With changing levels of complementarity in their capabilities, leadership roles will constantly shift. The research presented explores how people adapt their behavior to initiate or accommodate continuous leadership shifts in humanrobot collaboration and how this influences trust and understanding. We conducted an experiment in which participants were confronted with seemingly conflicting interests between robot and human in a collaborative task. This was embedded in a physical navigation task with a robot on a leash, inspired by the interaction between guide dogs and blind people. Explicit and implicit feedback factors from the task and the robot partner proved to trigger humans to reconsider when to lead and when to follow, while the outcome of this differed across participants. Overall the participants evaluated the collaboration more positively over time, while participants who took the lead more often valued the collaboration more negatively than other participants."
"van den Brule, Rik; Dotsch, Ron; Bijlstra, Gijsbert; Wigboldus, Daniel H. J.; Haselager, Pim",Do Robot Performance and Behavioral Style affect Human Trust?: A Multi-Method Approach,2014,2,160,156,4,"3 participants were excluded because they indicated they had prior knowledge about the experiment, 1 participant was excluded because of a computer error",Controlled Lab Environment,between-subjects,"Participants first watched a baseline video of a robot performing a task, then watched one of 16 videos with manipulated robot behavior and task performance, and then answered questions about the second robot.",Participants watched videos of a robot performing a ball-sorting task and rated the robot's trustworthiness.,Unspecified,Humanoid Robots,Research,Evaluation,Rating,passive observation,Participants passively observed videos of the robot.,media,Participants watched videos of the robot performing a task.,simulated,The robot was a simulated 3D model.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Questionnaires,,,Trust was measured using a questionnaire with Likert scale questions.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's task performance (making mistakes or not) and motion fluency (trembling or smooth) were directly manipulated to see how they affected trust.,"The robot was trusted more when it performed well and moved smoothly; trembling motions decreased trust, especially when the robot performed well.","The effect of motion fluency on trustworthiness was larger when the robot performed well, suggesting a possible anchoring effect from the baseline video.","Robot task performance and motion fluency significantly affect human trust judgments, with smooth motions leading to higher trust.",The robot's task was to remove brown balls from a conveyor belt. The human participant watched the robot perform this task and then rated its trustworthiness.,t-test; ANOVA; t-test,"Independent sample t-tests were used as manipulation checks to confirm that participants noticed the experimental manipulations. A between-subjects ANOVA was conducted to analyze the effects of gaze behavior, motion fluency, hesitation, and task performance on trustworthiness ratings. Post hoc tests were used to further examine significant interaction effects.",TRUE,Robot-nonverbal-communication; Robot-accuracy,Robot-nonverbal-communication; Robot-accuracy,,"The study manipulated the robot's motion fluency (trembling vs. smooth), which is a form of nonverbal communication, and the robot's task performance (making mistakes or not), which directly impacts accuracy. The paper states, 'For fluency of movement, we created a smooth style... and a trembling style...'. This clearly indicates a manipulation of the robot's physical movements, thus 'Robot-nonverbal-communication'. The paper also states, 'We included two levels of task performance. In the good performance condition, the robot made no mistakes. In the relatively bad performance condition, the robot made some mistakes...'. This is a direct manipulation of the robot's accuracy. The results showed that both motion fluency and task performance significantly affected trust, as stated in the paper: '...the robot was trusted more when it performed its motions smoothly... and ...a robot which made no mistakes was rated as more trustworthy...'. Therefore, both 'Robot-nonverbal-communication' and 'Robot-accuracy' are listed as factors that impacted trust. The study also manipulated gaze behavior and hesitation, but these did not impact trust, so they are not included in the factors that impacted trust.",10.1007/s12369-014-0231-5,http://link.springer.com/10.1007/s12369-014-0231-5,"An important aspect of a robot’s social behavior is to convey the right amount of trustworthiness. Task performance has shown to be an important source for trustworthiness judgments. Here, we argue that factors such as a robot’s behavioral style can play an important role as well. Our approach to studying the effects of a robot’s performance and behavioral style on human trust involves experiments with simulated robots in video human–robot interaction (VHRI) and immersive virtual environments (IVE). Although VHRI and IVE settings cannot substitute for the genuine interaction with a real robot, they can provide useful complementary approaches to experimental research in social human robot interaction. VHRI enables rapid prototyping of robot behaviors. Simulating human–robot interaction in IVEs can be a useful tool for measuring human responses to robots and help avoid the many constraints caused by real-world hardware. However, there are also difficulties with the generalization of results from one setting (e.g., VHRI) to another (e.g. IVE or the real world), which we discuss. In this paper, we use animated robot avatars in VHRI to rapidly identify robot behavioral styles that affect human trust assessment of the robot. In a subsequent study, we use an IVE to measure behavioral interaction between humans and an animated robot avatar equipped with behaviors from the VHRI experiment. Our findings reconfirm that a robot’s task performance influences its trustworthiness, but the effect of the behavioral style identified in the VHRI study did not influence the robot’s trustworthiness in the IVE study."
"van den Brule, Rik; Dotsch, Ron; Bijlstra, Gijsbert; Wigboldus, Daniel H. J.; Haselager, Pim",Do Robot Performance and Behavioral Style affect Human Trust?: A Multi-Method Approach,2014,2,87,81,6,"3 participants did not complete the full three blocks, 2 participants did not monitor the robot during the first block, 1 participant's behavioral data was not recorded due to a software error",Controlled Lab Environment,mixed design,"Participants performed a ball-sorting task in a virtual environment alongside a robot, with the ability to correct the robot's mistakes. They then completed a questionnaire.","Participants performed a ball-sorting task while monitoring a robot performing the same task, and could correct the robot's mistakes.",Unspecified,Humanoid Robots,Research,Supervision,Monitoring,minimal interaction,Participants interacted with a virtual robot in a simulated environment.,simulation,Participants were immersed in a virtual environment using a head-mounted display.,simulated,The robot was a simulated 3D model in a virtual environment.,pre-programmed (non-adaptive),The robot's actions were pre-programmed and did not adapt to the user.,Behavioral Measures; Questionnaires,,Eye-tracking Data; Performance Metrics,Trust was measured using questionnaires and behavioral measures such as monitoring time and number of corrections.,no modeling,No computational model of trust was used.,Empirical HRI Studies,Virtual/Simulated Studies,Direct Manipulation,The robot's task performance (making mistakes or not) and motion fluency (trembling or smooth) were directly manipulated to see how they affected trust and monitoring behavior.,"The robot was trusted more when it performed well, and a robot with bad performance was monitored more. Motion fluency did not significantly affect explicit trust ratings, but did affect the number of corrections when the robot performed badly.","The expected effect of motion fluency on explicit trust ratings was not found, and the explicit trust rating was not related to the monitoring behavior of the participants.","Robot task performance significantly affects both explicit trust ratings and monitoring behavior, with better performance leading to higher trust and less monitoring.",The robot's task was to remove brown balls from a conveyor belt. The human participant also performed the same task and could correct the robot's mistakes by pressing a button.,ANOVA; planned contrasts; post hoc t tests,"Between-subjects ANOVAs were used to analyze the effects of task performance and motion fluency on explicit trustworthiness ratings and calibration. A mixed-design ANOVA was used to analyze the effects of task performance, motion fluency, and ball speed (block number) on monitoring behavior. Planned contrasts were used to examine the effect of ball speed on monitoring behavior. Post hoc t-tests were used to examine the interaction effect of task performance and motion fluency on the number of corrections.",TRUE,Robot-nonverbal-communication; Robot-accuracy,Robot-accuracy,Robot-nonverbal-communication,"Similar to Experiment 1, this study manipulated the robot's motion fluency (trembling vs. smooth), which is a form of nonverbal communication, and the robot's task performance (making mistakes or not), which directly impacts accuracy. The paper states, 'Task performance and Motion Fluency were manipulated in a 2 (task performance: bad, good) × 2 (motion fluency: trembling, smooth) between subject design.' This indicates a manipulation of the robot's physical movements ('Robot-nonverbal-communication') and its accuracy ('Robot-accuracy'). The results showed that only task performance significantly affected trust, as stated in the paper: '...a robot with bad performance is trusted less... than a robot with good performance...'. Therefore, 'Robot-accuracy' is listed as a factor that impacted trust, while 'Robot-nonverbal-communication' is listed as a factor that did not impact trust. Although the study also manipulated ball speed, this is a task constraint and not a robot factor, so it is not included in the factors that impacted or did not impact trust.",10.1007/s12369-014-0231-5,http://link.springer.com/10.1007/s12369-014-0231-5,"An important aspect of a robot’s social behavior is to convey the right amount of trustworthiness. Task performance has shown to be an important source for trustworthiness judgments. Here, we argue that factors such as a robot’s behavioral style can play an important role as well. Our approach to studying the effects of a robot’s performance and behavioral style on human trust involves experiments with simulated robots in video human–robot interaction (VHRI) and immersive virtual environments (IVE). Although VHRI and IVE settings cannot substitute for the genuine interaction with a real robot, they can provide useful complementary approaches to experimental research in social human robot interaction. VHRI enables rapid prototyping of robot behaviors. Simulating human–robot interaction in IVEs can be a useful tool for measuring human responses to robots and help avoid the many constraints caused by real-world hardware. However, there are also difficulties with the generalization of results from one setting (e.g., VHRI) to another (e.g. IVE or the real world), which we discuss. In this paper, we use animated robot avatars in VHRI to rapidly identify robot behavioral styles that affect human trust assessment of the robot. In a subsequent study, we use an IVE to measure behavioral interaction between humans and an animated robot avatar equipped with behaviors from the VHRI experiment. Our findings reconfirm that a robot’s task performance influences its trustworthiness, but the effect of the behavioral style identified in the VHRI study did not influence the robot’s trustworthiness in the IVE study."